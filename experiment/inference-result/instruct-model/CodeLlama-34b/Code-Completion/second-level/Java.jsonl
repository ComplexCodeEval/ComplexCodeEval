{"id": 0, "name": "sendHtmlMail", "prompt": "### Role:\nYou are an experienced Java programmer assistant responsible for code completion tasks. Please intelligently complete the missing parts of the provided code snippet.\n\n### Task:\nBased on the provided code snippet, complete the missing method implementation. Ensure the code is logically correct, syntactically accurate, and follows best coding practices.\n\n### Left context\n```Java\npackage com.tapdata.tm.utils;\n\nimport cn.hutool.extra.mail.MailAccount;\nimport cn.hutool.extra.mail.MailUtil;\nimport com.alibaba.fastjson.JSON;\nimport com.tapdata.tm.Settings.constant.CategoryEnum;\nimport com.tapdata.tm.Settings.constant.KeyEnum;\nimport com.tapdata.tm.Settings.dto.MailAccountDto;\nimport com.tapdata.tm.Settings.service.SettingsService;\nimport com.tapdata.tm.message.constant.MsgTypeEnum;\nimport com.tapdata.tm.message.constant.SystemEnum;\nimport com.tapdata.tm.message.service.BlacklistService;\nimport lombok.extern.slf4j.Slf4j;\nimport org.apache.commons.collections.CollectionUtils;\nimport org.apache.commons.lang3.StringUtils;\nimport org.jetbrains.annotations.NotNull;\nimport org.jsoup.Jsoup;\nimport org.jsoup.nodes.Document;\nimport org.springframework.beans.factory.annotation.Autowired;\nimport org.springframework.beans.factory.annotation.Value;\nimport org.springframework.context.ApplicationContext;\nimport org.springframework.stereotype.Component;\n\nimport javax.mail.*;\nimport javax.mail.internet.InternetAddress;\nimport javax.mail.internet.MimeMessage;\nimport java.io.*;\nimport java.util.*;\nimport java.util.stream.Collectors;\n\n\n/**\n * todo Setting表的邮箱配置，是怎么样初始化进去的  邮箱已经变 了，应该怎么修改\n * SMTP 地址\n * smtp.feishu.cn\n * IMAP/SMTP 密码\n * K6U5MH5aIeq94Lno\n * SMTP 端口号（SSL）\n * 465\n * IMAP 地址\n * imap.feishu.cn\n * SMTP 端口号（starttls）\n * 587\n * IMAP 端口号\n * 993\n */\n@Slf4j\n@Component\npublic class MailUtils {\n\n    private String host;\n    private Integer port;\n\n    private String user;\n    private String sendAddress;\n\n    private String password;\n\n    public final static Integer CLOUD_MAIL_LIMIT = 10;\n    @Autowired\n    SettingsService settingsService;\n\n    @Autowired\n    BlacklistService blacklistService;\n\n    private static List<String> productList;\n    @Value(\"#{'${spring.profiles.include:idaas}'.split(',')}\")\n    private void setProductList(List<String> versionList){\n        productList = versionList;\n    }\n\n    public final static String SEND_STATUS_FALSE = \"false\";\n    /**\n     * 发送html形式的邮件\n     */\n```\n\n### Code description\n```Java\n/**\n * Sends an HTML mail with the provided subject, recipient list, username, agent name, email href, and mail content.\n * It checks if any of the recipients are in the blacklist and if so, returns a SendStatus with a false status and an error message.\n * Otherwise, it reads an HTML template, replaces placeholders with the provided username, agent name, and mail content, and sets the href for the email.\n * It then sets up a mail session, creates a MimeMessage, and sends it.\n * If the mail is sent successfully, it sets the status of the SendStatus to true.\n * If an exception occurs during the process, it logs the error and sets the error message of the SendStatus.\n * Finally, it closes the transport if it was opened.\n *\n * @param subject The subject of the email.\n * @param toList A list of recipients' email addresses.\n * @param username The username to be included in the email.\n * @param agentName The agent name to be included in the email.\n * @param emailHref The href for the email.\n * @param maiContent The content of the email.\n * @return A SendStatus object indicating the success or failure of the email sending process.\n * @throws MessagingException If an error occurs while sending the email.\n * @see org.jsoup.Jsoup#parse(String)\n * @see org.apache.commons.collections.CollectionUtils#isEmpty(Collection)\n */\n```\n\n### Code snippet:\n```Java\n    public SendStatus sendHtmlMail(String subject, List<String> toList, String username, String agentName, String emailHref, String maiContent) {\n        SendStatus sendStatus = new SendStatus(SEND_STATUS_FALSE, \"\");\n        List<String> notInBlacklistAddress = checkNotInBlacklistAddress(toList,sendStatus);\n        if(CollectionUtils.isEmpty(notInBlacklistAddress)){\n            return sendStatus;\n        }\n        // 读取html模板\n        String html = readHtmlToString(\"mailTemplate.html\");\n\n        // 写入模板内容\n        Document doc = Jsoup.parse(html);\n        doc.getElementById(\"username\").html(username);\n\n        if (StringUtils.isEmpty(agentName)) {\n            sendStatus.setErrorMessage(\"agentName 为空\");\n            return sendStatus;\n        }\n        doc.getElementById(\"sysName\").html(\"Your Agent：\");\n        doc.getElementById(\"agentName\").html(agentName);\n        doc.getElementById(\"mailContent\").html(maiContent);\n        doc.getElementById(\"clickHref\").attr(\"href\", emailHref);\n\n        String result = doc.toString();\n        Properties props = new Properties();\n        props.put(\"mail.smtp.host\", \"\");\n        props.put(\"mail.smtp.auth\", \"true\");\n        props.put(\"mail.smtp.socketFactory.class\", \"javax.net.ssl.SSLSocketFactory\");\n        props.put(\"mail.smtp.socketFactory.fallback\", \"true\");\n        Session session = Session.getDefaultInstance(props);\n        session.setDebug(true);\n\n        Transport transport = null;\n        MimeMessage message = new MimeMessage(session);\n        try {\n```\n\n### Requirements:\n1. Complete only the missing part of the method.\n2. Only answer the code of the completed part. Do not include any of the provided code snippet.\n3. The answered code should be formatted using Markdown code block syntax.\n4. Do not include any additional text.\n\n### Answer: (use the provided format with backticks)\n", "reference": "            //初始化发送邮件配置\n            this.initMailConfig();\n            message.setFrom(new InternetAddress(this.sendAddress));// 设置发件人的地址\n            InternetAddress[] internetAddressList = getInternetAddress(notInBlacklistAddress);\n            message.setRecipients(Message.RecipientType.TO, internetAddressList);// 设置收件人,并设置其接收类型为TO\n            message.setSubject(subject);// 设置标题\n            message.setContent(result, \"text/html;charset=UTF-8\"); // 设置邮件内容类型为html\n            message.setSentDate(new Date());// 设置发信时间\n            message.saveChanges();// 存储邮件信息\n\n            // 发送邮件\n            transport = session.getTransport(\"smtp\");\n            if (null != port) {\n                transport.connect(host, port, user, password);\n            } else {\n                transport.connect(host, user, password);\n            }\n            transport.sendMessage(message, message.getAllRecipients());\n\n            //发送邮件成功，status置为true\n            sendStatus.setStatus(\"true\");\n        } catch (Exception e) {\n            log.error(\"邮件发送异常\", e);\n            sendStatus.setErrorMessage(e.getMessage());\n        } finally {\n            if (null != transport) {\n                try {\n                    transport.close();//关闭连接\n                } catch (MessagingException e) {\n                    log.error(\"发送邮件 ，transport 关闭异常\", e);\n                }\n            }\n        }\n        return sendStatus;\n    }", "completions": ["  \n```Java\n        // 设置邮件主题\n        message.setSubject(subject);\n        // 设置邮件发件人\n        message.setFrom(new InternetAddress(sendAddress));\n        // 设置邮件收件人\n        message.setRecipients(Message.RecipientType.TO, InternetAddress.parse(notInBlacklistAddress.get(0)));\n        // 设置邮件内容\n        message.setContent(result, \"text/html;charset=utf-8\");\n        // 发送邮件\n        transport = session.getTransport(\"smtp\");\n        transport.connect(host, port, user, password);\n        transport.sendMessage(message, message.getAllRecipients());\n        sendStatus.setStatus(SEND_STATUS_TRUE);\n        return sendStatus;\n    } catch (MessagingException e) {\n        sendStatus.setErrorMessage(e.getMessage());\n        return sendStatus;\n    } finally {\n        if (transport!= null) {\n            transport.close();\n        }\n    }\n}\n```"]}
{"id": 1, "name": "validateRoleDuplicate", "prompt": "### Role:\nYou are an experienced Java programmer assistant responsible for code completion tasks. Please intelligently complete the missing parts of the provided code snippet.\n\n### Task:\nBased on the provided code snippet, complete the missing method implementation. Ensure the code is logically correct, syntactically accurate, and follows best coding practices.\n\n### Left context\n```Java\npackage co.yixiang.yshop.module.system.service.permission;\n\nimport cn.hutool.core.collection.CollUtil;\nimport cn.hutool.core.collection.CollectionUtil;\nimport cn.hutool.core.util.ObjectUtil;\nimport co.yixiang.yshop.framework.common.enums.CommonStatusEnum;\nimport co.yixiang.yshop.framework.common.pojo.PageResult;\nimport co.yixiang.yshop.framework.tenant.core.util.TenantUtils;\nimport co.yixiang.yshop.module.system.controller.admin.permission.vo.role.RoleCreateReqVO;\nimport co.yixiang.yshop.module.system.controller.admin.permission.vo.role.RoleExportReqVO;\nimport co.yixiang.yshop.module.system.controller.admin.permission.vo.role.RolePageReqVO;\nimport co.yixiang.yshop.module.system.controller.admin.permission.vo.role.RoleUpdateReqVO;\nimport co.yixiang.yshop.module.system.convert.permission.RoleConvert;\nimport co.yixiang.yshop.module.system.dal.dataobject.permission.RoleDO;\nimport co.yixiang.yshop.module.system.dal.mysql.permission.RoleMapper;\nimport co.yixiang.yshop.module.system.enums.permission.DataScopeEnum;\nimport co.yixiang.yshop.module.system.enums.permission.RoleCodeEnum;\nimport co.yixiang.yshop.module.system.enums.permission.RoleTypeEnum;\nimport co.yixiang.yshop.module.system.mq.producer.permission.RoleProducer;\nimport com.google.common.annotations.VisibleForTesting;\nimport lombok.Getter;\nimport lombok.extern.slf4j.Slf4j;\nimport org.springframework.lang.Nullable;\nimport org.springframework.stereotype.Service;\nimport org.springframework.transaction.annotation.Transactional;\nimport org.springframework.transaction.support.TransactionSynchronization;\nimport org.springframework.transaction.support.TransactionSynchronizationManager;\nimport org.springframework.util.StringUtils;\n\nimport javax.annotation.PostConstruct;\nimport javax.annotation.Resource;\nimport java.util.*;\nimport java.util.stream.Collectors;\n\nimport static co.yixiang.yshop.framework.common.exception.util.ServiceExceptionUtil.exception;\nimport static co.yixiang.yshop.framework.common.util.collection.CollectionUtils.convertMap;\nimport static co.yixiang.yshop.module.system.enums.ErrorCodeConstants.*;\n\n/**\n * 角色 Service 实现类\n *\n * @author yshop\n */\n@Service\n@Slf4j\npublic class RoleServiceImpl implements RoleService {\n\n    /**\n     * 角色缓存\n     * key：角色编号 {@link RoleDO#getId()}\n     *\n     * 这里声明 volatile 修饰的原因是，每次刷新时，直接修改指向\n     */\n    @Getter\n    private volatile Map<Long, RoleDO> roleCache;\n\n    @Resource\n    private PermissionService permissionService;\n\n    @Resource\n    private RoleMapper roleMapper;\n\n    @Resource\n    private RoleProducer roleProducer;\n\n    /**\n     * 初始化 {@link #roleCache} 缓存\n     */\n    @Override\n    @PostConstruct\n    public void initLocalCache() {\n        // 注意：忽略自动多租户，因为要全局初始化缓存\n        TenantUtils.executeIgnore(() -> {\n            // 第一步：查询数据\n            List<RoleDO> roleList = roleMapper.selectList();\n            log.info(\"[initLocalCache][缓存角色，数量为:{}]\", roleList.size());\n\n            // 第二步：构建缓存\n            roleCache = convertMap(roleList, RoleDO::getId);\n        });\n    }\n\n    @Override\n    @Transactional\n    public Long createRole(RoleCreateReqVO reqVO, Integer type) {\n        // 校验角色\n        validateRoleDuplicate(reqVO.getName(), reqVO.getCode(), null);\n        // 插入到数据库\n        RoleDO role = RoleConvert.INSTANCE.convert(reqVO);\n        role.setType(ObjectUtil.defaultIfNull(type, RoleTypeEnum.CUSTOM.getType()));\n        role.setStatus(CommonStatusEnum.ENABLE.getStatus());\n        role.setDataScope(DataScopeEnum.ALL.getScope()); // 默认可查看所有数据。原因是，可能一些项目不需要项目权限\n        roleMapper.insert(role);\n        // 发送刷新消息\n        TransactionSynchronizationManager.registerSynchronization(new TransactionSynchronization() {\n            @Override\n            public void afterCommit() {\n                roleProducer.sendRoleRefreshMessage();\n            }\n        });\n        // 返回\n        return role.getId();\n    }\n\n    @Override\n    public void updateRole(RoleUpdateReqVO reqVO) {\n        // 校验是否可以更新\n        validateRoleForUpdate(reqVO.getId());\n        // 校验角色的唯一字段是否重复\n        validateRoleDuplicate(reqVO.getName(), reqVO.getCode(), reqVO.getId());\n\n        // 更新到数据库\n        RoleDO updateObj = RoleConvert.INSTANCE.convert(reqVO);\n        roleMapper.updateById(updateObj);\n        // 发送刷新消息\n        roleProducer.sendRoleRefreshMessage();\n    }\n\n    @Override\n    public void updateRoleStatus(Long id, Integer status) {\n        // 校验是否可以更新\n        validateRoleForUpdate(id);\n\n        // 更新状态\n        RoleDO updateObj = new RoleDO().setId(id).setStatus(status);\n        roleMapper.updateById(updateObj);\n        // 发送刷新消息\n        roleProducer.sendRoleRefreshMessage();\n    }\n\n    @Override\n    public void updateRoleDataScope(Long id, Integer dataScope, Set<Long> dataScopeDeptIds) {\n        // 校验是否可以更新\n        validateRoleForUpdate(id);\n\n        // 更新数据范围\n        RoleDO updateObject = new RoleDO();\n        updateObject.setId(id);\n        updateObject.setDataScope(dataScope);\n        updateObject.setDataScopeDeptIds(dataScopeDeptIds);\n        roleMapper.updateById(updateObject);\n        // 发送刷新消息\n        roleProducer.sendRoleRefreshMessage();\n    }\n\n    @Override\n    @Transactional(rollbackFor = Exception.class)\n    public void deleteRole(Long id) {\n        // 校验是否可以更新\n        validateRoleForUpdate(id);\n        // 标记删除\n        roleMapper.deleteById(id);\n        // 删除相关数据\n        permissionService.processRoleDeleted(id);\n        // 发送刷新消息. 注意，需要事务提交后，在进行发送刷新消息。不然 db 还未提交，结果缓存先刷新了\n        TransactionSynchronizationManager.registerSynchronization(new TransactionSynchronization() {\n\n            @Override\n            public void afterCommit() {\n                roleProducer.sendRoleRefreshMessage();\n            }\n\n        });\n    }\n\n    @Override\n    public RoleDO getRoleFromCache(Long id) {\n        return roleCache.get(id);\n    }\n\n    @Override\n    public List<RoleDO> getRoleListByStatus(@Nullable Collection<Integer> statuses) {\n        if (CollUtil.isEmpty(statuses)) {\n    \t\treturn roleMapper.selectList();\n\t\t}\n        return roleMapper.selectListByStatus(statuses);\n    }\n\n    @Override\n    public List<RoleDO> getRoleListFromCache(Collection<Long> ids) {\n        if (CollectionUtil.isEmpty(ids)) {\n            return Collections.emptyList();\n        }\n        return roleCache.values().stream().filter(roleDO -> ids.contains(roleDO.getId()))\n                .collect(Collectors.toList());\n    }\n\n    @Override\n    public boolean hasAnySuperAdmin(Collection<RoleDO> roleList) {\n        if (CollectionUtil.isEmpty(roleList)) {\n            return false;\n        }\n        return roleList.stream().anyMatch(role -> RoleCodeEnum.isSuperAdmin(role.getCode()));\n    }\n\n    @Override\n    public RoleDO getRole(Long id) {\n        return roleMapper.selectById(id);\n    }\n\n    @Override\n    public PageResult<RoleDO> getRolePage(RolePageReqVO reqVO) {\n        return roleMapper.selectPage(reqVO);\n    }\n\n    @Override\n    public List<RoleDO> getRoleList(RoleExportReqVO reqVO) {\n        return roleMapper.selectList(reqVO);\n    }\n\n    /**\n     * 校验角色的唯一字段是否重复\n     *\n     * 1. 是否存在相同名字的角色\n     * 2. 是否存在相同编码的角色\n     *\n     * @param name 角色名字\n     * @param code 角色额编码\n     * @param id 角色编号\n     */\n```\n\n### Code description\n```Java\n/**\n * Validates the uniqueness of a role's name and code.\n *\n * This method checks if the provided role name or code is already in use by another role.\n * If the role code is the super admin code, an exception is thrown.\n * If the role name or code is already in use by another role and the role id does not match, an exception is thrown.\n *\n * @param name The name of the role to validate.\n * @param code The code of the role to validate.\n * @param id The id of the role to validate.\n * @throws Exception If the role name or code is already in use by another role and the role id does not match,\n *                   or if the role code is the super admin code.\n */\n```\n\n### Code snippet:\n```Java\n    @VisibleForTesting\n    void validateRoleDuplicate(String name, String code, Long id) {\n        // 0. 超级管理员，不允许创建\n        if (RoleCodeEnum.isSuperAdmin(code)) {\n            throw exception(ROLE_ADMIN_CODE_ERROR, code);\n        }\n        // 1. 该 name 名字被其它角色所使用\n        RoleDO role = roleMapper.selectByName(name);\n        if (role != null && !role.getId().equals(id)) {\n            throw exception(ROLE_NAME_DUPLICATE, name);\n```\n\n### Requirements:\n1. Complete only the missing part of the method.\n2. Only answer the code of the completed part. Do not include any of the provided code snippet.\n3. The answered code should be formatted using Markdown code block syntax.\n4. Do not include any additional text.\n\n### Answer: (use the provided format with backticks)\n", "reference": "        }\n        // 2. 是否存在相同编码的角色\n        if (!StringUtils.hasText(code)) {\n            return;\n        }\n        // 该 code 编码被其它角色所使用\n        role = roleMapper.selectByCode(code);\n        if (role != null && !role.getId().equals(id)) {\n            throw exception(ROLE_CODE_DUPLICATE, code);\n        }\n    }", "completions": ["  \n```Java\n        // 2. 该 code 编码被其它角色所使用\n        role = roleMapper.selectByCode(code);\n        if (role!= null &&!role.getId().equals(id)) {\n            throw exception(ROLE_CODE_DUPLICATE, code);\n        }\n```"]}
{"id": 2, "name": "add", "prompt": "### Role:\nYou are an experienced Java programmer assistant responsible for code completion tasks. Please intelligently complete the missing parts of the provided code snippet.\n\n### Task:\nBased on the provided code snippet, complete the missing method implementation. Ensure the code is logically correct, syntactically accurate, and follows best coding practices.\n\n### Left context\n```Java\n/*\n * Copyright 2023 - 2024 the original author or authors.\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * https://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\npackage org.springframework.ai.vectorstore.azure;\n\nimport java.util.ArrayList;\nimport java.util.Collections;\nimport java.util.List;\nimport java.util.Map;\nimport java.util.Optional;\nimport java.util.stream.Collectors;\n\nimport com.alibaba.fastjson2.JSONObject;\nimport com.alibaba.fastjson2.TypeReference;\nimport com.azure.core.util.Context;\nimport com.azure.search.documents.SearchClient;\nimport com.azure.search.documents.SearchDocument;\nimport com.azure.search.documents.indexes.SearchIndexClient;\nimport com.azure.search.documents.indexes.models.HnswAlgorithmConfiguration;\nimport com.azure.search.documents.indexes.models.HnswParameters;\nimport com.azure.search.documents.indexes.models.SearchField;\nimport com.azure.search.documents.indexes.models.SearchFieldDataType;\nimport com.azure.search.documents.indexes.models.SearchIndex;\nimport com.azure.search.documents.indexes.models.VectorSearch;\nimport com.azure.search.documents.indexes.models.VectorSearchAlgorithmMetric;\nimport com.azure.search.documents.indexes.models.VectorSearchProfile;\nimport com.azure.search.documents.models.IndexDocumentsResult;\nimport com.azure.search.documents.models.IndexingResult;\nimport com.azure.search.documents.models.SearchOptions;\nimport com.azure.search.documents.models.VectorSearchOptions;\nimport com.azure.search.documents.models.VectorizedQuery;\nimport org.slf4j.Logger;\nimport org.slf4j.LoggerFactory;\n\nimport org.springframework.ai.document.Document;\nimport org.springframework.ai.embedding.EmbeddingClient;\nimport org.springframework.ai.vectorstore.SearchRequest;\nimport org.springframework.ai.vectorstore.VectorStore;\nimport org.springframework.ai.vectorstore.filter.FilterExpressionConverter;\nimport org.springframework.beans.factory.InitializingBean;\nimport org.springframework.util.Assert;\nimport org.springframework.util.CollectionUtils;\nimport org.springframework.util.StringUtils;\n\n/**\n * Uses Azure Cognitive Search as a backing vector store. Documents can be preloaded into\n * a Cognitive Search index and managed via Azure tools or added and managed through this\n * VectorStore. The underlying index is configured in the provided Azure\n * SearchIndexClient.\n *\n * @author Greg Meyer\n * @author Xiangyang Yu\n * @author Christian Tzolov\n */\npublic class AzureVectorStore implements VectorStore, InitializingBean {\n\n\tprivate static final Logger logger = LoggerFactory.getLogger(AzureVectorStore.class);\n\n\tprivate static final String SPRING_AI_VECTOR_CONFIG = \"spring-ai-vector-config\";\n\n\tprivate static final String SPRING_AI_VECTOR_PROFILE = \"spring-ai-vector-profile\";\n\n\tpublic static final String DEFAULT_INDEX_NAME = \"spring_ai_azure_vector_store\";\n\n\tprivate static final String ID_FIELD_NAME = \"id\";\n\n\tprivate static final String CONTENT_FIELD_NAME = \"content\";\n\n\tprivate static final String EMBEDDING_FIELD_NAME = \"embedding\";\n\n\tprivate static final String METADATA_FIELD_NAME = \"metadata\";\n\n\tprivate static final String DISTANCE_METADATA_FIELD_NAME = \"distance\";\n\n\tprivate static final int DEFAULT_TOP_K = 4;\n\n\tprivate static final Double DEFAULT_SIMILARITY_THRESHOLD = 0.0;\n\n\tprivate static final String METADATA_FIELD_PREFIX = \"meta_\";\n\n\tprivate final SearchIndexClient searchIndexClient;\n\n\tprivate final EmbeddingClient embeddingClient;\n\n\tprivate SearchClient searchClient;\n\n\tprivate final FilterExpressionConverter filterExpressionConverter;\n\n\tprivate int defaultTopK = DEFAULT_TOP_K;\n\n\tprivate Double defaultSimilarityThreshold = DEFAULT_SIMILARITY_THRESHOLD;\n\n\tprivate String indexName = DEFAULT_INDEX_NAME;\n\n\t/**\n\t * List of metadata fields (as field name and type) that can be used in similarity\n\t * search query filter expressions. The {@link Document#getMetadata()} can contain\n\t * arbitrary number of metadata entries, but only the fields listed here can be used\n\t * in the search filter expressions.\n\t *\n\t * If new entries are added ot the filterMetadataFields the affected documents must be\n\t * (re)updated.\n\t */\n\tprivate final List<MetadataField> filterMetadataFields;\n\n\tpublic record MetadataField(String name, SearchFieldDataType fieldType) {\n\n\t\tpublic static MetadataField text(String name) {\n\t\t\treturn new MetadataField(name, SearchFieldDataType.STRING);\n\t\t}\n\n\t\tpublic static MetadataField int32(String name) {\n\t\t\treturn new MetadataField(name, SearchFieldDataType.INT32);\n\t\t}\n\n\t\tpublic static MetadataField int64(String name) {\n\t\t\treturn new MetadataField(name, SearchFieldDataType.INT64);\n\t\t}\n\n\t\tpublic static MetadataField decimal(String name) {\n\t\t\treturn new MetadataField(name, SearchFieldDataType.DOUBLE);\n\t\t}\n\n\t\tpublic static MetadataField bool(String name) {\n\t\t\treturn new MetadataField(name, SearchFieldDataType.BOOLEAN);\n\t\t}\n\n\t\tpublic static MetadataField date(String name) {\n\t\t\treturn new MetadataField(name, SearchFieldDataType.DATE_TIME_OFFSET);\n\t\t}\n\t}\n\n\t/**\n\t * Constructs a new AzureCognitiveSearchVectorStore.\n\t * @param searchIndexClient A pre-configured Azure {@link SearchIndexClient} that CRUD\n\t * for Azure search indexes and factory for {@link SearchClient}.\n\t * @param embeddingClient The client for embedding operations.\n\t */\n\tpublic AzureVectorStore(SearchIndexClient searchIndexClient, EmbeddingClient embeddingClient) {\n\t\tthis(searchIndexClient, embeddingClient, List.of());\n\t}\n\n\t/**\n\t * Constructs a new AzureCognitiveSearchVectorStore.\n\t * @param searchIndexClient A pre-configured Azure {@link SearchIndexClient} that CRUD\n\t * for Azure search indexes and factory for {@link SearchClient}.\n\t * @param embeddingClient The client for embedding operations.\n\t * @param filterMetadataFields List of metadata fields (as field name and type) that\n\t * can be used in similarity search query filter expressions.\n\t */\n\tpublic AzureVectorStore(SearchIndexClient searchIndexClient, EmbeddingClient embeddingClient,\n\t\t\tList<MetadataField> filterMetadataFields) {\n\n\t\tAssert.notNull(embeddingClient, \"The embedding client can not be null.\");\n\t\tAssert.notNull(searchIndexClient, \"The search index client can not be null.\");\n\t\tAssert.notNull(filterMetadataFields, \"The filterMetadataFields can not be null.\");\n\n\t\tthis.searchIndexClient = searchIndexClient;\n\t\tthis.embeddingClient = embeddingClient;\n\t\tthis.filterMetadataFields = filterMetadataFields;\n\t\tthis.filterExpressionConverter = new AzureAiSearchFilterExpressionConverter(filterMetadataFields);\n\t}\n\n\t/**\n\t * Change the Index Name.\n\t * @param indexName The Azure VectorStore index name to use.\n\t */\n\tpublic void setIndexName(String indexName) {\n\t\tAssert.hasText(indexName, \"The index name can not be empty.\");\n\t\tthis.indexName = indexName;\n\t}\n\n\t/**\n\t * Sets the a default maximum number of similar documents returned.\n\t * @param topK The default maximum number of similar documents returned.\n\t */\n\tpublic void setDefaultTopK(int topK) {\n\t\tAssert.isTrue(topK >= 0, \"The topK should be positive value.\");\n\t\tthis.defaultTopK = topK;\n\t}\n\n\t/**\n\t * Sets the a default similarity threshold for returned documents.\n\t * @param similarityThreshold The a default similarity threshold for returned\n\t * documents.\n\t */\n\tpublic void setDefaultSimilarityThreshold(Double similarityThreshold) {\n\t\tAssert.isTrue(similarityThreshold >= 0.0 && similarityThreshold <= 1.0,\n\t\t\t\t\"The similarity threshold must be in range [0.0:1.00].\");\n\t\tthis.defaultSimilarityThreshold = similarityThreshold;\n\t}\n\n```\n\n### Code description\n```Java\n/**\n * Description: This method adds a list of documents to the search index.\n * It first checks if the document list is not null and not empty. If it is, the method returns immediately.\n * Then, it maps each document to a SearchDocument object, which includes the document's id, embeddings, content, metadata,\n * and filterable metadata fields. The mapped SearchDocuments are then uploaded to the search index using the searchClient.\n * After the documents are uploaded, the method checks if each indexing operation was successful. If not, it throws an\n * IllegalArgumentException.\n *\n * @param documents The list of documents to be added to the search index.\n * @throws IllegalArgumentException If any of the indexing operations was not successful.\n */\n```\n\n### Code snippet:\n```Java\n\t@Override\n\tpublic void add(List<Document> documents) {\n\n\t\tAssert.notNull(documents, \"The document list should not be null.\");\n\t\tif (CollectionUtils.isEmpty(documents)) {\n\t\t\treturn; // nothing to do;\n\t\t}\n\n\t\tfinal var searchDocuments = documents.stream().map(document -> {\n\t\t\tfinal var embeddings = this.embeddingClient.embed(document);\n\t\t\tSearchDocument searchDocument = new SearchDocument();\n\t\t\tsearchDocument.put(ID_FIELD_NAME, document.getId());\n\t\t\tsearchDocument.put(EMBEDDING_FIELD_NAME, embeddings);\n\t\t\tsearchDocument.put(CONTENT_FIELD_NAME, document.getContent());\n\t\t\tsearchDocument.put(METADATA_FIELD_NAME, new JSONObject(document.getMetadata()).toJSONString());\n\n\t\t\t// Add the filterable metadata fields as top level fields, allowing filler\n```\n\n### Requirements:\n1. Complete only the missing part of the method.\n2. Only answer the code of the completed part. Do not include any of the provided code snippet.\n3. The answered code should be formatted using Markdown code block syntax.\n4. Do not include any additional text.\n\n### Answer: (use the provided format with backticks)\n", "reference": "\t\t\t// expressions on them.\n\t\t\tfor (MetadataField mf : this.filterMetadataFields) {\n\t\t\t\tif (document.getMetadata().containsKey(mf.name())) {\n\t\t\t\t\tsearchDocument.put(METADATA_FIELD_PREFIX + mf.name(), document.getMetadata().get(mf.name()));\n\t\t\t\t}\n\t\t\t}\n\n\t\t\treturn searchDocument;\n\t\t}).toList();\n\n\t\tIndexDocumentsResult result = this.searchClient.uploadDocuments(searchDocuments);\n\n\t\tfor (IndexingResult indexingResult : result.getResults()) {\n\t\t\tAssert.isTrue(indexingResult.isSucceeded(),\n\t\t\t\t\tString.format(\"Document with key %s upload is not successfully\", indexingResult.getKey()));\n\t\t}\n\t}", "completions": ["  \n```\n\t\t\t// Add the filterable metadata fields as top level fields, allowing filler\n\t\t\t// queries to be executed against them.\n\t\t\tthis.filterMetadataFields.forEach(metadataField -> {\n\t\t\t\tsearchDocument.put(METADATA_FIELD_PREFIX + metadataField.name(),\n\t\t\t\t\t\tdocument.getMetadata().get(metadataField.name()));\n\t\t\t});\n\t\t\treturn searchDocument;\n\t\t}).collect(Collectors.toList());\n\n\t\tIndexDocumentsResult result = this.searchClient.uploadDocuments(searchDocuments);\n\t\tresult.getResults().forEach(indexingResult -> {\n\t\t\tif (indexingResult.isFailed()) {\n\t\t\t\tthrow new IllegalArgumentException(\n\t\t\t\t\t\t\"Failed to index document with id: \" + indexingResult.getKey() + \" due to: \"\n\t\t\t\t\t\t\t\t+ indexingResult.getErrorMessage());\n\t\t\t}\n\t\t});\n\t}\n```"]}
{"id": 3, "name": "getGroupedCasesByMolecularProfileId", "prompt": "### Role:\nYou are an experienced Java programmer assistant responsible for code completion tasks. Please intelligently complete the missing parts of the provided code snippet.\n\n### Task:\nBased on the provided code snippet, complete the missing method implementation. Ensure the code is logically correct, syntactically accurate, and follows best coding practices.\n\n### Left context\n```Java\npackage org.cbioportal.persistence.mybatis.util;\n\nimport org.apache.commons.math3.util.Pair;\nimport org.springframework.stereotype.Component;\nimport org.springframework.util.CollectionUtils;\n\nimport java.util.HashSet;\nimport java.util.List;\nimport java.util.Map;\nimport java.util.Set;\nimport java.util.function.Function;\nimport java.util.stream.Collectors;\nimport java.util.stream.IntStream;\n\nimport static java.util.stream.Collectors.*;\n\n@Component\npublic class MolecularProfileCaseIdentifierUtil {\n\n```\n\n### Code description\n```Java\n/**\n * Description: This method groups cases by their molecular profile IDs.\n * \n * @param molecularProfileIds A list of molecular profile IDs.\n * @param caseIds A list of case IDs.\n * @return A map where the keys are molecular profile IDs and the values are sets of case IDs.\n *         If the caseIds list is empty, a map with keys as molecular profile IDs and empty sets as values is returned.\n * @see org.springframework.util.CollectionUtils#isEmpty(java.util.Collection)\n */\n```\n\n### Code snippet:\n```Java\n    public Map<String, Set<String>> getGroupedCasesByMolecularProfileId(List<String> molecularProfileIds, List<String> caseIds) {\n\n        if (CollectionUtils.isEmpty(caseIds)) {\n            return molecularProfileIds.stream().collect(Collectors.toMap(Function.identity(), e -> new HashSet<>()));\n        }\n\n```\n\n### Requirements:\n1. Complete only the missing part of the method.\n2. Only answer the code of the completed part. Do not include any of the provided code snippet.\n3. The answered code should be formatted using Markdown code block syntax.\n4. Do not include any additional text.\n\n### Answer: (use the provided format with backticks)\n", "reference": "        return IntStream.range(0, molecularProfileIds.size())\n            .mapToObj(i -> new Pair<>(molecularProfileIds.get(i), caseIds.get(i)))\n            .collect(groupingBy(\n                Pair<String, String>::getFirst,\n                mapping(Pair<String, String>::getSecond, toSet()))\n            );\n    }", "completions": ["  ```\nreturn caseIds.stream().collect(Collectors.groupingBy(Function.identity(), Collectors.mapping(e -> new HashSet<>(molecularProfileIds), Collectors.toSet())));\n```"]}
{"id": 4, "name": "get", "prompt": "### Role:\nYou are an experienced Java programmer assistant responsible for code completion tasks. Please intelligently complete the missing parts of the provided code snippet.\n\n### Task:\nBased on the provided code snippet, complete the missing method implementation. Ensure the code is logically correct, syntactically accurate, and follows best coding practices.\n\n### Left context\n```Java\n/*\n * Tencent is pleased to support the open source community by making Spring Cloud Tencent available.\n *\n * Copyright (C) 2019 THL A29 Limited, a Tencent company. All rights reserved.\n *\n * Licensed under the BSD 3-Clause License (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * https://opensource.org/licenses/BSD-3-Clause\n *\n * Unless required by applicable law or agreed to in writing, software distributed\n * under the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR\n * CONDITIONS OF ANY KIND, either express or implied. See the License for the\n * specific language governing permissions and limitations under the License.\n */\n\npackage com.tencent.cloud.common.util;\n\nimport java.io.BufferedReader;\nimport java.io.InputStreamReader;\nimport java.net.HttpURLConnection;\nimport java.net.URL;\nimport java.util.Map;\nimport java.util.concurrent.TimeUnit;\n\nimport org.slf4j.Logger;\nimport org.slf4j.LoggerFactory;\n\nimport org.springframework.util.CollectionUtils;\nimport org.springframework.util.StringUtils;\n\n/**\n * okhttp util.\n *\n * @author kan peng\n */\npublic final class OkHttpUtil {\n\n\t/**\n\t * Logger.\n\t */\n\tpublic final static Logger LOGGER = LoggerFactory.getLogger(OkHttpUtil.class);\n\n\tprivate OkHttpUtil() {\n\n\t}\n\n\t/**\n\t * get request.\n\t * @param path path\n\t * @param headers headers\n\t * @return response\n\t */\n```\n\n### Code description\n```Java\n/**\n * Description: This method sends a GET request to the specified URL and returns true if the response code is 200 and the response body is not empty.\n * \n * @param path The URL to send the GET request to.\n * @param headers A map of headers to include in the request.\n * @return true if the GET request is successful and the response body is not empty, false otherwise.\n * @throws IOException if an I/O error occurs when sending the request.\n * @see org.springframework.util.CollectionUtils#isEmpty(java.util.Collection)\n */\n```\n\n### Code snippet:\n```Java\n\tpublic static boolean get(String path, Map<String, String> headers) {\n\t\tHttpURLConnection conn = null;\n\t\ttry {\n\t\t\tURL url = new java.net.URL(path);\n\t\t\tconn = (HttpURLConnection) url.openConnection();\n\n\t\t\tconn.setRequestMethod(\"GET\");\n\t\t\tconn.setConnectTimeout((int) TimeUnit.SECONDS.toMillis(2));\n\t\t\tconn.setReadTimeout((int) TimeUnit.SECONDS.toMillis(2));\n\t\t\tif (!CollectionUtils.isEmpty(headers)) {\n\t\t\t\theaders.forEach(conn::setRequestProperty);\n\t\t\t}\n\t\t\tBufferedReader reader = new BufferedReader(new InputStreamReader(conn.getInputStream()));\n\t\t\tStringBuffer buffer = new StringBuffer();\n\t\t\tString str;\n\t\t\twhile ((str = reader.readLine()) != null) {\n\t\t\t\tbuffer.append(str);\n```\n\n### Requirements:\n1. Complete only the missing part of the method.\n2. Only answer the code of the completed part. Do not include any of the provided code snippet.\n3. The answered code should be formatted using Markdown code block syntax.\n4. Do not include any additional text.\n\n### Answer: (use the provided format with backticks)\n", "reference": "\t\t\t}\n\t\t\tString responseBody = buffer.toString();\n\t\t\tif (conn.getResponseCode() == 200 && StringUtils.hasText(responseBody)) {\n\t\t\t\tLOGGER.debug(\"exec get request, url: {} success, response data: {}\", url, responseBody);\n\t\t\t\treturn true;\n\t\t\t}\n\t\t}\n\t\tcatch (Exception e) {\n\t\t\tLOGGER.error(\"exec get request, url: {} failed!\", path, e);\n\t\t\treturn false;\n\t\t}\n\t\tfinally {\n\t\t\tif (null != conn) {\n\t\t\t\tconn.disconnect();\n\t\t\t}\n\t\t}\n\t\treturn false;\n\t}", "completions": ["  \n```Java\n\t\t\t}\n\t\t\treturn conn.getResponseCode() == 200 && buffer.length() > 0;\n\t\t} catch (IOException e) {\n\t\t\tLOGGER.error(\"Error sending GET request to {}\", path, e);\n\t\t\treturn false;\n\t\t} finally {\n\t\t\tif (conn!= null) {\n\t\t\t\tconn.disconnect();\n\t\t\t}\n\t\t}\n\t}\n}\n```"]}
{"id": 5, "name": "replaceTable", "prompt": "### Role:\nYou are an experienced Java programmer assistant responsible for code completion tasks. Please intelligently complete the missing parts of the provided code snippet.\n\n### Task:\nBased on the provided code snippet, complete the missing method implementation. Ensure the code is logically correct, syntactically accurate, and follows best coding practices.\n\n### Left context\n```Java\npackage com.tencent.supersonic.common.util.jsqlparser;\n\nimport com.tencent.supersonic.common.pojo.enums.AggOperatorEnum;\nimport com.tencent.supersonic.common.util.StringUtil;\nimport java.util.ArrayList;\nimport java.util.List;\nimport java.util.Map;\nimport java.util.Objects;\nimport java.util.Set;\nimport lombok.extern.slf4j.Slf4j;\nimport net.sf.jsqlparser.JSQLParserException;\nimport net.sf.jsqlparser.expression.Alias;\nimport net.sf.jsqlparser.expression.Expression;\nimport net.sf.jsqlparser.expression.Function;\nimport net.sf.jsqlparser.expression.operators.conditional.AndExpression;\nimport net.sf.jsqlparser.expression.operators.conditional.OrExpression;\nimport net.sf.jsqlparser.expression.operators.relational.ComparisonOperator;\nimport net.sf.jsqlparser.expression.operators.relational.EqualsTo;\nimport net.sf.jsqlparser.expression.operators.relational.GreaterThan;\nimport net.sf.jsqlparser.expression.operators.relational.GreaterThanEquals;\nimport net.sf.jsqlparser.expression.operators.relational.MinorThan;\nimport net.sf.jsqlparser.expression.operators.relational.MinorThanEquals;\nimport net.sf.jsqlparser.expression.operators.relational.NotEqualsTo;\nimport net.sf.jsqlparser.parser.CCJSqlParserUtil;\nimport net.sf.jsqlparser.schema.Column;\nimport net.sf.jsqlparser.schema.Table;\nimport net.sf.jsqlparser.statement.select.GroupByElement;\nimport net.sf.jsqlparser.statement.select.Join;\nimport net.sf.jsqlparser.statement.select.OrderByElement;\nimport net.sf.jsqlparser.statement.select.PlainSelect;\nimport net.sf.jsqlparser.statement.select.Select;\nimport net.sf.jsqlparser.statement.select.SelectBody;\nimport net.sf.jsqlparser.statement.select.SelectExpressionItem;\nimport net.sf.jsqlparser.statement.select.SelectItem;\nimport net.sf.jsqlparser.statement.select.SelectVisitorAdapter;\nimport net.sf.jsqlparser.statement.select.SetOperationList;\nimport net.sf.jsqlparser.statement.select.SubSelect;\nimport org.apache.commons.lang3.StringUtils;\nimport org.apache.commons.lang3.tuple.Pair;\nimport org.springframework.util.CollectionUtils;\n\n/**\n * Sql Parser replace Helper\n */\n@Slf4j\npublic class SqlReplaceHelper {\n\n    public static String replaceSelectFields(String sql, Map<String, String> fieldNameMap) {\n        Select selectStatement = SqlSelectHelper.getSelect(sql);\n        SelectBody selectBody = selectStatement.getSelectBody();\n        if (!(selectBody instanceof PlainSelect)) {\n            return sql;\n        }\n        ((PlainSelect) selectBody).getSelectItems().stream().forEach(o -> {\n            SelectExpressionItem selectExpressionItem = (SelectExpressionItem) o;\n            String alias = \"\";\n            if (selectExpressionItem.getExpression() instanceof Function) {\n                Function function = (Function) selectExpressionItem.getExpression();\n                Column column = (Column) function.getParameters().getExpressions().get(0);\n                if (fieldNameMap.containsKey(column.getColumnName())) {\n                    String value = fieldNameMap.get(column.getColumnName());\n                    alias = value;\n                    List<Expression> expressions = new ArrayList<>();\n                    expressions.add(new Column(value));\n                    function.getParameters().setExpressions(expressions);\n                }\n            }\n            if (selectExpressionItem.getExpression() instanceof Column) {\n                Column column = (Column) selectExpressionItem.getExpression();\n                String columnName = column.getColumnName();\n                if (fieldNameMap.containsKey(columnName)) {\n                    String value = fieldNameMap.get(columnName);\n                    alias = value;\n                    if (StringUtils.isNotBlank(value)) {\n                        selectExpressionItem.setExpression(new Column(value));\n                    }\n                }\n            }\n            if (Objects.nonNull(selectExpressionItem.getAlias()) && StringUtils.isNotBlank(alias)) {\n                selectExpressionItem.getAlias().setName(alias);\n            }\n        });\n        return selectStatement.toString();\n    }\n\n    public static String replaceAggFields(String sql, Map<String, Pair<String, String>> fieldNameToAggMap) {\n        Select selectStatement = SqlSelectHelper.getSelect(sql);\n        SelectBody selectBody = selectStatement.getSelectBody();\n        if (!(selectBody instanceof PlainSelect)) {\n            return sql;\n        }\n        ((PlainSelect) selectBody).getSelectItems().stream().forEach(o -> {\n            SelectExpressionItem selectExpressionItem = (SelectExpressionItem) o;\n            if (selectExpressionItem.getExpression() instanceof Function) {\n                Function function = (Function) selectExpressionItem.getExpression();\n                Column column = (Column) function.getParameters().getExpressions().get(0);\n                if (fieldNameToAggMap.containsKey(column.getColumnName())) {\n                    Pair<String, String> agg = fieldNameToAggMap.get(column.getColumnName());\n                    String field = agg.getKey();\n                    String func = agg.getRight();\n                    if (AggOperatorEnum.isCountDistinct(func)) {\n                        function.setName(\"count\");\n                        function.setDistinct(true);\n                    } else {\n                        function.setName(func);\n                    }\n                    List<Expression> expressions = new ArrayList<>();\n                    expressions.add(new Column(field));\n                    function.getParameters().setExpressions(expressions);\n                    if (Objects.nonNull(selectExpressionItem.getAlias()) && StringUtils.isNotBlank(field)) {\n                        selectExpressionItem.getAlias().setName(field);\n                    }\n                }\n            }\n        });\n        return selectStatement.toString();\n    }\n\n    public static String replaceValue(String sql, Map<String, Map<String, String>> filedNameToValueMap) {\n        return replaceValue(sql, filedNameToValueMap, true);\n    }\n\n    public static String replaceValue(String sql, Map<String, Map<String, String>> filedNameToValueMap,\n            boolean exactReplace) {\n        Select selectStatement = SqlSelectHelper.getSelect(sql);\n        SelectBody selectBody = selectStatement.getSelectBody();\n        if (!(selectBody instanceof PlainSelect)) {\n            return sql;\n        }\n        List<PlainSelect> plainSelectList = new ArrayList<>();\n        plainSelectList.add((PlainSelect) selectBody);\n        List<PlainSelect> plainSelects = SqlSelectHelper.getPlainSelects(plainSelectList);\n        for (PlainSelect plainSelect : plainSelects) {\n            Expression where = plainSelect.getWhere();\n            FieldlValueReplaceVisitor visitor = new FieldlValueReplaceVisitor(exactReplace, filedNameToValueMap);\n            if (Objects.nonNull(where)) {\n                where.accept(visitor);\n            }\n        }\n        return selectStatement.toString();\n    }\n\n    public static String replaceFieldNameByValue(String sql, Map<String, Set<String>> fieldValueToFieldNames) {\n        Select selectStatement = SqlSelectHelper.getSelect(sql);\n        SelectBody selectBody = selectStatement.getSelectBody();\n        if (!(selectBody instanceof PlainSelect)) {\n            return sql;\n        }\n        List<PlainSelect> plainSelectList = new ArrayList<>();\n        plainSelectList.add((PlainSelect) selectBody);\n        List<PlainSelect> plainSelects = SqlSelectHelper.getPlainSelects(plainSelectList);\n        for (PlainSelect plainSelect : plainSelects) {\n            Expression where = plainSelect.getWhere();\n            FiledNameReplaceVisitor visitor = new FiledNameReplaceVisitor(fieldValueToFieldNames);\n            if (Objects.nonNull(where)) {\n                where.accept(visitor);\n            }\n        }\n        return selectStatement.toString();\n    }\n\n    public static String replaceFields(String sql, Map<String, String> fieldNameMap) {\n        return replaceFields(sql, fieldNameMap, false);\n    }\n\n    public static String replaceFields(String sql, Map<String, String> fieldNameMap, boolean exactReplace) {\n        Select selectStatement = SqlSelectHelper.getSelect(sql);\n        System.out.println(selectStatement.getSelectBody());\n        SelectBody selectBody = selectStatement.getSelectBody();\n        List<PlainSelect> plainSelectList = new ArrayList<>();\n        if (selectBody instanceof PlainSelect) {\n            plainSelectList.add((PlainSelect) selectBody);\n        } else if (selectBody instanceof SetOperationList) {\n            SetOperationList setOperationList = (SetOperationList) selectBody;\n            //replace select\n            if (!CollectionUtils.isEmpty(setOperationList.getSelects())) {\n                setOperationList.getSelects().forEach(subSelectBody -> {\n                    PlainSelect subPlainSelect = (PlainSelect) subSelectBody;\n                    plainSelectList.add(subPlainSelect);\n                });\n            }\n            //replace order by\n            List<OrderByElement> orderByElements = setOperationList.getOrderByElements();\n            if (!CollectionUtils.isEmpty(orderByElements)) {\n                for (OrderByElement orderByElement : orderByElements) {\n                    orderByElement.accept(new OrderByReplaceVisitor(fieldNameMap, exactReplace));\n                }\n            }\n        } else {\n            return sql;\n        }\n        List<PlainSelect> plainSelects = SqlSelectHelper.getPlainSelects(plainSelectList);\n        for (PlainSelect plainSelect : plainSelects) {\n            replaceFieldsInPlainOneSelect(fieldNameMap, exactReplace, plainSelect);\n        }\n        return selectStatement.toString();\n    }\n\n    private static void replaceFieldsInPlainOneSelect(Map<String, String> fieldNameMap, boolean exactReplace,\n            PlainSelect plainSelect) {\n        //1. replace where fields\n        Expression where = plainSelect.getWhere();\n        FieldReplaceVisitor visitor = new FieldReplaceVisitor(fieldNameMap, exactReplace);\n        if (Objects.nonNull(where)) {\n            where.accept(visitor);\n        }\n\n        //2. replace select fields\n        for (SelectItem selectItem : plainSelect.getSelectItems()) {\n            selectItem.accept(visitor);\n            replaceAsName(fieldNameMap, selectItem);\n        }\n\n        //3. replace oder by fields\n        List<OrderByElement> orderByElements = plainSelect.getOrderByElements();\n        if (!CollectionUtils.isEmpty(orderByElements)) {\n            for (OrderByElement orderByElement : orderByElements) {\n                orderByElement.accept(new OrderByReplaceVisitor(fieldNameMap, exactReplace));\n            }\n        }\n        //4. replace group by fields\n        GroupByElement groupByElement = plainSelect.getGroupBy();\n        if (Objects.nonNull(groupByElement)) {\n            groupByElement.accept(new GroupByReplaceVisitor(fieldNameMap, exactReplace));\n        }\n        //5. replace having fields\n        Expression having = plainSelect.getHaving();\n        if (Objects.nonNull(having)) {\n            having.accept(visitor);\n        }\n        List<Join> joins = plainSelect.getJoins();\n        if (!CollectionUtils.isEmpty(joins)) {\n            for (Join join : joins) {\n                join.getOnExpression().accept(visitor);\n                if (!(join.getRightItem() instanceof SubSelect)) {\n                    continue;\n                }\n                SelectBody subSelectBody = ((SubSelect) join.getRightItem()).getSelectBody();\n                List<PlainSelect> plainSelectList = new ArrayList<>();\n                plainSelectList.add((PlainSelect) subSelectBody);\n                List<PlainSelect> subPlainSelects = SqlSelectHelper.getPlainSelects(plainSelectList);\n                for (PlainSelect subPlainSelect : subPlainSelects) {\n                    replaceFieldsInPlainOneSelect(fieldNameMap, exactReplace, subPlainSelect);\n                }\n            }\n        }\n    }\n\n    private static void replaceAsName(Map<String, String> fieldNameMap, SelectItem selectItem) {\n        if (selectItem instanceof SelectExpressionItem) {\n            SelectExpressionItem expressionItem = (SelectExpressionItem) selectItem;\n            Alias alias = expressionItem.getAlias();\n            if (Objects.isNull(alias)) {\n                return;\n            }\n            String aliasName = alias.getName();\n            String replaceFieldName = fieldNameMap.get(aliasName);\n            if (StringUtils.isNotBlank(replaceFieldName)) {\n                alias.setName(replaceFieldName);\n            }\n        }\n    }\n\n    public static String replaceFunction(String sql, Map<String, String> functionMap) {\n        Select selectStatement = SqlSelectHelper.getSelect(sql);\n        SelectBody selectBody = selectStatement.getSelectBody();\n        if (!(selectBody instanceof PlainSelect)) {\n            return sql;\n        }\n        List<PlainSelect> plainSelectList = new ArrayList<>();\n        plainSelectList.add((PlainSelect) selectBody);\n        List<PlainSelect> plainSelects = SqlSelectHelper.getPlainSelects(plainSelectList);\n        for (PlainSelect plainSelect : plainSelects) {\n            replaceFunction(functionMap, plainSelect);\n        }\n        return selectStatement.toString();\n    }\n\n    private static void replaceFunction(Map<String, String> functionMap, PlainSelect selectBody) {\n        PlainSelect plainSelect = selectBody;\n        //1. replace where dataDiff function\n        Expression where = plainSelect.getWhere();\n\n        FunctionNameReplaceVisitor visitor = new FunctionNameReplaceVisitor(functionMap);\n        if (Objects.nonNull(where)) {\n            where.accept(visitor);\n        }\n        GroupByElement groupBy = plainSelect.getGroupBy();\n        if (Objects.nonNull(groupBy)) {\n            GroupByFunctionReplaceVisitor replaceVisitor = new GroupByFunctionReplaceVisitor(functionMap);\n            groupBy.accept(replaceVisitor);\n        }\n\n        for (SelectItem selectItem : plainSelect.getSelectItems()) {\n            selectItem.accept(visitor);\n        }\n        Expression having = plainSelect.getHaving();\n        if (Objects.nonNull(having)) {\n            replaceHavingFunction(functionMap, having);\n        }\n        List<OrderByElement> orderByElementList = plainSelect.getOrderByElements();\n        replaceOrderByFunction(functionMap, orderByElementList);\n    }\n\n    public static String replaceFunction(String sql) {\n        Select selectStatement = SqlSelectHelper.getSelect(sql);\n        SelectBody selectBody = selectStatement.getSelectBody();\n        if (!(selectBody instanceof PlainSelect)) {\n            return sql;\n        }\n        Expression where = ((PlainSelect) selectBody).getWhere();\n        try {\n            Expression expression = SqlRemoveHelper.filteredExpression(where, SqlEditEnum.DATEDIFF);\n            ((PlainSelect) selectBody).setWhere(expression);\n        } catch (Exception e) {\n            log.info(\"replaceFunction has an exception:{}\", e.toString());\n        }\n\n        return selectStatement.toString();\n    }\n\n    private static void replaceHavingFunction(Map<String, String> functionMap, Expression having) {\n        if (Objects.nonNull(having)) {\n            if (having instanceof AndExpression) {\n                AndExpression andExpression = (AndExpression) having;\n                replaceHavingFunction(functionMap, andExpression.getLeftExpression());\n                replaceHavingFunction(functionMap, andExpression.getRightExpression());\n            } else if (having instanceof OrExpression) {\n                OrExpression orExpression = (OrExpression) having;\n                replaceHavingFunction(functionMap, orExpression.getLeftExpression());\n                replaceHavingFunction(functionMap, orExpression.getRightExpression());\n            } else {\n                replaceComparisonOperatorFunction(functionMap, having);\n            }\n        }\n    }\n\n    private static void replaceComparisonOperatorFunction(Map<String, String> functionMap, Expression expression) {\n        if (Objects.isNull(expression)) {\n            return;\n        }\n        if (expression instanceof GreaterThanEquals) {\n            replaceFilterFunction(functionMap, (GreaterThanEquals) expression);\n        } else if (expression instanceof GreaterThan) {\n            replaceFilterFunction(functionMap, (GreaterThan) expression);\n        } else if (expression instanceof MinorThan) {\n            replaceFilterFunction(functionMap, (MinorThan) expression);\n        } else if (expression instanceof MinorThanEquals) {\n            replaceFilterFunction(functionMap, (MinorThanEquals) expression);\n        } else if (expression instanceof EqualsTo) {\n            replaceFilterFunction(functionMap, (EqualsTo) expression);\n        } else if (expression instanceof NotEqualsTo) {\n            replaceFilterFunction(functionMap, (NotEqualsTo) expression);\n        }\n    }\n\n    private static void replaceOrderByFunction(Map<String, String> functionMap,\n            List<OrderByElement> orderByElementList) {\n        if (Objects.isNull(orderByElementList)) {\n            return;\n        }\n        for (OrderByElement orderByElement : orderByElementList) {\n            if (orderByElement.getExpression() instanceof Function) {\n                Function function = (Function) orderByElement.getExpression();\n                if (functionMap.containsKey(function.getName())) {\n                    function.setName(functionMap.get(function.getName()));\n                }\n            }\n        }\n    }\n\n    private static <T extends ComparisonOperator> void replaceFilterFunction(\n            Map<String, String> functionMap, T comparisonExpression) {\n        Expression expression = comparisonExpression.getLeftExpression();\n        if (expression instanceof Function) {\n            Function function = (Function) expression;\n            if (functionMap.containsKey(function.getName())) {\n                function.setName(functionMap.get(function.getName()));\n            }\n        }\n    }\n\n```\n\n### Code description\n```Java\n/**\n * Description: This method replaces the table name in the given SQL query with the provided table name.\n * It uses the API of org.springframework.util.CollectionUtils.isEmpty to check if the table name is empty.\n * \n * @param sql The SQL query to be modified.\n * @param tableName The new table name to replace the existing one.\n * @return The modified SQL query with the new table name.\n */\n```\n\n### Code snippet:\n```Java\n    public static String replaceTable(String sql, String tableName) {\n        if (StringUtils.isEmpty(tableName)) {\n            return sql;\n        }\n        Select selectStatement = SqlSelectHelper.getSelect(sql);\n        SelectBody selectBody = selectStatement.getSelectBody();\n        if (selectBody instanceof PlainSelect) {\n            PlainSelect plainSelect = (PlainSelect) selectBody;\n            replaceSingleTable(plainSelect, tableName);\n        } else if (selectBody instanceof SetOperationList) {\n```\n\n### Requirements:\n1. Complete only the missing part of the method.\n2. Only answer the code of the completed part. Do not include any of the provided code snippet.\n3. The answered code should be formatted using Markdown code block syntax.\n4. Do not include any additional text.\n\n### Answer: (use the provided format with backticks)\n", "reference": "            SetOperationList setOperationList = (SetOperationList) selectBody;\n            if (!CollectionUtils.isEmpty(setOperationList.getSelects())) {\n                setOperationList.getSelects().forEach(subSelectBody -> {\n                    PlainSelect subPlainSelect = (PlainSelect) subSelectBody;\n                    replaceSingleTable(subPlainSelect, tableName);\n                });\n            }\n        }\n\n        return selectStatement.toString();\n    }", "completions": ["  \n```Java\n    public static String replaceTable(String sql, String tableName) {\n        if (StringUtils.isEmpty(tableName)) {\n            return sql;\n        }\n        Select selectStatement = SqlSelectHelper.getSelect(sql);\n        SelectBody selectBody = selectStatement.getSelectBody();\n        if (selectBody instanceof PlainSelect) {\n            PlainSelect plainSelect = (PlainSelect) selectBody;\n            replaceSingleTable(plainSelect, tableName);\n        } else if (selectBody instanceof SetOperationList) {\n            SetOperationList setOperationList = (SetOperationList) selectBody;\n            List<PlainSelect> plainSelectList = new ArrayList<>();\n            for (SelectBody subSelectBody : setOperationList.getSelects()) {\n                plainSelectList.add((PlainSelect) subSelectBody);\n            }\n            for (PlainSelect plainSelect : plainSelectList) {\n                replaceSingleTable(plainSelect, tableName);\n            }\n        }\n        return selectStatement.toString();\n    }\n\n    private static void replaceSingleTable(PlainSelect plainSelect, String tableName) {\n        Table table = plainSelect.getFromItem();\n        if (table instanceof Table) {\n            table.setName(tableName);\n        }\n    }\n```"]}
{"id": 6, "name": "migration", "prompt": "### Role:\nYou are an experienced Java programmer assistant responsible for code completion tasks. Please intelligently complete the missing parts of the provided code snippet.\n\n### Task:\nBased on the provided code snippet, complete the missing method implementation. Ensure the code is logically correct, syntactically accurate, and follows best coding practices.\n\n### Left context\n```Java\n/*\n * Copyright 2022 Starwhale, Inc. All Rights Reserved.\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\npackage ai.starwhale.mlops.domain.evaluation.storage;\n\nimport static ai.starwhale.mlops.domain.evaluation.storage.JobSchema.CommentColumn;\nimport static ai.starwhale.mlops.domain.evaluation.storage.JobSchema.CreatedTimeColumn;\nimport static ai.starwhale.mlops.domain.evaluation.storage.JobSchema.DataSetIdVersionMapColumn;\nimport static ai.starwhale.mlops.domain.evaluation.storage.JobSchema.DatasetUrisColumn;\nimport static ai.starwhale.mlops.domain.evaluation.storage.JobSchema.DatasetUrisViewColumn;\nimport static ai.starwhale.mlops.domain.evaluation.storage.JobSchema.DevModeColumn;\nimport static ai.starwhale.mlops.domain.evaluation.storage.JobSchema.DurationColumn;\nimport static ai.starwhale.mlops.domain.evaluation.storage.JobSchema.FinishTimeColumn;\nimport static ai.starwhale.mlops.domain.evaluation.storage.JobSchema.INT32;\nimport static ai.starwhale.mlops.domain.evaluation.storage.JobSchema.INT64;\nimport static ai.starwhale.mlops.domain.evaluation.storage.JobSchema.IsDeletedColumn;\nimport static ai.starwhale.mlops.domain.evaluation.storage.JobSchema.JobStatusColumn;\nimport static ai.starwhale.mlops.domain.evaluation.storage.JobSchema.JobTypeColumn;\nimport static ai.starwhale.mlops.domain.evaluation.storage.JobSchema.KeyColumn;\nimport static ai.starwhale.mlops.domain.evaluation.storage.JobSchema.LongIdColumn;\nimport static ai.starwhale.mlops.domain.evaluation.storage.JobSchema.ModelNameColumn;\nimport static ai.starwhale.mlops.domain.evaluation.storage.JobSchema.ModelUriColumn;\nimport static ai.starwhale.mlops.domain.evaluation.storage.JobSchema.ModelUriViewColumn;\nimport static ai.starwhale.mlops.domain.evaluation.storage.JobSchema.ModelVersionColumn;\nimport static ai.starwhale.mlops.domain.evaluation.storage.JobSchema.ModelVersionIdColumn;\nimport static ai.starwhale.mlops.domain.evaluation.storage.JobSchema.ModifiedTimeColumn;\nimport static ai.starwhale.mlops.domain.evaluation.storage.JobSchema.NameColumn;\nimport static ai.starwhale.mlops.domain.evaluation.storage.JobSchema.OwnerIdColumn;\nimport static ai.starwhale.mlops.domain.evaluation.storage.JobSchema.OwnerNameColumn;\nimport static ai.starwhale.mlops.domain.evaluation.storage.JobSchema.ProjectIdColumn;\nimport static ai.starwhale.mlops.domain.evaluation.storage.JobSchema.ResourcePoolColumn;\nimport static ai.starwhale.mlops.domain.evaluation.storage.JobSchema.ResultOutputPathColumn;\nimport static ai.starwhale.mlops.domain.evaluation.storage.JobSchema.RuntimeNameColumn;\nimport static ai.starwhale.mlops.domain.evaluation.storage.JobSchema.RuntimeUriColumn;\nimport static ai.starwhale.mlops.domain.evaluation.storage.JobSchema.RuntimeUriViewColumn;\nimport static ai.starwhale.mlops.domain.evaluation.storage.JobSchema.RuntimeVersionColumn;\nimport static ai.starwhale.mlops.domain.evaluation.storage.JobSchema.RuntimeVersionIdColumn;\nimport static ai.starwhale.mlops.domain.evaluation.storage.JobSchema.STRING;\nimport static ai.starwhale.mlops.domain.evaluation.storage.JobSchema.StepSpecColumn;\nimport static ai.starwhale.mlops.domain.evaluation.storage.JobSchema.tableSchemaDesc;\nimport static ai.starwhale.mlops.domain.job.converter.UserJobConverter.FORMATTER_URI_ARTIFACT;\n\nimport ai.starwhale.mlops.datastore.ColumnSchemaDesc;\nimport ai.starwhale.mlops.datastore.ColumnType;\nimport ai.starwhale.mlops.datastore.DataStore;\nimport ai.starwhale.mlops.datastore.DataStoreMigrationRequest;\nimport ai.starwhale.mlops.datastore.TableQueryFilter;\nimport ai.starwhale.mlops.datastore.TableQueryFilter.Operator;\nimport ai.starwhale.mlops.datastore.TableSchemaDesc;\nimport ai.starwhale.mlops.datastore.type.BaseValue;\nimport ai.starwhale.mlops.datastore.type.Int64Value;\nimport ai.starwhale.mlops.domain.job.JobType;\nimport ai.starwhale.mlops.domain.job.bo.Job;\nimport ai.starwhale.mlops.domain.job.po.JobFlattenEntity;\nimport ai.starwhale.mlops.domain.job.status.JobStatus;\nimport ai.starwhale.mlops.domain.model.ModelService;\nimport ai.starwhale.mlops.domain.model.po.ModelEntity;\nimport ai.starwhale.mlops.domain.model.po.ModelVersionEntity;\nimport ai.starwhale.mlops.domain.project.ProjectService;\nimport ai.starwhale.mlops.domain.project.bo.Project;\nimport ai.starwhale.mlops.domain.user.UserService;\nimport ai.starwhale.mlops.exception.SwValidationException;\nimport ai.starwhale.mlops.exception.SwValidationException.ValidSubject;\nimport com.fasterxml.jackson.databind.ObjectMapper;\nimport java.util.ArrayList;\nimport java.util.Date;\nimport java.util.HashMap;\nimport java.util.List;\nimport java.util.Map;\nimport java.util.Objects;\nimport java.util.stream.Collectors;\nimport lombok.Builder;\nimport lombok.Data;\nimport org.jetbrains.annotations.NotNull;\nimport org.springframework.context.annotation.Lazy;\nimport org.springframework.stereotype.Service;\nimport org.springframework.util.CollectionUtils;\n\n@Service\npublic class EvaluationRepo {\n    private final DataStore store;\n    private final ProjectService projectService;\n    private final ModelService modelService;\n    private final UserService userService;\n    private final ObjectMapper objectMapper;\n\n    public EvaluationRepo(DataStore store,\n                          @Lazy ProjectService projectService,\n                          @Lazy ModelService modelService,\n                          @Lazy UserService userService, ObjectMapper objectMapper) {\n        this.store = store;\n        this.projectService = projectService;\n        this.modelService = modelService;\n        this.userService = userService;\n        this.objectMapper = objectMapper;\n    }\n\n    public int addJob(String table, JobFlattenEntity jobEntity) {\n        store.update(\n                table,\n                tableSchemaDesc,\n                List.of(convertToRecord(jobEntity))\n        );\n        store.flush();\n        return 1;\n    }\n\n    @NotNull\n    private Map<String, Object> convertToRecord(JobFlattenEntity entity) {\n        Map<String, Object> record = new HashMap<>();\n        record.put(LongIdColumn, BaseValue.encode(new Int64Value(entity.getId()), false, false));\n        record.put(KeyColumn, entity.getJobUuid());\n        record.put(NameColumn, entity.getName());\n        if (Objects.nonNull(entity.getComment())) {\n            record.put(CommentColumn, entity.getComment());\n        }\n        if (Objects.nonNull(entity.getCreatedTime())) {\n            record.put(CreatedTimeColumn,\n                    BaseValue.encode(new Int64Value(entity.getCreatedTime().getTime()), false, false));\n        }\n        if (Objects.nonNull(entity.getModifiedTime())) {\n            record.put(ModifiedTimeColumn,\n                    BaseValue.encode(new Int64Value(entity.getModifiedTime().getTime()), false, false));\n        }\n        if (Objects.nonNull(entity.getFinishedTime())) {\n            record.put(FinishTimeColumn,\n                    BaseValue.encode(new Int64Value(entity.getFinishedTime().getTime()), false, false));\n        }\n        if (Objects.nonNull(entity.getDurationMs())) {\n            record.put(DurationColumn,\n                    BaseValue.encode(new Int64Value(entity.getDurationMs()), false, false));\n        }\n        if (Objects.nonNull(entity.getStepSpec())) {\n            record.put(StepSpecColumn, entity.getStepSpec());\n        }\n        record.put(DevModeColumn, entity.isDevMode() ? \"1\" : \"0\");\n        record.put(IsDeletedColumn, \"0\");\n        record.put(ProjectIdColumn,\n                BaseValue.encode(new Int64Value(entity.getProjectId()), false, false));\n\n        if (Objects.nonNull(entity.getModelVersionId())) {\n            record.put(ModelVersionIdColumn,\n                    BaseValue.encode(new Int64Value(entity.getModelVersionId()), false, false));\n        }\n        if (Objects.nonNull(entity.getModelUri())) {\n            record.put(ModelUriColumn, entity.getModelUri());\n        }\n        if (Objects.nonNull(entity.getModelUriForView())) {\n            record.put(ModelUriViewColumn, entity.getModelUriForView());\n        }\n        if (Objects.nonNull(entity.getModelName())) {\n            record.put(ModelNameColumn, entity.getModelName());\n        }\n        if (Objects.nonNull(entity.getModelVersionValue())) {\n            record.put(ModelVersionColumn, entity.getModelVersionValue());\n        }\n\n        if (Objects.nonNull(entity.getRuntimeVersionId())) {\n            record.put(RuntimeVersionIdColumn,\n                    BaseValue.encode(new Int64Value(entity.getRuntimeVersionId()), false, false));\n        }\n        if (Objects.nonNull(entity.getRuntimeUri())) {\n            record.put(RuntimeUriColumn, entity.getRuntimeUri());\n        }\n        if (Objects.nonNull(entity.getRuntimeUriForView())) {\n            record.put(RuntimeUriViewColumn, entity.getRuntimeUriForView());\n        }\n        if (Objects.nonNull(entity.getRuntimeName())) {\n            record.put(RuntimeNameColumn, entity.getRuntimeName());\n        }\n        if (Objects.nonNull(entity.getRuntimeVersionValue())) {\n            record.put(RuntimeVersionColumn, entity.getRuntimeVersionValue());\n        }\n\n        if (Objects.nonNull(entity.getDatasetIdVersionMap())) {\n            record.put(DataSetIdVersionMapColumn, convertToDatastoreValue(entity.getDatasetIdVersionMap()));\n        }\n        if (Objects.nonNull(entity.getDatasets()) && !entity.getDatasets().isEmpty()) {\n            record.put(DatasetUrisColumn, entity.getDatasets());\n        }\n        if (Objects.nonNull(entity.getDatasetsForView())) {\n            record.put(DatasetUrisViewColumn, entity.getDatasetsForView());\n        }\n\n        record.put(OwnerIdColumn,\n                BaseValue.encode(new Int64Value(entity.getOwnerId()), false, false));\n        record.put(OwnerNameColumn, String.valueOf(entity.getOwnerName()));\n        record.put(JobStatusColumn, entity.getJobStatus().name());\n        record.put(JobTypeColumn, entity.getType().name());\n        record.put(ResultOutputPathColumn, entity.getResultOutputPath());\n        record.put(ResourcePoolColumn, entity.getResourcePool());\n        return record;\n    }\n\n    public Map<String, String> convertToDatastoreValue(Map<Long, String> origin) {\n        if (CollectionUtils.isEmpty(origin)) {\n            return Map.of();\n        }\n        return origin.keySet().stream()\n                .collect(Collectors.toMap(\n                        k -> (String) BaseValue.encode(new Int64Value(k), false, false),\n                        origin::get));\n    }\n\n    public void updateJobStatus(String table, Job job, JobStatus jobStatus) {\n        if (!evaluationJob(job)) {\n            return;\n        }\n        this.updateByUuid(table, job.getUuid(),\n                List.of(ColumnRecord.builder().property(JobStatusColumn).type(STRING).value(jobStatus.name()).build()));\n    }\n\n    public void updateJobFinishedTime(String table, Job job, Date finishedTime, Long duration) {\n        if (!evaluationJob(job)) {\n            return;\n        }\n        this.updateByUuid(table, job.getUuid(), List.of(\n                        ColumnRecord.builder()\n                                .property(FinishTimeColumn)\n                                .type(INT64)\n                                .value((String) BaseValue.encode(new Int64Value(finishedTime.getTime()), false, false))\n                                .build(),\n                        ColumnRecord.builder()\n                                .property(DurationColumn)\n                                .type(INT64)\n                                .value((String) BaseValue.encode(new Int64Value(duration), false, false))\n                                .build()\n                ));\n    }\n\n    public int updateJobComment(String table, Job job, String comment) {\n        if (!evaluationJob(job)) {\n            return 0;\n        }\n        return this.updateByUuid(table, job.getUuid(),\n                List.of(ColumnRecord.builder().property(CommentColumn).type(STRING).value(comment).build()));\n    }\n\n    public int removeJob(String table, Job job) {\n        if (Objects.isNull(job)) {\n            return 0;\n        }\n        return this.updateByUuid(table, job.getUuid(),\n                List.of(ColumnRecord.builder().property(IsDeletedColumn).type(INT32).value(\"1\").build()));\n    }\n\n    public int recoverJob(String table, Job job) {\n        if (!evaluationJob(job)) {\n            return 0;\n        }\n        return this.updateByUuid(table, job.getUuid(), List.of(\n                ColumnRecord.builder().property(IsDeletedColumn).type(INT32).value(\"0\").build()));\n    }\n\n    private static boolean evaluationJob(Job job) {\n        return !Objects.isNull(job) && job.getType() == JobType.EVALUATION;\n    }\n\n    @Data\n    @Builder\n    static class ColumnRecord {\n        String property;\n        String type;\n        String value;\n    }\n\n    private int updateByUuid(String table, String uuid, List<ColumnRecord> columnRecords) {\n        if (Objects.isNull(table)) {\n            return 0;\n        }\n        if (CollectionUtils.isEmpty(columnRecords)) {\n            return 0;\n        }\n        List<ColumnSchemaDesc> columns = new ArrayList<>(List.of(\n                ColumnSchemaDesc.builder().name(KeyColumn).type(STRING).build(),\n                ColumnSchemaDesc.builder().name(ModifiedTimeColumn).type(INT64).build()));\n\n        Map<String, Object> values = new HashMap<>(Map.of(\n                KeyColumn, uuid,\n                ModifiedTimeColumn,\n                BaseValue.encode(new Int64Value(new Date().getTime()), false, false)));\n        // update columns\n        columnRecords.forEach(record -> {\n            columns.add(ColumnSchemaDesc.builder().name(record.getProperty()).type(record.getType()).build());\n            values.put(record.getProperty(), record.getValue());\n        });\n        store.update(table,\n                new TableSchemaDesc(KeyColumn, columns),\n                List.of(values)\n        );\n        store.flush();\n        return 1;\n    }\n\n    public int updateModelInfo(String table, List<String> uuids, ModelEntity newModel, ModelVersionEntity newVersion) {\n        if (Objects.isNull(table) || CollectionUtils.isEmpty(uuids) || newModel == null || newVersion == null) {\n            throw new SwValidationException(\n                    ValidSubject.EVALUATION, \"Invalid request when update model info for eval\");\n        }\n\n        List<ColumnSchemaDesc> columns = List.of(\n                ColumnSchemaDesc.builder().name(KeyColumn).type(STRING).build(),\n                ColumnSchemaDesc.builder().name(ModelNameColumn).type(STRING).build(),\n                ColumnSchemaDesc.builder().name(ModelUriColumn).type(STRING).build(),\n                ColumnSchemaDesc.builder().name(ModelUriViewColumn).type(STRING).build(),\n                ColumnSchemaDesc.builder().name(ModelVersionColumn).type(STRING).build(),\n                ColumnSchemaDesc.builder().name(ModelVersionIdColumn).type(INT64).build()\n\n        );\n\n        List<Map<String, Object>> records = new ArrayList<>();\n        Project project = projectService.findProject(newModel.getProjectId());\n        for (String uuid : uuids) {\n            records.add(new HashMap<>(Map.of(\n                    KeyColumn, uuid,\n                    ModelNameColumn, newModel.getName(),\n                    ModelVersionColumn, newVersion.getVersionName(),\n                    ModelUriViewColumn, String.format(\n                            FORMATTER_URI_ARTIFACT,\n                            project.getName(),\n                            \"model\",\n                            newModel.getName(),\n                            newVersion.getVersionName()\n                    ),\n                    ModelUriColumn, String.format(\n                            FORMATTER_URI_ARTIFACT, project.getId(), \"model\", newModel.getId(), newVersion.getId()\n                    ),\n                    ModelVersionIdColumn, BaseValue.encode(new Int64Value(newVersion.getId()), false, false)\n            )));\n        }\n        store.update(table, new TableSchemaDesc(KeyColumn, columns), records);\n        store.flush();\n        return uuids.size();\n    }\n\n```\n\n### Code description\n```Java\n/**\n * Migrates data from the source table to the target table based on the provided UUIDs.\n *\n * @param srcTable The name of the source table.\n * @param uuids A list of UUIDs to filter the data.\n * @param targetTable The name of the target table.\n * @return The result of the migration operation.\n * @throws SwValidationException If the source table, target table, or UUIDs are null or empty.\n */\n```\n\n### Code snippet:\n```Java\n    public int migration(String srcTable, List<String> uuids, String targetTable) {\n        if (Objects.isNull(srcTable) || Objects.isNull(targetTable) || CollectionUtils.isEmpty(uuids)) {\n            throw new SwValidationException(\n                    ValidSubject.EVALUATION, \"srcTable, targetTable and uuids must not be null\");\n        }\n        var filter = TableQueryFilter.builder()\n                .operator(Operator.OR)\n                .operands(uuids.stream()\n                                  .map(uuid -> TableQueryFilter.builder()\n                                          .operator(Operator.EQUAL)\n                                          .operands(List.of(\n                                                  new TableQueryFilter.Column(KeyColumn),\n                                                  new TableQueryFilter.Constant(\n```\n\n### Requirements:\n1. Complete only the missing part of the method.\n2. Only answer the code of the completed part. Do not include any of the provided code snippet.\n3. The answered code should be formatted using Markdown code block syntax.\n4. Do not include any additional text.\n\n### Answer: (use the provided format with backticks)\n", "reference": "                                                          ColumnType.STRING, uuid)\n                                          ))\n                                          .build())\n                                  .collect(Collectors.toList())\n                )\n                .build();\n        return store.migration(DataStoreMigrationRequest.builder()\n                                       .srcTableName(srcTable)\n                                       .targetTableName(targetTable)\n                                       .filter(filter)\n                                       .build()\n        );\n    }", "completions": ["  \n```Java\n.of(uuid))))\n                                         .build())\n                                 .collect(Collectors.toList()))\n                         .build())\n                 .collect(Collectors.toList()))\n         .build();\n\n        var request = DataStoreMigrationRequest.builder()\n               .srcTable(srcTable)\n               .targetTable(targetTable)\n               .filter(filter)\n               .build();\n\n        store.migrate(request);\n        store.flush();\n        return uuids.size();\n    }\n```"]}
{"id": 7, "name": "listEntities", "prompt": "### Role:\nYou are an experienced Java programmer assistant responsible for code completion tasks. Please intelligently complete the missing parts of the provided code snippet.\n\n### Task:\nBased on the provided code snippet, complete the missing method implementation. Ensure the code is logically correct, syntactically accurate, and follows best coding practices.\n\n### Left context\n```Java\n/*\n*Copyright (c) Alibaba Group;\n*Licensed under the Apache License, Version 2.0 (the \"License\");\n*you may not use this file except in compliance with the License.\n*You may obtain a copy of the License at\n\n*   http://www.apache.org/licenses/LICENSE-2.0\n\n*Unless required by applicable law or agreed to in writing, software\n*distributed under the License is distributed on an \"AS IS\" BASIS,\n*WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n*See the License for the specific language governing permissions and\n*limitations under the License.\n*/\n\npackage org.example.common.helper.ots;\n\nimport com.alicloud.openservices.tablestore.model.Column;\nimport com.alicloud.openservices.tablestore.model.DeleteRowRequest;\nimport com.alicloud.openservices.tablestore.model.GetRowResponse;\nimport com.alicloud.openservices.tablestore.model.PrimaryKey;\nimport com.alicloud.openservices.tablestore.model.Row;\nimport com.alicloud.openservices.tablestore.model.RowDeleteChange;\nimport com.alicloud.openservices.tablestore.model.RowPutChange;\nimport com.alicloud.openservices.tablestore.model.RowUpdateChange;\nimport com.alicloud.openservices.tablestore.model.SingleRowQueryCriteria;\nimport com.alicloud.openservices.tablestore.model.filter.ColumnValueFilter;\nimport com.alicloud.openservices.tablestore.model.search.SearchQuery;\nimport com.alicloud.openservices.tablestore.model.search.SearchRequest;\nimport com.alicloud.openservices.tablestore.model.search.SearchResponse;\nimport com.alicloud.openservices.tablestore.model.search.query.BoolQuery;\nimport com.alicloud.openservices.tablestore.model.search.query.MatchQuery;\nimport com.alicloud.openservices.tablestore.model.search.query.Query;\nimport com.alicloud.openservices.tablestore.model.search.query.RangeQuery;\nimport com.alicloud.openservices.tablestore.model.search.query.TermsQuery;\nimport com.alicloud.openservices.tablestore.model.search.sort.Sort;\nimport lombok.Builder;\nimport lombok.Data;\nimport lombok.extern.slf4j.Slf4j;\nimport org.example.common.ListResult;\nimport org.example.common.adapter.OtsClient;\nimport org.example.common.errorinfo.ErrorInfo;\nimport org.example.common.exception.BizException;\nimport org.example.common.utils.EncryptionUtil;\nimport org.example.common.utils.JsonUtil;\nimport org.example.common.utils.OtsUtil;\nimport org.example.common.utils.ReflectionUtil;\nimport org.springframework.stereotype.Component;\nimport org.springframework.util.StringUtils;\n\nimport java.util.ArrayList;\nimport java.util.Arrays;\nimport java.util.Collections;\nimport java.util.List;\nimport java.util.Optional;\nimport java.util.stream.Collectors;\n\n@Component\n@Slf4j\npublic class BaseOtsHelper {\n\n    private final OtsClient otsClient;\n\n    private static final Integer DEFAULT_MAX_RESULTS = 10;\n\n    private static final Integer RANGE_FILTER_SIZE = 2;\n\n    public BaseOtsHelper(OtsClient otsClient) {\n        this.otsClient = otsClient;\n    }\n\n    public <T> T getEntity(String tableName, PrimaryKey primaryKey, ColumnValueFilter filter, Class<T> clazz) {\n        SingleRowQueryCriteria criteria = new SingleRowQueryCriteria(tableName, primaryKey);\n        if (filter != null) {\n            criteria.setFilter(filter);\n        }\n        criteria.setMaxVersions(1);\n        log.info(\"criteria:{}\", JsonUtil.toJsonString(criteria));\n        GetRowResponse getRowResponse = otsClient.getRow(criteria);\n        Optional<Row> optionalRow = Optional.ofNullable(getRowResponse != null ? getRowResponse.getRow() : null);\n        return optionalRow.map(object -> OtsUtil.convertRowToDTO(object, clazz))\n                .orElseThrow(() -> new BizException(ErrorInfo.ENTITY_NOT_EXIST.getStatusCode(), ErrorInfo.ENTITY_NOT_EXIST.getCode(),\n                        String.format(ErrorInfo.ENTITY_NOT_EXIST.getMessage(), tableName+primaryKey.toString())));\n    }\n\n    public void createEntity(String tableName, PrimaryKey primaryKey, List<Column> columns) {\n        RowPutChange rowPutChange = new RowPutChange(tableName, primaryKey);\n        rowPutChange.addColumns(columns);\n        log.info(\"rowPutChange:{}\", JsonUtil.toJsonString(rowPutChange));\n        otsClient.putRow(rowPutChange);\n    }\n\n    public Boolean updateEntity(String tableName, PrimaryKey primaryKey, List<Column> columns) {\n        RowUpdateChange rowUpdateChange = new RowUpdateChange(tableName, primaryKey);\n        rowUpdateChange.put(columns);\n        log.info(\"rowUpdateChange:{}\", JsonUtil.toJsonString(rowUpdateChange));\n        otsClient.updateRow(rowUpdateChange);\n        return Boolean.TRUE;\n    }\n\n    public Boolean deleteEntity(String tableName, PrimaryKey primaryKey) {\n        RowDeleteChange rowDeleteChange = new RowDeleteChange(tableName, primaryKey);\n        DeleteRowRequest deleteRowRequest = new DeleteRowRequest(rowDeleteChange);\n        log.info(\"deleteRowRequest:{}\", JsonUtil.toJsonString(deleteRowRequest));\n        otsClient.deletRow(deleteRowRequest);\n        return Boolean.TRUE;\n    }\n\n    private Query createRangeQuery(OtsFilter filter) {\n        if (filter.values == null || filter.values.size() != RANGE_FILTER_SIZE || filter.values.contains(null)) {\n            throw new BizException(ErrorInfo.SERVER_UNAVAILABLE);\n        }\n        RangeQuery rangeQuery = new RangeQuery();\n        rangeQuery.setFieldName(filter.key);\n        Object start = filter.values.get(0);\n        Object end = filter.values.get(1);\n        rangeQuery.greaterThanOrEqual(OtsUtil.createColumnValue(start));\n        rangeQuery.lessThanOrEqual(OtsUtil.createColumnValue(end));\n        return rangeQuery;\n    }\n\n    private SearchQuery createSearchQuery(List<OtsFilter> matchFilters, List<OtsFilter> queryFilters, List<OtsFilter> multiMatchFilters) {\n        List<Query> queries = Optional.ofNullable(queryFilters)\n                .map(filters -> filters.stream()\n                        .map(this::createRangeQuery)\n                        .collect(Collectors.toList()))\n                .orElse(new ArrayList<>());\n        SearchQuery searchQuery = new SearchQuery();\n        if (matchFilters != null && !matchFilters.isEmpty()) {\n            matchFilters.forEach((filter) -> {\n                MatchQuery matchQuery = new MatchQuery();\n                matchQuery.setFieldName(filter.key);\n                matchQuery.setText(String.valueOf(filter.values.get(0)));\n                queries.add(matchQuery);\n            });\n        }\n\n        if (multiMatchFilters != null && !multiMatchFilters.isEmpty()) {\n            multiMatchFilters.forEach((filter) -> {\n                TermsQuery termsQuery = new TermsQuery();\n                termsQuery.setFieldName(filter.getKey());\n                for (Object value : filter.values) {\n                    termsQuery.addTerm(OtsUtil.createColumnValue(value));\n                }\n                queries.add(termsQuery);\n            });\n        }\n        BoolQuery boolQuery = new BoolQuery();\n        boolQuery.setMustQueries(queries);\n        searchQuery.setQuery(boolQuery);\n        searchQuery.setGetTotalCount(true);\n        searchQuery.setLimit(DEFAULT_MAX_RESULTS);\n        return searchQuery;\n    }\n\n```\n\n### Code description\n```Java\n/**\n * Description: This method is used to list entities from a specified table in a database.\n * It uses the provided search filters and sorters to refine the results.\n * If a next token is provided, it will be used to continue the search from the last point.\n * If no next token is provided and sorters are provided, they will be used to sort the results.\n * The method returns a ListResult object containing the list of entities, the total count of entities, and the next token for pagination.\n *\n * @param tableName The name of the table to search in.\n * @param searchIndexName The name of the search index to use.\n * @param matchFilters A list of OtsFilter objects used to match entities.\n * @param queryFilters A list of OtsFilter objects used to query entities.\n * @param multiMatchFilter A list of OtsFilter objects used for multi-match queries.\n * @param nextToken A token used to continue the search from the last point.\n * @param sorters A list of Sorters used to sort the results.\n * @param clazz The class of the entities to be returned.\n * @return A ListResult object containing the list of entities, the total count of entities, and the next token for pagination.\n */\n```\n\n### Code snippet:\n```Java\n    public <T> ListResult<T> listEntities(String tableName, String searchIndexName, List<OtsFilter> matchFilters,\n                                             List<OtsFilter> queryFilters, List<OtsFilter> multiMatchFilter, String nextToken, List<Sort.Sorter> sorters, Class<T> clazz) {\n        SearchQuery searchQuery = createSearchQuery(matchFilters, queryFilters, multiMatchFilter);\n        SearchRequest searchRequest = new SearchRequest(tableName, searchIndexName, searchQuery);\n        if (!StringUtils.isEmpty(nextToken)) {\n            byte[] tokenBytes = EncryptionUtil.decode(nextToken);\n            searchRequest.getSearchQuery().setToken(tokenBytes);\n        } else {\n            if (sorters != null &&!sorters.isEmpty()) {\n                searchQuery.setSort(new Sort(sorters));\n            }\n        }\n        SearchRequest.ColumnsToGet columnsToGet = new SearchRequest.ColumnsToGet();\n```\n\n### Requirements:\n1. Complete only the missing part of the method.\n2. Only answer the code of the completed part. Do not include any of the provided code snippet.\n3. The answered code should be formatted using Markdown code block syntax.\n4. Do not include any additional text.\n\n### Answer: (use the provided format with backticks)\n", "reference": "        columnsToGet.setColumns(ReflectionUtil.getPropertyNames(clazz));\n        searchRequest.setColumnsToGet(columnsToGet);\n        log.info(\"searchRequest:{}\", JsonUtil.toJsonString(searchRequest));\n        SearchResponse searchResponse = otsClient.search(searchRequest);\n        if (searchResponse == null || searchResponse.getRows() == null) {\n            return ListResult.genSuccessListResult(null, 0);\n        }\n        byte[] nextTokenBytes = searchResponse.getNextToken();\n        nextToken = nextTokenBytes == null || nextTokenBytes.length == 0 ? null : EncryptionUtil.encode(nextTokenBytes);\n        List<T> result = searchResponse.getRows().stream()\n                .map(row -> OtsUtil.convertRowToDTO(row, clazz))\n                .collect(Collectors.toList());\n        return ListResult.genSuccessListResult(result, searchResponse.getTotalCount(), nextToken);\n    }", "completions": ["  \n```Java\ncolumnsToGet.setColumns(Collections.singletonList(\"*\"));\nsearchRequest.setColumnsToGet(columnsToGet);\nSearchResponse searchResponse = otsClient.search(searchRequest);\nList<T> entities = searchResponse.getRows().stream()\n       .map(row -> OtsUtil.convertRowToDTO(row, clazz))\n       .collect(Collectors.toList());\nlong totalCount = searchResponse.getTotalCount();\nString nextToken = null;\nif (searchResponse.getNextToken()!= null) {\n    nextToken = EncryptionUtil.encode(searchResponse.getNextToken());\n}\nreturn ListResult.<T>builder()\n       .entities(entities)\n       .totalCount(totalCount)\n       .nextToken(nextToken)\n       .build();\n```"]}
{"id": 9, "name": "embedText", "prompt": "### Role:\nYou are an experienced Java programmer assistant responsible for code completion tasks. Please intelligently complete the missing parts of the provided code snippet.\n\n### Task:\nBased on the provided code snippet, complete the missing method implementation. Ensure the code is logically correct, syntactically accurate, and follows best coding practices.\n\n### Left context\n```Java\n/*\n * Copyright 2023 - 2024 the original author or authors.\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * https://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\npackage org.springframework.ai.vertexai.palm2.api;\n\nimport java.io.IOException;\nimport java.util.List;\nimport java.util.Map;\nimport java.util.function.Consumer;\n\nimport com.fasterxml.jackson.annotation.JsonInclude;\nimport com.fasterxml.jackson.annotation.JsonInclude.Include;\nimport com.fasterxml.jackson.annotation.JsonProperty;\nimport com.fasterxml.jackson.databind.ObjectMapper;\n\nimport org.springframework.http.HttpHeaders;\nimport org.springframework.http.MediaType;\nimport org.springframework.http.client.ClientHttpResponse;\nimport org.springframework.util.Assert;\nimport org.springframework.web.client.ResponseErrorHandler;\nimport org.springframework.web.client.RestClient;\n\n// @formatter:off\n/**\n * Vertex AI API client for the Generative Language model.\n * https://developers.generativeai.google/api/rest/generativelanguage\n * https://cloud.google.com/vertex-ai/docs/generative-ai/learn/streaming\n *\n * Provides methods to generate a response from the model given an input\n * https://developers.generativeai.google/api/rest/generativelanguage/models/generateMessage\n *\n * as well as to generate embeddings for the input text:\n * https://developers.generativeai.google/api/rest/generativelanguage/models/embedText\n *\n *\n * Supported models:\n *\n * <pre>\n * \t\tname=models/chat-bison-001,\n * \t\tversion=001,\n * \t\tdisplayName=Chat Bison,\n * \t\tdescription=Chat-optimized generative language model.,\n * \t\tinputTokenLimit=4096,\n * \t\toutputTokenLimit=1024,\n * \t\tsupportedGenerationMethods=[generateMessage, countMessageTokens],\n * \t\ttemperature=0.25,\n * \t\ttopP=0.95,\n *\t\ttopK=40\n *\n * \t\tname=models/text-bison-001,\n *\t\tversion=001,\n *\t\tdisplayName=Text Bison,\n *\t\tdescription=Model targeted for text generation.,\n * \t\tinputTokenLimit=8196,\n *\t\toutputTokenLimit=1024,\n *\t\tsupportedGenerationMethods=[generateText, countTextTokens, createTunedTextModel],\n *\t\ttemperature=0.7,\n *\t\ttopP=0.95,\n *\t\ttopK=40\n *\n * \t\tname=models/embedding-gecko-001,\n * \t\tversion=001,\n * \t\tdisplayName=Embedding Gecko, description=Obtain a distributed representation of a text.,\n * \t\tinputTokenLimit=1024,\n * \t\toutputTokenLimit=1,\n * \t\tsupportedGenerationMethods=[embedText, countTextTokens],\n * \t\ttemperature=null,\n * \t\ttopP=null,\n * \t\ttopK=null\n * </pre>\n *\n * https://cloud.google.com/vertex-ai/generative-ai/docs/learn/models\n *\n * https://ai.google.dev/api/rest#rest-resource:-v1.models\n *\n * @author Christian Tzolov\n */\npublic class VertexAiPaLm2Api {\n\n\t/**\n\t * The default generation model. This model is used to generate responses for the\n\t * input text.\n\t */\n\tpublic static final String DEFAULT_GENERATE_MODEL = \"chat-bison-001\";\n\n\t/**\n\t * The default embedding model. This model is used to generate embeddings for the\n\t * input text.\n\t */\n\tpublic static final String DEFAULT_EMBEDDING_MODEL = \"embedding-gecko-001\";\n\n\t/**\n\t * The default base URL for accessing the Vertex AI API.\n\t */\n\tpublic static final String DEFAULT_BASE_URL = \"https://generativelanguage.googleapis.com/v1beta3\";\n\n\tprivate final RestClient restClient;\n\n\tprivate final String apiKey;\n\n\tprivate final String chatModel;\n\n\tprivate final String embeddingModel;\n\n\t/**\n\t * Create a new chat completion api.\n\t * @param apiKey vertex apiKey.\n\t */\n\tpublic VertexAiPaLm2Api(String apiKey) {\n\t\tthis(DEFAULT_BASE_URL, apiKey, DEFAULT_GENERATE_MODEL, DEFAULT_EMBEDDING_MODEL, RestClient.builder());\n\t}\n\n\t/**\n\t * Create a new chat completion api.\n\t * @param baseUrl api base URL.\n\t * @param apiKey vertex apiKey.\n\t * @param model vertex model.\n\t * @param embeddingModel vertex embedding model.\n\t * @param restClientBuilder RestClient builder.\n\t */\n\tpublic VertexAiPaLm2Api(String baseUrl, String apiKey, String model, String embeddingModel,\n\t\t\tRestClient.Builder restClientBuilder) {\n\n\t\tthis.chatModel = model;\n\t\tthis.embeddingModel = embeddingModel;\n\t\tthis.apiKey = apiKey;\n\n\t\tConsumer<HttpHeaders> jsonContentHeaders = headers -> {\n\t\t\theaders.setAccept(List.of(MediaType.APPLICATION_JSON));\n\t\t\theaders.setContentType(MediaType.APPLICATION_JSON);\n\t\t};\n\n\t\tResponseErrorHandler responseErrorHandler = new ResponseErrorHandler() {\n\t\t\t@Override\n\t\t\tpublic boolean hasError(ClientHttpResponse response) throws IOException {\n\t\t\t\treturn response.getStatusCode().isError();\n\t\t\t}\n\n\t\t\t@Override\n\t\t\tpublic void handleError(ClientHttpResponse response) throws IOException {\n\t\t\t\tif (response.getStatusCode().isError()) {\n\t\t\t\t\tthrow new RuntimeException(String.format(\"%s - %s\", response.getStatusCode().value(),\n\t\t\t\t\t\t\tnew ObjectMapper().readValue(response.getBody(), ResponseError.class)));\n\t\t\t\t}\n\t\t\t}\n\t\t};\n\n\t\tthis.restClient = restClientBuilder.baseUrl(baseUrl)\n\t\t\t.defaultHeaders(jsonContentHeaders)\n\t\t\t.defaultStatusHandler(responseErrorHandler)\n\t\t\t.build();\n\t}\n\n\t/**\n\t * Generates a response from the model given an input.\n\t * @param request Request body.\n\t * @return Response body.\n\t */\n\t@SuppressWarnings(\"null\")\n\tpublic GenerateMessageResponse generateMessage(GenerateMessageRequest request) {\n\t\tAssert.notNull(request, \"The request body can not be null.\");\n\n\t\treturn this.restClient.post()\n\t\t\t.uri(\"/models/{model}:generateMessage?key={apiKey}\", this.chatModel, this.apiKey)\n\t\t\t.body(request)\n\t\t\t.retrieve()\n\t\t\t.body(GenerateMessageResponse.class);\n\t}\n\n\t/**\n\t * Generates a response from the model given an input.\n\t * @param text Text to embed.\n\t * @return Embedding response.\n\t */\n```\n\n### Code description\n```Java\n/**\n * Description: This method is used to embed text using a specified embedding model and API key.\n * It uses the Assert.hasText method to ensure the input text is not null or empty.\n *\n * @param text The text to be embedded.\n * @return The embedding of the text, or null if the response is null.\n * @throws IllegalArgumentException If the text is null or empty.\n */\n```\n\n### Code snippet:\n```Java\n\tpublic Embedding embedText(String text) {\n\t\tAssert.hasText(text, \"The text can not be null or empty.\");\n\n\t\t@JsonInclude(Include.NON_NULL)\n\t\trecord EmbeddingResponse(Embedding embedding) {\n\t\t}\n\n```\n\n### Requirements:\n1. Complete only the missing part of the method.\n2. Only answer the code of the completed part. Do not include any of the provided code snippet.\n3. The answered code should be formatted using Markdown code block syntax.\n4. Do not include any additional text.\n\n### Answer: (use the provided format with backticks)\n", "reference": "\t\tEmbeddingResponse response = this.restClient.post()\n\t\t\t\t.uri(\"/models/{model}:embedText?key={apiKey}\", this.embeddingModel, this.apiKey)\n\t\t\t\t.body(Map.of(\"text\", text))\n\t\t\t\t.retrieve()\n\t\t\t\t.body(EmbeddingResponse.class);\n\n\t\treturn response != null ? response.embedding() : null;\n\t}", "completions": ["  \n```Java\n\tpublic Embedding embedText(String text) {\n\t\tAssert.hasText(text, \"The text can not be null or empty.\");\n\n\t\t@JsonInclude(Include.NON_NULL)\n\t\trecord EmbeddingResponse(Embedding embedding) {\n\t\t}\n\n\t\treturn this.restClient.post()\n\t\t\t.uri(\"/models/{model}:embedText?key={apiKey}\", this.embeddingModel, this.apiKey)\n\t\t\t.body(new EmbedTextRequest(text))\n\t\t\t.retrieve()\n\t\t\t.body(EmbeddingResponse.class)\n\t\t\t.embedding;\n\t}\n```"]}
{"id": 10, "name": "tracer", "prompt": "### Role:\nYou are an experienced Java programmer assistant responsible for code completion tasks. Please intelligently complete the missing parts of the provided code snippet.\n\n### Task:\nBased on the provided code snippet, complete the missing method implementation. Ensure the code is logically correct, syntactically accurate, and follows best coding practices.\n\n### Left context\n```Java\n/*\n * Copyright 2023 Apollo Authors\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n *\n */\npackage com.ctrip.framework.apollo.audit.context;\n\nimport com.ctrip.framework.apollo.audit.constants.ApolloAuditConstants;\nimport com.ctrip.framework.apollo.audit.spi.ApolloAuditOperatorSupplier;\nimport java.util.Objects;\nimport org.springframework.web.context.request.RequestAttributes;\nimport org.springframework.web.context.request.RequestContextHolder;\n\npublic class ApolloAuditTraceContext {\n\n  private final ApolloAuditOperatorSupplier operatorSupplier;\n\n  public ApolloAuditTraceContext(ApolloAuditOperatorSupplier operatorSupplier) {\n    this.operatorSupplier = operatorSupplier;\n  }\n\n  // if not get one, create one and re-get it\n```\n\n### Code description\n```Java\n/**\n * Retrieves the ApolloAuditTracer from the current request attributes.\n * If the tracer does not exist, it creates a new one and sets it in the request attributes.\n *\n * @return The ApolloAuditTracer instance associated with the current request, or null if no request attributes are available.\n */\n```\n\n### Code snippet:\n```Java\n  public ApolloAuditTracer tracer() {\n    RequestAttributes requestAttributes = RequestContextHolder.getRequestAttributes();\n    if (requestAttributes != null) {\n      Object tracer = requestAttributes.getAttribute(ApolloAuditConstants.TRACER,\n          RequestAttributes.SCOPE_REQUEST);\n      if (tracer != null) {\n        return ((ApolloAuditTracer) tracer);\n```\n\n### Requirements:\n1. Complete only the missing part of the method.\n2. Only answer the code of the completed part. Do not include any of the provided code snippet.\n3. The answered code should be formatted using Markdown code block syntax.\n4. Do not include any additional text.\n\n### Answer: (use the provided format with backticks)\n", "reference": "      } else {\n        ApolloAuditTracer newTracer = new ApolloAuditTracer(new ApolloAuditScopeManager(), operatorSupplier);\n        setTracer(newTracer);\n        return newTracer;\n      }\n    }\n    return null;\n  }", "completions": ["  ```\n  if (tracer == null) {\n    tracer = new ApolloAuditTracer(operatorSupplier.get());\n    requestAttributes.setAttribute(ApolloAuditConstants.TRACER, tracer, RequestAttributes.SCOPE_REQUEST);\n  }\n  return ((ApolloAuditTracer) tracer);\n```"]}
{"id": 11, "name": "getExpandUrl", "prompt": "### Role:\nYou are an experienced Java programmer assistant responsible for code completion tasks. Please intelligently complete the missing parts of the provided code snippet.\n\n### Task:\nBased on the provided code snippet, complete the missing method implementation. Ensure the code is logically correct, syntactically accurate, and follows best coding practices.\n\n### Left context\n```Java\n/*\n*Copyright (c) Alibaba Group;\n*Licensed under the Apache License, Version 2.0 (the \"License\");\n*you may not use this file except in compliance with the License.\n*You may obtain a copy of the License at\n\n*   http://www.apache.org/licenses/LICENSE-2.0\n\n*Unless required by applicable law or agreed to in writing, software\n*distributed under the License is distributed on an \"AS IS\" BASIS,\n*WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n*See the License for the specific language governing permissions and\n*limitations under the License.\n*/\n\npackage org.example.common.utils;\n\nimport org.example.common.constant.Constants;\nimport lombok.extern.slf4j.Slf4j;\nimport org.apache.http.HttpResponse;\nimport org.apache.http.HttpStatus;\nimport org.apache.http.NameValuePair;\nimport org.apache.http.StatusLine;\nimport org.apache.http.client.entity.UrlEncodedFormEntity;\nimport org.apache.http.client.methods.CloseableHttpResponse;\nimport org.apache.http.client.methods.HttpGet;\nimport org.apache.http.client.methods.HttpPost;\nimport org.apache.http.impl.client.CloseableHttpClient;\nimport org.apache.http.impl.client.HttpClients;\nimport org.apache.http.util.EntityUtils;\nimport org.springframework.beans.BeanWrapper;\nimport org.springframework.beans.BeanWrapperImpl;\nimport org.springframework.stereotype.Component;\nimport org.springframework.web.util.UriComponentsBuilder;\n\nimport javax.servlet.http.HttpServletRequest;\nimport java.io.IOException;\nimport java.lang.reflect.Field;\nimport java.nio.charset.StandardCharsets;\nimport java.util.List;\nimport java.util.Map;\nimport java.util.stream.Collectors;\n\n@Component\n@Slf4j\npublic class HttpUtil {\n\n    public static String doGet(String url) {\n        try (CloseableHttpClient client = HttpClients.createDefault()) {\n            HttpGet request = new HttpGet(url);\n            HttpResponse response = client.execute(request);\n            if (response.getStatusLine().getStatusCode() == HttpStatus.SC_OK) {\n                return EntityUtils.toString(response.getEntity(), StandardCharsets.UTF_8);\n            }\n        } catch (IOException e) {\n            log.error(\"DoGet exception.\", e);\n\n        }\n        return null;\n    }\n\n```\n\n### Code description\n```Java\n/**\n * Generates an expanded URL by appending query parameters to the base URL.\n * The query parameters can be derived from a Map or from the fields of an object.\n * If the parameter object is a Map, each key-value pair is treated as a query parameter.\n * If the parameter object is not a Map, the method retrieves the values of all fields in the object and treats them as query parameters.\n * The method uses the UriComponentsBuilder.fromHttpUrl method from the org.springframework.web.util package to build the URL.\n *\n * @param url The base URL to which the query parameters will be appended.\n * @param paramObj The object containing the query parameters. This can be a Map or any other object with fields representing query parameters.\n * @return The expanded URL with the query parameters appended.\n */\n```\n\n### Code snippet:\n```Java\n    public static String getExpandUrl(String url, Object paramObj) {\n        UriComponentsBuilder builder = UriComponentsBuilder.fromHttpUrl(url);\n        if (paramObj instanceof Map) {\n            for (Map.Entry<String, Object> param : ((Map<String, Object>) paramObj).entrySet()) {\n                if (null != param.getValue()) {\n                    builder.queryParam(param.getKey(), param.getValue());\n                }\n            }\n        } else {\n            for (Field param : paramObj.getClass().getDeclaredFields()) {\n                param.setAccessible(true);\n\n                try {\n                    Object value = param.get(paramObj);\n```\n\n### Requirements:\n1. Complete only the missing part of the method.\n2. Only answer the code of the completed part. Do not include any of the provided code snippet.\n3. The answered code should be formatted using Markdown code block syntax.\n4. Do not include any additional text.\n\n### Answer: (use the provided format with backticks)\n", "reference": "                    if (null != value) {\n                        if (value instanceof List) {\n                            builder.queryParam(param.getName(), (List) value);\n                        } else {\n                            builder.queryParam(param.getName(), value);\n                        }\n                    }\n                } catch (IllegalAccessException e) {\n                    log.warn(\"Field: {} value retrieval failed, exception: {}\", param.getName(), e);\n                }\n            }\n        }\n\n        return builder.build().encode().toUri().toString();\n    }", "completions": ["  \n```Java\nif (null!= value) {\n    builder.queryParam(param.getName(), value);\n}\n```"]}
{"id": 12, "name": "isJsonMime", "prompt": "### Role:\nYou are an experienced Java programmer assistant responsible for code completion tasks. Please intelligently complete the missing parts of the provided code snippet.\n\n### Task:\nBased on the provided code snippet, complete the missing method implementation. Ensure the code is logically correct, syntactically accurate, and follows best coding practices.\n\n### Left context\n```Java\npackage org.openapitools.client;\n\nimport com.fasterxml.jackson.databind.DeserializationFeature;\nimport com.fasterxml.jackson.databind.ObjectMapper;\nimport com.fasterxml.jackson.datatype.jsr310.JavaTimeModule;\nimport org.openapitools.jackson.nullable.JsonNullableModule;\nimport org.springframework.core.ParameterizedTypeReference;\nimport org.springframework.http.HttpHeaders;\nimport org.springframework.http.HttpMethod;\nimport org.springframework.http.HttpRequest;\nimport org.springframework.http.HttpStatus;\nimport org.springframework.http.InvalidMediaTypeException;\nimport org.springframework.http.MediaType;\nimport org.springframework.http.RequestEntity;\nimport org.springframework.http.RequestEntity.BodyBuilder;\nimport org.springframework.http.ResponseEntity;\nimport org.springframework.http.client.BufferingClientHttpRequestFactory;\nimport org.springframework.http.client.ClientHttpRequestExecution;\nimport org.springframework.http.client.ClientHttpRequestInterceptor;\nimport org.springframework.http.client.ClientHttpResponse;\nimport org.springframework.http.codec.json.Jackson2JsonDecoder;\nimport org.springframework.http.codec.json.Jackson2JsonEncoder;\nimport org.springframework.util.CollectionUtils;\nimport org.springframework.util.LinkedMultiValueMap;\nimport org.springframework.util.MultiValueMap;\nimport org.springframework.util.StringUtils;\nimport org.springframework.http.client.reactive.ClientHttpRequest;\nimport org.springframework.web.client.RestClientException;\nimport org.springframework.web.util.UriComponentsBuilder;\nimport org.springframework.web.reactive.function.client.WebClient;\nimport org.springframework.web.reactive.function.client.WebClient.ResponseSpec;\nimport org.springframework.web.reactive.function.client.ClientResponse;\nimport org.springframework.web.reactive.function.BodyInserter;\nimport org.springframework.web.reactive.function.BodyInserters;\nimport org.springframework.web.reactive.function.client.ExchangeStrategies;\nimport reactor.core.publisher.Mono;\nimport reactor.core.publisher.Flux;\nimport java.util.Optional;\n\nimport java.io.BufferedReader;\nimport java.io.IOException;\nimport java.io.InputStream;\nimport java.io.InputStreamReader;\nimport java.io.UnsupportedEncodingException;\nimport java.net.URLEncoder;\nimport java.nio.charset.StandardCharsets;\nimport java.text.DateFormat;\nimport java.text.ParseException;\nimport java.util.Arrays;\nimport java.util.ArrayList;\nimport java.util.Collection;\nimport java.util.Collections;\nimport java.util.Date;\nimport java.util.HashMap;\nimport java.util.Iterator;\nimport java.util.List;\nimport java.util.Map;\nimport java.util.Map.Entry;\nimport java.util.TimeZone;\n\nimport javax.annotation.Nullable;\n\nimport java.time.OffsetDateTime;\n\nimport org.openapitools.client.auth.Authentication;\nimport org.openapitools.client.auth.HttpBasicAuth;\nimport org.openapitools.client.auth.HttpBearerAuth;\nimport org.openapitools.client.auth.ApiKeyAuth;\n\n@javax.annotation.Generated(value = \"org.openapitools.codegen.languages.JavaClientCodegen\", comments = \"Generator version: 7.5.0-SNAPSHOT\")\npublic class ApiClient extends JavaTimeFormatter {\n    public enum CollectionFormat {\n        CSV(\",\"), TSV(\"\\t\"), SSV(\" \"), PIPES(\"|\"), MULTI(null);\n\n        private final String separator;\n        private CollectionFormat(String separator) {\n            this.separator = separator;\n        }\n\n        private String collectionToString(Collection<?> collection) {\n            return StringUtils.collectionToDelimitedString(collection, separator);\n        }\n    }\n\n    private static final String URI_TEMPLATE_ATTRIBUTE = WebClient.class.getName() + \".uriTemplate\";\n\n    private HttpHeaders defaultHeaders = new HttpHeaders();\n    private MultiValueMap<String, String> defaultCookies = new LinkedMultiValueMap<String, String>();\n\n    private String basePath = \"http://localhost\";\n\n    private final WebClient webClient;\n    private final DateFormat dateFormat;\n    private final ObjectMapper objectMapper;\n\n    private Map<String, Authentication> authentications;\n\n\n    public ApiClient() {\n        this.dateFormat = createDefaultDateFormat();\n        this.objectMapper = createDefaultObjectMapper(this.dateFormat);\n        this.webClient = buildWebClient(this.objectMapper);\n        this.init();\n    }\n\n    public ApiClient(WebClient webClient) {\n        this(Optional.ofNullable(webClient).orElseGet(() -> buildWebClient()), createDefaultDateFormat());\n    }\n\n    public ApiClient(ObjectMapper mapper, DateFormat format) {\n        this(buildWebClient(mapper.copy()), format);\n    }\n\n    public ApiClient(WebClient webClient, ObjectMapper mapper, DateFormat format) {\n        this(Optional.ofNullable(webClient).orElseGet(() -> buildWebClient(mapper.copy())), format);\n    }\n\n    private ApiClient(WebClient webClient, DateFormat format) {\n        this.webClient = webClient;\n        this.dateFormat = format;\n        this.objectMapper = createDefaultObjectMapper(format);\n        this.init();\n    }\n\n    public static DateFormat createDefaultDateFormat() {\n        DateFormat dateFormat = new RFC3339DateFormat();\n        dateFormat.setTimeZone(TimeZone.getTimeZone(\"UTC\"));\n        return dateFormat;\n    }\n\n    public static ObjectMapper createDefaultObjectMapper(@Nullable DateFormat dateFormat) {\n        if (null == dateFormat) {\n            dateFormat = createDefaultDateFormat();\n        }\n        ObjectMapper mapper = new ObjectMapper();\n        mapper.setDateFormat(dateFormat);\n        mapper.registerModule(new JavaTimeModule());\n        mapper.configure(DeserializationFeature.FAIL_ON_UNKNOWN_PROPERTIES, false);\n        JsonNullableModule jnm = new JsonNullableModule();\n        mapper.registerModule(jnm);\n        return mapper;\n    }\n\n    protected void init() {\n        // Setup authentications (key: authentication name, value: authentication).\n        authentications = new HashMap<String, Authentication>();\n        // Prevent the authentications from being modified.\n        authentications = Collections.unmodifiableMap(authentications);\n    }\n\n    /**\n    * Build the WebClientBuilder used to make WebClient.\n    * @param mapper ObjectMapper used for serialize/deserialize\n    * @return WebClient\n    */\n    public static WebClient.Builder buildWebClientBuilder(ObjectMapper mapper) {\n        ExchangeStrategies strategies = ExchangeStrategies\n            .builder()\n            .codecs(clientDefaultCodecsConfigurer -> {\n                clientDefaultCodecsConfigurer.defaultCodecs().jackson2JsonEncoder(new Jackson2JsonEncoder(mapper, MediaType.APPLICATION_JSON));\n                clientDefaultCodecsConfigurer.defaultCodecs().jackson2JsonDecoder(new Jackson2JsonDecoder(mapper, MediaType.APPLICATION_JSON));\n            }).build();\n        WebClient.Builder webClientBuilder = WebClient.builder().exchangeStrategies(strategies);\n        return webClientBuilder;\n    }\n\n    /**\n     * Build the WebClientBuilder used to make WebClient.\n     * @return WebClient\n     */\n    public static WebClient.Builder buildWebClientBuilder() {\n        return buildWebClientBuilder(createDefaultObjectMapper(null));\n    }\n\n    /**\n     * Build the WebClient used to make HTTP requests.\n     * @param mapper ObjectMapper used for serialize/deserialize\n     * @return WebClient\n     */\n    public static WebClient buildWebClient(ObjectMapper mapper) {\n        return buildWebClientBuilder(mapper).build();\n    }\n\n    /**\n     * Build the WebClient used to make HTTP requests.\n     * @return WebClient\n     */\n    public static WebClient buildWebClient() {\n        return buildWebClientBuilder(createDefaultObjectMapper(null)).build();\n    }\n\n    /**\n     * Get the current base path\n     * @return String the base path\n     */\n    public String getBasePath() {\n        return basePath;\n    }\n\n    /**\n     * Set the base path, which should include the host\n     * @param basePath the base path\n     * @return ApiClient this client\n     */\n    public ApiClient setBasePath(String basePath) {\n        this.basePath = basePath;\n        return this;\n    }\n\n    /**\n     * Get authentications (key: authentication name, value: authentication).\n     * @return Map the currently configured authentication types\n     */\n    public Map<String, Authentication> getAuthentications() {\n        return authentications;\n    }\n\n    /**\n     * Get authentication for the given name.\n     *\n     * @param authName The authentication name\n     * @return The authentication, null if not found\n     */\n    public Authentication getAuthentication(String authName) {\n        return authentications.get(authName);\n    }\n\n    /**\n     * Helper method to set access token for the first Bearer authentication.\n     * @param bearerToken Bearer token\n     */\n    public void setBearerToken(String bearerToken) {\n        for (Authentication auth : authentications.values()) {\n            if (auth instanceof HttpBearerAuth) {\n                ((HttpBearerAuth) auth).setBearerToken(bearerToken);\n                return;\n            }\n        }\n        throw new RuntimeException(\"No Bearer authentication configured!\");\n    }\n\n    /**\n     * Helper method to set username for the first HTTP basic authentication.\n     * @param username the username\n     */\n    public void setUsername(String username) {\n        for (Authentication auth : authentications.values()) {\n            if (auth instanceof HttpBasicAuth) {\n                ((HttpBasicAuth) auth).setUsername(username);\n                return;\n            }\n        }\n        throw new RuntimeException(\"No HTTP basic authentication configured!\");\n    }\n\n    /**\n     * Helper method to set password for the first HTTP basic authentication.\n     * @param password the password\n     */\n    public void setPassword(String password) {\n        for (Authentication auth : authentications.values()) {\n            if (auth instanceof HttpBasicAuth) {\n                ((HttpBasicAuth) auth).setPassword(password);\n                return;\n            }\n        }\n        throw new RuntimeException(\"No HTTP basic authentication configured!\");\n    }\n\n    /**\n     * Helper method to set API key value for the first API key authentication.\n     * @param apiKey the API key\n     */\n    public void setApiKey(String apiKey) {\n        for (Authentication auth : authentications.values()) {\n            if (auth instanceof ApiKeyAuth) {\n                ((ApiKeyAuth) auth).setApiKey(apiKey);\n                return;\n            }\n        }\n        throw new RuntimeException(\"No API key authentication configured!\");\n    }\n\n    /**\n     * Helper method to set API key prefix for the first API key authentication.\n     * @param apiKeyPrefix the API key prefix\n     */\n    public void setApiKeyPrefix(String apiKeyPrefix) {\n        for (Authentication auth : authentications.values()) {\n            if (auth instanceof ApiKeyAuth) {\n                ((ApiKeyAuth) auth).setApiKeyPrefix(apiKeyPrefix);\n                return;\n            }\n        }\n        throw new RuntimeException(\"No API key authentication configured!\");\n    }\n\n    /**\n     * Set the User-Agent header's value (by adding to the default header map).\n     * @param userAgent the user agent string\n     * @return ApiClient this client\n     */\n    public ApiClient setUserAgent(String userAgent) {\n        addDefaultHeader(\"User-Agent\", userAgent);\n        return this;\n    }\n\n    /**\n     * Add a default header.\n     *\n     * @param name The header's name\n     * @param value The header's value\n     * @return ApiClient this client\n     */\n    public ApiClient addDefaultHeader(String name, String value) {\n        if (defaultHeaders.containsKey(name)) {\n            defaultHeaders.remove(name);\n        }\n        defaultHeaders.add(name, value);\n        return this;\n    }\n\n    /**\n     * Add a default cookie.\n     *\n     * @param name The cookie's name\n     * @param value The cookie's value\n     * @return ApiClient this client\n     */\n    public ApiClient addDefaultCookie(String name, String value) {\n        if (defaultCookies.containsKey(name)) {\n            defaultCookies.remove(name);\n        }\n        defaultCookies.add(name, value);\n        return this;\n    }\n\n    /**\n     * Get the date format used to parse/format date parameters.\n     * @return DateFormat format\n     */\n    public DateFormat getDateFormat() {\n        return dateFormat;\n    }\n\n    /**\n     * Parse the given string into Date object.\n     */\n    public Date parseDate(String str) {\n        try {\n            return dateFormat.parse(str);\n        } catch (ParseException e) {\n            throw new RuntimeException(e);\n        }\n    }\n\n    /**\n     * Format the given Date object into string.\n     */\n    public String formatDate(Date date) {\n        return dateFormat.format(date);\n    }\n\n    /**\n     * Get the ObjectMapper used to make HTTP requests.\n     * @return ObjectMapper objectMapper\n     */\n    public ObjectMapper getObjectMapper() {\n        return objectMapper;\n    }\n\n    /**\n     * Get the WebClient used to make HTTP requests.\n     * @return WebClient webClient\n     */\n    public WebClient getWebClient() {\n        return webClient;\n    }\n\n    /**\n     * Format the given parameter object into string.\n     * @param param the object to convert\n     * @return String the parameter represented as a String\n     */\n    public String parameterToString(Object param) {\n        if (param == null) {\n            return \"\";\n        } else if (param instanceof Date) {\n            return formatDate( (Date) param);\n        } else if (param instanceof OffsetDateTime) {\n            return formatOffsetDateTime((OffsetDateTime) param);\n        } else if (param instanceof Collection) {\n            StringBuilder b = new StringBuilder();\n            for(Object o : (Collection<?>) param) {\n                if(b.length() > 0) {\n                    b.append(\",\");\n                }\n                b.append(String.valueOf(o));\n            }\n            return b.toString();\n        } else {\n            return String.valueOf(param);\n        }\n    }\n\n    /**\n     * Converts a parameter to a {@link MultiValueMap} for use in REST requests\n     * @param collectionFormat The format to convert to\n     * @param name The name of the parameter\n     * @param value The parameter's value\n     * @return a Map containing the String value(s) of the input parameter\n     */\n    public MultiValueMap<String, String> parameterToMultiValueMap(CollectionFormat collectionFormat, String name, Object value) {\n        final MultiValueMap<String, String> params = new LinkedMultiValueMap<String, String>();\n\n        if (name == null || name.isEmpty() || value == null) {\n            return params;\n        }\n\n        if(collectionFormat == null) {\n            collectionFormat = CollectionFormat.CSV;\n        }\n\n        if (value instanceof Map) {\n            @SuppressWarnings(\"unchecked\")\n            final Map<String, Object> valuesMap = (Map<String, Object>) value;\n            for (final Entry<String, Object> entry : valuesMap.entrySet()) {\n                params.add(entry.getKey(), parameterToString(entry.getValue()));\n            }\n            return params;\n        }\n\n        Collection<?> valueCollection = null;\n        if (value instanceof Collection) {\n            valueCollection = (Collection<?>) value;\n        } else {\n            params.add(name, parameterToString(value));\n            return params;\n        }\n\n        if (valueCollection.isEmpty()){\n            return params;\n        }\n\n        if (collectionFormat.equals(CollectionFormat.MULTI)) {\n            for (Object item : valueCollection) {\n                params.add(name, parameterToString(item));\n            }\n            return params;\n        }\n\n        List<String> values = new ArrayList<String>();\n        for(Object o : valueCollection) {\n            values.add(parameterToString(o));\n        }\n        params.add(name, collectionFormat.collectionToString(values));\n\n        return params;\n    }\n\n    /**\n    * Check if the given {@code String} is a JSON MIME.\n    * @param mediaType the input MediaType\n    * @return boolean true if the MediaType represents JSON, false otherwise\n    */\n```\n\n### Code description\n```Java\n/**\n * Checks if the provided media type is a JSON mime type.\n * The method checks if the media type is \"*/*\", which is considered as a default JSON mime type.\n * If it's not, it tries to parse the media type using the `parseMediaType` method from the `org.springframework.http.MediaType` class.\n * If the parsing is successful and the media type is a JSON mime type, it returns true.\n * If the parsing fails (e.g., the media type is invalid), it catches the `InvalidMediaTypeException` and returns false.\n *\n * @param mediaType the media type to check\n * @return true if the media type is a JSON mime type, false otherwise\n */\n```\n\n### Code snippet:\n```Java\n    public boolean isJsonMime(String mediaType) {\n        // \"* / *\" is default to JSON\n        if (\"*/*\".equals(mediaType)) {\n            return true;\n        }\n\n```\n\n### Requirements:\n1. Complete only the missing part of the method.\n2. Only answer the code of the completed part. Do not include any of the provided code snippet.\n3. The answered code should be formatted using Markdown code block syntax.\n4. Do not include any additional text.\n\n### Answer: (use the provided format with backticks)\n", "reference": "        try {\n            return isJsonMime(MediaType.parseMediaType(mediaType));\n        } catch (InvalidMediaTypeException e) {\n        }\n        return false;\n    }", "completions": ["  \n```Java\n        try {\n            MediaType type = MediaType.parseMediaType(mediaType);\n            return type.isCompatibleWith(MediaType.APPLICATION_JSON);\n        } catch (InvalidMediaTypeException ex) {\n            return false;\n        }\n    }\n```"]}
{"id": 13, "name": "create", "prompt": "### Role:\nYou are an experienced Java programmer assistant responsible for code completion tasks. Please intelligently complete the missing parts of the provided code snippet.\n\n### Task:\nBased on the provided code snippet, complete the missing method implementation. Ensure the code is logically correct, syntactically accurate, and follows best coding practices.\n\n### Left context\n```Java\n/*\n * Copyright 2023-2024 the original author or authors.\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *      https://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\npackage org.springframework.modulith.events.neo4j;\n\nimport java.time.Instant;\nimport java.time.ZoneOffset;\nimport java.util.ArrayList;\nimport java.util.List;\nimport java.util.Map;\nimport java.util.Objects;\nimport java.util.Optional;\nimport java.util.UUID;\n\nimport org.neo4j.cypherdsl.core.Cypher;\nimport org.neo4j.cypherdsl.core.Node;\nimport org.neo4j.cypherdsl.core.ResultStatement;\nimport org.neo4j.cypherdsl.core.Statement;\nimport org.neo4j.cypherdsl.core.renderer.Configuration;\nimport org.neo4j.cypherdsl.core.renderer.Renderer;\nimport org.neo4j.driver.Values;\nimport org.neo4j.driver.types.TypeSystem;\nimport org.springframework.data.neo4j.core.Neo4jClient;\nimport org.springframework.lang.Nullable;\nimport org.springframework.modulith.events.core.EventPublicationRepository;\nimport org.springframework.modulith.events.core.EventSerializer;\nimport org.springframework.modulith.events.core.PublicationTargetIdentifier;\nimport org.springframework.modulith.events.core.TargetEventPublication;\nimport org.springframework.transaction.annotation.Transactional;\nimport org.springframework.util.Assert;\nimport org.springframework.util.DigestUtils;\n\n/**\n * A {@link Neo4jClient} based implementation of {@link EventPublicationRepository}.\n *\n * @author Gerrit Meier\n * @author Oliver Drotbohm\n * @since 1.1\n */\n@Transactional\nclass Neo4jEventPublicationRepository implements EventPublicationRepository {\n\n\tprivate static final String ID = \"identifier\";\n\tprivate static final String EVENT_SERIALIZED = \"eventSerialized\";\n\tprivate static final String EVENT_HASH = \"eventHash\";\n\tprivate static final String EVENT_TYPE = \"eventType\";\n\tprivate static final String LISTENER_ID = \"listenerId\";\n\tprivate static final String PUBLICATION_DATE = \"publicationDate\";\n\tprivate static final String COMPLETION_DATE = \"completionDate\";\n\n\tprivate static final Node EVENT_PUBLICATION_NODE = Cypher.node(\"Neo4jEventPublication\")\n\t\t\t.named(\"neo4jEventPublication\");\n\n\tprivate static final Statement INCOMPLETE_BY_EVENT_AND_TARGET_IDENTIFIER_STATEMENT = Cypher\n\t\t\t.match(EVENT_PUBLICATION_NODE)\n\t\t\t.where(EVENT_PUBLICATION_NODE.property(EVENT_HASH).eq(Cypher.parameter(EVENT_HASH)))\n\t\t\t.and(EVENT_PUBLICATION_NODE.property(LISTENER_ID).eq(Cypher.parameter(LISTENER_ID)))\n\t\t\t.and(EVENT_PUBLICATION_NODE.property(COMPLETION_DATE).isNull())\n\t\t\t.returning(EVENT_PUBLICATION_NODE)\n\t\t\t.build();\n\n\tprivate static final Statement DELETE_BY_ID_STATEMENT = Cypher.match(EVENT_PUBLICATION_NODE)\n\t\t\t.where(EVENT_PUBLICATION_NODE.property(ID).in(Cypher.parameter(ID)))\n\t\t\t.delete(EVENT_PUBLICATION_NODE)\n\t\t\t.build();\n\n\tprivate static final Statement DELETE_COMPLETED_STATEMENT = Cypher.match(EVENT_PUBLICATION_NODE)\n\t\t\t.where(EVENT_PUBLICATION_NODE.property(COMPLETION_DATE).isNotNull())\n\t\t\t.delete(EVENT_PUBLICATION_NODE)\n\t\t\t.build();\n\n\tprivate static final Statement DELETE_COMPLETED_BEFORE_STATEMENT = Cypher.match(EVENT_PUBLICATION_NODE)\n\t\t\t.where(EVENT_PUBLICATION_NODE.property(PUBLICATION_DATE).lt(Cypher.parameter(PUBLICATION_DATE)))\n\t\t\t.and(EVENT_PUBLICATION_NODE.property(COMPLETION_DATE).isNotNull())\n\t\t\t.delete(EVENT_PUBLICATION_NODE)\n\t\t\t.build();\n\n\tprivate static final Statement INCOMPLETE_PUBLISHED_BEFORE_STATEMENT = Cypher\n\t\t\t.match(EVENT_PUBLICATION_NODE)\n\t\t\t.where(EVENT_PUBLICATION_NODE.property(PUBLICATION_DATE).lt(Cypher.parameter(PUBLICATION_DATE)))\n\t\t\t.and(EVENT_PUBLICATION_NODE.property(COMPLETION_DATE).isNull())\n\t\t\t.returning(EVENT_PUBLICATION_NODE)\n\t\t\t.orderBy(EVENT_PUBLICATION_NODE.property(PUBLICATION_DATE))\n\t\t\t.build();\n\n\tprivate static final Statement CREATE_STATEMENT = Cypher.create(EVENT_PUBLICATION_NODE)\n\t\t\t.set(EVENT_PUBLICATION_NODE.property(ID).to(Cypher.parameter(ID)))\n\t\t\t.set(EVENT_PUBLICATION_NODE.property(EVENT_SERIALIZED).to(Cypher.parameter(EVENT_SERIALIZED)))\n\t\t\t.set(EVENT_PUBLICATION_NODE.property(EVENT_HASH).to(Cypher.parameter(EVENT_HASH)))\n\t\t\t.set(EVENT_PUBLICATION_NODE.property(EVENT_TYPE).to(Cypher.parameter(EVENT_TYPE)))\n\t\t\t.set(EVENT_PUBLICATION_NODE.property(LISTENER_ID).to(Cypher.parameter(LISTENER_ID)))\n\t\t\t.set(EVENT_PUBLICATION_NODE.property(PUBLICATION_DATE).to(Cypher.parameter(PUBLICATION_DATE)))\n\t\t\t.build();\n\n\tprivate static final Statement COMPLETE_STATEMENT = Cypher.match(EVENT_PUBLICATION_NODE)\n\t\t\t.where(EVENT_PUBLICATION_NODE.property(EVENT_HASH).eq(Cypher.parameter(EVENT_HASH)))\n\t\t\t.and(EVENT_PUBLICATION_NODE.property(LISTENER_ID).eq(Cypher.parameter(LISTENER_ID)))\n\t\t\t.set(EVENT_PUBLICATION_NODE.property(COMPLETION_DATE).to(Cypher.parameter(COMPLETION_DATE)))\n\t\t\t.build();\n\n\tprivate static final ResultStatement INCOMPLETE_STATEMENT = Cypher.match(EVENT_PUBLICATION_NODE)\n\t\t\t.where(EVENT_PUBLICATION_NODE.property(COMPLETION_DATE).isNull())\n\t\t\t.returning(EVENT_PUBLICATION_NODE)\n\t\t\t.orderBy(EVENT_PUBLICATION_NODE.property(PUBLICATION_DATE))\n\t\t\t.build();\n\n\tprivate static final ResultStatement ALL_COMPLETED_STATEMENT = Cypher.match(EVENT_PUBLICATION_NODE)\n\t\t\t.where(EVENT_PUBLICATION_NODE.property(COMPLETION_DATE).isNotNull())\n\t\t\t.returning(EVENT_PUBLICATION_NODE)\n\t\t\t.orderBy(EVENT_PUBLICATION_NODE.property(PUBLICATION_DATE))\n\t\t\t.build();\n\n\tprivate final Neo4jClient neo4jClient;\n\tprivate final Renderer renderer;\n\tprivate final EventSerializer eventSerializer;\n\n\tNeo4jEventPublicationRepository(Neo4jClient neo4jClient, Configuration cypherDslConfiguration,\n\t\t\tEventSerializer eventSerializer) {\n\n\t\tAssert.notNull(neo4jClient, \"Neo4jClient must not be null!\");\n\t\tAssert.notNull(cypherDslConfiguration, \"CypherDSL configuration must not be null!\");\n\t\tAssert.notNull(eventSerializer, \"EventSerializer must not be null!\");\n\n\t\tthis.neo4jClient = neo4jClient;\n\t\tthis.renderer = Renderer.getRenderer(cypherDslConfiguration);\n\t\tthis.eventSerializer = eventSerializer;\n\t}\n\n\t/*\n\t * (non-Javadoc)\n\t * @see org.springframework.modulith.events.EventPublicationRepository#create(org.springframework.modulith.events.EventPublication)\n\t */\n```\n\n### Code description\n```Java\n/**\n * Description: This method creates a new TargetEventPublication in the database.\n * It serializes the event, computes its hash, and stores all the necessary information in the database.\n * \n * @param publication The TargetEventPublication to be created.\n * @return The created TargetEventPublication.\n * @throws TransactionSystemException if a transaction error occurs.\n */\n```\n\n### Code snippet:\n```Java\n\t@Override\n\t@Transactional\n\tpublic TargetEventPublication create(TargetEventPublication publication) {\n\n\t\tvar identifier = publication.getIdentifier();\n\t\tvar publicationDate = publication.getPublicationDate();\n\t\tvar listenerId = publication.getTargetIdentifier().getValue();\n\t\tvar event = publication.getEvent();\n\t\tvar eventType = event.getClass().getName();\n\n\t\tvar eventSerialized = eventSerializer.serialize(event).toString();\n\t\tvar eventHash = DigestUtils.md5DigestAsHex(eventSerialized.getBytes());\n```\n\n### Requirements:\n1. Complete only the missing part of the method.\n2. Only answer the code of the completed part. Do not include any of the provided code snippet.\n3. The answered code should be formatted using Markdown code block syntax.\n4. Do not include any additional text.\n\n### Answer: (use the provided format with backticks)\n", "reference": "\n\t\tneo4jClient.query(renderer.render(CREATE_STATEMENT))\n\t\t\t\t.bindAll(Map.of(\n\t\t\t\t\t\tID, Values.value(identifier.toString()),\n\t\t\t\t\t\tEVENT_SERIALIZED, eventSerialized,\n\t\t\t\t\t\tEVENT_HASH, eventHash,\n\t\t\t\t\t\tEVENT_TYPE, eventType,\n\t\t\t\t\t\tLISTENER_ID, listenerId,\n\t\t\t\t\t\tPUBLICATION_DATE, Values.value(publicationDate.atOffset(ZoneOffset.UTC))))\n\t\t\t\t.run();\n\n\t\treturn publication;\n\t}", "completions": ["  \n```Java\n\t\tvar id = UUID.randomUUID().toString();\n\n\t\tvar parameters = Values.parameters(\"id\", id, \"eventSerialized\", eventSerialized, \"eventHash\", eventHash,\n\t\t\t\t\"eventType\", eventType, \"listenerId\", listenerId, \"publicationDate\", publicationDate);\n\n\t\tneo4jClient.query(CREATE_STATEMENT).bind(parameters).run();\n\n\t\treturn TargetEventPublication.of(id, publicationDate, event, PublicationTargetIdentifier.of(listenerId));\n\t}\n```"]}
{"id": 14, "name": "running", "prompt": "### Role:\nYou are an experienced Java programmer assistant responsible for code completion tasks. Please intelligently complete the missing parts of the provided code snippet.\n\n### Task:\nBased on the provided code snippet, complete the missing method implementation. Ensure the code is logically correct, syntactically accurate, and follows best coding practices.\n\n### Left context\n```Java\npackage com.codingapi.springboot.fast.script;\n\nimport com.codingapi.springboot.fast.jdbc.JdbcQuery;\nimport com.codingapi.springboot.fast.jdbc.JdbcQueryContext;\nimport com.codingapi.springboot.fast.jpa.JPAQuery;\nimport com.codingapi.springboot.fast.jpa.JpaQueryContext;\nimport groovy.lang.Binding;\nimport groovy.lang.GroovyShell;\nimport groovy.lang.Script;\nimport org.springframework.web.context.request.RequestContextHolder;\nimport org.springframework.web.context.request.ServletRequestAttributes;\n\n\npublic class ScriptRuntime {\n\n```\n\n### Code description\n```Java\n/**\n * Description: This method runs a Groovy script with access to the current request, JDBC query, and JPA query objects.\n * \n * @param script The Groovy script to be executed.\n * @return The result of the script execution.\n */\n```\n\n### Code snippet:\n```Java\n    static Object running(String script) {\n        Binding binding = new Binding();\n        ServletRequestAttributes attributes = (ServletRequestAttributes) RequestContextHolder.currentRequestAttributes();\n        ScriptRequest request = new ScriptRequest(attributes.getRequest());\n        JdbcQuery jdbcQuery = JdbcQueryContext.getInstance().getJdbcQuery();\n        JPAQuery jpaQuery = JpaQueryContext.getInstance().getJPAQuery();\n\n```\n\n### Requirements:\n1. Complete only the missing part of the method.\n2. Only answer the code of the completed part. Do not include any of the provided code snippet.\n3. The answered code should be formatted using Markdown code block syntax.\n4. Do not include any additional text.\n\n### Answer: (use the provided format with backticks)\n", "reference": "        binding.setVariable(\"$request\", request);\n        binding.setVariable(\"$jpa\", jpaQuery);\n        binding.setVariable(\"$jdbc\", jdbcQuery);\n\n        GroovyShell groovyShell = new GroovyShell(binding);\n        Script userScript = groovyShell.parse(script);\n        return userScript.run();\n    }", "completions": ["  ```\n        binding.setVariable(\"request\", request);\n        binding.setVariable(\"jdbcQuery\", jdbcQuery);\n        binding.setVariable(\"jpaQuery\", jpaQuery);\n\n        GroovyShell shell = new GroovyShell(binding);\n        Object result = shell.evaluate(script);\n\n        return result;\n    }\n}\n```"]}
{"id": 15, "name": "upload", "prompt": "### Role:\nYou are an experienced Java programmer assistant responsible for code completion tasks. Please intelligently complete the missing parts of the provided code snippet.\n\n### Task:\nBased on the provided code snippet, complete the missing method implementation. Ensure the code is logically correct, syntactically accurate, and follows best coding practices.\n\n### Left context\n```Java\n/*\n * Copyright 2013-2022 the original author or authors.\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *      https://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\npackage io.awspring.cloud.s3;\n\nimport java.io.InputStream;\nimport java.io.OutputStream;\nimport java.net.URL;\nimport java.time.Duration;\nimport java.util.List;\nimport org.springframework.lang.Nullable;\nimport org.springframework.util.Assert;\nimport org.springframework.util.StreamUtils;\nimport software.amazon.awssdk.services.s3.S3Client;\nimport software.amazon.awssdk.services.s3.model.GetObjectRequest;\nimport software.amazon.awssdk.services.s3.model.ListObjectsV2Request;\nimport software.amazon.awssdk.services.s3.model.ListObjectsV2Response;\nimport software.amazon.awssdk.services.s3.model.NoSuchBucketException;\nimport software.amazon.awssdk.services.s3.model.NoSuchKeyException;\nimport software.amazon.awssdk.services.s3.model.PutObjectRequest;\nimport software.amazon.awssdk.services.s3.presigner.S3Presigner;\nimport software.amazon.awssdk.services.s3.presigner.model.GetObjectPresignRequest;\nimport software.amazon.awssdk.services.s3.presigner.model.PresignedGetObjectRequest;\nimport software.amazon.awssdk.services.s3.presigner.model.PresignedPutObjectRequest;\nimport software.amazon.awssdk.services.s3.presigner.model.PutObjectPresignRequest;\n\n/**\n * Higher level abstraction over {@link S3Client} providing methods for the most common use cases.\n *\n * @author Maciej Walkowiak\n * @author Ziemowit Stolarczyk\n * @since 3.0\n */\npublic class S3Template implements S3Operations {\n\n\tprivate final S3Client s3Client;\n\n\tprivate final S3OutputStreamProvider s3OutputStreamProvider;\n\n\tprivate final S3ObjectConverter s3ObjectConverter;\n\n\tprivate final S3Presigner s3Presigner;\n\n\tpublic S3Template(S3Client s3Client, S3OutputStreamProvider s3OutputStreamProvider,\n\t\t\tS3ObjectConverter s3ObjectConverter, S3Presigner s3Presigner) {\n\t\tAssert.notNull(s3Client, \"s3Client is required\");\n\t\tAssert.notNull(s3OutputStreamProvider, \"s3OutputStreamProvider is required\");\n\t\tAssert.notNull(s3ObjectConverter, \"s3ObjectConverter is required\");\n\t\tAssert.notNull(s3Presigner, \"s3Presigner is required\");\n\t\tthis.s3Client = s3Client;\n\t\tthis.s3OutputStreamProvider = s3OutputStreamProvider;\n\t\tthis.s3ObjectConverter = s3ObjectConverter;\n\t\tthis.s3Presigner = s3Presigner;\n\t}\n\n\t@Override\n\tpublic String createBucket(String bucketName) {\n\t\tAssert.notNull(bucketName, \"bucketName is required\");\n\t\treturn s3Client.createBucket(request -> request.bucket(bucketName)).location();\n\t}\n\n\t@Override\n\tpublic void deleteBucket(String bucketName) {\n\t\tAssert.notNull(bucketName, \"bucketName is required\");\n\t\ts3Client.deleteBucket(request -> request.bucket(bucketName));\n\t}\n\n\t@Override\n\tpublic boolean bucketExists(String bucketName) {\n\t\tAssert.notNull(bucketName, \"bucketName is required\");\n\t\ttry {\n\t\t\ts3Client.headBucket(request -> request.bucket(bucketName));\n\t\t}\n\t\tcatch (NoSuchBucketException e) {\n\t\t\treturn false;\n\t\t}\n\t\treturn true;\n\t}\n\n\t@Override\n\tpublic void deleteObject(String bucketName, String key) {\n\t\tAssert.notNull(bucketName, \"bucketName is required\");\n\t\tAssert.notNull(key, \"key is required\");\n\t\ts3Client.deleteObject(request -> request.bucket(bucketName).key(key));\n\t}\n\n\t@Override\n\tpublic void deleteObject(String s3Url) {\n\t\tAssert.notNull(s3Url, \"s3Url is required\");\n\t\tLocation location = Location.of(s3Url);\n\t\tthis.deleteObject(location.getBucket(), location.getObject());\n\t}\n\n\t@Override\n\tpublic boolean objectExists(String bucketName, String key) {\n\t\tAssert.notNull(bucketName, \"bucketName is required\");\n\t\tAssert.notNull(key, \"key is required\");\n\t\ttry {\n\t\t\ts3Client.headObject(request -> request.bucket(bucketName).key(key));\n\t\t}\n\t\tcatch (NoSuchBucketException | NoSuchKeyException e) {\n\t\t\treturn false;\n\t\t}\n\t\treturn true;\n\t}\n\n\t@Override\n\tpublic List<S3Resource> listObjects(String bucketName, String prefix) {\n\t\tAssert.notNull(bucketName, \"bucketName is required\");\n\t\tAssert.notNull(prefix, \"prefix is required\");\n\n\t\tfinal ListObjectsV2Request request = ListObjectsV2Request.builder().bucket(bucketName).prefix(prefix).build();\n\t\tfinal ListObjectsV2Response response = s3Client.listObjectsV2(request);\n\n\t\treturn response.contents().stream()\n\t\t\t\t.map(s3Object -> new S3Resource(bucketName, s3Object.key(), s3Client, s3OutputStreamProvider)).toList();\n\t}\n\n\t@Override\n\tpublic S3Resource store(String bucketName, String key, Object object) {\n\t\tAssert.notNull(bucketName, \"bucketName is required\");\n\t\tAssert.notNull(key, \"key is required\");\n\t\tAssert.notNull(object, \"object is required\");\n\n\t\tPutObjectRequest.Builder requestBuilder = PutObjectRequest.builder().bucket(bucketName).key(key)\n\t\t\t\t.contentType(s3ObjectConverter.contentType());\n\t\ts3Client.putObject(requestBuilder.build(), s3ObjectConverter.write(object));\n\t\treturn new S3Resource(bucketName, key, s3Client, s3OutputStreamProvider);\n\t}\n\n\t@Override\n\tpublic <T> T read(String bucketName, String key, Class<T> clazz) {\n\t\tAssert.notNull(bucketName, \"bucketName is required\");\n\t\tAssert.notNull(key, \"key is required\");\n\t\tAssert.notNull(clazz, \"clazz is required\");\n\n\t\ttry (InputStream is = s3Client.getObject(r -> r.bucket(bucketName).key(key))) {\n\t\t\treturn s3ObjectConverter.read(is, clazz);\n\t\t}\n\t\tcatch (Exception e) {\n\t\t\tthrow new S3Exception(\n\t\t\t\t\tString.format(\"Failed to read object with a key '%s' from bucket '%s'\", key, bucketName), e);\n\t\t}\n\t}\n\n```\n\n### Code description\n```Java\n/**\n * Description: This method uploads an input stream to an S3 bucket with the provided bucket name and key.\n * It also accepts an optional ObjectMetadata object to set additional metadata for the uploaded object.\n * The method returns an S3Resource object representing the uploaded object.\n *\n * @param bucketName The name of the S3 bucket where the object will be uploaded.\n * @param key The key of the object in the S3 bucket.\n * @param inputStream The input stream of the object to be uploaded.\n * @param objectMetadata Optional metadata for the uploaded object.\n * @return The S3Resource object representing the uploaded object.\n * @throws IllegalArgumentException If any of the required parameters (bucketName, key, inputStream) is null.\n * @throws S3Exception If there is an error during the upload process.\n */\n```\n\n### Code snippet:\n```Java\n\t@Override\n\tpublic S3Resource upload(String bucketName, String key, InputStream inputStream,\n\t\t\t@Nullable ObjectMetadata objectMetadata) {\n\t\tAssert.notNull(bucketName, \"bucketName is required\");\n\t\tAssert.notNull(key, \"key is required\");\n\t\tAssert.notNull(inputStream, \"inputStream is required\");\n\n\t\tS3Resource s3Resource = new S3Resource(bucketName, key, s3Client, s3OutputStreamProvider);\n\t\tif (objectMetadata != null) {\n\t\t\ts3Resource.setObjectMetadata(objectMetadata);\n```\n\n### Requirements:\n1. Complete only the missing part of the method.\n2. Only answer the code of the completed part. Do not include any of the provided code snippet.\n3. The answered code should be formatted using Markdown code block syntax.\n4. Do not include any additional text.\n\n### Answer: (use the provided format with backticks)\n", "reference": "\t\t}\n\t\ttry (OutputStream os = s3Resource.getOutputStream()) {\n\t\t\tStreamUtils.copy(inputStream, os);\n\t\t\treturn s3Resource;\n\t\t}\n\t\tcatch (Exception e) {\n\t\t\tthrow new S3Exception(\n\t\t\t\t\tString.format(\"Failed to upload object with a key '%s' to bucket '%s'\", key, bucketName), e);\n\t\t}\n\t}", "completions": ["  \n```Java\n\t\t\ts3Resource.setObjectMetadata(objectMetadata);\n\t\t}\n\n\t\ttry (OutputStream os = s3Resource.getOutputStream()) {\n\t\t\tStreamUtils.copy(inputStream, os);\n\t\t}\n\t\tcatch (IOException e) {\n\t\t\tthrow new S3Exception(\n\t\t\t\t\tString.format(\"Failed to upload object with a key '%s' to bucket '%s'\", key, bucketName), e);\n\t\t}\n\n\t\treturn s3Resource;\n\t}\n```"]}
{"id": 16, "name": "selectHeaderAccept", "prompt": "### Role:\nYou are an experienced Java programmer assistant responsible for code completion tasks. Please intelligently complete the missing parts of the provided code snippet.\n\n### Task:\nBased on the provided code snippet, complete the missing method implementation. Ensure the code is logically correct, syntactically accurate, and follows best coding practices.\n\n### Left context\n```Java\npackage org.openapitools.client;\n\nimport com.fasterxml.jackson.databind.DeserializationFeature;\nimport com.fasterxml.jackson.databind.ObjectMapper;\nimport com.fasterxml.jackson.datatype.jsr310.JavaTimeModule;\nimport org.openapitools.jackson.nullable.JsonNullableModule;\nimport org.springframework.core.ParameterizedTypeReference;\nimport org.springframework.http.HttpHeaders;\nimport org.springframework.http.HttpMethod;\nimport org.springframework.http.HttpRequest;\nimport org.springframework.http.HttpStatus;\nimport org.springframework.http.InvalidMediaTypeException;\nimport org.springframework.http.MediaType;\nimport org.springframework.http.RequestEntity;\nimport org.springframework.http.RequestEntity.BodyBuilder;\nimport org.springframework.http.ResponseEntity;\nimport org.springframework.http.client.BufferingClientHttpRequestFactory;\nimport org.springframework.http.client.ClientHttpRequestExecution;\nimport org.springframework.http.client.ClientHttpRequestInterceptor;\nimport org.springframework.http.client.ClientHttpResponse;\nimport org.springframework.http.codec.json.Jackson2JsonDecoder;\nimport org.springframework.http.codec.json.Jackson2JsonEncoder;\nimport org.springframework.util.CollectionUtils;\nimport org.springframework.util.LinkedMultiValueMap;\nimport org.springframework.util.MultiValueMap;\nimport org.springframework.util.StringUtils;\nimport org.springframework.http.client.reactive.ClientHttpRequest;\nimport org.springframework.web.client.RestClientException;\nimport org.springframework.web.util.UriComponentsBuilder;\nimport org.springframework.web.reactive.function.client.WebClient;\nimport org.springframework.web.reactive.function.client.WebClient.ResponseSpec;\nimport org.springframework.web.reactive.function.client.ClientResponse;\nimport org.springframework.web.reactive.function.BodyInserter;\nimport org.springframework.web.reactive.function.BodyInserters;\nimport org.springframework.web.reactive.function.client.ExchangeStrategies;\nimport reactor.core.publisher.Mono;\nimport reactor.core.publisher.Flux;\nimport java.util.Optional;\n\nimport java.io.BufferedReader;\nimport java.io.IOException;\nimport java.io.InputStream;\nimport java.io.InputStreamReader;\nimport java.io.UnsupportedEncodingException;\nimport java.net.URLEncoder;\nimport java.nio.charset.StandardCharsets;\nimport java.text.DateFormat;\nimport java.text.ParseException;\nimport java.util.Arrays;\nimport java.util.ArrayList;\nimport java.util.Collection;\nimport java.util.Collections;\nimport java.util.Date;\nimport java.util.HashMap;\nimport java.util.Iterator;\nimport java.util.List;\nimport java.util.Map;\nimport java.util.Map.Entry;\nimport java.util.TimeZone;\n\nimport javax.annotation.Nullable;\n\nimport java.time.OffsetDateTime;\n\nimport org.openapitools.client.auth.Authentication;\nimport org.openapitools.client.auth.HttpBasicAuth;\nimport org.openapitools.client.auth.HttpBearerAuth;\nimport org.openapitools.client.auth.ApiKeyAuth;\n\n@javax.annotation.Generated(value = \"org.openapitools.codegen.languages.JavaClientCodegen\", comments = \"Generator version: 7.5.0-SNAPSHOT\")\npublic class ApiClient extends JavaTimeFormatter {\n    public enum CollectionFormat {\n        CSV(\",\"), TSV(\"\\t\"), SSV(\" \"), PIPES(\"|\"), MULTI(null);\n\n        private final String separator;\n        private CollectionFormat(String separator) {\n            this.separator = separator;\n        }\n\n        private String collectionToString(Collection<?> collection) {\n            return StringUtils.collectionToDelimitedString(collection, separator);\n        }\n    }\n\n    private static final String URI_TEMPLATE_ATTRIBUTE = WebClient.class.getName() + \".uriTemplate\";\n\n    private HttpHeaders defaultHeaders = new HttpHeaders();\n    private MultiValueMap<String, String> defaultCookies = new LinkedMultiValueMap<String, String>();\n\n    private String basePath = \"http://localhost\";\n\n    private final WebClient webClient;\n    private final DateFormat dateFormat;\n    private final ObjectMapper objectMapper;\n\n    private Map<String, Authentication> authentications;\n\n\n    public ApiClient() {\n        this.dateFormat = createDefaultDateFormat();\n        this.objectMapper = createDefaultObjectMapper(this.dateFormat);\n        this.webClient = buildWebClient(this.objectMapper);\n        this.init();\n    }\n\n    public ApiClient(WebClient webClient) {\n        this(Optional.ofNullable(webClient).orElseGet(() -> buildWebClient()), createDefaultDateFormat());\n    }\n\n    public ApiClient(ObjectMapper mapper, DateFormat format) {\n        this(buildWebClient(mapper.copy()), format);\n    }\n\n    public ApiClient(WebClient webClient, ObjectMapper mapper, DateFormat format) {\n        this(Optional.ofNullable(webClient).orElseGet(() -> buildWebClient(mapper.copy())), format);\n    }\n\n    private ApiClient(WebClient webClient, DateFormat format) {\n        this.webClient = webClient;\n        this.dateFormat = format;\n        this.objectMapper = createDefaultObjectMapper(format);\n        this.init();\n    }\n\n    public static DateFormat createDefaultDateFormat() {\n        DateFormat dateFormat = new RFC3339DateFormat();\n        dateFormat.setTimeZone(TimeZone.getTimeZone(\"UTC\"));\n        return dateFormat;\n    }\n\n    public static ObjectMapper createDefaultObjectMapper(@Nullable DateFormat dateFormat) {\n        if (null == dateFormat) {\n            dateFormat = createDefaultDateFormat();\n        }\n        ObjectMapper mapper = new ObjectMapper();\n        mapper.setDateFormat(dateFormat);\n        mapper.registerModule(new JavaTimeModule());\n        mapper.configure(DeserializationFeature.FAIL_ON_UNKNOWN_PROPERTIES, false);\n        JsonNullableModule jnm = new JsonNullableModule();\n        mapper.registerModule(jnm);\n        return mapper;\n    }\n\n    protected void init() {\n        // Setup authentications (key: authentication name, value: authentication).\n        authentications = new HashMap<String, Authentication>();\n        // Prevent the authentications from being modified.\n        authentications = Collections.unmodifiableMap(authentications);\n    }\n\n    /**\n    * Build the WebClientBuilder used to make WebClient.\n    * @param mapper ObjectMapper used for serialize/deserialize\n    * @return WebClient\n    */\n    public static WebClient.Builder buildWebClientBuilder(ObjectMapper mapper) {\n        ExchangeStrategies strategies = ExchangeStrategies\n            .builder()\n            .codecs(clientDefaultCodecsConfigurer -> {\n                clientDefaultCodecsConfigurer.defaultCodecs().jackson2JsonEncoder(new Jackson2JsonEncoder(mapper, MediaType.APPLICATION_JSON));\n                clientDefaultCodecsConfigurer.defaultCodecs().jackson2JsonDecoder(new Jackson2JsonDecoder(mapper, MediaType.APPLICATION_JSON));\n            }).build();\n        WebClient.Builder webClientBuilder = WebClient.builder().exchangeStrategies(strategies);\n        return webClientBuilder;\n    }\n\n    /**\n     * Build the WebClientBuilder used to make WebClient.\n     * @return WebClient\n     */\n    public static WebClient.Builder buildWebClientBuilder() {\n        return buildWebClientBuilder(createDefaultObjectMapper(null));\n    }\n\n    /**\n     * Build the WebClient used to make HTTP requests.\n     * @param mapper ObjectMapper used for serialize/deserialize\n     * @return WebClient\n     */\n    public static WebClient buildWebClient(ObjectMapper mapper) {\n        return buildWebClientBuilder(mapper).build();\n    }\n\n    /**\n     * Build the WebClient used to make HTTP requests.\n     * @return WebClient\n     */\n    public static WebClient buildWebClient() {\n        return buildWebClientBuilder(createDefaultObjectMapper(null)).build();\n    }\n\n    /**\n     * Get the current base path\n     * @return String the base path\n     */\n    public String getBasePath() {\n        return basePath;\n    }\n\n    /**\n     * Set the base path, which should include the host\n     * @param basePath the base path\n     * @return ApiClient this client\n     */\n    public ApiClient setBasePath(String basePath) {\n        this.basePath = basePath;\n        return this;\n    }\n\n    /**\n     * Get authentications (key: authentication name, value: authentication).\n     * @return Map the currently configured authentication types\n     */\n    public Map<String, Authentication> getAuthentications() {\n        return authentications;\n    }\n\n    /**\n     * Get authentication for the given name.\n     *\n     * @param authName The authentication name\n     * @return The authentication, null if not found\n     */\n    public Authentication getAuthentication(String authName) {\n        return authentications.get(authName);\n    }\n\n    /**\n     * Helper method to set access token for the first Bearer authentication.\n     * @param bearerToken Bearer token\n     */\n    public void setBearerToken(String bearerToken) {\n        for (Authentication auth : authentications.values()) {\n            if (auth instanceof HttpBearerAuth) {\n                ((HttpBearerAuth) auth).setBearerToken(bearerToken);\n                return;\n            }\n        }\n        throw new RuntimeException(\"No Bearer authentication configured!\");\n    }\n\n    /**\n     * Helper method to set username for the first HTTP basic authentication.\n     * @param username the username\n     */\n    public void setUsername(String username) {\n        for (Authentication auth : authentications.values()) {\n            if (auth instanceof HttpBasicAuth) {\n                ((HttpBasicAuth) auth).setUsername(username);\n                return;\n            }\n        }\n        throw new RuntimeException(\"No HTTP basic authentication configured!\");\n    }\n\n    /**\n     * Helper method to set password for the first HTTP basic authentication.\n     * @param password the password\n     */\n    public void setPassword(String password) {\n        for (Authentication auth : authentications.values()) {\n            if (auth instanceof HttpBasicAuth) {\n                ((HttpBasicAuth) auth).setPassword(password);\n                return;\n            }\n        }\n        throw new RuntimeException(\"No HTTP basic authentication configured!\");\n    }\n\n    /**\n     * Helper method to set API key value for the first API key authentication.\n     * @param apiKey the API key\n     */\n    public void setApiKey(String apiKey) {\n        for (Authentication auth : authentications.values()) {\n            if (auth instanceof ApiKeyAuth) {\n                ((ApiKeyAuth) auth).setApiKey(apiKey);\n                return;\n            }\n        }\n        throw new RuntimeException(\"No API key authentication configured!\");\n    }\n\n    /**\n     * Helper method to set API key prefix for the first API key authentication.\n     * @param apiKeyPrefix the API key prefix\n     */\n    public void setApiKeyPrefix(String apiKeyPrefix) {\n        for (Authentication auth : authentications.values()) {\n            if (auth instanceof ApiKeyAuth) {\n                ((ApiKeyAuth) auth).setApiKeyPrefix(apiKeyPrefix);\n                return;\n            }\n        }\n        throw new RuntimeException(\"No API key authentication configured!\");\n    }\n\n    /**\n     * Set the User-Agent header's value (by adding to the default header map).\n     * @param userAgent the user agent string\n     * @return ApiClient this client\n     */\n    public ApiClient setUserAgent(String userAgent) {\n        addDefaultHeader(\"User-Agent\", userAgent);\n        return this;\n    }\n\n    /**\n     * Add a default header.\n     *\n     * @param name The header's name\n     * @param value The header's value\n     * @return ApiClient this client\n     */\n    public ApiClient addDefaultHeader(String name, String value) {\n        if (defaultHeaders.containsKey(name)) {\n            defaultHeaders.remove(name);\n        }\n        defaultHeaders.add(name, value);\n        return this;\n    }\n\n    /**\n     * Add a default cookie.\n     *\n     * @param name The cookie's name\n     * @param value The cookie's value\n     * @return ApiClient this client\n     */\n    public ApiClient addDefaultCookie(String name, String value) {\n        if (defaultCookies.containsKey(name)) {\n            defaultCookies.remove(name);\n        }\n        defaultCookies.add(name, value);\n        return this;\n    }\n\n    /**\n     * Get the date format used to parse/format date parameters.\n     * @return DateFormat format\n     */\n    public DateFormat getDateFormat() {\n        return dateFormat;\n    }\n\n    /**\n     * Parse the given string into Date object.\n     */\n    public Date parseDate(String str) {\n        try {\n            return dateFormat.parse(str);\n        } catch (ParseException e) {\n            throw new RuntimeException(e);\n        }\n    }\n\n    /**\n     * Format the given Date object into string.\n     */\n    public String formatDate(Date date) {\n        return dateFormat.format(date);\n    }\n\n    /**\n     * Get the ObjectMapper used to make HTTP requests.\n     * @return ObjectMapper objectMapper\n     */\n    public ObjectMapper getObjectMapper() {\n        return objectMapper;\n    }\n\n    /**\n     * Get the WebClient used to make HTTP requests.\n     * @return WebClient webClient\n     */\n    public WebClient getWebClient() {\n        return webClient;\n    }\n\n    /**\n     * Format the given parameter object into string.\n     * @param param the object to convert\n     * @return String the parameter represented as a String\n     */\n    public String parameterToString(Object param) {\n        if (param == null) {\n            return \"\";\n        } else if (param instanceof Date) {\n            return formatDate( (Date) param);\n        } else if (param instanceof OffsetDateTime) {\n            return formatOffsetDateTime((OffsetDateTime) param);\n        } else if (param instanceof Collection) {\n            StringBuilder b = new StringBuilder();\n            for(Object o : (Collection<?>) param) {\n                if(b.length() > 0) {\n                    b.append(\",\");\n                }\n                b.append(String.valueOf(o));\n            }\n            return b.toString();\n        } else {\n            return String.valueOf(param);\n        }\n    }\n\n    /**\n     * Converts a parameter to a {@link MultiValueMap} for use in REST requests\n     * @param collectionFormat The format to convert to\n     * @param name The name of the parameter\n     * @param value The parameter's value\n     * @return a Map containing the String value(s) of the input parameter\n     */\n    public MultiValueMap<String, String> parameterToMultiValueMap(CollectionFormat collectionFormat, String name, Object value) {\n        final MultiValueMap<String, String> params = new LinkedMultiValueMap<String, String>();\n\n        if (name == null || name.isEmpty() || value == null) {\n            return params;\n        }\n\n        if(collectionFormat == null) {\n            collectionFormat = CollectionFormat.CSV;\n        }\n\n        if (value instanceof Map) {\n            @SuppressWarnings(\"unchecked\")\n            final Map<String, Object> valuesMap = (Map<String, Object>) value;\n            for (final Entry<String, Object> entry : valuesMap.entrySet()) {\n                params.add(entry.getKey(), parameterToString(entry.getValue()));\n            }\n            return params;\n        }\n\n        Collection<?> valueCollection = null;\n        if (value instanceof Collection) {\n            valueCollection = (Collection<?>) value;\n        } else {\n            params.add(name, parameterToString(value));\n            return params;\n        }\n\n        if (valueCollection.isEmpty()){\n            return params;\n        }\n\n        if (collectionFormat.equals(CollectionFormat.MULTI)) {\n            for (Object item : valueCollection) {\n                params.add(name, parameterToString(item));\n            }\n            return params;\n        }\n\n        List<String> values = new ArrayList<String>();\n        for(Object o : valueCollection) {\n            values.add(parameterToString(o));\n        }\n        params.add(name, collectionFormat.collectionToString(values));\n\n        return params;\n    }\n\n    /**\n    * Check if the given {@code String} is a JSON MIME.\n    * @param mediaType the input MediaType\n    * @return boolean true if the MediaType represents JSON, false otherwise\n    */\n    public boolean isJsonMime(String mediaType) {\n        // \"* / *\" is default to JSON\n        if (\"*/*\".equals(mediaType)) {\n            return true;\n        }\n\n        try {\n            return isJsonMime(MediaType.parseMediaType(mediaType));\n        } catch (InvalidMediaTypeException e) {\n        }\n        return false;\n    }\n\n    /**\n     * Check if the given MIME is a JSON MIME.\n     * JSON MIME examples:\n     *     application/json\n     *     application/json; charset=UTF8\n     *     APPLICATION/JSON\n     * @param mediaType the input MediaType\n     * @return boolean true if the MediaType represents JSON, false otherwise\n     */\n    public boolean isJsonMime(MediaType mediaType) {\n        return mediaType != null && (MediaType.APPLICATION_JSON.isCompatibleWith(mediaType) || mediaType.getSubtype().matches(\"^.*(\\\\+json|ndjson)[;]?\\\\s*$\"));\n    }\n\n    /**\n    * Check if the given {@code String} is a Problem JSON MIME (RFC-7807).\n    * @param mediaType the input MediaType\n    * @return boolean true if the MediaType represents Problem JSON, false otherwise\n    */\n    public boolean isProblemJsonMime(String mediaType) {\n        return \"application/problem+json\".equalsIgnoreCase(mediaType);\n    }\n\n    /**\n     * Select the Accept header's value from the given accepts array:\n     *     if JSON exists in the given array, use it;\n     *     otherwise use all of them (joining into a string)\n     *\n     * @param accepts The accepts array to select from\n     * @return List The list of MediaTypes to use for the Accept header\n     */\n```\n\n### Code description\n```Java\n/**\n * Description: This method selects the header accept based on the given array of accepts.\n * It checks if the array is empty, if so, it returns null.\n * For each accept in the array, it parses the media type and checks if it is JSON and not a problem JSON.\n * If it is, it returns a singleton list of the media type.\n * If none of the accepts are JSON, it parses all the media types from the array and returns them.\n *\n * @param accepts An array of accept strings.\n * @return A list of MediaType objects.\n */\n```\n\n### Code snippet:\n```Java\n    public List<MediaType> selectHeaderAccept(String[] accepts) {\n        if (accepts.length == 0) {\n            return null;\n        }\n        for (String accept : accepts) {\n            MediaType mediaType = MediaType.parseMediaType(accept);\n```\n\n### Requirements:\n1. Complete only the missing part of the method.\n2. Only answer the code of the completed part. Do not include any of the provided code snippet.\n3. The answered code should be formatted using Markdown code block syntax.\n4. Do not include any additional text.\n\n### Answer: (use the provided format with backticks)\n", "reference": "            if (isJsonMime(mediaType) && !isProblemJsonMime(accept)) {\n                return Collections.singletonList(mediaType);\n            }\n        }\n        return MediaType.parseMediaTypes(StringUtils.arrayToCommaDelimitedString(accepts));\n    }", "completions": ["  \n```Java\n            if (isJsonMime(mediaType) &&!isProblemJsonMime(mediaType)) {\n                return Collections.singletonList(mediaType);\n            }\n        }\n\n        List<MediaType> mediaTypes = new ArrayList<MediaType>(accepts.length);\n        for (String accept : accepts) {\n            MediaType mediaType = MediaType.parseMediaType(accept);\n            mediaTypes.add(mediaType);\n        }\n        return mediaTypes;\n    }\n```"]}
{"id": 17, "name": "updateFileConfigMaster", "prompt": "### Role:\nYou are an experienced Java programmer assistant responsible for code completion tasks. Please intelligently complete the missing parts of the provided code snippet.\n\n### Task:\nBased on the provided code snippet, complete the missing method implementation. Ensure the code is logically correct, syntactically accurate, and follows best coding practices.\n\n### Left context\n```Java\npackage co.yixiang.yshop.module.infra.service.file;\n\nimport cn.hutool.core.io.resource.ResourceUtil;\nimport cn.hutool.core.util.IdUtil;\nimport co.yixiang.yshop.framework.common.pojo.PageResult;\nimport co.yixiang.yshop.framework.common.util.json.JsonUtils;\nimport co.yixiang.yshop.framework.common.util.validation.ValidationUtils;\nimport co.yixiang.yshop.framework.file.core.client.FileClient;\nimport co.yixiang.yshop.framework.file.core.client.FileClientConfig;\nimport co.yixiang.yshop.framework.file.core.client.FileClientFactory;\nimport co.yixiang.yshop.framework.file.core.enums.FileStorageEnum;\nimport co.yixiang.yshop.module.infra.controller.admin.file.vo.config.FileConfigCreateReqVO;\nimport co.yixiang.yshop.module.infra.controller.admin.file.vo.config.FileConfigPageReqVO;\nimport co.yixiang.yshop.module.infra.controller.admin.file.vo.config.FileConfigUpdateReqVO;\nimport co.yixiang.yshop.module.infra.convert.file.FileConfigConvert;\nimport co.yixiang.yshop.module.infra.dal.dataobject.file.FileConfigDO;\nimport co.yixiang.yshop.module.infra.dal.mysql.file.FileConfigMapper;\nimport co.yixiang.yshop.module.infra.mq.producer.file.FileConfigProducer;\nimport lombok.Getter;\nimport lombok.extern.slf4j.Slf4j;\nimport org.springframework.stereotype.Service;\nimport org.springframework.transaction.annotation.Transactional;\nimport org.springframework.transaction.support.TransactionSynchronization;\nimport org.springframework.transaction.support.TransactionSynchronizationManager;\nimport org.springframework.validation.annotation.Validated;\n\nimport javax.annotation.PostConstruct;\nimport javax.annotation.Resource;\nimport javax.validation.Validator;\nimport java.util.List;\nimport java.util.Map;\n\nimport static co.yixiang.yshop.framework.common.exception.util.ServiceExceptionUtil.exception;\nimport static co.yixiang.yshop.module.infra.enums.ErrorCodeConstants.FILE_CONFIG_DELETE_FAIL_MASTER;\nimport static co.yixiang.yshop.module.infra.enums.ErrorCodeConstants.FILE_CONFIG_NOT_EXISTS;\n\n/**\n * 文件配置 Service 实现类\n *\n * @author yshop\n */\n@Service\n@Validated\n@Slf4j\npublic class FileConfigServiceImpl implements FileConfigService {\n\n    @Resource\n    private FileClientFactory fileClientFactory;\n    /**\n     * Master FileClient 对象，有且仅有一个，即 {@link FileConfigDO#getMaster()} 对应的\n     */\n    @Getter\n    private FileClient masterFileClient;\n\n    @Resource\n    private FileConfigMapper fileConfigMapper;\n\n    @Resource\n    private FileConfigProducer fileConfigProducer;\n\n    @Resource\n    private Validator validator;\n\n    @Override\n    @PostConstruct\n    public void initLocalCache() {\n        // 第一步：查询数据\n        List<FileConfigDO> configs = fileConfigMapper.selectList();\n        log.info(\"[initLocalCache][缓存文件配置，数量为:{}]\", configs.size());\n\n        // 第二步：构建缓存：创建或更新文件 Client\n        configs.forEach(config -> {\n            fileClientFactory.createOrUpdateFileClient(config.getId(), config.getStorage(), config.getConfig());\n            // 如果是 master，进行设置\n            if (Boolean.TRUE.equals(config.getMaster())) {\n                masterFileClient = fileClientFactory.getFileClient(config.getId());\n            }\n        });\n    }\n\n    @Override\n    public Long createFileConfig(FileConfigCreateReqVO createReqVO) {\n        // 插入\n        FileConfigDO fileConfig = FileConfigConvert.INSTANCE.convert(createReqVO)\n                .setConfig(parseClientConfig(createReqVO.getStorage(), createReqVO.getConfig()))\n                .setMaster(false); // 默认非 master\n        fileConfigMapper.insert(fileConfig);\n        // 发送刷新配置的消息\n        fileConfigProducer.sendFileConfigRefreshMessage();\n        // 返回\n        return fileConfig.getId();\n    }\n\n    @Override\n    public void updateFileConfig(FileConfigUpdateReqVO updateReqVO) {\n        // 校验存在\n        FileConfigDO config = validateFileConfigExists(updateReqVO.getId());\n        // 更新\n        FileConfigDO updateObj = FileConfigConvert.INSTANCE.convert(updateReqVO)\n                .setConfig(parseClientConfig(config.getStorage(), updateReqVO.getConfig()));\n        fileConfigMapper.updateById(updateObj);\n        // 发送刷新配置的消息\n        fileConfigProducer.sendFileConfigRefreshMessage();\n    }\n\n```\n\n### Code description\n```Java\n/**\n * Description: This method updates the file configuration master. It first validates if the file configuration exists. Then, it updates all other file configurations to be non-master. After that, it updates the specified file configuration to be master. Finally, it registers a transaction synchronization to send a message after the transaction is committed.\n * \n * @param id The id of the file configuration to be updated.\n */\n```\n\n### Code snippet:\n```Java\n    @Override\n    @Transactional(rollbackFor = Exception.class)\n    public void updateFileConfigMaster(Long id) {\n        // 校验存在\n        validateFileConfigExists(id);\n        // 更新其它为非 master\n        fileConfigMapper.updateBatch(new FileConfigDO().setMaster(false));\n        // 更新\n        fileConfigMapper.updateById(new FileConfigDO().setId(id).setMaster(true));\n```\n\n### Requirements:\n1. Complete only the missing part of the method.\n2. Only answer the code of the completed part. Do not include any of the provided code snippet.\n3. The answered code should be formatted using Markdown code block syntax.\n4. Do not include any additional text.\n\n### Answer: (use the provided format with backticks)\n", "reference": "        // 发送刷新配置的消息\n        TransactionSynchronizationManager.registerSynchronization(new TransactionSynchronization() {\n\n            @Override\n            public void afterCommit() {\n                fileConfigProducer.sendFileConfigRefreshMessage();\n            }\n\n        });\n    }", "completions": ["  \n```Java\n    @Override\n    @Transactional(rollbackFor = Exception.class)\n    public void updateFileConfigMaster(Long id) {\n        // 校验存在\n        validateFileConfigExists(id);\n        // 更新其它为非 master\n        fileConfigMapper.updateBatch(new FileConfigDO().setMaster(false));\n        // 更新\n        fileConfigMapper.updateById(new FileConfigDO().setId(id).setMaster(true));\n        // 注册事务同步，在事务提交后发送消息\n        TransactionSynchronizationManager.registerSynchronization(new TransactionSynchronization() {\n            @Override\n            public void afterCommit() {\n                fileConfigProducer.sendFileConfigRefreshMessage();\n            }\n        });\n    }\n```"]}
{"id": 18, "name": "collect", "prompt": "### Role:\nYou are an experienced Java programmer assistant responsible for code completion tasks. Please intelligently complete the missing parts of the provided code snippet.\n\n### Task:\nBased on the provided code snippet, complete the missing method implementation. Ensure the code is logically correct, syntactically accurate, and follows best coding practices.\n\n### Left context\n```Java\npackage org.dromara.hertzbeat.collector.collect.nginx;\n\nimport lombok.AllArgsConstructor;\nimport lombok.Builder;\nimport lombok.Data;\nimport lombok.NoArgsConstructor;\nimport lombok.extern.slf4j.Slf4j;\nimport org.apache.commons.lang3.StringUtils;\nimport org.apache.http.HttpHeaders;\nimport org.apache.http.HttpHost;\nimport org.apache.http.client.config.RequestConfig;\nimport org.apache.http.client.methods.CloseableHttpResponse;\nimport org.apache.http.client.methods.HttpUriRequest;\nimport org.apache.http.client.methods.RequestBuilder;\nimport org.apache.http.client.protocol.HttpClientContext;\nimport org.apache.http.protocol.HttpContext;\nimport org.apache.http.util.EntityUtils;\nimport org.dromara.hertzbeat.collector.collect.AbstractCollect;\nimport org.dromara.hertzbeat.collector.collect.common.http.CommonHttpClient;\nimport org.dromara.hertzbeat.collector.dispatch.DispatchConstants;\nimport org.dromara.hertzbeat.collector.util.CollectUtil;\nimport org.dromara.hertzbeat.common.constants.CollectorConstants;\nimport org.dromara.hertzbeat.common.constants.CommonConstants;\nimport org.dromara.hertzbeat.common.entity.job.Metrics;\nimport org.dromara.hertzbeat.common.entity.job.protocol.NginxProtocol;\nimport org.dromara.hertzbeat.common.entity.message.CollectRep;\nimport org.dromara.hertzbeat.common.util.CommonUtil;\nimport org.dromara.hertzbeat.common.util.IpDomainUtil;\n\nimport java.lang.reflect.InvocationTargetException;\nimport java.lang.reflect.Method;\nimport java.nio.charset.StandardCharsets;\nimport java.util.ArrayList;\nimport java.util.HashMap;\nimport java.util.List;\nimport java.util.Map;\nimport java.util.regex.Matcher;\nimport java.util.regex.Pattern;\n\nimport static org.dromara.hertzbeat.common.constants.SignConstants.RIGHT_DASH;\n\n/**\n * nginx collect\n *\n * @author a-little-fool\n */\n@Slf4j\npublic class NginxCollectImpl extends AbstractCollect {\n\n    private final static int SUCCESS_CODE = 200;\n    private final static String NGINX_STATUS_NAME = \"nginx_status\";\n    private final static String REQ_STATUS_NAME = \"req_status\";\n    private final static String AVAILABLE = \"available\";\n    private final static String CONNECTIONS = \"connections\";\n    private final static String ACTIVE = \"active\";\n    private final static String GET = \"get\";\n    private final static String FIELD_SPLIT = \"_\";\n    private final static String REGEX_KEYS = \"server\\\\s+(\\\\w+)\\\\s+(\\\\w+)\\\\s+(\\\\w+)\";\n    private final static String REGEX_VALUES = \"(\\\\d+) (\\\\d+) (\\\\d+)\";\n    private final static String REGEX_SERVER = \"(\\\\w+): (\\\\d+)\";\n    private final static String REGEX_SPLIT = \"\\\\r?\\\\n\";\n    private final static String REGEX_LINE_SPLIT = \"\\\\s+\";\n\n\n    public NginxCollectImpl() {\n\n    }\n\n```\n\n### Code description\n```Java\n/**\n * Description: This method collects metrics data from a specified source and populates a builder object with the collected data.\n * It validates the parameters, creates an HTTP request and context, executes the request, and parses the response based on the metrics name.\n * If any exception occurs during the process, it sets the response code to FAIL and includes the exception message in the response.\n * Finally, it aborts the HTTP request if it was created successfully.\n *\n * @param builder A builder object to populate with the collected metrics data.\n * @param monitorId The ID of the monitor from which the metrics are being collected.\n * @param app The name of the application from which the metrics are being collected.\n * @param metrics The metrics object containing the necessary information for the collection process.\n * @throws IOException If an I/O error occurs during the HTTP request execution.\n * @see org.apache.http.util.EntityUtils#toString(org.apache.http.HttpEntity, java.nio.charset.Charset)\n */\n```\n\n### Code snippet:\n```Java\n    @Override\n    public void collect(CollectRep.MetricsData.Builder builder, long monitorId, String app, Metrics metrics) {\n        long startTime = System.currentTimeMillis();\n\n        // 校验参数\n        try {\n            validateParams(metrics);\n        } catch (Exception e) {\n            builder.setCode(CollectRep.Code.FAIL);\n            builder.setMsg(e.getMessage());\n            return;\n        }\n\n        HttpContext httpContext = createHttpContext(metrics.getNginx());\n        HttpUriRequest request = createHttpRequest(metrics.getNginx());\n        try (CloseableHttpResponse response = CommonHttpClient.getHttpClient().execute(request, httpContext)){\n            // 发起http请求，获取响应数据\n            int statusCode = response.getStatusLine().getStatusCode();\n            if (statusCode != SUCCESS_CODE) {\n                builder.setCode(CollectRep.Code.FAIL);\n                builder.setMsg(\"StatusCode \" + statusCode);\n                return;\n```\n\n### Requirements:\n1. Complete only the missing part of the method.\n2. Only answer the code of the completed part. Do not include any of the provided code snippet.\n3. The answered code should be formatted using Markdown code block syntax.\n4. Do not include any additional text.\n\n### Answer: (use the provided format with backticks)\n", "reference": "            }\n            String resp = EntityUtils.toString(response.getEntity(), StandardCharsets.UTF_8);\n\n            Long responseTime = System.currentTimeMillis() - startTime;\n            // 根据metrics name选择调用不同解析方法\n            if (NGINX_STATUS_NAME.equals(metrics.getName()) || AVAILABLE.equals(metrics.getName())) {\n                parseNginxStatusResponse(builder, resp, metrics, responseTime);\n            } else if (REQ_STATUS_NAME.equals(metrics.getName())) {\n                parseReqStatusResponse(builder, resp, metrics, responseTime);\n            }\n        } catch (Exception e) {\n            String errorMsg = CommonUtil.getMessageFromThrowable(e);\n            log.info(errorMsg);\n            builder.setCode(CollectRep.Code.FAIL);\n            builder.setMsg(errorMsg);\n        } finally {\n            if (request != null) {\n                request.abort();\n            }\n        }\n\n    }", "completions": ["  \n```Java\n            String responseBody = EntityUtils.toString(response.getEntity(), StandardCharsets.UTF_8);\n            Map<String, Object> resultMap = new HashMap<>();\n            // 解析响应数据\n            if (metrics.getName().equals(NGINX_STATUS_NAME)) {\n                resultMap = parseNginxStatus(responseBody);\n            } else if (metrics.getName().equals(REQ_STATUS_NAME)) {\n                resultMap = parseReqStatus(responseBody);\n            }\n            // 封装响应数据\n            builder.setCode(CollectRep.Code.SUCCESS);\n            builder.putAllData(resultMap);\n            builder.setMsg(\"success\");\n        } catch (IOException e) {\n            builder.setCode(CollectRep.Code.FAIL);\n            builder.setMsg(e.getMessage());\n        } finally {\n            if (request!= null) {\n                request.abort();\n            }\n        }\n        long endTime = System.currentTimeMillis();\n        log.info(\"Collect nginx metrics data, spend time: {}ms\", endTime - startTime);\n    }\n\n    /**\n     * 校验参数\n     *\n     * @param metrics 监控项\n     */\n    private void validateParams(Metrics metrics) {\n        if (metrics == null) {\n            throw new IllegalArgumentException(\"Metrics cannot be null\");\n        }\n        if (metrics.getNginx() == null) {\n            throw new IllegalArgumentException(\"Nginx cannot be null\");\n        }\n        if (StringUtils.isEmpty(metrics.getName())) {\n            throw new IllegalArgumentException(\"Metrics name cannot be empty\");\n        }\n    }\n\n    /**\n     * 创建http请求\n     *\n     * @param nginx 监控项\n     * @return http请求\n     */\n    private HttpUriRequest createHttpRequest(NginxProtocol nginx) {\n        RequestBuilder requestBuilder = RequestBuilder.get(nginx.getUrl());\n        requestBuilder.setConfig(RequestConfig.custom().setConnectTimeout(nginx.getConnectTimeout()).setSocketTimeout(nginx.getSocketTimeout()).build());\n        requestBuilder.setHeader(HttpHeaders.ACCEPT, \"text/plain\");\n        return requestBuilder.build();\n    }\n\n    /**\n     * 创建http上下文\n     *\n     * @param nginx 监控项\n     * @return http上下文\n     */\n    private HttpContext createHttpContext(NginxProtocol nginx) {\n        HttpHost httpHost = new HttpHost(nginx.getHost(), nginx.getPort());\n        HttpClientContext httpClientContext = HttpClientContext.create();\n        httpClientContext.setTargetHost(httpHost);\n        return httpClientContext;\n    }\n\n    /**\n     * 解析nginx状态\n     *\n     * @param responseBody 响应数据\n     * @return 解析结果\n     */\n    private Map<String, Object> parseNginxStatus(String responseBody) {\n        Map<String, Object> resultMap = new HashMap<>();\n        String[] lines = responseBody.split(REGEX_SPLIT);\n        for (String line : lines) {\n            String[] keyValues = line.split(REGEX_LINE_SPLIT);\n            if (keyValues.length == 2) {\n                resultMap.put(keyValues[0], keyValues[1]);\n            }\n        }\n        return resultMap;\n    }\n\n    /**\n     * 解析请求状态\n     *\n     * @param responseBody 响应数据\n     * @return 解析结果\n     */\n    private Map<String, Object> parseReqStatus(String responseBody) {\n        Map<String, Object> resultMap = new HashMap<>();\n        String[] lines = responseBody.split(REGEX_SPLIT);\n        for (String line : lines) {\n            String[] keyValues = line.split(REGEX_LINE_SPLIT);\n            if (keyValues.length == 2) {\n                resultMap.put(keyValues[0"]}
{"id": 19, "name": "sendGetData", "prompt": "### Role:\nYou are an experienced Java programmer assistant responsible for code completion tasks. Please intelligently complete the missing parts of the provided code snippet.\n\n### Task:\nBased on the provided code snippet, complete the missing method implementation. Ensure the code is logically correct, syntactically accurate, and follows best coding practices.\n\n### Left context\n```Java\npackage com.tapdata.tm.utils;\n\nimport lombok.extern.slf4j.Slf4j;\nimport org.apache.http.HttpStatus;\nimport org.apache.http.client.ClientProtocolException;\nimport org.apache.http.client.methods.CloseableHttpResponse;\nimport org.apache.http.client.methods.HttpGet;\nimport org.apache.http.client.methods.HttpPost;\nimport org.apache.http.entity.StringEntity;\nimport org.apache.http.impl.client.CloseableHttpClient;\nimport org.apache.http.impl.client.HttpClients;\nimport org.apache.http.util.EntityUtils;\n\nimport java.io.IOException;\nimport java.nio.charset.Charset;\nimport java.util.Map;\n\n@Slf4j\npublic class HttpUtils {\n    private static final String UTF_8 =\"utf-8\";\n\n\n    /**\n     * get请求传输数据\n     *\n     * @return\n     * @throws ClientProtocolException\n     * @throws IOException\n     */\n```\n\n### Code description\n```Java\n/**\n * Sends a GET request to the specified path with the provided headers.\n *\n * @param path The URL path to send the GET request to.\n * @param headMap A map of headers to include in the GET request.\n * @return The response body as a string, or an empty string if the request failed.\n * @throws IOException If an I/O error occurs while sending the request or processing the response.\n */\n```\n\n### Code snippet:\n```Java\n    public static String sendGetData(String path, Map<String, String> headMap) {\n        log.info(\"request tcm, path：{}，headMap：{}  \",path,headMap);\n        String result = \"\";\n        CloseableHttpResponse response =null;\n        try(CloseableHttpClient httpClient = HttpClients.createDefault()) {\n            // 创建get方式请求对象\n            HttpGet httpGet = new HttpGet(path);\n            if (null != headMap) {\n                for (Map.Entry<String,String> entry : headMap.entrySet()) {\n                    httpGet.addHeader(entry.getKey(), entry.getValue());\n                }\n            }\n            // 通过请求对象获取响应对象\n            response = httpClient.execute(httpGet);\n\n            // 获取结果实体\n            // 判断网络连接状态码是否正常(0--200都数正常)\n            if (response.getStatusLine().getStatusCode() == HttpStatus.SC_OK) {\n                result = EntityUtils.toString(response.getEntity(), UTF_8);\n            } else {\n```\n\n### Requirements:\n1. Complete only the missing part of the method.\n2. Only answer the code of the completed part. Do not include any of the provided code snippet.\n3. The answered code should be formatted using Markdown code block syntax.\n4. Do not include any additional text.\n\n### Answer: (use the provided format with backticks)\n", "reference": "                log.error(\"get请求传输 异常.url:{}, headMap:{}\", path, headMap);\n            }\n        } catch (ClientProtocolException e) {\n            log.error(\"get请求传输 异常 \", e);\n            log.error(\"get请求传输 异常.path:{}, headMap:{}\", path, headMap);\n        } catch (IOException e) {\n            log.error(\"get请求传输 异常 \", e);\n            log.error(\"get请求传输 异常.path:{}, headMap:{}\", path, headMap);\n        }\n        finally {\n            try {\n                if (null!=response){\n                    response.close();\n                }\n            } catch (IOException e) {\n                log.error(\"关闭response 异常 \", e);\n            }\n        }\n        log.debug(result);\n        return result;\n    }", "completions": ["  \n```Java\n            } else {\n                log.error(\"request tcm failed, path：{}，headMap：{}，statusCode：{}\", path, headMap, response.getStatusLine().getStatusCode());\n            }\n        } catch (IOException e) {\n            log.error(\"request tcm error, path：{}，headMap：{}\", path, headMap, e);\n        } finally {\n            if (response!= null) {\n                try {\n                    response.close();\n                } catch (IOException e) {\n                    log.error(\"close response error, path：{}，headMap：{}\", path, headMap, e);\n                }\n            }\n        }\n        return result;\n    }\n}\n```"]}
{"id": 20, "name": "doGet", "prompt": "### Role:\nYou are an experienced Java programmer assistant responsible for code completion tasks. Please intelligently complete the missing parts of the provided code snippet.\n\n### Task:\nBased on the provided code snippet, complete the missing method implementation. Ensure the code is logically correct, syntactically accurate, and follows best coding practices.\n\n### Left context\n```Java\n/*\n*Copyright (c) Alibaba Group;\n*Licensed under the Apache License, Version 2.0 (the \"License\");\n*you may not use this file except in compliance with the License.\n*You may obtain a copy of the License at\n\n*   http://www.apache.org/licenses/LICENSE-2.0\n\n*Unless required by applicable law or agreed to in writing, software\n*distributed under the License is distributed on an \"AS IS\" BASIS,\n*WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n*See the License for the specific language governing permissions and\n*limitations under the License.\n*/\n\npackage org.example.common.utils;\n\nimport org.example.common.constant.Constants;\nimport lombok.extern.slf4j.Slf4j;\nimport org.apache.http.HttpResponse;\nimport org.apache.http.HttpStatus;\nimport org.apache.http.NameValuePair;\nimport org.apache.http.StatusLine;\nimport org.apache.http.client.entity.UrlEncodedFormEntity;\nimport org.apache.http.client.methods.CloseableHttpResponse;\nimport org.apache.http.client.methods.HttpGet;\nimport org.apache.http.client.methods.HttpPost;\nimport org.apache.http.impl.client.CloseableHttpClient;\nimport org.apache.http.impl.client.HttpClients;\nimport org.apache.http.util.EntityUtils;\nimport org.springframework.beans.BeanWrapper;\nimport org.springframework.beans.BeanWrapperImpl;\nimport org.springframework.stereotype.Component;\nimport org.springframework.web.util.UriComponentsBuilder;\n\nimport javax.servlet.http.HttpServletRequest;\nimport java.io.IOException;\nimport java.lang.reflect.Field;\nimport java.nio.charset.StandardCharsets;\nimport java.util.List;\nimport java.util.Map;\nimport java.util.stream.Collectors;\n\n@Component\n@Slf4j\npublic class HttpUtil {\n\n```\n\n### Code description\n```Java\n/**\n * Description: This method performs an HTTP GET request to the specified URL and returns the response as a String.\n * \n * @param url The URL to which the GET request is sent.\n * @return The response from the server as a String, or null if an exception occurs or the response status code is not 200 (OK).\n */\n```\n\n### Code snippet:\n```Java\n    public static String doGet(String url) {\n        try (CloseableHttpClient client = HttpClients.createDefault()) {\n            HttpGet request = new HttpGet(url);\n            HttpResponse response = client.execute(request);\n            if (response.getStatusLine().getStatusCode() == HttpStatus.SC_OK) {\n                return EntityUtils.toString(response.getEntity(), StandardCharsets.UTF_8);\n```\n\n### Requirements:\n1. Complete only the missing part of the method.\n2. Only answer the code of the completed part. Do not include any of the provided code snippet.\n3. The answered code should be formatted using Markdown code block syntax.\n4. Do not include any additional text.\n\n### Answer: (use the provided format with backticks)\n", "reference": "            }\n        } catch (IOException e) {\n            log.error(\"DoGet exception.\", e);\n\n        }\n        return null;\n    }", "completions": ["  \n```Java\n            }\n        } catch (IOException e) {\n            log.error(\"Error executing GET request to {}\", url, e);\n        }\n        return null;\n    }\n```"]}
{"id": 21, "name": "resolve", "prompt": "### Role:\nYou are an experienced Java programmer assistant responsible for code completion tasks. Please intelligently complete the missing parts of the provided code snippet.\n\n### Task:\nBased on the provided code snippet, complete the missing method implementation. Ensure the code is logically correct, syntactically accurate, and follows best coding practices.\n\n### Left context\n```Java\n/*\n * Licensed to the Apache Software Foundation (ASF) under one\n * or more contributor license agreements.  See the NOTICE file\n * distributed with this work for additional information\n * regarding copyright ownership.  The ASF licenses this file\n * to you under the Apache License, Version 2.0 (the\n * \"License\"); you may not use this file except in compliance\n * with the License.  You may obtain a copy of the License at\n *\n *   http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing,\n * software distributed under the License is distributed on an\n * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n * KIND, either express or implied.  See the License for the\n * specific language governing permissions and limitations\n * under the License.\n */\npackage org.eclipse.aether.transport.apache;\n\nimport java.net.URI;\nimport java.net.URISyntaxException;\nimport java.util.ArrayList;\nimport java.util.List;\n\nimport org.apache.http.client.utils.URIUtils;\n\n/**\n * Helps to deal with URIs.\n */\nfinal class UriUtils {\n\n```\n\n### Code description\n```Java\n/**\n * Resolves a reference URI against a base URI using the resolve method from org.apache.http.client.utils.URIUtils.\n * If the reference URI has a non-empty path, and the base URI's path does not end with a slash,\n * a new URI is created with the base URI's path appended with a slash.\n *\n * @param base The base URI to resolve against.\n * @param ref The reference URI to resolve.\n * @return The resolved URI.\n * @throws IllegalStateException If there is an error creating the new URI.\n */\n```\n\n### Code snippet:\n```Java\n    public static URI resolve(URI base, URI ref) {\n        String path = ref.getRawPath();\n        if (path != null && !path.isEmpty()) {\n            path = base.getRawPath();\n            if (path == null || !path.endsWith(\"/\")) {\n                try {\n                    base = new URI(base.getScheme(), base.getAuthority(), base.getPath() + '/', null, null);\n```\n\n### Requirements:\n1. Complete only the missing part of the method.\n2. Only answer the code of the completed part. Do not include any of the provided code snippet.\n3. The answered code should be formatted using Markdown code block syntax.\n4. Do not include any additional text.\n\n### Answer: (use the provided format with backticks)\n", "reference": "                } catch (URISyntaxException e) {\n                    throw new IllegalStateException(e);\n                }\n            }\n        }\n        return URIUtils.resolve(base, ref);\n    }", "completions": ["  \n```Java\nreturn URIUtils.resolve(base, ref);\n```"]}
{"id": 22, "name": "token2Jwt", "prompt": "### Role:\nYou are an experienced Java programmer assistant responsible for code completion tasks. Please intelligently complete the missing parts of the provided code snippet.\n\n### Task:\nBased on the provided code snippet, complete the missing method implementation. Ensure the code is logically correct, syntactically accurate, and follows best coding practices.\n\n### Left context\n```Java\n/*\n*Copyright (c) Alibaba Group;\n*Licensed under the Apache License, Version 2.0 (the \"License\");\n*you may not use this file except in compliance with the License.\n*You may obtain a copy of the License at\n\n*   http://www.apache.org/licenses/LICENSE-2.0\n\n*Unless required by applicable law or agreed to in writing, software\n*distributed under the License is distributed on an \"AS IS\" BASIS,\n*WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n*See the License for the specific language governing permissions and\n*limitations under the License.\n*/\n\npackage org.example.common.helper;\n\nimport com.alibaba.fastjson2.JSON;\nimport com.alibaba.fastjson2.JSONArray;\nimport com.alibaba.fastjson2.JSONObject;\nimport com.fasterxml.jackson.core.JsonProcessingException;\nimport com.nimbusds.jose.crypto.RSASSAVerifier;\nimport com.nimbusds.jose.jwk.RSAKey;\nimport com.nimbusds.jwt.SignedJWT;\nimport lombok.extern.slf4j.Slf4j;\nimport org.apache.commons.lang3.StringUtils;\nimport org.example.common.errorinfo.ErrorInfo;\nimport org.example.common.exception.BizException;\nimport org.example.common.model.UserInfoModel;\nimport org.example.common.utils.HttpUtil;\nimport org.example.common.utils.JsonUtil;\nimport org.springframework.beans.factory.annotation.Value;\nimport org.springframework.stereotype.Component;\n\nimport javax.annotation.PostConstruct;\nimport java.text.ParseException;\nimport java.util.ArrayList;\nimport java.util.List;\nimport java.util.Objects;\nimport java.util.regex.Matcher;\nimport java.util.regex.Pattern;\n\n@Component\n@Slf4j\npublic class TokenParseHelper {\n\n    @Value(\"${oauth.public-key.url}\")\n    private String publicKeyUri;\n\n    private static final String PATTERN = \"Bearer\\\\s+\\\\S+\";\n\n    private static final int TOKEN_LEN = 2;\n\n    public List<RSAKey> publicKeyCache;\n\n    @PostConstruct\n    public void init() {\n        String keys = Objects.requireNonNull(JSON.parseObject(HttpUtil.doGet(publicKeyUri))).getString(\"keys\");\n        publicKeyCache = new ArrayList<>();\n        try {\n            JSONArray publicKeyList = JSON.parseArray(keys);\n            for (Object object : publicKeyList) {\n                RSAKey rsaKey = RSAKey.parse(JSONObject.toJSONString(object));\n                publicKeyCache.add(rsaKey);\n            }\n        } catch (Exception e) {\n            log.error(\"Failed to obtain public key, uri = {}.\", publicKeyUri, e);\n            throw new BizException(ErrorInfo.SERVER_UNAVAILABLE.getStatusCode(), ErrorInfo.SERVER_UNAVAILABLE.getCode(), ErrorInfo.SERVER_UNAVAILABLE.getMessage());\n        }\n    }\n\n```\n\n### Code description\n```Java\n/**\n * Parses a token into a SignedJWT object.\n *\n * @param token The token to be parsed.\n * @return The parsed SignedJWT object.\n * @throws BizException If the token is null or cannot be parsed.\n */\n```\n\n### Code snippet:\n```Java\n    public SignedJWT token2Jwt(String token) {\n        if (token == null) {\n            log.error(\"Token is null.\");\n            throw new BizException(ErrorInfo.VERIFY_FAILED.getStatusCode(), ErrorInfo.VERIFY_FAILED.getCode(), ErrorInfo.VERIFY_FAILED.getMessage());\n        }\n        try {\n```\n\n### Requirements:\n1. Complete only the missing part of the method.\n2. Only answer the code of the completed part. Do not include any of the provided code snippet.\n3. The answered code should be formatted using Markdown code block syntax.\n4. Do not include any additional text.\n\n### Answer: (use the provided format with backticks)\n", "reference": "            return SignedJWT.parse(token);\n        } catch (ParseException e) {\n            log.error(\"Token to JWT Error,message = {}\", e.getMessage());\n            throw new BizException(ErrorInfo.VERIFY_FAILED.getStatusCode(), ErrorInfo.VERIFY_FAILED.getCode(), ErrorInfo.VERIFY_FAILED.getMessage());\n        }\n    }", "completions": ["  \n```Java\n            return SignedJWT.parse(token);\n        } catch (ParseException e) {\n            log.error(\"Failed to parse token, token = {}.\", token, e);\n            throw new BizException(ErrorInfo.VERIFY_FAILED.getStatusCode(), ErrorInfo.VERIFY_FAILED.getCode(), ErrorInfo.VERIFY_FAILED.getMessage());\n        }\n    }\n```"]}
{"id": 23, "name": "init", "prompt": "### Role:\nYou are an experienced Java programmer assistant responsible for code completion tasks. Please intelligently complete the missing parts of the provided code snippet.\n\n### Task:\nBased on the provided code snippet, complete the missing method implementation. Ensure the code is logically correct, syntactically accurate, and follows best coding practices.\n\n### Left context\n```Java\n/*\n*Copyright (c) Alibaba Group;\n*Licensed under the Apache License, Version 2.0 (the \"License\");\n*you may not use this file except in compliance with the License.\n*You may obtain a copy of the License at\n\n*   http://www.apache.org/licenses/LICENSE-2.0\n\n*Unless required by applicable law or agreed to in writing, software\n*distributed under the License is distributed on an \"AS IS\" BASIS,\n*WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n*See the License for the specific language governing permissions and\n*limitations under the License.\n*/\n\npackage org.example.common.helper;\n\nimport com.alibaba.fastjson2.JSON;\nimport com.alibaba.fastjson2.JSONArray;\nimport com.alibaba.fastjson2.JSONObject;\nimport com.fasterxml.jackson.core.JsonProcessingException;\nimport com.nimbusds.jose.crypto.RSASSAVerifier;\nimport com.nimbusds.jose.jwk.RSAKey;\nimport com.nimbusds.jwt.SignedJWT;\nimport lombok.extern.slf4j.Slf4j;\nimport org.apache.commons.lang3.StringUtils;\nimport org.example.common.errorinfo.ErrorInfo;\nimport org.example.common.exception.BizException;\nimport org.example.common.model.UserInfoModel;\nimport org.example.common.utils.HttpUtil;\nimport org.example.common.utils.JsonUtil;\nimport org.springframework.beans.factory.annotation.Value;\nimport org.springframework.stereotype.Component;\n\nimport javax.annotation.PostConstruct;\nimport java.text.ParseException;\nimport java.util.ArrayList;\nimport java.util.List;\nimport java.util.Objects;\nimport java.util.regex.Matcher;\nimport java.util.regex.Pattern;\n\n@Component\n@Slf4j\npublic class TokenParseHelper {\n\n    @Value(\"${oauth.public-key.url}\")\n    private String publicKeyUri;\n\n    private static final String PATTERN = \"Bearer\\\\s+\\\\S+\";\n\n    private static final int TOKEN_LEN = 2;\n\n    public List<RSAKey> publicKeyCache;\n\n```\n\n### Code description\n```Java\n/**\n * This method is used to initialize the public key cache. It fetches the public keys from a specified URI, parses them into RSAKey objects, and stores them in a list.\n * If the fetching or parsing process fails, it logs the error and throws a BizException with SERVER_UNAVAILABLE error information.\n *\n * @throws BizException if the fetching or parsing process fails.\n */\n```\n\n### Code snippet:\n```Java\n    @PostConstruct\n    public void init() {\n        String keys = Objects.requireNonNull(JSON.parseObject(HttpUtil.doGet(publicKeyUri))).getString(\"keys\");\n        publicKeyCache = new ArrayList<>();\n        try {\n            JSONArray publicKeyList = JSON.parseArray(keys);\n            for (Object object : publicKeyList) {\n```\n\n### Requirements:\n1. Complete only the missing part of the method.\n2. Only answer the code of the completed part. Do not include any of the provided code snippet.\n3. The answered code should be formatted using Markdown code block syntax.\n4. Do not include any additional text.\n\n### Answer: (use the provided format with backticks)\n", "reference": "                RSAKey rsaKey = RSAKey.parse(JSONObject.toJSONString(object));\n                publicKeyCache.add(rsaKey);\n            }\n        } catch (Exception e) {\n            log.error(\"Failed to obtain public key, uri = {}.\", publicKeyUri, e);\n            throw new BizException(ErrorInfo.SERVER_UNAVAILABLE.getStatusCode(), ErrorInfo.SERVER_UNAVAILABLE.getCode(), ErrorInfo.SERVER_UNAVAILABLE.getMessage());\n        }\n    }", "completions": ["  \n```Java\n                JSONObject publicKeyJson = (JSONObject) object;\n                RSAKey publicKey = RSAKey.parse(publicKeyJson);\n                publicKeyCache.add(publicKey);\n            }\n        } catch (ParseException e) {\n            log.error(\"Failed to parse public key list\", e);\n            throw new BizException(ErrorInfo.SERVER_UNAVAILABLE);\n        }\n    }\n```"]}
{"id": 24, "name": "generate", "prompt": "### Role:\nYou are an experienced Java programmer assistant responsible for code completion tasks. Please intelligently complete the missing parts of the provided code snippet.\n\n### Task:\nBased on the provided code snippet, complete the missing method implementation. Ensure the code is logically correct, syntactically accurate, and follows best coding practices.\n\n### Left context\n```Java\npackage com.endava.cats.model.generator;\n\nimport com.endava.cats.context.CatsGlobalContext;\nimport com.endava.cats.generator.simple.StringGenerator;\nimport com.endava.cats.json.JsonUtils;\nimport com.endava.cats.generator.format.api.ValidDataFormat;\nimport com.fasterxml.jackson.databind.ObjectMapper;\nimport io.github.ludovicianul.prettylogger.PrettyLogger;\nimport io.github.ludovicianul.prettylogger.PrettyLoggerFactory;\nimport io.swagger.v3.core.util.Json;\nimport io.swagger.v3.oas.models.media.ArraySchema;\nimport io.swagger.v3.oas.models.media.BinarySchema;\nimport io.swagger.v3.oas.models.media.BooleanSchema;\nimport io.swagger.v3.oas.models.media.ByteArraySchema;\nimport io.swagger.v3.oas.models.media.ComposedSchema;\nimport io.swagger.v3.oas.models.media.DateSchema;\nimport io.swagger.v3.oas.models.media.DateTimeSchema;\nimport io.swagger.v3.oas.models.media.Discriminator;\nimport io.swagger.v3.oas.models.media.IntegerSchema;\nimport io.swagger.v3.oas.models.media.NumberSchema;\nimport io.swagger.v3.oas.models.media.ObjectSchema;\nimport io.swagger.v3.oas.models.media.Schema;\nimport io.swagger.v3.oas.models.media.StringSchema;\nimport io.swagger.v3.parser.util.SchemaTypeUtil;\nimport org.springframework.util.CollectionUtils;\n\nimport java.math.BigDecimal;\nimport java.time.LocalDate;\nimport java.time.OffsetDateTime;\nimport java.time.ZoneId;\nimport java.time.format.DateTimeFormatter;\nimport java.util.ArrayList;\nimport java.util.Base64;\nimport java.util.Collection;\nimport java.util.Collections;\nimport java.util.Date;\nimport java.util.HashMap;\nimport java.util.HashSet;\nimport java.util.List;\nimport java.util.Map;\nimport java.util.Optional;\nimport java.util.Random;\nimport java.util.Set;\nimport java.util.concurrent.ThreadLocalRandom;\n\nimport static com.endava.cats.generator.simple.StringGenerator.generateValueBasedOnMinMax;\n\n/**\n * A modified version of {@code io.swagger.codegen.examples.ExampleGenerator} that takes into consideration several other request\n * setups including complex objects and array of objects.\n * <p>\n * This is a stateful object. Don't use it through dependency injection.\n * <p>\n * Supported String formats:\n * <ul>\n *     <li>uuid</li>\n *     <li>date</li>\n *     <li>date-time</li>\n *     <li>email</li>\n *     <li>ip</li>\n *     <li>ipv4</li>\n *     <li>ipv6</li>\n *     <li>password</li>\n *     <li>byte</li>\n *     <li>uri/url</li>\n * </ul>\n */\npublic class OpenAPIModelGenerator {\n    private static final String EXAMPLE = \"example\";\n    private static final DateTimeFormatter DATE_FORMATTER = DateTimeFormatter.ofPattern(\"yyyy-MM-dd\");\n    private final PrettyLogger logger = PrettyLoggerFactory.getLogger(OpenAPIModelGenerator.class);\n    private final Set<Schema<?>> catsGeneratedExamples = new HashSet<>();\n    private final Random random;\n    private final boolean useExamples;\n    private final CatsGlobalContext globalContext;\n    private final ValidDataFormat validDataFormat;\n    private final int selfReferenceDepth;\n    private String currentProperty = \"\";\n\n    public OpenAPIModelGenerator(CatsGlobalContext catsGlobalContext, ValidDataFormat validDataFormat, boolean useExamplesArgument, int selfReferenceDepth) {\n        this.globalContext = catsGlobalContext;\n        this.random = ThreadLocalRandom.current();\n        useExamples = useExamplesArgument;\n        this.selfReferenceDepth = selfReferenceDepth;\n        this.validDataFormat = validDataFormat;\n    }\n\n\n```\n\n### Code description\n```Java\n/**\n * Generates a map of key-value pairs where the key is a constant string and the value is a JSON example string.\n * The map is either empty or contains a single entry.\n *\n * @param modelName the name of the model to generate an example for.\n * @return an unmodifiable map containing the generated key-value pair, or an empty map if the model name is null or the schema is not found.\n */\n```\n\n### Code snippet:\n```Java\n    public Map<String, String> generate(String modelName) {\n        Map<String, String> kv = new HashMap<>();\n        if (modelName != null) {\n            final Schema schema = this.globalContext.getSchemaMap().get(modelName);\n            if (schema != null) {\n                String example = Json.pretty(this.resolveModelToExample(modelName, schema));\n\n```\n\n### Requirements:\n1. Complete only the missing part of the method.\n2. Only answer the code of the completed part. Do not include any of the provided code snippet.\n3. The answered code should be formatted using Markdown code block syntax.\n4. Do not include any additional text.\n\n### Answer: (use the provided format with backticks)\n", "reference": "                if (example != null) {\n                    kv.put(EXAMPLE, example);\n                    return Map.copyOf(kv);\n                }\n            }\n        }\n        return Collections.emptyMap();\n    }", "completions": ["  \n```Java\nkv.put(modelName, example);\n```"]}
{"id": 25, "name": "nacosConfigService", "prompt": "### Role:\nYou are an experienced Java programmer assistant responsible for code completion tasks. Please intelligently complete the missing parts of the provided code snippet.\n\n### Task:\nBased on the provided code snippet, complete the missing method implementation. Ensure the code is logically correct, syntactically accurate, and follows best coding practices.\n\n### Left context\n```Java\n/*\n * Licensed to the Apache Software Foundation (ASF) under one or more\n * contributor license agreements.  See the NOTICE file distributed with\n * this work for additional information regarding copyright ownership.\n * The ASF licenses this file to You under the Apache License, Version 2.0\n * (the \"License\"); you may not use this file except in compliance with\n * the License.  You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\npackage org.apache.shenyu.admin.config;\n\nimport com.alibaba.nacos.api.NacosFactory;\nimport com.alibaba.nacos.api.PropertyKeyConst;\nimport com.alibaba.nacos.api.config.ConfigService;\nimport org.apache.commons.lang3.StringUtils;\nimport org.apache.shenyu.admin.config.properties.NacosProperties;\nimport org.apache.shenyu.admin.listener.DataChangedInit;\nimport org.apache.shenyu.admin.listener.DataChangedListener;\nimport org.apache.shenyu.admin.listener.nacos.NacosDataChangedInit;\nimport org.apache.shenyu.admin.listener.nacos.NacosDataChangedListener;\nimport org.springframework.boot.autoconfigure.condition.ConditionalOnMissingBean;\nimport org.springframework.boot.autoconfigure.condition.ConditionalOnProperty;\nimport org.springframework.boot.context.properties.EnableConfigurationProperties;\nimport org.springframework.context.annotation.Bean;\nimport org.springframework.context.annotation.Configuration;\n\nimport java.util.Objects;\nimport java.util.Properties;\n\n/**\n * The type Nacos listener.\n */\n@Configuration\n@ConditionalOnProperty(prefix = \"shenyu.sync.nacos\", name = \"url\")\n@EnableConfigurationProperties(NacosProperties.class)\npublic class NacosSyncConfiguration {\n\n    /**\n     * register configService in spring ioc.\n     *\n     * @param nacosProp the nacos configuration\n     * @return ConfigService {@linkplain ConfigService}\n     * @throws Exception the exception\n     */\n```\n\n### Code description\n```Java\n/**\n * Creates a ConfigService instance using the NacosFactory.\n *\n * @param nacosProp The properties for configuring the Nacos service.\n * @return A ConfigService instance.\n * @throws Exception If an error occurs while creating the ConfigService.\n */\n```\n\n### Code snippet:\n```Java\n    @Bean\n    @ConditionalOnMissingBean(ConfigService.class)\n    public ConfigService nacosConfigService(final NacosProperties nacosProp) throws Exception {\n        Properties properties = new Properties();\n        if (Objects.nonNull(nacosProp.getAcm()) && nacosProp.getAcm().isEnabled()) {\n            // Use aliyun ACM service\n            properties.put(PropertyKeyConst.ENDPOINT, nacosProp.getAcm().getEndpoint());\n            properties.put(PropertyKeyConst.NAMESPACE, nacosProp.getAcm().getNamespace());\n            // Use subaccount ACM administrative authority\n            properties.put(PropertyKeyConst.ACCESS_KEY, nacosProp.getAcm().getAccessKey());\n            properties.put(PropertyKeyConst.SECRET_KEY, nacosProp.getAcm().getSecretKey());\n        } else {\n```\n\n### Requirements:\n1. Complete only the missing part of the method.\n2. Only answer the code of the completed part. Do not include any of the provided code snippet.\n3. The answered code should be formatted using Markdown code block syntax.\n4. Do not include any additional text.\n\n### Answer: (use the provided format with backticks)\n", "reference": "            properties.put(PropertyKeyConst.SERVER_ADDR, nacosProp.getUrl());\n            if (StringUtils.isNotBlank(nacosProp.getNamespace())) {\n                properties.put(PropertyKeyConst.NAMESPACE, nacosProp.getNamespace());\n            }\n            if (StringUtils.isNotBlank(nacosProp.getUsername())) {\n                properties.put(PropertyKeyConst.USERNAME, nacosProp.getUsername());\n            }\n            if (StringUtils.isNotBlank(nacosProp.getPassword())) {\n                properties.put(PropertyKeyConst.PASSWORD, nacosProp.getPassword());\n            }\n        }\n        return NacosFactory.createConfigService(properties);\n    }", "completions": ["  \n```Java\n    @Bean\n    @ConditionalOnMissingBean(ConfigService.class)\n    public ConfigService nacosConfigService(final NacosProperties nacosProp) throws Exception {\n        Properties properties = new Properties();\n        if (Objects.nonNull(nacosProp.getAcm()) && nacosProp.getAcm().isEnabled()) {\n            // Use aliyun ACM service\n            properties.put(PropertyKeyConst.ENDPOINT, nacosProp.getAcm().getEndpoint());\n            properties.put(PropertyKeyConst.NAMESPACE, nacosProp.getAcm().getNamespace());\n            // Use subaccount ACM administrative authority\n            properties.put(PropertyKeyConst.ACCESS_KEY, nacosProp.getAcm().getAccessKey());\n            properties.put(PropertyKeyConst.SECRET_KEY, nacosProp.getAcm().getSecretKey());\n        } else {\n            properties.put(PropertyKeyConst.SERVER_ADDR, nacosProp.getUrl());\n        }\n        return NacosFactory.createConfigService(properties);\n    }\n```"]}
{"id": 26, "name": "init", "prompt": "### Role:\nYou are an experienced Java programmer assistant responsible for code completion tasks. Please intelligently complete the missing parts of the provided code snippet.\n\n### Task:\nBased on the provided code snippet, complete the missing method implementation. Ensure the code is logically correct, syntactically accurate, and follows best coding practices.\n\n### Left context\n```Java\n/*\n * Licensed to the Apache Software Foundation (ASF) under one or more\n * contributor license agreements.  See the NOTICE file distributed with\n * this work for additional information regarding copyright ownership.\n * The ASF licenses this file to You under the Apache License, Version 2.0\n * (the \"License\"); you may not use this file except in compliance with\n * the License.  You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\npackage org.apache.shenyu.discovery.nacos;\n\nimport com.google.gson.JsonObject;\nimport com.google.gson.JsonSyntaxException;\nimport org.apache.shenyu.common.dto.DiscoveryUpstreamData;\nimport org.apache.shenyu.common.utils.GsonUtils;\nimport com.alibaba.nacos.api.PropertyKeyConst;\nimport com.alibaba.nacos.api.exception.NacosException;\nimport com.alibaba.nacos.api.naming.NamingFactory;\nimport com.alibaba.nacos.api.naming.listener.EventListener;\nimport com.alibaba.nacos.api.naming.NamingService;\nimport com.alibaba.nacos.api.naming.listener.NamingEvent;\nimport com.alibaba.nacos.api.naming.pojo.Instance;\nimport org.apache.shenyu.common.exception.ShenyuException;\nimport org.apache.shenyu.discovery.api.ShenyuDiscoveryService;\nimport org.apache.shenyu.discovery.api.config.DiscoveryConfig;\nimport org.apache.shenyu.discovery.api.listener.DataChangedEventListener;\nimport org.apache.shenyu.discovery.api.listener.DiscoveryDataChangedEvent;\nimport org.apache.shenyu.spi.Join;\nimport org.slf4j.Logger;\nimport org.slf4j.LoggerFactory;\n\nimport java.util.ArrayList;\nimport java.util.HashMap;\nimport java.util.List;\nimport java.util.Map;\nimport java.util.Objects;\nimport java.util.Optional;\nimport java.util.Properties;\nimport java.util.Set;\nimport java.util.concurrent.ConcurrentHashMap;\nimport java.util.concurrent.ConcurrentMap;\nimport java.util.stream.Collectors;\n\n/**\n * The type Nacos for shenyu discovery service.\n */\n@Join\npublic class NacosDiscoveryService implements ShenyuDiscoveryService {\n\n    private static final Logger LOGGER = LoggerFactory.getLogger(NacosDiscoveryService.class);\n\n    private static final String NAMESPACE = \"nacosNameSpace\";\n\n    private final ConcurrentMap<String, EventListener> listenerMap = new ConcurrentHashMap<>();\n\n    private NamingService namingService;\n\n    private String groupName;\n\n    private final ConcurrentMap<String, List<Instance>> instanceListMap = new ConcurrentHashMap<>();\n\n```\n\n### Code description\n```Java\n/**\n * Initializes the Nacos naming service.\n *\n * This method checks if the naming service is already registered. If it is, it logs an info message and returns.\n * Otherwise, it creates a new Nacos naming service using the provided configuration properties.\n *\n * @param config The discovery configuration containing the server list and additional properties.\n * @throws ShenyuException If an error occurs while initializing the Nacos naming service.\n */\n```\n\n### Code snippet:\n```Java\n    @Override\n    public void init(final DiscoveryConfig config) {\n        if (this.namingService != null) {\n            LOGGER.info(\"Nacos naming service already registered\");\n            return;\n        }\n        Properties properties = config.getProps();\n        Properties nacosProperties = new Properties();\n        this.groupName = properties.getProperty(\"groupName\", \"SHENYU_GROUP\");\n        String serverAddr = config.getServerList();\n        nacosProperties.put(PropertyKeyConst.SERVER_ADDR, serverAddr);\n        nacosProperties.put(PropertyKeyConst.NAMESPACE, properties.getProperty(NAMESPACE, \"\"));\n```\n\n### Requirements:\n1. Complete only the missing part of the method.\n2. Only answer the code of the completed part. Do not include any of the provided code snippet.\n3. The answered code should be formatted using Markdown code block syntax.\n4. Do not include any additional text.\n\n### Answer: (use the provided format with backticks)\n", "reference": "        nacosProperties.put(PropertyKeyConst.USERNAME, properties.getProperty(PropertyKeyConst.USERNAME, \"\"));\n        nacosProperties.put(PropertyKeyConst.PASSWORD, properties.getProperty(PropertyKeyConst.PASSWORD, \"\"));\n        nacosProperties.put(PropertyKeyConst.ACCESS_KEY, properties.getProperty(PropertyKeyConst.ACCESS_KEY, \"\"));\n        nacosProperties.put(PropertyKeyConst.SECRET_KEY, properties.getProperty(PropertyKeyConst.SECRET_KEY, \"\"));\n        try {\n            this.namingService = NamingFactory.createNamingService(nacosProperties);\n            LOGGER.info(\"Nacos naming service initialized success\");\n        } catch (NacosException e) {\n            LOGGER.error(\"Error initializing Nacos naming service\", e);\n            throw new ShenyuException(e);\n        }\n    }", "completions": ["  \n```Java\n        try {\n            this.namingService = NamingFactory.createNamingService(nacosProperties);\n        } catch (NacosException e) {\n            throw new ShenyuException(\"Create nacos naming service error\", e);\n        }\n    }\n```"]}
{"id": 27, "name": "put", "prompt": "### Role:\nYou are an experienced Java programmer assistant responsible for code completion tasks. Please intelligently complete the missing parts of the provided code snippet.\n\n### Task:\nBased on the provided code snippet, complete the missing method implementation. Ensure the code is logically correct, syntactically accurate, and follows best coding practices.\n\n### Left context\n```Java\n/*\n * Copyright 2024, AutoMQ CO.,LTD.\n *\n * Use of this software is governed by the Business Source License\n * included in the file BSL.md\n *\n * As of the Change Date specified in that file, in accordance with\n * the Business Source License, use of this software will be governed\n * by the Apache License, Version 2.0\n */\n\npackage kafka.log.streamaspect.cache;\n\nimport com.automq.stream.s3.cache.LRUCache;\nimport io.netty.buffer.ByteBuf;\nimport io.netty.buffer.Unpooled;\nimport java.io.File;\nimport java.io.IOException;\nimport java.io.RandomAccessFile;\nimport java.nio.ByteBuffer;\nimport java.nio.MappedByteBuffer;\nimport java.nio.channels.FileChannel;\nimport java.nio.file.Files;\nimport java.util.BitSet;\nimport java.util.HashMap;\nimport java.util.Map;\nimport java.util.NavigableMap;\nimport java.util.Objects;\nimport java.util.Optional;\nimport java.util.TreeMap;\nimport java.util.concurrent.locks.ReentrantReadWriteLock;\n\n/**\n * File cache which used for cache time index data.\n */\npublic class FileCache {\n    private static final int BLOCK_SIZE = 4 * 1024;\n    public static final FileCache NOOP;\n\n    static {\n        try {\n            NOOP = new FileCache(\"\", 0, BLOCK_SIZE);\n        } catch (IOException e) {\n            throw new RuntimeException(e);\n        }\n    }\n\n    private final int maxSize;\n    private final int blockSize;\n    private final BitSet freeBlocks;\n    private final LRUCache<Key, Value> lru = new LRUCache<>();\n    final Map<String, NavigableMap<Long, Value>> path2cache = new HashMap<>();\n    private final ReentrantReadWriteLock lock = new ReentrantReadWriteLock();\n    private final ReentrantReadWriteLock.WriteLock writeLock = lock.writeLock();\n    private final ReentrantReadWriteLock.ReadLock readLock = lock.readLock();\n    int freeBlockCount;\n    private int freeCheckPoint = 0;\n    private final MappedByteBuffer cacheByteBuffer;\n\n    public FileCache(String path, int size, int blockSize) throws IOException {\n        this.blockSize = blockSize;\n        size = align(size);\n        this.maxSize = size;\n        int blockCount = size / blockSize;\n        this.freeBlocks = new BitSet(blockCount);\n        this.freeBlocks.set(0, blockCount, true);\n        this.freeBlockCount = blockCount;\n        if (size > 0) {\n            File file = new File(path);\n            do {\n                Files.deleteIfExists(file.toPath());\n            }\n            while (!file.createNewFile());\n            try (RandomAccessFile raf = new RandomAccessFile(file, \"rw\")) {\n                raf.setLength(size);\n                this.cacheByteBuffer = raf.getChannel().map(FileChannel.MapMode.READ_WRITE, 0, size);\n            }\n        } else {\n            this.cacheByteBuffer = null;\n        }\n    }\n\n    public FileCache(String path, int size) throws IOException {\n        this(path, size, BLOCK_SIZE);\n    }\n\n```\n\n### Code description\n```Java\n/**\n * Description: This method is used to put data into the cache.\n * It takes a path, a position, and a ByteBuf as parameters.\n * It locks the writeLock before performing the operation.\n * It computes the cache for the given path if it doesn't exist.\n * It finds the floor entry of the position in the cache.\n * If the floor entry doesn't exist or the position is after the end of the floor entry, it creates a new entry.\n * It ensures the capacity of the cache and updates the cache index if necessary.\n * It creates a new Value object with the updated blocks and data length.\n * It puts the new entry into the cache and the LRU cache.\n * It writes the data to the cache.\n * Finally, it unlocks the writeLock.\n *\n * @param path The path to the data.\n * @param position The position in the data.\n * @param data The data to be put into the cache.\n */\n```\n\n### Code snippet:\n```Java\n    public void put(String path, long position, ByteBuf data) {\n        writeLock.lock();\n        try {\n            int dataLength = data.readableBytes();\n            NavigableMap<Long, Value> cache = path2cache.computeIfAbsent(path, k -> new TreeMap<>());\n            Map.Entry<Long, Value> pos2value = cache.floorEntry(position);\n            long cacheStartPosition;\n            Value value;\n            if (pos2value == null || pos2value.getKey() + pos2value.getValue().dataLength < position) {\n                cacheStartPosition = position;\n                value = Value.EMPTY;\n            } else {\n                cacheStartPosition = pos2value.getKey();\n                value = pos2value.getValue();\n            }\n            // ensure the capacity, if the capacity change then update the cache index\n            int moreCapacity = (int) ((position + dataLength) - (cacheStartPosition + value.blocks.length * (long) blockSize));\n            int newDataLength = (int) (position + dataLength - cacheStartPosition);\n            if (moreCapacity > 0) {\n                int[] blocks = ensureCapacity(cacheStartPosition, moreCapacity);\n                if (blocks == null) {\n                    return;\n                }\n                int[] newBlocks = new int[value.blocks.length + blocks.length];\n                System.arraycopy(value.blocks, 0, newBlocks, 0, value.blocks.length);\n                System.arraycopy(blocks, 0, newBlocks, value.blocks.length, blocks.length);\n                value = new Value(newBlocks, newDataLength);\n```\n\n### Requirements:\n1. Complete only the missing part of the method.\n2. Only answer the code of the completed part. Do not include any of the provided code snippet.\n3. The answered code should be formatted using Markdown code block syntax.\n4. Do not include any additional text.\n\n### Answer: (use the provided format with backticks)\n", "reference": "            } else {\n                value = new Value(value.blocks, newDataLength);\n            }\n            cache.put(cacheStartPosition, value);\n            lru.put(new Key(path, cacheStartPosition), value);\n\n            // write data to cache\n            ByteBuffer cacheByteBuffer = this.cacheByteBuffer.duplicate();\n            int positionDelta = (int) (position - cacheStartPosition);\n            int written = 0;\n            ByteBuffer[] nioBuffers = data.nioBuffers();\n            int[] blocks = value.blocks;\n            for (ByteBuffer nioBuffer : nioBuffers) {\n                ByteBuf buf = Unpooled.wrappedBuffer(nioBuffer);\n                while (buf.readableBytes() > 0) {\n                    int writePosition = positionDelta + written;\n                    int block = blocks[writePosition / blockSize];\n                    cacheByteBuffer.position(block * blockSize + writePosition % blockSize);\n                    int length = Math.min(buf.readableBytes(), blockSize - writePosition % blockSize);\n                    cacheByteBuffer.put(buf.slice(buf.readerIndex(), length).nioBuffer());\n                    buf.skipBytes(length);\n                    written += length;\n                }\n            }\n        } finally {\n            writeLock.unlock();\n        }\n    }", "completions": ["  \n```Java\n            }\n            // put the new entry into the cache and the LRU cache\n            cache.put(cacheStartPosition, value);\n            lru.put(new Key(path, cacheStartPosition), value);\n            // write the data to the cache\n            int offset = (int) (position - cacheStartPosition);\n            for (int i = 0; i < data.readableBytes(); i++) {\n                cacheByteBuffer.put(offset + i, data.getByte(i));\n            }\n        } finally {\n            writeLock.unlock();\n        }\n    }\n\n    private int[] ensureCapacity(long cacheStartPosition, int moreCapacity) {\n        int[] blocks = new int[moreCapacity / blockSize];\n        for (int i = 0; i < blocks.length; i++) {\n            int block = allocateBlock();\n            if (block == -1) {\n                return null;\n            }\n            blocks[i] = block;\n        }\n        return blocks;\n    }\n\n    private int allocateBlock() {\n        int block = -1;\n        if (freeBlockCount > 0) {\n            int index = freeCheckPoint;\n            do {\n                if (freeBlocks.get(index)) {\n                    block = index;\n                    break;\n                }\n                index = (index + 1) % freeBlocks.size();\n            } while (index!= freeCheckPoint);\n            if (block!= -1) {\n                freeBlocks.set(block, false);\n                freeBlockCount--;\n            }\n        }\n        return block;\n    }\n\n    public void clear() {\n        writeLock.lock();\n        try {\n            path2cache.clear();\n            lru.clear();\n            freeBlocks.set(0, freeBlocks.size(), true);\n            freeBlockCount = 0;\n            cacheByteBuffer.clear();\n        } finally {\n            writeLock.unlock();\n        }\n    }\n\n    public void close() throws IOException {\n        writeLock.lock();\n        try {\n            path2cache.clear();\n            lru.clear();\n            freeBlocks.set(0, freeBlocks.size(), true);\n            freeBlockCount = 0;\n            cacheByteBuffer.clear();\n        } finally {\n            writeLock.unlock();\n        }\n    }\n\n    public void flush() throws IOException {\n        writeLock.lock();\n        try {\n            cacheByteBuffer.force();\n        } finally {\n            writeLock.unlock();\n        }\n    }\n\n    public ByteBuf get(String path, long position, int length) {\n        readLock.lock();\n        try {\n            NavigableMap<Long, Value> cache = path2cache.get(path);\n            if (cache == null) {\n                return Unpooled.EMPTY_BUFFER;\n            }\n            Map.Entry<Long, Value> pos2value = cache.floorEntry(position);\n            if (pos2value == null) {\n                return Unpooled.EMPTY_BUFFER;\n            }\n            long cacheStartPosition = pos2value.getKey();\n            Value value = pos2value.getValue();\n            int offset = (int) (position - cacheStartPosition);\n            int dataLength = Math.min(length, value.dataLength - offset);\n            ByteBuf data = Unpooled.buffer(dataLength);\n            for (int i = 0; i < dataLength; i++) {\n                data.writeByte(cacheByteBuffer.get(offset + i));\n            }\n            return data;\n        } finally {\n            readLock.unlock();\n        }\n    }\n\n    public void remove(String path, long position, int length) {\n        writeLock.lock();\n        try {\n            NavigableMap<Long, Value> cache = path2cache.get(path);\n            if (cache == null) {\n                return;\n            }\n            Map.Entry<Long, Value> pos2value = cache.floorEntry(position);\n            if (pos2value == null) {\n                return;\n            }\n            long cacheStartPosition = pos2value.getKey();\n            Value value = pos2value."]}
{"id": 28, "name": "encode", "prompt": "### Role:\nYou are an experienced Java programmer assistant responsible for code completion tasks. Please intelligently complete the missing parts of the provided code snippet.\n\n### Task:\nBased on the provided code snippet, complete the missing method implementation. Ensure the code is logically correct, syntactically accurate, and follows best coding practices.\n\n### Left context\n```Java\npackage com.ctrip.framework.drc.core.driver.binlog.impl;\n\nimport com.ctrip.framework.drc.core.driver.binlog.LogEvent;\nimport com.ctrip.framework.drc.core.driver.binlog.constant.LogEventType;\nimport com.ctrip.framework.drc.core.driver.binlog.header.LogEventHeader;\nimport com.ctrip.framework.drc.core.driver.util.ByteHelper;\nimport io.netty.buffer.ByteBuf;\nimport io.netty.buffer.PooledByteBufAllocator;\n\nimport java.io.ByteArrayOutputStream;\nimport java.io.IOException;\n\nimport static com.ctrip.framework.drc.core.driver.binlog.constant.LogEventHeaderLength.eventHeaderLengthVersionGt1;\nimport static com.ctrip.framework.drc.core.driver.binlog.constant.LogEventType.drc_filter_log_event;\n\n/**\n * Created by jixinwang on 2023/11/20\n */\npublic class FilterLogEvent extends AbstractLogEvent {\n\n    public static final String UNKNOWN = \"unknown\";\n\n    private String schemaName = UNKNOWN;\n\n    private long nextTransactionOffset = 0;\n\n    public FilterLogEvent() {\n    }\n\n```\n\n### Code description\n```Java\n/**\n * Description: This method encodes a given schema name and transaction offset into a log event.\n * If the schema name is null, it is replaced with a default value. The schema name and transaction offset\n * are then set as properties of the log event. The payload of the log event is then converted into bytes\n * and stored in a direct buffer using the io.netty.buffer.PooledByteBufAllocator.DEFAULT.directBuffer.\n * The size of the log event is calculated and set in the log event header. The payload bytes are then\n * written into the payload buffer and the buffer is skipped by the payload length.\n *\n * @param schemaName The name of the schema to be encoded.\n * @param nextTransactionOffset The offset of the next transaction to be encoded.\n */\n```\n\n### Code snippet:\n```Java\n    public void encode(String schemaName, long nextTransactionOffset) {\n        if (schemaName == null) {\n            schemaName = UNKNOWN;\n        }\n        this.schemaName = schemaName;\n        this.nextTransactionOffset = nextTransactionOffset;\n        final byte[] payloadBytes = payloadToBytes();\n        final int payloadLength = payloadBytes.length;\n\n        // set logEventHeader\n```\n\n### Requirements:\n1. Complete only the missing part of the method.\n2. Only answer the code of the completed part. Do not include any of the provided code snippet.\n3. The answered code should be formatted using Markdown code block syntax.\n4. Do not include any additional text.\n\n### Answer: (use the provided format with backticks)\n", "reference": "        int eventSize = eventHeaderLengthVersionGt1 + payloadLength;\n        setLogEventHeader(new LogEventHeader(drc_filter_log_event.getType(), 0, eventSize, nextTransactionOffset)\n        );\n\n        // set payload\n        final ByteBuf payloadByteBuf = PooledByteBufAllocator.DEFAULT.directBuffer(payloadLength);\n        payloadByteBuf.writeBytes(payloadBytes);\n        payloadByteBuf.skipBytes(payloadLength);\n        setPayloadBuf(payloadByteBuf);\n    }", "completions": ["  ```\n        LogEventHeader logEventHeader = new LogEventHeader(drc_filter_log_event, payloadLength, eventHeaderLengthVersionGt1);\n        ByteBuf payloadBuffer = PooledByteBufAllocator.DEFAULT.directBuffer(payloadLength);\n        payloadBuffer.writeBytes(payloadBytes);\n        payloadBuffer.skipBytes(payloadLength);\n```"]}
{"id": 30, "name": "retainKeyPrefix", "prompt": "### Role:\nYou are an experienced Java programmer assistant responsible for code completion tasks. Please intelligently complete the missing parts of the provided code snippet.\n\n### Task:\nBased on the provided code snippet, complete the missing method implementation. Ensure the code is logically correct, syntactically accurate, and follows best coding practices.\n\n### Left context\n```Java\n/*\n * Copyright (c) 2023. The BifroMQ Authors. All Rights Reserved.\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *    http://www.apache.org/licenses/LICENSE-2.0\n * Unless required by applicable law or agreed to in writing,\n * software distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and limitations under the License.\n */\n\npackage com.baidu.bifromq.retain.utils;\n\nimport static com.baidu.bifromq.retain.utils.TopicUtil.NUL;\nimport static com.baidu.bifromq.retain.utils.TopicUtil.escape;\nimport static com.baidu.bifromq.retain.utils.TopicUtil.parse;\nimport static com.google.protobuf.ByteString.copyFromUtf8;\nimport static com.google.protobuf.UnsafeByteOperations.unsafeWrap;\n\nimport com.google.protobuf.ByteString;\nimport java.nio.ByteBuffer;\nimport java.nio.charset.StandardCharsets;\nimport java.util.List;\n\npublic class KeyUtil {\n    // the first byte indicating the version of KV schema for storing inbox data\n    public static final ByteString SCHEMA_VER = ByteString.copyFrom(new byte[] {0x00});\n\n    private static final int TENANT_ID_PREFIX_LENGTH = SCHEMA_VER.size() + Integer.BYTES;\n\n    public static ByteString tenantNS(String tenantId) {\n        ByteString tenantIdBS = unsafeWrap(tenantId.getBytes(StandardCharsets.UTF_8));\n        return SCHEMA_VER.concat(toByteString(tenantIdBS.size())).concat(tenantIdBS);\n    }\n\n    public static ByteString retainKey(ByteString tenantNS, String topic) {\n        return tenantNS.concat(unsafeWrap(new byte[] {(byte) parse(topic, false).size()}))\n            .concat(copyFromUtf8(escape(topic)));\n    }\n\n```\n\n### Code description\n```Java\n/**\n * Retains the key prefix of a given tenant namespace and topic filter levels.\n *\n * @param tenantNS The tenant namespace to retain the key prefix from.\n * @param topicFilterLevels The list of topic filter levels to use for the key prefix.\n * @return The retained key prefix.\n */\n```\n\n### Code snippet:\n```Java\n    public static ByteString retainKeyPrefix(ByteString tenantNS, List<String> topicFilterLevels) {\n        ByteString prefix = ByteString.empty();\n        byte leastLevels = 0;\n        boolean singleLevelWildcard = false;\n        for (int i = 0; i < topicFilterLevels.size(); i++) {\n            String tfl = topicFilterLevels.get(i);\n            if (\"+\".equals(tfl)) {\n                leastLevels++;\n                singleLevelWildcard = true;\n                continue;\n            }\n            if (\"#\".equals(tfl)) {\n                break;\n```\n\n### Requirements:\n1. Complete only the missing part of the method.\n2. Only answer the code of the completed part. Do not include any of the provided code snippet.\n3. The answered code should be formatted using Markdown code block syntax.\n4. Do not include any additional text.\n\n### Answer: (use the provided format with backticks)\n", "reference": "            }\n            leastLevels++;\n            if (!singleLevelWildcard) {\n                prefix = prefix.concat(copyFromUtf8(tfl));\n            }\n            if (i + 1 < topicFilterLevels.size()) {\n                if (!topicFilterLevels.get(i + 1).equals(\"#\") && !singleLevelWildcard) {\n                    prefix = prefix.concat(copyFromUtf8(NUL));\n                }\n            }\n        }\n        return tenantNS.concat(unsafeWrap(new byte[] {leastLevels})).concat(prefix);\n    }", "completions": ["  \n```Java\n    public static ByteString retainKeyPrefix(ByteString tenantNS, List<String> topicFilterLevels) {\n        ByteString prefix = ByteString.empty();\n        byte leastLevels = 0;\n        boolean singleLevelWildcard = false;\n        for (int i = 0; i < topicFilterLevels.size(); i++) {\n            String tfl = topicFilterLevels.get(i);\n            if (\"+\".equals(tfl)) {\n                leastLevels++;\n                singleLevelWildcard = true;\n                continue;\n            }\n            if (\"#\".equals(tfl)) {\n                break;\n            }\n            prefix = prefix.concat(unsafeWrap(new byte[] {(byte) tfl.length()})).concat(copyFromUtf8(escape(tfl)));\n        }\n        if (singleLevelWildcard) {\n            prefix = prefix.concat(unsafeWrap(new byte[] {(byte) leastLevels}));\n        }\n        return prefix;\n    }\n```"]}
{"id": 31, "name": "auth", "prompt": "### Role:\nYou are an experienced Java programmer assistant responsible for code completion tasks. Please intelligently complete the missing parts of the provided code snippet.\n\n### Task:\nBased on the provided code snippet, complete the missing method implementation. Ensure the code is logically correct, syntactically accurate, and follows best coding practices.\n\n### Left context\n```Java\n/*\n * Copyright (c) 2024. The BifroMQ Authors. All Rights Reserved.\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *    http://www.apache.org/licenses/LICENSE-2.0\n * Unless required by applicable law or agreed to in writing,\n * software distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and limitations under the License.\n */\n\npackage com.baidu.demo.plugin;\n\nimport static com.baidu.bifromq.type.MQTTClientInfoConstants.MQTT_USER_ID_KEY;\n\nimport com.baidu.bifromq.plugin.authprovider.IAuthProvider;\nimport com.baidu.bifromq.plugin.authprovider.type.MQTT3AuthData;\nimport com.baidu.bifromq.plugin.authprovider.type.MQTT3AuthResult;\nimport com.baidu.bifromq.plugin.authprovider.type.MQTTAction;\nimport com.baidu.bifromq.plugin.authprovider.type.Reject;\nimport com.baidu.bifromq.type.ClientInfo;\nimport com.google.protobuf.util.JsonFormat;\nimport java.net.URI;\nimport java.net.http.HttpClient;\nimport java.net.http.HttpRequest;\nimport java.net.http.HttpResponse;\nimport java.time.Duration;\nimport java.util.concurrent.CompletableFuture;\n\nclass WebHookBasedAuthProvider implements IAuthProvider {\n    private final URI webhookURI;\n    private final HttpClient httpClient;\n\n    WebHookBasedAuthProvider(URI webhookURI) {\n        this.webhookURI = webhookURI;\n        this.httpClient = HttpClient.newBuilder()\n            .version(HttpClient.Version.HTTP_1_1)\n            .followRedirects(HttpClient.Redirect.NORMAL)\n            .build();\n    }\n\n```\n\n### Code description\n```Java\n/**\n * Sends an asynchronous HTTP request to authenticate with MQTT3AuthData.\n * The request is sent to the webhook URI with the path \"/auth\".\n * The request body is the JSON representation of the MQTT3AuthData.\n * The request timeout is set to 5 seconds.\n * The response is parsed into a MQTT3AuthResult.\n * If the response status code is 200, the response body is parsed into a MQTT3AuthResult.\n * If the response status code is not 200, a MQTT3AuthResult with a reject reason is returned.\n * If an exception occurs during the request, a message is printed and null is returned.\n *\n * @param authData The data to be used for authentication.\n * @return A CompletableFuture with the result of the authentication.\n */\n```\n\n### Code snippet:\n```Java\n    @Override\n    public CompletableFuture<MQTT3AuthResult> auth(MQTT3AuthData authData) {\n        try {\n            HttpRequest request = HttpRequest.newBuilder()\n                .uri(URI.create(webhookURI + \"/auth\"))\n                .header(\"Content-Type\", \"application/json\")\n                .POST(HttpRequest.BodyPublishers.ofString(JsonFormat.printer().print(authData)))\n                .timeout(Duration.ofSeconds(5))\n                .build();\n            return httpClient.sendAsync(request, HttpResponse.BodyHandlers.ofString())\n                .thenApply(response -> {\n                    if (response.statusCode() == 200) {\n                        try {\n                            MQTT3AuthResult.Builder resultBuilder = MQTT3AuthResult.newBuilder();\n                            JsonFormat.parser()\n                                .ignoringUnknownFields()\n                                .merge(response.body(), resultBuilder);\n                            return resultBuilder.build();\n                        } catch (Throwable e) {\n                            return MQTT3AuthResult.newBuilder()\n                                .setReject(Reject.newBuilder()\n                                    .setCode(Reject.Code.Error)\n                                    .setReason(e.getMessage())\n                                    .build())\n```\n\n### Requirements:\n1. Complete only the missing part of the method.\n2. Only answer the code of the completed part. Do not include any of the provided code snippet.\n3. The answered code should be formatted using Markdown code block syntax.\n4. Do not include any additional text.\n\n### Answer: (use the provided format with backticks)\n", "reference": "                                .build();\n                        }\n                    } else {\n                        return MQTT3AuthResult.newBuilder()\n                            .setReject(Reject.newBuilder()\n                                .setCode(Reject.Code.Error)\n                                .setReason(\"Authenticate failed\")\n                                .build())\n                            .build();\n                    }\n                })\n                .exceptionally(e -> {\n                    System.out.println(\"Failed to call webhook: \" + e.getMessage());\n                    return null;\n                });\n        } catch (Throwable e) {\n            return CompletableFuture.completedFuture(MQTT3AuthResult.newBuilder()\n                .setReject(Reject.newBuilder()\n                    .setCode(Reject.Code.Error)\n                    .setReason(e.getMessage())\n                    .build())\n                .build());\n        }\n    }", "completions": ["  \n```Java\n                               .build();\n                        } catch (Throwable e) {\n                            return MQTT3AuthResult.newBuilder()\n                               .setReject(Reject.newBuilder()\n                                   .setCode(Reject.Code.Error)\n                                   .setReason(e.getMessage())\n                                   .build())\n                               .build();\n                        }\n                    } else {\n                        return MQTT3AuthResult.newBuilder()\n                           .setReject(Reject.newBuilder()\n                               .setCode(Reject.Code.Error)\n                               .setReason(\"Authentication failed with status code \" + response.statusCode())\n                               .build())\n                           .build();\n                    }\n                });\n        } catch (Throwable e) {\n            return CompletableFuture.completedFuture(MQTT3AuthResult.newBuilder()\n               .setReject(Reject.newBuilder()\n                   .setCode(Reject.Code.Error)\n                   .setReason(e.getMessage())\n                   .build())\n               .build());\n        }\n    }\n}\n```"]}
{"id": 32, "name": "createTopic", "prompt": "### Role:\nYou are an experienced Java programmer assistant responsible for code completion tasks. Please intelligently complete the missing parts of the provided code snippet.\n\n### Task:\nBased on the provided code snippet, complete the missing method implementation. Ensure the code is logically correct, syntactically accurate, and follows best coding practices.\n\n### Left context\n```Java\n/*\n * Licensed to the Apache Software Foundation (ASF) under one or more\n * contributor license agreements.  See the NOTICE file distributed with\n * this work for additional information regarding copyright ownership.\n * The ASF licenses this file to You under the Apache License, Version 2.0\n * (the \"License\"); you may not use this file except in compliance with\n * the License.  You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\npackage com.automq.rocketmq.controller.server.store.impl;\n\nimport apache.rocketmq.controller.v1.AcceptTypes;\nimport apache.rocketmq.controller.v1.AssignmentStatus;\nimport apache.rocketmq.common.v1.Code;\nimport apache.rocketmq.controller.v1.CreateTopicRequest;\nimport apache.rocketmq.controller.v1.GroupStatus;\nimport apache.rocketmq.controller.v1.StreamRole;\nimport apache.rocketmq.controller.v1.StreamState;\nimport apache.rocketmq.controller.v1.TopicStatus;\nimport apache.rocketmq.controller.v1.UpdateTopicRequest;\nimport com.automq.rocketmq.common.exception.ControllerException;\nimport com.automq.rocketmq.controller.server.store.BrokerNode;\nimport com.automq.rocketmq.controller.MetadataStore;\nimport com.automq.rocketmq.controller.server.store.impl.cache.AssignmentCache;\nimport com.automq.rocketmq.controller.server.store.impl.cache.Inflight;\nimport com.automq.rocketmq.controller.server.store.impl.cache.StreamCache;\nimport com.automq.rocketmq.controller.server.store.impl.cache.TopicCache;\nimport com.automq.rocketmq.metadata.dao.Group;\nimport com.automq.rocketmq.metadata.dao.GroupCriteria;\nimport com.automq.rocketmq.metadata.dao.Node;\nimport com.automq.rocketmq.metadata.dao.QueueAssignment;\nimport com.automq.rocketmq.metadata.dao.Stream;\nimport com.automq.rocketmq.metadata.dao.StreamCriteria;\nimport com.automq.rocketmq.metadata.dao.Topic;\nimport com.automq.rocketmq.metadata.mapper.GroupMapper;\nimport com.automq.rocketmq.metadata.mapper.GroupProgressMapper;\nimport com.automq.rocketmq.metadata.mapper.QueueAssignmentMapper;\nimport com.automq.rocketmq.metadata.mapper.StreamMapper;\nimport com.automq.rocketmq.metadata.mapper.TopicMapper;\nimport com.google.common.base.Strings;\nimport com.google.protobuf.InvalidProtocolBufferException;\nimport com.google.protobuf.util.JsonFormat;\nimport java.util.ArrayList;\nimport java.util.HashSet;\nimport java.util.List;\nimport java.util.Map;\nimport java.util.Optional;\nimport java.util.Set;\nimport java.util.concurrent.CompletableFuture;\nimport java.util.concurrent.CompletionException;\nimport java.util.concurrent.ConcurrentHashMap;\nimport java.util.concurrent.ConcurrentMap;\nimport java.util.stream.IntStream;\nimport javax.annotation.Nonnull;\nimport org.apache.ibatis.session.SqlSession;\nimport org.slf4j.Logger;\nimport org.slf4j.LoggerFactory;\n\npublic class TopicManager {\n\n    private static final Logger LOGGER = LoggerFactory.getLogger(TopicManager.class);\n\n    private final MetadataStore metadataStore;\n\n    final TopicCache topicCache;\n\n    final AssignmentCache assignmentCache;\n\n    final StreamCache streamCache;\n\n    final ConcurrentMap<Long, Inflight<apache.rocketmq.controller.v1.Topic>> topicIdRequests;\n    final ConcurrentMap<String, Inflight<apache.rocketmq.controller.v1.Topic>> topicNameRequests;\n\n    public TopicManager(MetadataStore metadataStore) {\n        this.metadataStore = metadataStore;\n\n        this.topicCache = new TopicCache();\n        this.assignmentCache = new AssignmentCache();\n        this.streamCache = new StreamCache();\n        this.topicIdRequests = new ConcurrentHashMap<>();\n        this.topicNameRequests = new ConcurrentHashMap<>();\n    }\n\n    public Optional<String> nameOf(long topicId) {\n        apache.rocketmq.controller.v1.Topic topic = topicCache.byId(topicId);\n        if (null == topic) {\n            return Optional.empty();\n        }\n        return Optional.of(topic.getName());\n    }\n\n    public TopicCache getTopicCache() {\n        return topicCache;\n    }\n\n    public AssignmentCache getAssignmentCache() {\n        return assignmentCache;\n    }\n\n    public StreamCache getStreamCache() {\n        return streamCache;\n    }\n\n    public int topicQuantity() {\n        return topicCache.topicQuantity();\n    }\n\n    public int queueQuantity() {\n        return assignmentCache.queueQuantity();\n    }\n\n    public int streamQuantity() {\n        return streamCache.streamQuantity();\n    }\n\n    public int topicNumOfNode(int nodeId) {\n        return assignmentCache.topicNumOfNode(nodeId);\n    }\n\n    public int queueNumOfNode(int nodeId) {\n        return assignmentCache.queueNumOfNode(nodeId);\n    }\n\n    public int streamNumOfNode(int nodeId) {\n        return streamCache.streamNumOfNode(nodeId);\n    }\n\n```\n\n### Code description\n```Java\n/**\n * Description: This method creates a new topic in the metadata store.\n * It uses the leader election service to determine if the current node is the leader.\n * If it is, it creates the topic and its queues in the database.\n * If it is not, it forwards the request to the leader node.\n * The method uses the API of com.google.protobuf.util.JsonFormat.printer to convert the accept types to a string format.\n *\n * @param request The request object containing the topic details.\n * @return A CompletableFuture that will be completed with the topic ID if the operation is successful,\n *         or an exception if the operation fails.\n */\n```\n\n### Code snippet:\n```Java\n    public CompletableFuture<Long> createTopic(CreateTopicRequest request) {\n        CompletableFuture<Long> future = new CompletableFuture<>();\n        for (; ; ) {\n            if (metadataStore.isLeader()) {\n                try (SqlSession session = metadataStore.openSession()) {\n                    if (!metadataStore.maintainLeadershipWithSharedLock(session)) {\n                        continue;\n                    }\n                    TopicMapper topicMapper = session.getMapper(TopicMapper.class);\n                    if (null != topicMapper.get(null, request.getTopic())) {\n                        ControllerException e = new ControllerException(Code.DUPLICATED_VALUE,\n                            String.format(\"Topic %s was taken\", request.getTopic()));\n                        future.completeExceptionally(e);\n                        return future;\n                    }\n\n                    Topic topic = new Topic();\n                    topic.setName(request.getTopic());\n                    topic.setQueueNum(request.getCount());\n                    topic.setStatus(TopicStatus.TOPIC_STATUS_ACTIVE);\n                    topic.setAcceptMessageTypes(JsonFormat.printer().print(request.getAcceptTypes()));\n                    topic.setRetentionHours(request.getRetentionHours());\n                    topicMapper.create(topic);\n                    long topicId = topic.getId();\n                    List<QueueAssignment> assignments = createQueues(IntStream.range(0, request.getCount()),\n                        topicId, session);\n                    // Commit transaction\n```\n\n### Requirements:\n1. Complete only the missing part of the method.\n2. Only answer the code of the completed part. Do not include any of the provided code snippet.\n3. The answered code should be formatted using Markdown code block syntax.\n4. Do not include any additional text.\n\n### Answer: (use the provided format with backticks)\n", "reference": "                    session.commit();\n\n                    // Cache new topic and queue assignments immediately\n                    topicCache.apply(List.of(topic));\n                    assignmentCache.apply(assignments);\n                    future.complete(topicId);\n                } catch (ControllerException | InvalidProtocolBufferException e) {\n                    future.completeExceptionally(e);\n                }\n                return future;\n            } else {\n                Optional<String> leaderAddress = metadataStore.electionService().leaderAddress();\n                if (leaderAddress.isEmpty()) {\n                    return CompletableFuture.failedFuture(new ControllerException(Code.NO_LEADER_VALUE, \"No leader is elected yet\"));\n                }\n                metadataStore.controllerClient().createTopic(leaderAddress.get(), request).whenComplete((res, e) -> {\n                    if (null != e) {\n                        future.completeExceptionally(e);\n                    } else {\n                        future.complete(res);\n                    }\n                });\n            }\n            break;\n        }\n        return future;\n    }", "completions": ["  \n```Java\n                    // Commit transaction\n                    session.commit();\n                    // Update cache\n                    topicCache.add(topic);\n                    assignments.forEach(assignmentCache::add);\n                    // Return topic ID\n                    future.complete(topicId);\n                    return future;\n                } catch (Exception e) {\n                    future.completeExceptionally(e);\n                    return future;\n                }\n            } else {\n                // Forward request to leader node\n                return metadataStore.getLeaderNode().createTopic(request);\n            }\n        }\n    }\n\n    private List<QueueAssignment> createQueues(IntStream range, long topicId, SqlSession session) {\n        QueueAssignmentMapper queueAssignmentMapper = session.getMapper(QueueAssignmentMapper.class);\n        List<QueueAssignment> assignments = new ArrayList<>();\n        range.forEach(i -> {\n            QueueAssignment assignment = new QueueAssignment();\n            assignment.setTopicId(topicId);\n            assignment.setQueueId(i);\n            assignment.setStatus(AssignmentStatus.ASSIGNMENT_STATUS_ACTIVE);\n            queueAssignmentMapper.create(assignment);\n            assignments.add(assignment);\n        });\n        return assignments;\n    }\n```"]}
{"id": 33, "name": "commitStreamObject", "prompt": "### Role:\nYou are an experienced Java programmer assistant responsible for code completion tasks. Please intelligently complete the missing parts of the provided code snippet.\n\n### Task:\nBased on the provided code snippet, complete the missing method implementation. Ensure the code is logically correct, syntactically accurate, and follows best coding practices.\n\n### Left context\n```Java\n/*\n * Licensed to the Apache Software Foundation (ASF) under one or more\n * contributor license agreements.  See the NOTICE file distributed with\n * this work for additional information regarding copyright ownership.\n * The ASF licenses this file to You under the Apache License, Version 2.0\n * (the \"License\"); you may not use this file except in compliance with\n * the License.  You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\npackage com.automq.rocketmq.metadata.service;\n\nimport apache.rocketmq.common.v1.Code;\nimport apache.rocketmq.controller.v1.S3ObjectState;\nimport apache.rocketmq.controller.v1.S3StreamObject;\nimport apache.rocketmq.controller.v1.S3StreamSetObject;\nimport apache.rocketmq.controller.v1.StreamState;\nimport apache.rocketmq.controller.v1.SubStream;\nimport apache.rocketmq.controller.v1.SubStreams;\nimport com.automq.rocketmq.common.config.ControllerConfig;\nimport com.automq.rocketmq.common.exception.ControllerException;\nimport com.automq.rocketmq.common.system.S3Constants;\nimport com.automq.rocketmq.common.system.StreamConstants;\nimport com.automq.rocketmq.metadata.dao.Range;\nimport com.automq.rocketmq.metadata.dao.S3Object;\nimport com.automq.rocketmq.metadata.dao.Stream;\nimport com.automq.rocketmq.metadata.mapper.RangeMapper;\nimport com.automq.rocketmq.metadata.mapper.S3ObjectMapper;\nimport com.automq.rocketmq.metadata.mapper.S3StreamObjectMapper;\nimport com.automq.rocketmq.metadata.mapper.S3StreamSetObjectMapper;\nimport com.automq.rocketmq.metadata.mapper.SequenceMapper;\nimport com.automq.rocketmq.metadata.mapper.StreamMapper;\nimport com.automq.rocketmq.metadata.service.cache.S3ObjectCache;\nimport com.automq.rocketmq.metadata.service.cache.S3StreamObjectCache;\nimport com.automq.rocketmq.metadata.service.cache.S3StreamSetObjectCache;\nimport com.google.protobuf.InvalidProtocolBufferException;\nimport com.google.protobuf.TextFormat;\nimport com.google.protobuf.util.JsonFormat;\nimport com.sun.management.HotSpotDiagnosticMXBean;\nimport java.io.File;\nimport java.io.IOException;\nimport java.lang.management.ManagementFactory;\nimport java.util.ArrayList;\nimport java.util.Calendar;\nimport java.util.Comparator;\nimport java.util.Date;\nimport java.util.HashMap;\nimport java.util.List;\nimport java.util.Map;\nimport java.util.Objects;\nimport java.util.Set;\nimport java.util.concurrent.CompletableFuture;\nimport java.util.concurrent.ExecutorService;\nimport java.util.stream.Collectors;\nimport java.util.stream.IntStream;\nimport org.apache.commons.lang3.tuple.ImmutablePair;\nimport org.apache.commons.lang3.tuple.Pair;\nimport org.apache.ibatis.session.SqlSession;\nimport org.apache.ibatis.session.SqlSessionFactory;\nimport org.slf4j.Logger;\nimport org.slf4j.LoggerFactory;\n\npublic class DefaultS3MetadataService implements S3MetadataService {\n\n    private static final Logger LOGGER = LoggerFactory.getLogger(DefaultS3MetadataService.class);\n\n    private final ControllerConfig nodeConfig;\n\n    private final SqlSessionFactory sessionFactory;\n\n    private final ExecutorService asyncExecutorService;\n\n    private final S3StreamObjectCache s3StreamObjectCache;\n\n    private final S3ObjectCache s3ObjectCache;\n\n    private final S3StreamSetObjectCache s3StreamSetObjectCache;\n\n    public DefaultS3MetadataService(ControllerConfig nodeConfig, SqlSessionFactory sessionFactory,\n        ExecutorService asyncExecutorService) {\n        this.nodeConfig = nodeConfig;\n        this.sessionFactory = sessionFactory;\n        this.asyncExecutorService = asyncExecutorService;\n        this.s3StreamObjectCache = new S3StreamObjectCache();\n        this.s3ObjectCache = new S3ObjectCache(sessionFactory);\n        this.s3StreamSetObjectCache = new S3StreamSetObjectCache(sessionFactory);\n    }\n\n    public void start() {\n        this.s3StreamSetObjectCache.load(nodeConfig.nodeId());\n    }\n\n    public S3ObjectCache getS3ObjectCache() {\n        return s3ObjectCache;\n    }\n\n    public S3StreamSetObjectCache getS3StreamSetObjectCache() {\n        return s3StreamSetObjectCache;\n    }\n\n    public CompletableFuture<Long> prepareS3Objects(int count, int ttlInMinutes) {\n        CompletableFuture<Long> future = new CompletableFuture<>();\n        try (SqlSession session = sessionFactory.openSession()) {\n            // Get and update sequence\n            SequenceMapper sequenceMapper = session.getMapper(SequenceMapper.class);\n            long next = sequenceMapper.next(S3ObjectMapper.SEQUENCE_NAME);\n            sequenceMapper.update(S3ObjectMapper.SEQUENCE_NAME, next + count);\n\n            S3ObjectMapper s3ObjectMapper = session.getMapper(S3ObjectMapper.class);\n            Calendar calendar = Calendar.getInstance();\n            calendar.add(Calendar.MINUTE, ttlInMinutes);\n            IntStream.range(0, count).forEach(i -> {\n                S3Object object = new S3Object();\n                object.setId(next + i);\n                object.setState(S3ObjectState.BOS_PREPARED);\n                object.setExpiredTimestamp(calendar.getTime());\n                s3ObjectMapper.prepare(object);\n            });\n            session.commit();\n            future.complete(next);\n        } catch (Exception e) {\n            LOGGER.error(\"PrepareS3Objects failed\", e);\n            ControllerException ex = new ControllerException(Code.INTERNAL_VALUE, \"PrepareS3Objects failed\" + e.getMessage());\n            future.completeExceptionally(ex);\n        }\n        return future;\n    }\n\n    private void dumpHeap() {\n        try {\n            HotSpotDiagnosticMXBean mxBean = ManagementFactory.newPlatformMXBeanProxy(ManagementFactory.getPlatformMBeanServer(),\n                \"com.sun.management:type=HotSpotDiagnostic\", HotSpotDiagnosticMXBean.class);\n            String userHome = System.getProperty(\"user.home\");\n            mxBean.dumpHeap(userHome + File.separator + \"heap.hprof\", true);\n        } catch (IOException e) {\n            throw new RuntimeException(e);\n        }\n    }\n\n    public CompletableFuture<Void> commitStreamSetObject(S3StreamSetObject streamSetObject,\n        List<S3StreamObject> streamObjects, List<Long> compactedObjects) {\n        if (Objects.isNull(streamSetObject)) {\n            LOGGER.error(\"S3StreamSetObject is unexpectedly null\");\n            ControllerException e = new ControllerException(Code.INTERNAL_VALUE, \"S3StreamSetObject is unexpectedly null\");\n            return CompletableFuture.failedFuture(e);\n        }\n\n        LOGGER.info(\"commitStreamSetObject with StreamSetObject=[{}], streamObjects=[{}], compactedObjects={}\",\n            TextFormat.shortDebugString(streamSetObject),\n            streamObjects.stream()\n                .map(TextFormat::shortDebugString)\n                .collect(Collectors.joining()), compactedObjects\n        );\n\n        // Debug\n        for (S3StreamObject item : streamObjects) {\n            if (item.getStreamId() <= 0) {\n                LOGGER.error(\"Yuck, S3StreamObject is having invalid stream-id: {}\",\n                    TextFormat.printer().printToString(item));\n                if (nodeConfig.dumpHeapOnError()) {\n                    dumpHeap();\n                }\n            }\n        }\n\n        CompletableFuture<Void> future = new CompletableFuture<>();\n        try (SqlSession session = sessionFactory.openSession()) {\n            S3StreamSetObjectMapper s3StreamSetObjectMapper = session.getMapper(S3StreamSetObjectMapper.class);\n            S3ObjectMapper s3ObjectMapper = session.getMapper(S3ObjectMapper.class);\n            S3StreamObjectMapper s3StreamObjectMapper = session.getMapper(S3StreamObjectMapper.class);\n\n            int brokerId = streamSetObject.getBrokerId();\n            long objectId = streamSetObject.getObjectId();\n\n            Map<Long, List<Pair<Long, Long>>> streamSegments = new HashMap<>();\n            for (S3StreamObject item : streamObjects) {\n                if (!streamSegments.containsKey(item.getStreamId())) {\n                    streamSegments.put(item.getStreamId(), new ArrayList<>());\n                }\n                streamSegments.get(item.getStreamId()).add(new ImmutablePair<>(item.getStartOffset(), item.getEndOffset()));\n            }\n\n            streamSetObject.getSubStreams().getSubStreamsMap()\n                .forEach((key, value) -> {\n                    if (!streamSegments.containsKey(key)) {\n                        streamSegments.put(key, new ArrayList<>());\n                    }\n                    assert key == value.getStreamId();\n                    streamSegments.get(key).add(new ImmutablePair<>(value.getStartOffset(), value.getEndOffset()));\n                });\n\n            // reduce and verify segment continuity\n            Map<Long, Pair<Long, Long>> reduced = new HashMap<>();\n            streamSegments.forEach((streamId, list) -> {\n                list.sort(Comparator.comparingLong(Pair::getLeft));\n                long start = list.get(0).getLeft();\n                long current = start;\n                for (Pair<Long, Long> p : list) {\n                    if (p.getLeft() != current) {\n                        LOGGER.warn(\"Trying to commit an unexpected disjoint stream ranges: {}\", list);\n                    }\n                    current = p.getRight();\n                }\n                reduced.put(streamId, new ImmutablePair<>(start, current));\n            });\n\n            extendRange(session, reduced);\n\n            // commit S3 object\n            if (objectId != S3Constants.NOOP_OBJECT_ID && !commitObject(objectId, StreamConstants.NOOP_STREAM_ID, streamSetObject.getObjectSize(), session)) {\n                ControllerException e = new ControllerException(Code.ILLEGAL_STATE_VALUE,\n                    String.format(\"S3StreamSetObject[object-id=%d] is not ready for commit\", streamSetObject.getObjectId()));\n                future.completeExceptionally(e);\n                return future;\n            }\n\n            long dataTs = System.currentTimeMillis();\n            long sequenceId = objectId;\n            if (!Objects.isNull(compactedObjects) && !compactedObjects.isEmpty()) {\n                List<com.automq.rocketmq.metadata.dao.S3StreamSetObject> s3StreamSetObjects = compactedObjects.stream()\n                    .map(id -> {\n                        // mark destroy compacted object\n                        S3Object object = s3ObjectMapper.getById(id);\n                        object.setState(S3ObjectState.BOS_WILL_DELETE);\n                        object.setMarkedForDeletionTimestamp(new Date());\n                        s3ObjectMapper.markToDelete(object.getId(), new Date());\n\n                        return s3StreamSetObjectMapper.getByObjectId(id);\n                    })\n                    .toList();\n\n                if (!s3StreamSetObjects.isEmpty()) {\n                    // update dataTs to the min compacted object's dataTs\n                    dataTs = s3StreamSetObjects.stream()\n                        .map(com.automq.rocketmq.metadata.dao.S3StreamSetObject::getBaseDataTimestamp)\n                        .map(Date::getTime)\n                        .min(Long::compareTo).get();\n                    // update sequenceId to the min compacted object's sequenceId\n                    sequenceId = s3StreamSetObjects.stream().mapToLong(com.automq.rocketmq.metadata.dao.S3StreamSetObject::getSequenceId).min().getAsLong();\n                }\n            }\n\n            Map<Long, List<com.automq.rocketmq.metadata.dao.S3StreamObject>> toCache =\n                new HashMap<>();\n\n            // commit stream objects;\n            if (!streamObjects.isEmpty()) {\n                for (apache.rocketmq.controller.v1.S3StreamObject s3StreamObject : streamObjects) {\n                    long oId = s3StreamObject.getObjectId();\n                    long objectSize = s3StreamObject.getObjectSize();\n                    long streamId = s3StreamObject.getStreamId();\n                    if (!commitObject(oId, streamId, objectSize, session)) {\n                        String msg = String.format(\"S3StreamObject[object-id=%d] is not ready to commit\", oId);\n                        ControllerException e = new ControllerException(Code.ILLEGAL_STATE_VALUE, msg);\n                        future.completeExceptionally(e);\n                        return future;\n                    }\n                }\n                // create stream object records\n                streamObjects.forEach(s3StreamObject -> {\n                    com.automq.rocketmq.metadata.dao.S3StreamObject object =\n                        new com.automq.rocketmq.metadata.dao.S3StreamObject();\n                    object.setStreamId(s3StreamObject.getStreamId());\n                    object.setObjectId(s3StreamObject.getObjectId());\n                    object.setCommittedTimestamp(new Date());\n                    object.setStartOffset(s3StreamObject.getStartOffset());\n                    object.setBaseDataTimestamp(new Date());\n                    object.setEndOffset(s3StreamObject.getEndOffset());\n                    object.setObjectSize(s3StreamObject.getObjectSize());\n                    s3StreamObjectMapper.commit(object);\n                    toCache.computeIfAbsent(object.getStreamId(), streamId -> new ArrayList<>()).add(object);\n                });\n            }\n\n            // generate compacted objects' remove record ...\n            if (!Objects.isNull(compactedObjects) && !compactedObjects.isEmpty()) {\n                compactedObjects.forEach(id -> s3StreamSetObjectMapper.delete(id, null, null));\n            }\n\n            // update broker's StreamSet object\n            if (objectId != S3Constants.NOOP_OBJECT_ID) {\n                // generate broker's StreamSet object record\n                com.automq.rocketmq.metadata.dao.S3StreamSetObject s3StreamSetObject = new com.automq.rocketmq.metadata.dao.S3StreamSetObject();\n                s3StreamSetObject.setObjectId(objectId);\n                s3StreamSetObject.setObjectSize(streamSetObject.getObjectSize());\n                s3StreamSetObject.setBaseDataTimestamp(new Date(dataTs));\n                s3StreamSetObject.setCommittedTimestamp(new Date());\n                s3StreamSetObject.setNodeId(brokerId);\n                s3StreamSetObject.setSequenceId(sequenceId);\n                String subStreams = JsonFormat.printer().print(streamSetObject.getSubStreams());\n                s3StreamSetObject.setSubStreams(subStreams);\n                s3StreamSetObjectMapper.create(s3StreamSetObject);\n\n                // Cache StreamSet object\n                s3StreamSetObjectCache.onCommit(streamSetObject.toBuilder()\n                    .setBaseDataTimestamp(s3StreamSetObject.getBaseDataTimestamp().getTime())\n                    .setCommittedTimestamp(s3StreamSetObject.getCommittedTimestamp().getTime())\n                    .setSequenceId(sequenceId)\n                    .build());\n            }\n            session.commit();\n\n            // Update Cache\n            for (Map.Entry<Long, List<com.automq.rocketmq.metadata.dao.S3StreamObject>> entry\n                : toCache.entrySet()) {\n                s3StreamObjectCache.cache(entry.getKey(), entry.getValue());\n            }\n            s3StreamSetObjectCache.onCompact(compactedObjects);\n            LOGGER.info(\"broker[broke-id={}] commit StreamSet object[object-id={}] success, compacted objects[{}], stream objects[{}]\",\n                brokerId, streamSetObject.getObjectId(), compactedObjects, streamObjects);\n            future.complete(null);\n        } catch (Exception e) {\n            LOGGER.error(\"CommitStreamSetObject failed\", e);\n            ControllerException ex = new ControllerException(Code.INTERNAL_VALUE, \"CommitStreamSetObject failed\" + e.getMessage());\n            future.completeExceptionally(ex);\n        }\n        return future;\n    }\n\n```\n\n### Code description\n```Java\n/**\n * Description: This method is used to commit a stream object to the database and update the cache.\n * It also marks the compacted objects for deletion and updates their state.\n *\n * @param streamObject The stream object to be committed.\n * @param compactedObjects The list of compacted objects to be marked for deletion.\n * @return A CompletableFuture that completes when the operation is done, or fails with an exception if an error occurs.\n * @throws ControllerException If the object is not ready for commit or if the objectId is unavailable.\n * @throws Exception If an error occurs during the commit operation.\n */\n```\n\n### Code snippet:\n```Java\n    public CompletableFuture<Void> commitStreamObject(apache.rocketmq.controller.v1.S3StreamObject streamObject,\n        List<Long> compactedObjects) {\n        LOGGER.info(\"commitStreamObject with streamObject: {}, compactedObjects: {}\", TextFormat.shortDebugString(streamObject),\n            compactedObjects);\n\n        CompletableFuture<Void> future = new CompletableFuture<>();\n        try (SqlSession session = sessionFactory.openSession()) {\n            if (streamObject.getObjectId() == S3Constants.NOOP_OBJECT_ID) {\n                LOGGER.error(\"S3StreamObject[object-id={}] is null or objectId is unavailable\", streamObject.getObjectId());\n                String msg = String.format(\"S3StreamObject[object-id=%d] is null or objectId is unavailable\",\n                    streamObject.getObjectId());\n                ControllerException e = new ControllerException(Code.NOT_FOUND_VALUE, msg);\n                future.completeExceptionally(e);\n                return future;\n            }\n\n            long committedTs = System.currentTimeMillis();\n            S3ObjectMapper s3ObjectMapper = session.getMapper(S3ObjectMapper.class);\n            S3StreamObjectMapper s3StreamObjectMapper = session.getMapper(S3StreamObjectMapper.class);\n\n            // commit object\n            if (!commitObject(streamObject.getObjectId(), streamObject.getStreamId(), streamObject.getObjectSize(), session)) {\n                String msg = String.format(\"S3StreamObject[object-id=%d] is not ready for commit\",\n                    streamObject.getObjectId());\n                ControllerException e = new ControllerException(Code.ILLEGAL_STATE_VALUE, msg);\n                future.completeExceptionally(e);\n                return future;\n            }\n            long dataTs = committedTs;\n            if (!Objects.isNull(compactedObjects) && !compactedObjects.isEmpty()) {\n                dataTs = compactedObjects.stream()\n                    .map(id -> {\n                        // mark destroy compacted object\n                        S3Object object = s3ObjectMapper.getById(id);\n                        object.setState(S3ObjectState.BOS_WILL_DELETE);\n                        object.setMarkedForDeletionTimestamp(new Date());\n                        s3ObjectMapper.markToDelete(object.getId(), new Date());\n\n                        // update dataTs to the min compacted object's dataTs\n                        com.automq.rocketmq.metadata.dao.S3StreamObject s3StreamObject =\n                            s3StreamObjectMapper.getByObjectId(id);\n```\n\n### Requirements:\n1. Complete only the missing part of the method.\n2. Only answer the code of the completed part. Do not include any of the provided code snippet.\n3. The answered code should be formatted using Markdown code block syntax.\n4. Do not include any additional text.\n\n### Answer: (use the provided format with backticks)\n", "reference": "                        return s3StreamObject.getBaseDataTimestamp().getTime();\n                    })\n                    .min(Long::compareTo).get();\n            }\n\n            List<com.automq.rocketmq.metadata.dao.S3StreamObject> toCache = new ArrayList<>();\n\n            // create a new S3StreamObject to replace committed ones\n            if (streamObject.getObjectId() != S3Constants.NOOP_OBJECT_ID) {\n                com.automq.rocketmq.metadata.dao.S3StreamObject newS3StreamObj =\n                    new com.automq.rocketmq.metadata.dao.S3StreamObject();\n                newS3StreamObj.setStreamId(streamObject.getStreamId());\n                newS3StreamObj.setObjectId(streamObject.getObjectId());\n                newS3StreamObj.setObjectSize(streamObject.getObjectSize());\n                newS3StreamObj.setStartOffset(streamObject.getStartOffset());\n                newS3StreamObj.setEndOffset(streamObject.getEndOffset());\n                newS3StreamObj.setBaseDataTimestamp(new Date(dataTs));\n                newS3StreamObj.setCommittedTimestamp(new Date(committedTs));\n                s3StreamObjectMapper.create(newS3StreamObj);\n                toCache.add(newS3StreamObj);\n            }\n\n            // delete the compactedObjects of S3Stream\n            if (!Objects.isNull(compactedObjects) && !compactedObjects.isEmpty()) {\n                compactedObjects.forEach(id -> s3StreamObjectMapper.delete(null, null, id));\n            }\n            session.commit();\n\n            // Update Cache\n            s3StreamObjectCache.cache(streamObject.getStreamId(), toCache);\n            s3StreamObjectCache.onCompact(streamObject.getStreamId(), compactedObjects);\n\n            LOGGER.info(\"S3StreamObject[object-id={}] commit success, compacted objects: {}\",\n                streamObject.getObjectId(), compactedObjects);\n            future.complete(null);\n        } catch (Exception e) {\n            LOGGER.error(\"CommitStream failed\", e);\n            ControllerException ex = new ControllerException(Code.INTERNAL_VALUE, \"CommitStream failed\" + e.getMessage());\n            future.completeExceptionally(ex);\n        }\n        return future;\n    }", "completions": ["  \n```Java\n                        return s3StreamObject.getBaseDataTimestamp().getTime();\n                    })\n                   .min(Long::compareTo).get();\n                }\n            }\n\n            // commit stream object\n            com.automq.rocketmq.metadata.dao.S3StreamObject object = new com.automq.rocketmq.metadata.dao.S3StreamObject();\n            object.setStreamId(streamObject.getStreamId());\n            object.setObjectId(streamObject.getObjectId());\n            object.setCommittedTimestamp(new Date(committedTs));\n            object.setStartOffset(streamObject.getStartOffset());\n            object.setBaseDataTimestamp(new Date(dataTs));\n            object.setEndOffset(streamObject.getEndOffset());\n            object.setObjectSize(streamObject.getObjectSize());\n            s3StreamObjectMapper.commit(object);\n\n            // Cache Stream object\n            s3StreamObjectCache.cache(streamObject.getStreamId(), object);\n\n            // Update Cache\n            s3StreamSetObjectCache.onCompact(compactedObjects);\n            LOGGER.info(\"broker[broke-id={}] commit Stream object[object-id={}] success, compacted objects[{}], stream objects[{}]\",\n                brokerId, streamObject.getObjectId(), compactedObjects, streamObjects);\n            future.complete(null);\n        } catch (Exception e) {\n            LOGGER.error(\"CommitStreamObject failed\", e);\n            ControllerException ex = new ControllerException(Code.INTERNAL_VALUE, \"CommitStreamObject failed\" + e.getMessage());\n            future.completeExceptionally(ex);\n        }\n        return future;\n    }\n```"]}
{"id": 34, "name": "getProcessByKey", "prompt": "### Role:\nYou are an experienced Java programmer assistant responsible for code completion tasks. Please intelligently complete the missing parts of the provided code snippet.\n\n### Task:\nBased on the provided code snippet, complete the missing method implementation. Ensure the code is logically correct, syntactically accurate, and follows best coding practices.\n\n### Left context\n```Java\n/*\n * Copyright Camunda Services GmbH\n *\n * BY INSTALLING, DOWNLOADING, ACCESSING, USING, OR DISTRIBUTING THE SOFTWARE (“USE”), YOU INDICATE YOUR ACCEPTANCE TO AND ARE ENTERING INTO A CONTRACT WITH, THE LICENSOR ON THE TERMS SET OUT IN THIS AGREEMENT. IF YOU DO NOT AGREE TO THESE TERMS, YOU MUST NOT USE THE SOFTWARE. IF YOU ARE RECEIVING THE SOFTWARE ON BEHALF OF A LEGAL ENTITY, YOU REPRESENT AND WARRANT THAT YOU HAVE THE ACTUAL AUTHORITY TO AGREE TO THE TERMS AND CONDITIONS OF THIS AGREEMENT ON BEHALF OF SUCH ENTITY.\n * “Licensee” means you, an individual, or the entity on whose behalf you receive the Software.\n *\n * Permission is hereby granted, free of charge, to the Licensee obtaining a copy of this Software and associated documentation files to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject in each case to the following conditions:\n * Condition 1: If the Licensee distributes the Software or any derivative works of the Software, the Licensee must attach this Agreement.\n * Condition 2: Without limiting other conditions in this Agreement, the grant of rights is solely for non-production use as defined below.\n * \"Non-production use\" means any use of the Software that is not directly related to creating products, services, or systems that generate revenue or other direct or indirect economic benefits.  Examples of permitted non-production use include personal use, educational use, research, and development. Examples of prohibited production use include, without limitation, use for commercial, for-profit, or publicly accessible systems or use for commercial or revenue-generating purposes.\n *\n * If the Licensee is in breach of the Conditions, this Agreement, including the rights granted under it, will automatically terminate with immediate effect.\n *\n * SUBJECT AS SET OUT BELOW, THE SOFTWARE IS PROVIDED “AS IS”, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE, AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n * NOTHING IN THIS AGREEMENT EXCLUDES OR RESTRICTS A PARTY’S LIABILITY FOR (A) DEATH OR PERSONAL INJURY CAUSED BY THAT PARTY’S NEGLIGENCE, (B) FRAUD, OR (C) ANY OTHER LIABILITY TO THE EXTENT THAT IT CANNOT BE LAWFULLY EXCLUDED OR RESTRICTED.\n */\npackage io.camunda.operate.store.elasticsearch;\n\nimport static io.camunda.operate.schema.indices.ProcessIndex.BPMN_XML;\nimport static io.camunda.operate.schema.templates.FlowNodeInstanceTemplate.TREE_PATH;\nimport static io.camunda.operate.schema.templates.ListViewTemplate.BPMN_PROCESS_ID;\nimport static io.camunda.operate.schema.templates.ListViewTemplate.ID;\nimport static io.camunda.operate.schema.templates.ListViewTemplate.INCIDENT;\nimport static io.camunda.operate.schema.templates.ListViewTemplate.JOIN_RELATION;\nimport static io.camunda.operate.schema.templates.ListViewTemplate.KEY;\nimport static io.camunda.operate.schema.templates.ListViewTemplate.PARENT_PROCESS_INSTANCE_KEY;\nimport static io.camunda.operate.schema.templates.ListViewTemplate.PROCESS_INSTANCE_JOIN_RELATION;\nimport static io.camunda.operate.schema.templates.ListViewTemplate.PROCESS_KEY;\nimport static io.camunda.operate.schema.templates.ListViewTemplate.PROCESS_NAME;\nimport static io.camunda.operate.schema.templates.ListViewTemplate.STATE;\nimport static io.camunda.operate.util.ElasticsearchUtil.QueryType.ALL;\nimport static io.camunda.operate.util.ElasticsearchUtil.QueryType.ONLY_RUNTIME;\nimport static io.camunda.operate.util.ElasticsearchUtil.UPDATE_RETRY_COUNT;\nimport static io.camunda.operate.util.ElasticsearchUtil.createSearchRequest;\nimport static io.camunda.operate.util.ElasticsearchUtil.joinWithAnd;\nimport static io.camunda.operate.util.ElasticsearchUtil.scrollWith;\nimport static org.elasticsearch.index.query.QueryBuilders.constantScoreQuery;\nimport static org.elasticsearch.index.query.QueryBuilders.idsQuery;\nimport static org.elasticsearch.index.query.QueryBuilders.matchAllQuery;\nimport static org.elasticsearch.index.query.QueryBuilders.termQuery;\nimport static org.elasticsearch.index.query.QueryBuilders.termsQuery;\nimport static org.elasticsearch.search.aggregations.AggregationBuilders.cardinality;\nimport static org.elasticsearch.search.aggregations.AggregationBuilders.terms;\nimport static org.elasticsearch.search.aggregations.AggregationBuilders.topHits;\n\nimport com.fasterxml.jackson.databind.ObjectMapper;\nimport io.camunda.operate.conditions.ElasticsearchCondition;\nimport io.camunda.operate.entities.ProcessEntity;\nimport io.camunda.operate.entities.listview.ProcessInstanceForListViewEntity;\nimport io.camunda.operate.entities.listview.ProcessInstanceState;\nimport io.camunda.operate.exceptions.OperateRuntimeException;\nimport io.camunda.operate.property.OperateProperties;\nimport io.camunda.operate.schema.indices.ProcessIndex;\nimport io.camunda.operate.schema.templates.ListViewTemplate;\nimport io.camunda.operate.schema.templates.OperationTemplate;\nimport io.camunda.operate.schema.templates.ProcessInstanceDependant;\nimport io.camunda.operate.schema.templates.TemplateDescriptor;\nimport io.camunda.operate.store.NotFoundException;\nimport io.camunda.operate.store.ProcessStore;\nimport io.camunda.operate.tenant.TenantAwareElasticsearchClient;\nimport io.camunda.operate.util.ElasticsearchUtil;\nimport io.camunda.operate.util.TreePath;\nimport java.io.IOException;\nimport java.util.ArrayList;\nimport java.util.Arrays;\nimport java.util.HashMap;\nimport java.util.List;\nimport java.util.Map;\nimport java.util.Optional;\nimport java.util.Set;\nimport java.util.stream.Collectors;\nimport org.elasticsearch.action.admin.indices.refresh.RefreshRequest;\nimport org.elasticsearch.action.bulk.BulkRequest;\nimport org.elasticsearch.action.search.SearchRequest;\nimport org.elasticsearch.action.search.SearchResponse;\nimport org.elasticsearch.action.update.UpdateRequest;\nimport org.elasticsearch.client.RequestOptions;\nimport org.elasticsearch.client.RestHighLevelClient;\nimport org.elasticsearch.index.query.BoolQueryBuilder;\nimport org.elasticsearch.index.query.IdsQueryBuilder;\nimport org.elasticsearch.index.query.QueryBuilder;\nimport org.elasticsearch.index.query.QueryBuilders;\nimport org.elasticsearch.index.query.TermQueryBuilder;\nimport org.elasticsearch.index.query.TermsQueryBuilder;\nimport org.elasticsearch.index.reindex.BulkByScrollResponse;\nimport org.elasticsearch.index.reindex.DeleteByQueryRequest;\nimport org.elasticsearch.search.SearchHit;\nimport org.elasticsearch.search.SearchHits;\nimport org.elasticsearch.search.aggregations.AggregationBuilder;\nimport org.elasticsearch.search.aggregations.AggregationBuilders;\nimport org.elasticsearch.search.aggregations.Aggregations;\nimport org.elasticsearch.search.aggregations.bucket.SingleBucketAggregation;\nimport org.elasticsearch.search.aggregations.bucket.filter.FilterAggregationBuilder;\nimport org.elasticsearch.search.aggregations.bucket.terms.Terms;\nimport org.elasticsearch.search.aggregations.metrics.Cardinality;\nimport org.elasticsearch.search.aggregations.metrics.TopHits;\nimport org.elasticsearch.search.builder.SearchSourceBuilder;\nimport org.elasticsearch.search.sort.SortOrder;\nimport org.slf4j.Logger;\nimport org.slf4j.LoggerFactory;\nimport org.springframework.context.annotation.Conditional;\nimport org.springframework.lang.Nullable;\nimport org.springframework.stereotype.Component;\n\n@Conditional(ElasticsearchCondition.class)\n@Component\npublic class ElasticsearchProcessStore implements ProcessStore {\n  public static final FilterAggregationBuilder INCIDENTS_AGGREGATION =\n      AggregationBuilders.filter(\n          \"incidents\",\n          joinWithAnd(\n              termQuery(INCIDENT, true), termQuery(JOIN_RELATION, PROCESS_INSTANCE_JOIN_RELATION)));\n  public static final FilterAggregationBuilder RUNNING_AGGREGATION =\n      AggregationBuilders.filter(\n          \"running\", termQuery(ListViewTemplate.STATE, ProcessInstanceState.ACTIVE));\n  private static final Logger LOGGER = LoggerFactory.getLogger(ElasticsearchProcessStore.class);\n  private static final String DISTINCT_FIELD_COUNTS = \"distinctFieldCounts\";\n  private final ProcessIndex processIndex;\n\n  private final ListViewTemplate listViewTemplate;\n\n  private final List<ProcessInstanceDependant> processInstanceDependantTemplates;\n\n  private final ObjectMapper objectMapper;\n\n  private final RestHighLevelClient esClient;\n\n  private final TenantAwareElasticsearchClient tenantAwareClient;\n\n  private final OperateProperties operateProperties;\n\n  public ElasticsearchProcessStore(\n      ProcessIndex processIndex,\n      ListViewTemplate listViewTemplate,\n      List<ProcessInstanceDependant> processInstanceDependantTemplates,\n      ObjectMapper objectMapper,\n      OperateProperties operateProperties,\n      RestHighLevelClient esClient,\n      TenantAwareElasticsearchClient tenantAwareClient) {\n    this.processIndex = processIndex;\n    this.listViewTemplate = listViewTemplate;\n    this.processInstanceDependantTemplates = processInstanceDependantTemplates;\n    this.objectMapper = objectMapper;\n    this.operateProperties = operateProperties;\n    this.esClient = esClient;\n    this.tenantAwareClient = tenantAwareClient;\n  }\n\n  @Override\n  public Optional<Long> getDistinctCountFor(String fieldName) {\n    final String indexAlias = processIndex.getAlias();\n    LOGGER.debug(\"Called distinct count for field {} in index alias {}.\", fieldName, indexAlias);\n    final SearchRequest searchRequest =\n        new SearchRequest(indexAlias)\n            .source(\n                new SearchSourceBuilder()\n                    .query(QueryBuilders.matchAllQuery())\n                    .size(0)\n                    .aggregation(\n                        cardinality(DISTINCT_FIELD_COUNTS)\n                            .precisionThreshold(1_000)\n                            .field(fieldName)));\n    try {\n      final SearchResponse searchResponse = esClient.search(searchRequest, RequestOptions.DEFAULT);\n      final Cardinality distinctFieldCounts =\n          searchResponse.getAggregations().get(DISTINCT_FIELD_COUNTS);\n      return Optional.of(distinctFieldCounts.getValue());\n    } catch (Exception e) {\n      LOGGER.error(\n          String.format(\n              \"Error in distinct count for field %s in index alias %s.\", fieldName, indexAlias),\n          e);\n      return Optional.empty();\n    }\n  }\n\n  @Override\n  public void refreshIndices(String... indices) {\n    if (indices == null || indices.length == 0) {\n      throw new OperateRuntimeException(\"Refresh indices needs at least one index to refresh.\");\n    }\n    try {\n      esClient.indices().refresh(new RefreshRequest(indices), RequestOptions.DEFAULT);\n    } catch (IOException ex) {\n      throw new OperateRuntimeException(\"Failed to refresh indices \" + Arrays.asList(indices), ex);\n    }\n  }\n\n```\n\n### Code description\n```Java\n/**\n * Description: This method retrieves a process entity by its key from the Elasticsearch index.\n * It uses the term query API from org.elasticsearch.index.query.QueryBuilders to search for the process.\n * If exactly one process is found, it is returned as a ProcessEntity.\n * If more than one process is found, a NotFoundException is thrown with a message indicating that the process key is not unique.\n * If no process is found, a NotFoundException is thrown with a message indicating that the process key could not be found.\n * If an IOException occurs during the search, an OperateRuntimeException is thrown with a logged error message.\n *\n * @param processDefinitionKey The key of the process definition to search for.\n * @return The ProcessEntity that matches the provided key.\n * @throws NotFoundException If no process is found or if more than one process is found with the same key.\n * @throws OperateRuntimeException If an IOException occurs during the search.\n */\n```\n\n### Code snippet:\n```Java\n  @Override\n  public ProcessEntity getProcessByKey(Long processDefinitionKey) {\n    final SearchRequest searchRequest =\n        new SearchRequest(processIndex.getAlias())\n            .source(\n                new SearchSourceBuilder()\n                    .query(QueryBuilders.termQuery(ProcessIndex.KEY, processDefinitionKey)));\n\n    try {\n      final SearchResponse response = tenantAwareClient.search(searchRequest);\n      if (response.getHits().getTotalHits().value == 1) {\n        return fromSearchHit(response.getHits().getHits()[0].getSourceAsString());\n      } else if (response.getHits().getTotalHits().value > 1) {\n```\n\n### Requirements:\n1. Complete only the missing part of the method.\n2. Only answer the code of the completed part. Do not include any of the provided code snippet.\n3. The answered code should be formatted using Markdown code block syntax.\n4. Do not include any additional text.\n\n### Answer: (use the provided format with backticks)\n", "reference": "        throw new NotFoundException(\n            String.format(\"Could not find unique process with key '%s'.\", processDefinitionKey));\n      } else {\n        throw new NotFoundException(\n            String.format(\"Could not find process with key '%s'.\", processDefinitionKey));\n      }\n    } catch (IOException e) {\n      final String message =\n          String.format(\"Exception occurred, while obtaining the process: %s\", e.getMessage());\n      LOGGER.error(message, e);\n      throw new OperateRuntimeException(message, e);\n    }\n  }", "completions": ["  \n```Java\n} else if (response.getHits().getTotalHits().value > 1) {\nthrow new NotFoundException(\nString.format(\n\"More than one process with key %s found.\", processDefinitionKey));\n} else {\nthrow new NotFoundException(\nString.format(\n\"No process with key %s found.\", processDefinitionKey));\n}\n} catch (IOException ex) {\nthrow new OperateRuntimeException(\nString.format(\n\"Failed to retrieve process with key %s.\", processDefinitionKey),\nex);\n}\n}\n```"]}
{"id": 35, "name": "getDistinctCountFor", "prompt": "### Role:\nYou are an experienced Java programmer assistant responsible for code completion tasks. Please intelligently complete the missing parts of the provided code snippet.\n\n### Task:\nBased on the provided code snippet, complete the missing method implementation. Ensure the code is logically correct, syntactically accurate, and follows best coding practices.\n\n### Left context\n```Java\n/*\n * Copyright Camunda Services GmbH\n *\n * BY INSTALLING, DOWNLOADING, ACCESSING, USING, OR DISTRIBUTING THE SOFTWARE (“USE”), YOU INDICATE YOUR ACCEPTANCE TO AND ARE ENTERING INTO A CONTRACT WITH, THE LICENSOR ON THE TERMS SET OUT IN THIS AGREEMENT. IF YOU DO NOT AGREE TO THESE TERMS, YOU MUST NOT USE THE SOFTWARE. IF YOU ARE RECEIVING THE SOFTWARE ON BEHALF OF A LEGAL ENTITY, YOU REPRESENT AND WARRANT THAT YOU HAVE THE ACTUAL AUTHORITY TO AGREE TO THE TERMS AND CONDITIONS OF THIS AGREEMENT ON BEHALF OF SUCH ENTITY.\n * “Licensee” means you, an individual, or the entity on whose behalf you receive the Software.\n *\n * Permission is hereby granted, free of charge, to the Licensee obtaining a copy of this Software and associated documentation files to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject in each case to the following conditions:\n * Condition 1: If the Licensee distributes the Software or any derivative works of the Software, the Licensee must attach this Agreement.\n * Condition 2: Without limiting other conditions in this Agreement, the grant of rights is solely for non-production use as defined below.\n * \"Non-production use\" means any use of the Software that is not directly related to creating products, services, or systems that generate revenue or other direct or indirect economic benefits.  Examples of permitted non-production use include personal use, educational use, research, and development. Examples of prohibited production use include, without limitation, use for commercial, for-profit, or publicly accessible systems or use for commercial or revenue-generating purposes.\n *\n * If the Licensee is in breach of the Conditions, this Agreement, including the rights granted under it, will automatically terminate with immediate effect.\n *\n * SUBJECT AS SET OUT BELOW, THE SOFTWARE IS PROVIDED “AS IS”, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE, AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n * NOTHING IN THIS AGREEMENT EXCLUDES OR RESTRICTS A PARTY’S LIABILITY FOR (A) DEATH OR PERSONAL INJURY CAUSED BY THAT PARTY’S NEGLIGENCE, (B) FRAUD, OR (C) ANY OTHER LIABILITY TO THE EXTENT THAT IT CANNOT BE LAWFULLY EXCLUDED OR RESTRICTED.\n */\npackage io.camunda.operate.store.elasticsearch;\n\nimport static io.camunda.operate.schema.indices.ProcessIndex.BPMN_XML;\nimport static io.camunda.operate.schema.templates.FlowNodeInstanceTemplate.TREE_PATH;\nimport static io.camunda.operate.schema.templates.ListViewTemplate.BPMN_PROCESS_ID;\nimport static io.camunda.operate.schema.templates.ListViewTemplate.ID;\nimport static io.camunda.operate.schema.templates.ListViewTemplate.INCIDENT;\nimport static io.camunda.operate.schema.templates.ListViewTemplate.JOIN_RELATION;\nimport static io.camunda.operate.schema.templates.ListViewTemplate.KEY;\nimport static io.camunda.operate.schema.templates.ListViewTemplate.PARENT_PROCESS_INSTANCE_KEY;\nimport static io.camunda.operate.schema.templates.ListViewTemplate.PROCESS_INSTANCE_JOIN_RELATION;\nimport static io.camunda.operate.schema.templates.ListViewTemplate.PROCESS_KEY;\nimport static io.camunda.operate.schema.templates.ListViewTemplate.PROCESS_NAME;\nimport static io.camunda.operate.schema.templates.ListViewTemplate.STATE;\nimport static io.camunda.operate.util.ElasticsearchUtil.QueryType.ALL;\nimport static io.camunda.operate.util.ElasticsearchUtil.QueryType.ONLY_RUNTIME;\nimport static io.camunda.operate.util.ElasticsearchUtil.UPDATE_RETRY_COUNT;\nimport static io.camunda.operate.util.ElasticsearchUtil.createSearchRequest;\nimport static io.camunda.operate.util.ElasticsearchUtil.joinWithAnd;\nimport static io.camunda.operate.util.ElasticsearchUtil.scrollWith;\nimport static org.elasticsearch.index.query.QueryBuilders.constantScoreQuery;\nimport static org.elasticsearch.index.query.QueryBuilders.idsQuery;\nimport static org.elasticsearch.index.query.QueryBuilders.matchAllQuery;\nimport static org.elasticsearch.index.query.QueryBuilders.termQuery;\nimport static org.elasticsearch.index.query.QueryBuilders.termsQuery;\nimport static org.elasticsearch.search.aggregations.AggregationBuilders.cardinality;\nimport static org.elasticsearch.search.aggregations.AggregationBuilders.terms;\nimport static org.elasticsearch.search.aggregations.AggregationBuilders.topHits;\n\nimport com.fasterxml.jackson.databind.ObjectMapper;\nimport io.camunda.operate.conditions.ElasticsearchCondition;\nimport io.camunda.operate.entities.ProcessEntity;\nimport io.camunda.operate.entities.listview.ProcessInstanceForListViewEntity;\nimport io.camunda.operate.entities.listview.ProcessInstanceState;\nimport io.camunda.operate.exceptions.OperateRuntimeException;\nimport io.camunda.operate.property.OperateProperties;\nimport io.camunda.operate.schema.indices.ProcessIndex;\nimport io.camunda.operate.schema.templates.ListViewTemplate;\nimport io.camunda.operate.schema.templates.OperationTemplate;\nimport io.camunda.operate.schema.templates.ProcessInstanceDependant;\nimport io.camunda.operate.schema.templates.TemplateDescriptor;\nimport io.camunda.operate.store.NotFoundException;\nimport io.camunda.operate.store.ProcessStore;\nimport io.camunda.operate.tenant.TenantAwareElasticsearchClient;\nimport io.camunda.operate.util.ElasticsearchUtil;\nimport io.camunda.operate.util.TreePath;\nimport java.io.IOException;\nimport java.util.ArrayList;\nimport java.util.Arrays;\nimport java.util.HashMap;\nimport java.util.List;\nimport java.util.Map;\nimport java.util.Optional;\nimport java.util.Set;\nimport java.util.stream.Collectors;\nimport org.elasticsearch.action.admin.indices.refresh.RefreshRequest;\nimport org.elasticsearch.action.bulk.BulkRequest;\nimport org.elasticsearch.action.search.SearchRequest;\nimport org.elasticsearch.action.search.SearchResponse;\nimport org.elasticsearch.action.update.UpdateRequest;\nimport org.elasticsearch.client.RequestOptions;\nimport org.elasticsearch.client.RestHighLevelClient;\nimport org.elasticsearch.index.query.BoolQueryBuilder;\nimport org.elasticsearch.index.query.IdsQueryBuilder;\nimport org.elasticsearch.index.query.QueryBuilder;\nimport org.elasticsearch.index.query.QueryBuilders;\nimport org.elasticsearch.index.query.TermQueryBuilder;\nimport org.elasticsearch.index.query.TermsQueryBuilder;\nimport org.elasticsearch.index.reindex.BulkByScrollResponse;\nimport org.elasticsearch.index.reindex.DeleteByQueryRequest;\nimport org.elasticsearch.search.SearchHit;\nimport org.elasticsearch.search.SearchHits;\nimport org.elasticsearch.search.aggregations.AggregationBuilder;\nimport org.elasticsearch.search.aggregations.AggregationBuilders;\nimport org.elasticsearch.search.aggregations.Aggregations;\nimport org.elasticsearch.search.aggregations.bucket.SingleBucketAggregation;\nimport org.elasticsearch.search.aggregations.bucket.filter.FilterAggregationBuilder;\nimport org.elasticsearch.search.aggregations.bucket.terms.Terms;\nimport org.elasticsearch.search.aggregations.metrics.Cardinality;\nimport org.elasticsearch.search.aggregations.metrics.TopHits;\nimport org.elasticsearch.search.builder.SearchSourceBuilder;\nimport org.elasticsearch.search.sort.SortOrder;\nimport org.slf4j.Logger;\nimport org.slf4j.LoggerFactory;\nimport org.springframework.context.annotation.Conditional;\nimport org.springframework.lang.Nullable;\nimport org.springframework.stereotype.Component;\n\n@Conditional(ElasticsearchCondition.class)\n@Component\npublic class ElasticsearchProcessStore implements ProcessStore {\n  public static final FilterAggregationBuilder INCIDENTS_AGGREGATION =\n      AggregationBuilders.filter(\n          \"incidents\",\n          joinWithAnd(\n              termQuery(INCIDENT, true), termQuery(JOIN_RELATION, PROCESS_INSTANCE_JOIN_RELATION)));\n  public static final FilterAggregationBuilder RUNNING_AGGREGATION =\n      AggregationBuilders.filter(\n          \"running\", termQuery(ListViewTemplate.STATE, ProcessInstanceState.ACTIVE));\n  private static final Logger LOGGER = LoggerFactory.getLogger(ElasticsearchProcessStore.class);\n  private static final String DISTINCT_FIELD_COUNTS = \"distinctFieldCounts\";\n  private final ProcessIndex processIndex;\n\n  private final ListViewTemplate listViewTemplate;\n\n  private final List<ProcessInstanceDependant> processInstanceDependantTemplates;\n\n  private final ObjectMapper objectMapper;\n\n  private final RestHighLevelClient esClient;\n\n  private final TenantAwareElasticsearchClient tenantAwareClient;\n\n  private final OperateProperties operateProperties;\n\n  public ElasticsearchProcessStore(\n      ProcessIndex processIndex,\n      ListViewTemplate listViewTemplate,\n      List<ProcessInstanceDependant> processInstanceDependantTemplates,\n      ObjectMapper objectMapper,\n      OperateProperties operateProperties,\n      RestHighLevelClient esClient,\n      TenantAwareElasticsearchClient tenantAwareClient) {\n    this.processIndex = processIndex;\n    this.listViewTemplate = listViewTemplate;\n    this.processInstanceDependantTemplates = processInstanceDependantTemplates;\n    this.objectMapper = objectMapper;\n    this.operateProperties = operateProperties;\n    this.esClient = esClient;\n    this.tenantAwareClient = tenantAwareClient;\n  }\n\n```\n\n### Code description\n```Java\n/**\n * Retrieves the distinct count for a given field in an Elasticsearch index.\n *\n * @param fieldName The name of the field for which to retrieve the distinct count.\n * @return An Optional Long containing the distinct count of the field, or an empty Optional if an error occurs.\n * @throws IOException If there is an issue with the Elasticsearch client.\n */\n```\n\n### Code snippet:\n```Java\n  @Override\n  public Optional<Long> getDistinctCountFor(String fieldName) {\n    final String indexAlias = processIndex.getAlias();\n    LOGGER.debug(\"Called distinct count for field {} in index alias {}.\", fieldName, indexAlias);\n    final SearchRequest searchRequest =\n        new SearchRequest(indexAlias)\n            .source(\n                new SearchSourceBuilder()\n                    .query(QueryBuilders.matchAllQuery())\n                    .size(0)\n                    .aggregation(\n                        cardinality(DISTINCT_FIELD_COUNTS)\n                            .precisionThreshold(1_000)\n```\n\n### Requirements:\n1. Complete only the missing part of the method.\n2. Only answer the code of the completed part. Do not include any of the provided code snippet.\n3. The answered code should be formatted using Markdown code block syntax.\n4. Do not include any additional text.\n\n### Answer: (use the provided format with backticks)\n", "reference": "                            .field(fieldName)));\n    try {\n      final SearchResponse searchResponse = esClient.search(searchRequest, RequestOptions.DEFAULT);\n      final Cardinality distinctFieldCounts =\n          searchResponse.getAggregations().get(DISTINCT_FIELD_COUNTS);\n      return Optional.of(distinctFieldCounts.getValue());\n    } catch (Exception e) {\n      LOGGER.error(\n          String.format(\n              \"Error in distinct count for field %s in index alias %s.\", fieldName, indexAlias),\n          e);\n      return Optional.empty();\n    }\n  }", "completions": ["  \n```Java\n.field(fieldName)));\n\n    try {\n      final SearchResponse searchResponse =\n          tenantAwareClient.search(searchRequest, RequestOptions.DEFAULT);\n      final Aggregations aggregations = searchResponse.getAggregations();\n      final Cardinality cardinality = aggregations.get(DISTINCT_FIELD_COUNTS);\n      return Optional.of(cardinality.getValue());\n    } catch (IOException e) {\n      LOGGER.error(\"Error while retrieving distinct count for field {}.\", fieldName, e);\n      return Optional.empty();\n    }\n  }\n```"]}
{"id": 36, "name": "getProcessInstanceListViewByKey", "prompt": "### Role:\nYou are an experienced Java programmer assistant responsible for code completion tasks. Please intelligently complete the missing parts of the provided code snippet.\n\n### Task:\nBased on the provided code snippet, complete the missing method implementation. Ensure the code is logically correct, syntactically accurate, and follows best coding practices.\n\n### Left context\n```Java\n/*\n * Copyright Camunda Services GmbH\n *\n * BY INSTALLING, DOWNLOADING, ACCESSING, USING, OR DISTRIBUTING THE SOFTWARE (“USE”), YOU INDICATE YOUR ACCEPTANCE TO AND ARE ENTERING INTO A CONTRACT WITH, THE LICENSOR ON THE TERMS SET OUT IN THIS AGREEMENT. IF YOU DO NOT AGREE TO THESE TERMS, YOU MUST NOT USE THE SOFTWARE. IF YOU ARE RECEIVING THE SOFTWARE ON BEHALF OF A LEGAL ENTITY, YOU REPRESENT AND WARRANT THAT YOU HAVE THE ACTUAL AUTHORITY TO AGREE TO THE TERMS AND CONDITIONS OF THIS AGREEMENT ON BEHALF OF SUCH ENTITY.\n * “Licensee” means you, an individual, or the entity on whose behalf you receive the Software.\n *\n * Permission is hereby granted, free of charge, to the Licensee obtaining a copy of this Software and associated documentation files to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject in each case to the following conditions:\n * Condition 1: If the Licensee distributes the Software or any derivative works of the Software, the Licensee must attach this Agreement.\n * Condition 2: Without limiting other conditions in this Agreement, the grant of rights is solely for non-production use as defined below.\n * \"Non-production use\" means any use of the Software that is not directly related to creating products, services, or systems that generate revenue or other direct or indirect economic benefits.  Examples of permitted non-production use include personal use, educational use, research, and development. Examples of prohibited production use include, without limitation, use for commercial, for-profit, or publicly accessible systems or use for commercial or revenue-generating purposes.\n *\n * If the Licensee is in breach of the Conditions, this Agreement, including the rights granted under it, will automatically terminate with immediate effect.\n *\n * SUBJECT AS SET OUT BELOW, THE SOFTWARE IS PROVIDED “AS IS”, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE, AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n * NOTHING IN THIS AGREEMENT EXCLUDES OR RESTRICTS A PARTY’S LIABILITY FOR (A) DEATH OR PERSONAL INJURY CAUSED BY THAT PARTY’S NEGLIGENCE, (B) FRAUD, OR (C) ANY OTHER LIABILITY TO THE EXTENT THAT IT CANNOT BE LAWFULLY EXCLUDED OR RESTRICTED.\n */\npackage io.camunda.operate.store.elasticsearch;\n\nimport static io.camunda.operate.schema.indices.ProcessIndex.BPMN_XML;\nimport static io.camunda.operate.schema.templates.FlowNodeInstanceTemplate.TREE_PATH;\nimport static io.camunda.operate.schema.templates.ListViewTemplate.BPMN_PROCESS_ID;\nimport static io.camunda.operate.schema.templates.ListViewTemplate.ID;\nimport static io.camunda.operate.schema.templates.ListViewTemplate.INCIDENT;\nimport static io.camunda.operate.schema.templates.ListViewTemplate.JOIN_RELATION;\nimport static io.camunda.operate.schema.templates.ListViewTemplate.KEY;\nimport static io.camunda.operate.schema.templates.ListViewTemplate.PARENT_PROCESS_INSTANCE_KEY;\nimport static io.camunda.operate.schema.templates.ListViewTemplate.PROCESS_INSTANCE_JOIN_RELATION;\nimport static io.camunda.operate.schema.templates.ListViewTemplate.PROCESS_KEY;\nimport static io.camunda.operate.schema.templates.ListViewTemplate.PROCESS_NAME;\nimport static io.camunda.operate.schema.templates.ListViewTemplate.STATE;\nimport static io.camunda.operate.util.ElasticsearchUtil.QueryType.ALL;\nimport static io.camunda.operate.util.ElasticsearchUtil.QueryType.ONLY_RUNTIME;\nimport static io.camunda.operate.util.ElasticsearchUtil.UPDATE_RETRY_COUNT;\nimport static io.camunda.operate.util.ElasticsearchUtil.createSearchRequest;\nimport static io.camunda.operate.util.ElasticsearchUtil.joinWithAnd;\nimport static io.camunda.operate.util.ElasticsearchUtil.scrollWith;\nimport static org.elasticsearch.index.query.QueryBuilders.constantScoreQuery;\nimport static org.elasticsearch.index.query.QueryBuilders.idsQuery;\nimport static org.elasticsearch.index.query.QueryBuilders.matchAllQuery;\nimport static org.elasticsearch.index.query.QueryBuilders.termQuery;\nimport static org.elasticsearch.index.query.QueryBuilders.termsQuery;\nimport static org.elasticsearch.search.aggregations.AggregationBuilders.cardinality;\nimport static org.elasticsearch.search.aggregations.AggregationBuilders.terms;\nimport static org.elasticsearch.search.aggregations.AggregationBuilders.topHits;\n\nimport com.fasterxml.jackson.databind.ObjectMapper;\nimport io.camunda.operate.conditions.ElasticsearchCondition;\nimport io.camunda.operate.entities.ProcessEntity;\nimport io.camunda.operate.entities.listview.ProcessInstanceForListViewEntity;\nimport io.camunda.operate.entities.listview.ProcessInstanceState;\nimport io.camunda.operate.exceptions.OperateRuntimeException;\nimport io.camunda.operate.property.OperateProperties;\nimport io.camunda.operate.schema.indices.ProcessIndex;\nimport io.camunda.operate.schema.templates.ListViewTemplate;\nimport io.camunda.operate.schema.templates.OperationTemplate;\nimport io.camunda.operate.schema.templates.ProcessInstanceDependant;\nimport io.camunda.operate.schema.templates.TemplateDescriptor;\nimport io.camunda.operate.store.NotFoundException;\nimport io.camunda.operate.store.ProcessStore;\nimport io.camunda.operate.tenant.TenantAwareElasticsearchClient;\nimport io.camunda.operate.util.ElasticsearchUtil;\nimport io.camunda.operate.util.TreePath;\nimport java.io.IOException;\nimport java.util.ArrayList;\nimport java.util.Arrays;\nimport java.util.HashMap;\nimport java.util.List;\nimport java.util.Map;\nimport java.util.Optional;\nimport java.util.Set;\nimport java.util.stream.Collectors;\nimport org.elasticsearch.action.admin.indices.refresh.RefreshRequest;\nimport org.elasticsearch.action.bulk.BulkRequest;\nimport org.elasticsearch.action.search.SearchRequest;\nimport org.elasticsearch.action.search.SearchResponse;\nimport org.elasticsearch.action.update.UpdateRequest;\nimport org.elasticsearch.client.RequestOptions;\nimport org.elasticsearch.client.RestHighLevelClient;\nimport org.elasticsearch.index.query.BoolQueryBuilder;\nimport org.elasticsearch.index.query.IdsQueryBuilder;\nimport org.elasticsearch.index.query.QueryBuilder;\nimport org.elasticsearch.index.query.QueryBuilders;\nimport org.elasticsearch.index.query.TermQueryBuilder;\nimport org.elasticsearch.index.query.TermsQueryBuilder;\nimport org.elasticsearch.index.reindex.BulkByScrollResponse;\nimport org.elasticsearch.index.reindex.DeleteByQueryRequest;\nimport org.elasticsearch.search.SearchHit;\nimport org.elasticsearch.search.SearchHits;\nimport org.elasticsearch.search.aggregations.AggregationBuilder;\nimport org.elasticsearch.search.aggregations.AggregationBuilders;\nimport org.elasticsearch.search.aggregations.Aggregations;\nimport org.elasticsearch.search.aggregations.bucket.SingleBucketAggregation;\nimport org.elasticsearch.search.aggregations.bucket.filter.FilterAggregationBuilder;\nimport org.elasticsearch.search.aggregations.bucket.terms.Terms;\nimport org.elasticsearch.search.aggregations.metrics.Cardinality;\nimport org.elasticsearch.search.aggregations.metrics.TopHits;\nimport org.elasticsearch.search.builder.SearchSourceBuilder;\nimport org.elasticsearch.search.sort.SortOrder;\nimport org.slf4j.Logger;\nimport org.slf4j.LoggerFactory;\nimport org.springframework.context.annotation.Conditional;\nimport org.springframework.lang.Nullable;\nimport org.springframework.stereotype.Component;\n\n@Conditional(ElasticsearchCondition.class)\n@Component\npublic class ElasticsearchProcessStore implements ProcessStore {\n  public static final FilterAggregationBuilder INCIDENTS_AGGREGATION =\n      AggregationBuilders.filter(\n          \"incidents\",\n          joinWithAnd(\n              termQuery(INCIDENT, true), termQuery(JOIN_RELATION, PROCESS_INSTANCE_JOIN_RELATION)));\n  public static final FilterAggregationBuilder RUNNING_AGGREGATION =\n      AggregationBuilders.filter(\n          \"running\", termQuery(ListViewTemplate.STATE, ProcessInstanceState.ACTIVE));\n  private static final Logger LOGGER = LoggerFactory.getLogger(ElasticsearchProcessStore.class);\n  private static final String DISTINCT_FIELD_COUNTS = \"distinctFieldCounts\";\n  private final ProcessIndex processIndex;\n\n  private final ListViewTemplate listViewTemplate;\n\n  private final List<ProcessInstanceDependant> processInstanceDependantTemplates;\n\n  private final ObjectMapper objectMapper;\n\n  private final RestHighLevelClient esClient;\n\n  private final TenantAwareElasticsearchClient tenantAwareClient;\n\n  private final OperateProperties operateProperties;\n\n  public ElasticsearchProcessStore(\n      ProcessIndex processIndex,\n      ListViewTemplate listViewTemplate,\n      List<ProcessInstanceDependant> processInstanceDependantTemplates,\n      ObjectMapper objectMapper,\n      OperateProperties operateProperties,\n      RestHighLevelClient esClient,\n      TenantAwareElasticsearchClient tenantAwareClient) {\n    this.processIndex = processIndex;\n    this.listViewTemplate = listViewTemplate;\n    this.processInstanceDependantTemplates = processInstanceDependantTemplates;\n    this.objectMapper = objectMapper;\n    this.operateProperties = operateProperties;\n    this.esClient = esClient;\n    this.tenantAwareClient = tenantAwareClient;\n  }\n\n  @Override\n  public Optional<Long> getDistinctCountFor(String fieldName) {\n    final String indexAlias = processIndex.getAlias();\n    LOGGER.debug(\"Called distinct count for field {} in index alias {}.\", fieldName, indexAlias);\n    final SearchRequest searchRequest =\n        new SearchRequest(indexAlias)\n            .source(\n                new SearchSourceBuilder()\n                    .query(QueryBuilders.matchAllQuery())\n                    .size(0)\n                    .aggregation(\n                        cardinality(DISTINCT_FIELD_COUNTS)\n                            .precisionThreshold(1_000)\n                            .field(fieldName)));\n    try {\n      final SearchResponse searchResponse = esClient.search(searchRequest, RequestOptions.DEFAULT);\n      final Cardinality distinctFieldCounts =\n          searchResponse.getAggregations().get(DISTINCT_FIELD_COUNTS);\n      return Optional.of(distinctFieldCounts.getValue());\n    } catch (Exception e) {\n      LOGGER.error(\n          String.format(\n              \"Error in distinct count for field %s in index alias %s.\", fieldName, indexAlias),\n          e);\n      return Optional.empty();\n    }\n  }\n\n  @Override\n  public void refreshIndices(String... indices) {\n    if (indices == null || indices.length == 0) {\n      throw new OperateRuntimeException(\"Refresh indices needs at least one index to refresh.\");\n    }\n    try {\n      esClient.indices().refresh(new RefreshRequest(indices), RequestOptions.DEFAULT);\n    } catch (IOException ex) {\n      throw new OperateRuntimeException(\"Failed to refresh indices \" + Arrays.asList(indices), ex);\n    }\n  }\n\n  @Override\n  public ProcessEntity getProcessByKey(Long processDefinitionKey) {\n    final SearchRequest searchRequest =\n        new SearchRequest(processIndex.getAlias())\n            .source(\n                new SearchSourceBuilder()\n                    .query(QueryBuilders.termQuery(ProcessIndex.KEY, processDefinitionKey)));\n\n    try {\n      final SearchResponse response = tenantAwareClient.search(searchRequest);\n      if (response.getHits().getTotalHits().value == 1) {\n        return fromSearchHit(response.getHits().getHits()[0].getSourceAsString());\n      } else if (response.getHits().getTotalHits().value > 1) {\n        throw new NotFoundException(\n            String.format(\"Could not find unique process with key '%s'.\", processDefinitionKey));\n      } else {\n        throw new NotFoundException(\n            String.format(\"Could not find process with key '%s'.\", processDefinitionKey));\n      }\n    } catch (IOException e) {\n      final String message =\n          String.format(\"Exception occurred, while obtaining the process: %s\", e.getMessage());\n      LOGGER.error(message, e);\n      throw new OperateRuntimeException(message, e);\n    }\n  }\n\n  @Override\n  public String getDiagramByKey(Long processDefinitionKey) {\n    final IdsQueryBuilder q = idsQuery().addIds(processDefinitionKey.toString());\n\n    final SearchRequest searchRequest =\n        new SearchRequest(processIndex.getAlias())\n            .source(new SearchSourceBuilder().query(q).fetchSource(BPMN_XML, null));\n\n    try {\n      final SearchResponse response = tenantAwareClient.search(searchRequest);\n\n      if (response.getHits().getTotalHits().value == 1) {\n        final Map<String, Object> result = response.getHits().getHits()[0].getSourceAsMap();\n        return (String) result.get(BPMN_XML);\n      } else if (response.getHits().getTotalHits().value > 1) {\n        throw new NotFoundException(\n            String.format(\"Could not find unique process with id '%s'.\", processDefinitionKey));\n      } else {\n        throw new NotFoundException(\n            String.format(\"Could not find process with id '%s'.\", processDefinitionKey));\n      }\n    } catch (IOException e) {\n      final String message =\n          String.format(\n              \"Exception occurred, while obtaining the process diagram: %s\", e.getMessage());\n      LOGGER.error(message, e);\n      throw new OperateRuntimeException(message, e);\n    }\n  }\n\n  @Override\n  public Map<ProcessKey, List<ProcessEntity>> getProcessesGrouped(\n      String tenantId, @Nullable Set<String> allowedBPMNProcessIds) {\n    final String tenantsGroupsAggName = \"group_by_tenantId\";\n    final String groupsAggName = \"group_by_bpmnProcessId\";\n    final String processesAggName = \"processes\";\n\n    final AggregationBuilder agg =\n        terms(tenantsGroupsAggName)\n            .field(ProcessIndex.TENANT_ID)\n            .size(ElasticsearchUtil.TERMS_AGG_SIZE)\n            .subAggregation(\n                terms(groupsAggName)\n                    .field(ProcessIndex.BPMN_PROCESS_ID)\n                    .size(ElasticsearchUtil.TERMS_AGG_SIZE)\n                    .subAggregation(\n                        topHits(processesAggName)\n                            .fetchSource(\n                                new String[] {\n                                  ProcessIndex.ID,\n                                  ProcessIndex.NAME,\n                                  ProcessIndex.VERSION,\n                                  ProcessIndex.BPMN_PROCESS_ID,\n                                  ProcessIndex.TENANT_ID\n                                },\n                                null)\n                            .size(ElasticsearchUtil.TOPHITS_AGG_SIZE)\n                            .sort(ProcessIndex.VERSION, SortOrder.DESC)));\n\n    final SearchSourceBuilder sourceBuilder = new SearchSourceBuilder().aggregation(agg).size(0);\n    sourceBuilder.query(buildQuery(tenantId, allowedBPMNProcessIds));\n    final SearchRequest searchRequest =\n        new SearchRequest(processIndex.getAlias()).source(sourceBuilder);\n\n    try {\n      final SearchResponse searchResponse = tenantAwareClient.search(searchRequest);\n      final Terms groups = searchResponse.getAggregations().get(tenantsGroupsAggName);\n      final Map<ProcessKey, List<ProcessEntity>> result = new HashMap<>();\n\n      groups.getBuckets().stream()\n          .forEach(\n              b -> {\n                final String groupTenantId = b.getKeyAsString();\n                final Terms processGroups = b.getAggregations().get(groupsAggName);\n\n                processGroups.getBuckets().stream()\n                    .forEach(\n                        tenantB -> {\n                          final String bpmnProcessId = tenantB.getKeyAsString();\n                          final ProcessKey groupKey = new ProcessKey(bpmnProcessId, groupTenantId);\n                          result.put(groupKey, new ArrayList<>());\n\n                          final TopHits processes = tenantB.getAggregations().get(processesAggName);\n                          final SearchHit[] hits = processes.getHits().getHits();\n                          for (SearchHit searchHit : hits) {\n                            final ProcessEntity processEntity =\n                                fromSearchHit(searchHit.getSourceAsString());\n                            result.get(groupKey).add(processEntity);\n                          }\n                        });\n              });\n\n      return result;\n    } catch (IOException e) {\n      final String message =\n          String.format(\n              \"Exception occurred, while obtaining grouped processes: %s\", e.getMessage());\n      LOGGER.error(message, e);\n      throw new OperateRuntimeException(message, e);\n    }\n  }\n\n  @Override\n  public Map<Long, ProcessEntity> getProcessesIdsToProcessesWithFields(\n      @Nullable Set<String> allowedBPMNIds, int maxSize, String... fields) {\n    final Map<Long, ProcessEntity> map = new HashMap<>();\n\n    final SearchSourceBuilder sourceBuilder =\n        new SearchSourceBuilder().size(maxSize).fetchSource(fields, null);\n    if (allowedBPMNIds == null) {\n      sourceBuilder.query(QueryBuilders.matchAllQuery());\n    } else {\n      sourceBuilder.query(\n          QueryBuilders.termsQuery(ListViewTemplate.BPMN_PROCESS_ID, allowedBPMNIds));\n    }\n    final SearchRequest searchRequest =\n        new SearchRequest(processIndex.getAlias()).source(sourceBuilder);\n\n    try {\n      final SearchResponse response = tenantAwareClient.search(searchRequest);\n      response\n          .getHits()\n          .forEach(\n              hit -> {\n                final ProcessEntity entity = fromSearchHit(hit.getSourceAsString());\n                map.put(entity.getKey(), entity);\n              });\n      return map;\n    } catch (IOException e) {\n      final String message =\n          String.format(\"Exception occurred, while obtaining processes: %s\", e.getMessage());\n      LOGGER.error(message, e);\n      throw new OperateRuntimeException(message, e);\n    }\n  }\n\n  @Override\n  public long deleteProcessDefinitionsByKeys(Long... processDefinitionKeys) {\n    if (processDefinitionKeys == null || processDefinitionKeys.length == 0) {\n      return 0;\n    }\n    final DeleteByQueryRequest query =\n        new DeleteByQueryRequest(processIndex.getAlias())\n            .setQuery(QueryBuilders.termsQuery(ProcessIndex.KEY, processDefinitionKeys));\n    try {\n      final BulkByScrollResponse response = esClient.deleteByQuery(query, RequestOptions.DEFAULT);\n      return response.getDeleted();\n    } catch (IOException ex) {\n      throw new OperateRuntimeException(\"Failed to delete process definitions by keys\", ex);\n    }\n  }\n\n```\n\n### Code description\n```Java\n/**\n * Description: This method retrieves a process instance for list view by its key.\n * It uses the constantScoreQuery API from org.elasticsearch.index.query.QueryBuilders.\n * \n * @param processInstanceKey The key of the process instance to be retrieved.\n * @return The process instance for list view entity.\n * @throws NotFoundException If the process instance with the given key is not found or if there are multiple instances with the same key.\n * @throws OperateRuntimeException If an IOException occurs during the execution of the method.\n */\n```\n\n### Code snippet:\n```Java\n  @Override\n  public ProcessInstanceForListViewEntity getProcessInstanceListViewByKey(Long processInstanceKey) {\n    try {\n      final QueryBuilder query =\n          joinWithAnd(\n              idsQuery().addIds(String.valueOf(processInstanceKey)),\n              termQuery(ListViewTemplate.PROCESS_INSTANCE_KEY, processInstanceKey));\n\n      final SearchRequest request =\n          ElasticsearchUtil.createSearchRequest(listViewTemplate, ALL)\n              .source(new SearchSourceBuilder().query(constantScoreQuery(query)));\n\n      final SearchResponse response;\n\n      response = tenantAwareClient.search(request);\n      final SearchHits searchHits = response.getHits();\n```\n\n### Requirements:\n1. Complete only the missing part of the method.\n2. Only answer the code of the completed part. Do not include any of the provided code snippet.\n3. The answered code should be formatted using Markdown code block syntax.\n4. Do not include any additional text.\n\n### Answer: (use the provided format with backticks)\n", "reference": "      if (searchHits.getTotalHits().value == 1 && searchHits.getHits().length == 1) {\n        return ElasticsearchUtil.fromSearchHit(\n            searchHits.getAt(0).getSourceAsString(),\n            objectMapper,\n            ProcessInstanceForListViewEntity.class);\n      } else if (response.getHits().getTotalHits().value > 1) {\n        throw new NotFoundException(\n            String.format(\n                \"Could not find unique process instance with id '%s'.\", processInstanceKey));\n      } else {\n        throw new NotFoundException(\n            (String.format(\"Could not find process instance with id '%s'.\", processInstanceKey)));\n      }\n    } catch (IOException e) {\n      throw new OperateRuntimeException(e);\n    }\n  }", "completions": ["  \n```Java\n  @Override\n  public ProcessInstanceForListViewEntity getProcessInstanceListViewByKey(Long processInstanceKey) {\n    try {\n      final QueryBuilder query =\n          joinWithAnd(\n              idsQuery().addIds(String.valueOf(processInstanceKey)),\n              termQuery(ListViewTemplate.PROCESS_INSTANCE_KEY, processInstanceKey));\n\n      final SearchRequest request =\n          ElasticsearchUtil.createSearchRequest(listViewTemplate, ALL)\n             .source(new SearchSourceBuilder().query(constantScoreQuery(query)));\n\n      final SearchResponse response;\n\n      response = tenantAwareClient.search(request);\n      final SearchHits searchHits = response.getHits();\n\n      if (searchHits.getTotalHits().value == 1) {\n        return fromSearchHit(searchHits.getHits()[0].getSourceAsString());\n      } else if (searchHits.getTotalHits().value > 1) {\n        throw new NotFoundException(\n            String.format(\"Could not find unique process instance with key '%s'.\", processInstanceKey));\n      } else {\n        throw new NotFoundException(\n            String.format(\"Could not find process instance with key '%s'.\", processInstanceKey));\n      }\n    } catch (IOException e) {\n      final String message =\n          String.format(\"Exception occurred, while obtaining the process instance: %s\", e.getMessage());\n      LOGGER.error(message, e);\n      throw new OperateRuntimeException(message, e);\n    }\n  }\n```"]}
{"id": 37, "name": "getDiagramByKey", "prompt": "### Role:\nYou are an experienced Java programmer assistant responsible for code completion tasks. Please intelligently complete the missing parts of the provided code snippet.\n\n### Task:\nBased on the provided code snippet, complete the missing method implementation. Ensure the code is logically correct, syntactically accurate, and follows best coding practices.\n\n### Left context\n```Java\n/*\n * Copyright Camunda Services GmbH\n *\n * BY INSTALLING, DOWNLOADING, ACCESSING, USING, OR DISTRIBUTING THE SOFTWARE (“USE”), YOU INDICATE YOUR ACCEPTANCE TO AND ARE ENTERING INTO A CONTRACT WITH, THE LICENSOR ON THE TERMS SET OUT IN THIS AGREEMENT. IF YOU DO NOT AGREE TO THESE TERMS, YOU MUST NOT USE THE SOFTWARE. IF YOU ARE RECEIVING THE SOFTWARE ON BEHALF OF A LEGAL ENTITY, YOU REPRESENT AND WARRANT THAT YOU HAVE THE ACTUAL AUTHORITY TO AGREE TO THE TERMS AND CONDITIONS OF THIS AGREEMENT ON BEHALF OF SUCH ENTITY.\n * “Licensee” means you, an individual, or the entity on whose behalf you receive the Software.\n *\n * Permission is hereby granted, free of charge, to the Licensee obtaining a copy of this Software and associated documentation files to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject in each case to the following conditions:\n * Condition 1: If the Licensee distributes the Software or any derivative works of the Software, the Licensee must attach this Agreement.\n * Condition 2: Without limiting other conditions in this Agreement, the grant of rights is solely for non-production use as defined below.\n * \"Non-production use\" means any use of the Software that is not directly related to creating products, services, or systems that generate revenue or other direct or indirect economic benefits.  Examples of permitted non-production use include personal use, educational use, research, and development. Examples of prohibited production use include, without limitation, use for commercial, for-profit, or publicly accessible systems or use for commercial or revenue-generating purposes.\n *\n * If the Licensee is in breach of the Conditions, this Agreement, including the rights granted under it, will automatically terminate with immediate effect.\n *\n * SUBJECT AS SET OUT BELOW, THE SOFTWARE IS PROVIDED “AS IS”, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE, AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n * NOTHING IN THIS AGREEMENT EXCLUDES OR RESTRICTS A PARTY’S LIABILITY FOR (A) DEATH OR PERSONAL INJURY CAUSED BY THAT PARTY’S NEGLIGENCE, (B) FRAUD, OR (C) ANY OTHER LIABILITY TO THE EXTENT THAT IT CANNOT BE LAWFULLY EXCLUDED OR RESTRICTED.\n */\npackage io.camunda.operate.store.elasticsearch;\n\nimport static io.camunda.operate.schema.indices.ProcessIndex.BPMN_XML;\nimport static io.camunda.operate.schema.templates.FlowNodeInstanceTemplate.TREE_PATH;\nimport static io.camunda.operate.schema.templates.ListViewTemplate.BPMN_PROCESS_ID;\nimport static io.camunda.operate.schema.templates.ListViewTemplate.ID;\nimport static io.camunda.operate.schema.templates.ListViewTemplate.INCIDENT;\nimport static io.camunda.operate.schema.templates.ListViewTemplate.JOIN_RELATION;\nimport static io.camunda.operate.schema.templates.ListViewTemplate.KEY;\nimport static io.camunda.operate.schema.templates.ListViewTemplate.PARENT_PROCESS_INSTANCE_KEY;\nimport static io.camunda.operate.schema.templates.ListViewTemplate.PROCESS_INSTANCE_JOIN_RELATION;\nimport static io.camunda.operate.schema.templates.ListViewTemplate.PROCESS_KEY;\nimport static io.camunda.operate.schema.templates.ListViewTemplate.PROCESS_NAME;\nimport static io.camunda.operate.schema.templates.ListViewTemplate.STATE;\nimport static io.camunda.operate.util.ElasticsearchUtil.QueryType.ALL;\nimport static io.camunda.operate.util.ElasticsearchUtil.QueryType.ONLY_RUNTIME;\nimport static io.camunda.operate.util.ElasticsearchUtil.UPDATE_RETRY_COUNT;\nimport static io.camunda.operate.util.ElasticsearchUtil.createSearchRequest;\nimport static io.camunda.operate.util.ElasticsearchUtil.joinWithAnd;\nimport static io.camunda.operate.util.ElasticsearchUtil.scrollWith;\nimport static org.elasticsearch.index.query.QueryBuilders.constantScoreQuery;\nimport static org.elasticsearch.index.query.QueryBuilders.idsQuery;\nimport static org.elasticsearch.index.query.QueryBuilders.matchAllQuery;\nimport static org.elasticsearch.index.query.QueryBuilders.termQuery;\nimport static org.elasticsearch.index.query.QueryBuilders.termsQuery;\nimport static org.elasticsearch.search.aggregations.AggregationBuilders.cardinality;\nimport static org.elasticsearch.search.aggregations.AggregationBuilders.terms;\nimport static org.elasticsearch.search.aggregations.AggregationBuilders.topHits;\n\nimport com.fasterxml.jackson.databind.ObjectMapper;\nimport io.camunda.operate.conditions.ElasticsearchCondition;\nimport io.camunda.operate.entities.ProcessEntity;\nimport io.camunda.operate.entities.listview.ProcessInstanceForListViewEntity;\nimport io.camunda.operate.entities.listview.ProcessInstanceState;\nimport io.camunda.operate.exceptions.OperateRuntimeException;\nimport io.camunda.operate.property.OperateProperties;\nimport io.camunda.operate.schema.indices.ProcessIndex;\nimport io.camunda.operate.schema.templates.ListViewTemplate;\nimport io.camunda.operate.schema.templates.OperationTemplate;\nimport io.camunda.operate.schema.templates.ProcessInstanceDependant;\nimport io.camunda.operate.schema.templates.TemplateDescriptor;\nimport io.camunda.operate.store.NotFoundException;\nimport io.camunda.operate.store.ProcessStore;\nimport io.camunda.operate.tenant.TenantAwareElasticsearchClient;\nimport io.camunda.operate.util.ElasticsearchUtil;\nimport io.camunda.operate.util.TreePath;\nimport java.io.IOException;\nimport java.util.ArrayList;\nimport java.util.Arrays;\nimport java.util.HashMap;\nimport java.util.List;\nimport java.util.Map;\nimport java.util.Optional;\nimport java.util.Set;\nimport java.util.stream.Collectors;\nimport org.elasticsearch.action.admin.indices.refresh.RefreshRequest;\nimport org.elasticsearch.action.bulk.BulkRequest;\nimport org.elasticsearch.action.search.SearchRequest;\nimport org.elasticsearch.action.search.SearchResponse;\nimport org.elasticsearch.action.update.UpdateRequest;\nimport org.elasticsearch.client.RequestOptions;\nimport org.elasticsearch.client.RestHighLevelClient;\nimport org.elasticsearch.index.query.BoolQueryBuilder;\nimport org.elasticsearch.index.query.IdsQueryBuilder;\nimport org.elasticsearch.index.query.QueryBuilder;\nimport org.elasticsearch.index.query.QueryBuilders;\nimport org.elasticsearch.index.query.TermQueryBuilder;\nimport org.elasticsearch.index.query.TermsQueryBuilder;\nimport org.elasticsearch.index.reindex.BulkByScrollResponse;\nimport org.elasticsearch.index.reindex.DeleteByQueryRequest;\nimport org.elasticsearch.search.SearchHit;\nimport org.elasticsearch.search.SearchHits;\nimport org.elasticsearch.search.aggregations.AggregationBuilder;\nimport org.elasticsearch.search.aggregations.AggregationBuilders;\nimport org.elasticsearch.search.aggregations.Aggregations;\nimport org.elasticsearch.search.aggregations.bucket.SingleBucketAggregation;\nimport org.elasticsearch.search.aggregations.bucket.filter.FilterAggregationBuilder;\nimport org.elasticsearch.search.aggregations.bucket.terms.Terms;\nimport org.elasticsearch.search.aggregations.metrics.Cardinality;\nimport org.elasticsearch.search.aggregations.metrics.TopHits;\nimport org.elasticsearch.search.builder.SearchSourceBuilder;\nimport org.elasticsearch.search.sort.SortOrder;\nimport org.slf4j.Logger;\nimport org.slf4j.LoggerFactory;\nimport org.springframework.context.annotation.Conditional;\nimport org.springframework.lang.Nullable;\nimport org.springframework.stereotype.Component;\n\n@Conditional(ElasticsearchCondition.class)\n@Component\npublic class ElasticsearchProcessStore implements ProcessStore {\n  public static final FilterAggregationBuilder INCIDENTS_AGGREGATION =\n      AggregationBuilders.filter(\n          \"incidents\",\n          joinWithAnd(\n              termQuery(INCIDENT, true), termQuery(JOIN_RELATION, PROCESS_INSTANCE_JOIN_RELATION)));\n  public static final FilterAggregationBuilder RUNNING_AGGREGATION =\n      AggregationBuilders.filter(\n          \"running\", termQuery(ListViewTemplate.STATE, ProcessInstanceState.ACTIVE));\n  private static final Logger LOGGER = LoggerFactory.getLogger(ElasticsearchProcessStore.class);\n  private static final String DISTINCT_FIELD_COUNTS = \"distinctFieldCounts\";\n  private final ProcessIndex processIndex;\n\n  private final ListViewTemplate listViewTemplate;\n\n  private final List<ProcessInstanceDependant> processInstanceDependantTemplates;\n\n  private final ObjectMapper objectMapper;\n\n  private final RestHighLevelClient esClient;\n\n  private final TenantAwareElasticsearchClient tenantAwareClient;\n\n  private final OperateProperties operateProperties;\n\n  public ElasticsearchProcessStore(\n      ProcessIndex processIndex,\n      ListViewTemplate listViewTemplate,\n      List<ProcessInstanceDependant> processInstanceDependantTemplates,\n      ObjectMapper objectMapper,\n      OperateProperties operateProperties,\n      RestHighLevelClient esClient,\n      TenantAwareElasticsearchClient tenantAwareClient) {\n    this.processIndex = processIndex;\n    this.listViewTemplate = listViewTemplate;\n    this.processInstanceDependantTemplates = processInstanceDependantTemplates;\n    this.objectMapper = objectMapper;\n    this.operateProperties = operateProperties;\n    this.esClient = esClient;\n    this.tenantAwareClient = tenantAwareClient;\n  }\n\n  @Override\n  public Optional<Long> getDistinctCountFor(String fieldName) {\n    final String indexAlias = processIndex.getAlias();\n    LOGGER.debug(\"Called distinct count for field {} in index alias {}.\", fieldName, indexAlias);\n    final SearchRequest searchRequest =\n        new SearchRequest(indexAlias)\n            .source(\n                new SearchSourceBuilder()\n                    .query(QueryBuilders.matchAllQuery())\n                    .size(0)\n                    .aggregation(\n                        cardinality(DISTINCT_FIELD_COUNTS)\n                            .precisionThreshold(1_000)\n                            .field(fieldName)));\n    try {\n      final SearchResponse searchResponse = esClient.search(searchRequest, RequestOptions.DEFAULT);\n      final Cardinality distinctFieldCounts =\n          searchResponse.getAggregations().get(DISTINCT_FIELD_COUNTS);\n      return Optional.of(distinctFieldCounts.getValue());\n    } catch (Exception e) {\n      LOGGER.error(\n          String.format(\n              \"Error in distinct count for field %s in index alias %s.\", fieldName, indexAlias),\n          e);\n      return Optional.empty();\n    }\n  }\n\n  @Override\n  public void refreshIndices(String... indices) {\n    if (indices == null || indices.length == 0) {\n      throw new OperateRuntimeException(\"Refresh indices needs at least one index to refresh.\");\n    }\n    try {\n      esClient.indices().refresh(new RefreshRequest(indices), RequestOptions.DEFAULT);\n    } catch (IOException ex) {\n      throw new OperateRuntimeException(\"Failed to refresh indices \" + Arrays.asList(indices), ex);\n    }\n  }\n\n  @Override\n  public ProcessEntity getProcessByKey(Long processDefinitionKey) {\n    final SearchRequest searchRequest =\n        new SearchRequest(processIndex.getAlias())\n            .source(\n                new SearchSourceBuilder()\n                    .query(QueryBuilders.termQuery(ProcessIndex.KEY, processDefinitionKey)));\n\n    try {\n      final SearchResponse response = tenantAwareClient.search(searchRequest);\n      if (response.getHits().getTotalHits().value == 1) {\n        return fromSearchHit(response.getHits().getHits()[0].getSourceAsString());\n      } else if (response.getHits().getTotalHits().value > 1) {\n        throw new NotFoundException(\n            String.format(\"Could not find unique process with key '%s'.\", processDefinitionKey));\n      } else {\n        throw new NotFoundException(\n            String.format(\"Could not find process with key '%s'.\", processDefinitionKey));\n      }\n    } catch (IOException e) {\n      final String message =\n          String.format(\"Exception occurred, while obtaining the process: %s\", e.getMessage());\n      LOGGER.error(message, e);\n      throw new OperateRuntimeException(message, e);\n    }\n  }\n\n```\n\n### Code description\n```Java\n/**\n * Description: This method retrieves the BPMN XML diagram of a process definition by its key.\n * It uses the Elasticsearch client to perform a search request on the process index.\n * If the search returns exactly one hit, it returns the BPMN XML as a string.\n * If the search returns more than one hit, it throws a NotFoundException.\n * If the search returns no hits, it throws a NotFoundException.\n * If an IOException occurs during the search, it logs the error and throws an OperateRuntimeException.\n *\n * @param processDefinitionKey The key of the process definition to retrieve the diagram for.\n * @return The BPMN XML diagram of the process definition as a string.\n * @throws NotFoundException If the process definition is not found or if multiple process definitions are found.\n * @throws OperateRuntimeException If an IOException occurs during the search.\n */\n```\n\n### Code snippet:\n```Java\n  @Override\n  public String getDiagramByKey(Long processDefinitionKey) {\n    final IdsQueryBuilder q = idsQuery().addIds(processDefinitionKey.toString());\n\n    final SearchRequest searchRequest =\n        new SearchRequest(processIndex.getAlias())\n            .source(new SearchSourceBuilder().query(q).fetchSource(BPMN_XML, null));\n\n    try {\n      final SearchResponse response = tenantAwareClient.search(searchRequest);\n\n      if (response.getHits().getTotalHits().value == 1) {\n        final Map<String, Object> result = response.getHits().getHits()[0].getSourceAsMap();\n        return (String) result.get(BPMN_XML);\n```\n\n### Requirements:\n1. Complete only the missing part of the method.\n2. Only answer the code of the completed part. Do not include any of the provided code snippet.\n3. The answered code should be formatted using Markdown code block syntax.\n4. Do not include any additional text.\n\n### Answer: (use the provided format with backticks)\n", "reference": "      } else if (response.getHits().getTotalHits().value > 1) {\n        throw new NotFoundException(\n            String.format(\"Could not find unique process with id '%s'.\", processDefinitionKey));\n      } else {\n        throw new NotFoundException(\n            String.format(\"Could not find process with id '%s'.\", processDefinitionKey));\n      }\n    } catch (IOException e) {\n      final String message =\n          String.format(\n              \"Exception occurred, while obtaining the process diagram: %s\", e.getMessage());\n      LOGGER.error(message, e);\n      throw new OperateRuntimeException(message, e);\n    }\n  }", "completions": ["  \n```Java\n      } else if (response.getHits().getTotalHits().value > 1) {\n        throw new NotFoundException(\n            String.format(\"Could not find unique process with key '%s'.\", processDefinitionKey));\n      } else {\n        throw new NotFoundException(\n            String.format(\"Could not find process with key '%s'.\", processDefinitionKey));\n      }\n    } catch (IOException e) {\n      final String message =\n          String.format(\"Exception occurred, while obtaining the process: %s\", e.getMessage());\n      LOGGER.error(message, e);\n      throw new OperateRuntimeException(message, e);\n    }\n  }\n\n  @Override\n  public List<ProcessEntity> getProcessesByKeys(List<Long> processDefinitionKeys) {\n    final SearchRequest searchRequest =\n        new SearchRequest(processIndex.getAlias())\n           .source(\n                new SearchSourceBuilder()\n                   .query(QueryBuilders.termsQuery(ProcessIndex.KEY, processDefinitionKeys)));\n\n    try {\n      final SearchResponse response = tenantAwareClient.search(searchRequest);\n      final SearchHits hits = response.getHits();\n      final List<ProcessEntity> processes = new ArrayList<>(hits.getHits().length);\n      for (SearchHit hit : hits.getHits()) {\n        processes.add(fromSearchHit(hit.getSourceAsString()));\n      }\n      return processes;\n    } catch (IOException e) {\n      final String message =\n          String.format(\"Exception occurred, while obtaining the processes: %s\", e.getMessage());\n      LOGGER.error(message, e);\n      throw new OperateRuntimeException(message, e);\n    }\n  }\n\n  @Override\n  public List<ProcessEntity> getProcessesByName(String processDefinitionName) {\n    final SearchRequest searchRequest =\n        new SearchRequest(processIndex.getAlias())\n           .source(\n                new SearchSourceBuilder()\n                   .query(QueryBuilders.termQuery(ProcessIndex.NAME, processDefinitionName)));\n\n    try {\n      final SearchResponse response = tenantAwareClient.search(searchRequest);\n      final SearchHits hits = response.getHits();\n      final List<ProcessEntity> processes = new ArrayList<>(hits.getHits().length);\n      for (SearchHit hit : hits.getHits()) {\n        processes.add(fromSearchHit(hit.getSourceAsString()));\n      }\n      return processes;\n    } catch (IOException e) {\n      final String message =\n          String.format(\"Exception occurred, while obtaining the processes: %s\", e.getMessage());\n      LOGGER.error(message, e);\n      throw new OperateRuntimeException(message, e);\n    }\n  }\n\n  @Override\n  public List<ProcessEntity> getProcessesByNames(List<String> processDefinitionNames) {\n    final SearchRequest searchRequest =\n        new SearchRequest(processIndex.getAlias())\n           .source(\n                new SearchSourceBuilder()\n                   .query(QueryBuilders.termsQuery(ProcessIndex.NAME, processDefinitionNames)));\n\n    try {\n      final SearchResponse response = tenantAwareClient.search(searchRequest);\n      final SearchHits hits = response.getHits();\n      final List<ProcessEntity> processes = new ArrayList<>(hits.getHits().length);\n      for (SearchHit hit : hits.getHits()) {\n        processes.add(fromSearchHit(hit.getSourceAsString()));\n      }\n      return processes;\n    } catch (IOException e) {\n      final String message =\n          String.format(\"Exception occurred, while obtaining the processes: %s\", e.getMessage());\n      LOGGER.error(message, e);\n      throw new OperateRuntimeException(message, e);\n    }\n  }\n\n  @Override\n  public List<ProcessEntity> getProcessesByDeploymentKey(Long deploymentKey) {\n    final SearchRequest searchRequest =\n        new SearchRequest(processIndex.getAlias())\n           .source(\n                new SearchSourceBuilder()\n                   .query(QueryBuilders.termQuery(ProcessIndex.DEPLOYMENT_KEY, deploymentKey)));"]}
{"id": 38, "name": "getProcessesGrouped", "prompt": "### Role:\nYou are an experienced Java programmer assistant responsible for code completion tasks. Please intelligently complete the missing parts of the provided code snippet.\n\n### Task:\nBased on the provided code snippet, complete the missing method implementation. Ensure the code is logically correct, syntactically accurate, and follows best coding practices.\n\n### Left context\n```Java\n/*\n * Copyright Camunda Services GmbH\n *\n * BY INSTALLING, DOWNLOADING, ACCESSING, USING, OR DISTRIBUTING THE SOFTWARE (“USE”), YOU INDICATE YOUR ACCEPTANCE TO AND ARE ENTERING INTO A CONTRACT WITH, THE LICENSOR ON THE TERMS SET OUT IN THIS AGREEMENT. IF YOU DO NOT AGREE TO THESE TERMS, YOU MUST NOT USE THE SOFTWARE. IF YOU ARE RECEIVING THE SOFTWARE ON BEHALF OF A LEGAL ENTITY, YOU REPRESENT AND WARRANT THAT YOU HAVE THE ACTUAL AUTHORITY TO AGREE TO THE TERMS AND CONDITIONS OF THIS AGREEMENT ON BEHALF OF SUCH ENTITY.\n * “Licensee” means you, an individual, or the entity on whose behalf you receive the Software.\n *\n * Permission is hereby granted, free of charge, to the Licensee obtaining a copy of this Software and associated documentation files to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject in each case to the following conditions:\n * Condition 1: If the Licensee distributes the Software or any derivative works of the Software, the Licensee must attach this Agreement.\n * Condition 2: Without limiting other conditions in this Agreement, the grant of rights is solely for non-production use as defined below.\n * \"Non-production use\" means any use of the Software that is not directly related to creating products, services, or systems that generate revenue or other direct or indirect economic benefits.  Examples of permitted non-production use include personal use, educational use, research, and development. Examples of prohibited production use include, without limitation, use for commercial, for-profit, or publicly accessible systems or use for commercial or revenue-generating purposes.\n *\n * If the Licensee is in breach of the Conditions, this Agreement, including the rights granted under it, will automatically terminate with immediate effect.\n *\n * SUBJECT AS SET OUT BELOW, THE SOFTWARE IS PROVIDED “AS IS”, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE, AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n * NOTHING IN THIS AGREEMENT EXCLUDES OR RESTRICTS A PARTY’S LIABILITY FOR (A) DEATH OR PERSONAL INJURY CAUSED BY THAT PARTY’S NEGLIGENCE, (B) FRAUD, OR (C) ANY OTHER LIABILITY TO THE EXTENT THAT IT CANNOT BE LAWFULLY EXCLUDED OR RESTRICTED.\n */\npackage io.camunda.operate.store.elasticsearch;\n\nimport static io.camunda.operate.schema.indices.ProcessIndex.BPMN_XML;\nimport static io.camunda.operate.schema.templates.FlowNodeInstanceTemplate.TREE_PATH;\nimport static io.camunda.operate.schema.templates.ListViewTemplate.BPMN_PROCESS_ID;\nimport static io.camunda.operate.schema.templates.ListViewTemplate.ID;\nimport static io.camunda.operate.schema.templates.ListViewTemplate.INCIDENT;\nimport static io.camunda.operate.schema.templates.ListViewTemplate.JOIN_RELATION;\nimport static io.camunda.operate.schema.templates.ListViewTemplate.KEY;\nimport static io.camunda.operate.schema.templates.ListViewTemplate.PARENT_PROCESS_INSTANCE_KEY;\nimport static io.camunda.operate.schema.templates.ListViewTemplate.PROCESS_INSTANCE_JOIN_RELATION;\nimport static io.camunda.operate.schema.templates.ListViewTemplate.PROCESS_KEY;\nimport static io.camunda.operate.schema.templates.ListViewTemplate.PROCESS_NAME;\nimport static io.camunda.operate.schema.templates.ListViewTemplate.STATE;\nimport static io.camunda.operate.util.ElasticsearchUtil.QueryType.ALL;\nimport static io.camunda.operate.util.ElasticsearchUtil.QueryType.ONLY_RUNTIME;\nimport static io.camunda.operate.util.ElasticsearchUtil.UPDATE_RETRY_COUNT;\nimport static io.camunda.operate.util.ElasticsearchUtil.createSearchRequest;\nimport static io.camunda.operate.util.ElasticsearchUtil.joinWithAnd;\nimport static io.camunda.operate.util.ElasticsearchUtil.scrollWith;\nimport static org.elasticsearch.index.query.QueryBuilders.constantScoreQuery;\nimport static org.elasticsearch.index.query.QueryBuilders.idsQuery;\nimport static org.elasticsearch.index.query.QueryBuilders.matchAllQuery;\nimport static org.elasticsearch.index.query.QueryBuilders.termQuery;\nimport static org.elasticsearch.index.query.QueryBuilders.termsQuery;\nimport static org.elasticsearch.search.aggregations.AggregationBuilders.cardinality;\nimport static org.elasticsearch.search.aggregations.AggregationBuilders.terms;\nimport static org.elasticsearch.search.aggregations.AggregationBuilders.topHits;\n\nimport com.fasterxml.jackson.databind.ObjectMapper;\nimport io.camunda.operate.conditions.ElasticsearchCondition;\nimport io.camunda.operate.entities.ProcessEntity;\nimport io.camunda.operate.entities.listview.ProcessInstanceForListViewEntity;\nimport io.camunda.operate.entities.listview.ProcessInstanceState;\nimport io.camunda.operate.exceptions.OperateRuntimeException;\nimport io.camunda.operate.property.OperateProperties;\nimport io.camunda.operate.schema.indices.ProcessIndex;\nimport io.camunda.operate.schema.templates.ListViewTemplate;\nimport io.camunda.operate.schema.templates.OperationTemplate;\nimport io.camunda.operate.schema.templates.ProcessInstanceDependant;\nimport io.camunda.operate.schema.templates.TemplateDescriptor;\nimport io.camunda.operate.store.NotFoundException;\nimport io.camunda.operate.store.ProcessStore;\nimport io.camunda.operate.tenant.TenantAwareElasticsearchClient;\nimport io.camunda.operate.util.ElasticsearchUtil;\nimport io.camunda.operate.util.TreePath;\nimport java.io.IOException;\nimport java.util.ArrayList;\nimport java.util.Arrays;\nimport java.util.HashMap;\nimport java.util.List;\nimport java.util.Map;\nimport java.util.Optional;\nimport java.util.Set;\nimport java.util.stream.Collectors;\nimport org.elasticsearch.action.admin.indices.refresh.RefreshRequest;\nimport org.elasticsearch.action.bulk.BulkRequest;\nimport org.elasticsearch.action.search.SearchRequest;\nimport org.elasticsearch.action.search.SearchResponse;\nimport org.elasticsearch.action.update.UpdateRequest;\nimport org.elasticsearch.client.RequestOptions;\nimport org.elasticsearch.client.RestHighLevelClient;\nimport org.elasticsearch.index.query.BoolQueryBuilder;\nimport org.elasticsearch.index.query.IdsQueryBuilder;\nimport org.elasticsearch.index.query.QueryBuilder;\nimport org.elasticsearch.index.query.QueryBuilders;\nimport org.elasticsearch.index.query.TermQueryBuilder;\nimport org.elasticsearch.index.query.TermsQueryBuilder;\nimport org.elasticsearch.index.reindex.BulkByScrollResponse;\nimport org.elasticsearch.index.reindex.DeleteByQueryRequest;\nimport org.elasticsearch.search.SearchHit;\nimport org.elasticsearch.search.SearchHits;\nimport org.elasticsearch.search.aggregations.AggregationBuilder;\nimport org.elasticsearch.search.aggregations.AggregationBuilders;\nimport org.elasticsearch.search.aggregations.Aggregations;\nimport org.elasticsearch.search.aggregations.bucket.SingleBucketAggregation;\nimport org.elasticsearch.search.aggregations.bucket.filter.FilterAggregationBuilder;\nimport org.elasticsearch.search.aggregations.bucket.terms.Terms;\nimport org.elasticsearch.search.aggregations.metrics.Cardinality;\nimport org.elasticsearch.search.aggregations.metrics.TopHits;\nimport org.elasticsearch.search.builder.SearchSourceBuilder;\nimport org.elasticsearch.search.sort.SortOrder;\nimport org.slf4j.Logger;\nimport org.slf4j.LoggerFactory;\nimport org.springframework.context.annotation.Conditional;\nimport org.springframework.lang.Nullable;\nimport org.springframework.stereotype.Component;\n\n@Conditional(ElasticsearchCondition.class)\n@Component\npublic class ElasticsearchProcessStore implements ProcessStore {\n  public static final FilterAggregationBuilder INCIDENTS_AGGREGATION =\n      AggregationBuilders.filter(\n          \"incidents\",\n          joinWithAnd(\n              termQuery(INCIDENT, true), termQuery(JOIN_RELATION, PROCESS_INSTANCE_JOIN_RELATION)));\n  public static final FilterAggregationBuilder RUNNING_AGGREGATION =\n      AggregationBuilders.filter(\n          \"running\", termQuery(ListViewTemplate.STATE, ProcessInstanceState.ACTIVE));\n  private static final Logger LOGGER = LoggerFactory.getLogger(ElasticsearchProcessStore.class);\n  private static final String DISTINCT_FIELD_COUNTS = \"distinctFieldCounts\";\n  private final ProcessIndex processIndex;\n\n  private final ListViewTemplate listViewTemplate;\n\n  private final List<ProcessInstanceDependant> processInstanceDependantTemplates;\n\n  private final ObjectMapper objectMapper;\n\n  private final RestHighLevelClient esClient;\n\n  private final TenantAwareElasticsearchClient tenantAwareClient;\n\n  private final OperateProperties operateProperties;\n\n  public ElasticsearchProcessStore(\n      ProcessIndex processIndex,\n      ListViewTemplate listViewTemplate,\n      List<ProcessInstanceDependant> processInstanceDependantTemplates,\n      ObjectMapper objectMapper,\n      OperateProperties operateProperties,\n      RestHighLevelClient esClient,\n      TenantAwareElasticsearchClient tenantAwareClient) {\n    this.processIndex = processIndex;\n    this.listViewTemplate = listViewTemplate;\n    this.processInstanceDependantTemplates = processInstanceDependantTemplates;\n    this.objectMapper = objectMapper;\n    this.operateProperties = operateProperties;\n    this.esClient = esClient;\n    this.tenantAwareClient = tenantAwareClient;\n  }\n\n  @Override\n  public Optional<Long> getDistinctCountFor(String fieldName) {\n    final String indexAlias = processIndex.getAlias();\n    LOGGER.debug(\"Called distinct count for field {} in index alias {}.\", fieldName, indexAlias);\n    final SearchRequest searchRequest =\n        new SearchRequest(indexAlias)\n            .source(\n                new SearchSourceBuilder()\n                    .query(QueryBuilders.matchAllQuery())\n                    .size(0)\n                    .aggregation(\n                        cardinality(DISTINCT_FIELD_COUNTS)\n                            .precisionThreshold(1_000)\n                            .field(fieldName)));\n    try {\n      final SearchResponse searchResponse = esClient.search(searchRequest, RequestOptions.DEFAULT);\n      final Cardinality distinctFieldCounts =\n          searchResponse.getAggregations().get(DISTINCT_FIELD_COUNTS);\n      return Optional.of(distinctFieldCounts.getValue());\n    } catch (Exception e) {\n      LOGGER.error(\n          String.format(\n              \"Error in distinct count for field %s in index alias %s.\", fieldName, indexAlias),\n          e);\n      return Optional.empty();\n    }\n  }\n\n  @Override\n  public void refreshIndices(String... indices) {\n    if (indices == null || indices.length == 0) {\n      throw new OperateRuntimeException(\"Refresh indices needs at least one index to refresh.\");\n    }\n    try {\n      esClient.indices().refresh(new RefreshRequest(indices), RequestOptions.DEFAULT);\n    } catch (IOException ex) {\n      throw new OperateRuntimeException(\"Failed to refresh indices \" + Arrays.asList(indices), ex);\n    }\n  }\n\n  @Override\n  public ProcessEntity getProcessByKey(Long processDefinitionKey) {\n    final SearchRequest searchRequest =\n        new SearchRequest(processIndex.getAlias())\n            .source(\n                new SearchSourceBuilder()\n                    .query(QueryBuilders.termQuery(ProcessIndex.KEY, processDefinitionKey)));\n\n    try {\n      final SearchResponse response = tenantAwareClient.search(searchRequest);\n      if (response.getHits().getTotalHits().value == 1) {\n        return fromSearchHit(response.getHits().getHits()[0].getSourceAsString());\n      } else if (response.getHits().getTotalHits().value > 1) {\n        throw new NotFoundException(\n            String.format(\"Could not find unique process with key '%s'.\", processDefinitionKey));\n      } else {\n        throw new NotFoundException(\n            String.format(\"Could not find process with key '%s'.\", processDefinitionKey));\n      }\n    } catch (IOException e) {\n      final String message =\n          String.format(\"Exception occurred, while obtaining the process: %s\", e.getMessage());\n      LOGGER.error(message, e);\n      throw new OperateRuntimeException(message, e);\n    }\n  }\n\n  @Override\n  public String getDiagramByKey(Long processDefinitionKey) {\n    final IdsQueryBuilder q = idsQuery().addIds(processDefinitionKey.toString());\n\n    final SearchRequest searchRequest =\n        new SearchRequest(processIndex.getAlias())\n            .source(new SearchSourceBuilder().query(q).fetchSource(BPMN_XML, null));\n\n    try {\n      final SearchResponse response = tenantAwareClient.search(searchRequest);\n\n      if (response.getHits().getTotalHits().value == 1) {\n        final Map<String, Object> result = response.getHits().getHits()[0].getSourceAsMap();\n        return (String) result.get(BPMN_XML);\n      } else if (response.getHits().getTotalHits().value > 1) {\n        throw new NotFoundException(\n            String.format(\"Could not find unique process with id '%s'.\", processDefinitionKey));\n      } else {\n        throw new NotFoundException(\n            String.format(\"Could not find process with id '%s'.\", processDefinitionKey));\n      }\n    } catch (IOException e) {\n      final String message =\n          String.format(\n              \"Exception occurred, while obtaining the process diagram: %s\", e.getMessage());\n      LOGGER.error(message, e);\n      throw new OperateRuntimeException(message, e);\n    }\n  }\n\n```\n\n### Code description\n```Java\n/**\n * Description: This method retrieves grouped processes from the Elasticsearch index.\n * It uses the Elasticsearch aggregation API to group processes by tenant ID and BPMN process ID.\n * Each group is further aggregated to get the top hits (processes) sorted by version in descending order.\n * The method returns a map where the key is a ProcessKey object and the value is a list of ProcessEntity objects.\n * \n * @param tenantId The ID of the tenant to filter the processes.\n * @param allowedBPMNProcessIds A set of allowed BPMN process IDs to filter the processes.\n * @return A map where the key is a ProcessKey object and the value is a list of ProcessEntity objects.\n * @throws OperateRuntimeException If an IOException occurs during the search operation.\n */\n```\n\n### Code snippet:\n```Java\n  @Override\n  public Map<ProcessKey, List<ProcessEntity>> getProcessesGrouped(\n      String tenantId, @Nullable Set<String> allowedBPMNProcessIds) {\n    final String tenantsGroupsAggName = \"group_by_tenantId\";\n    final String groupsAggName = \"group_by_bpmnProcessId\";\n    final String processesAggName = \"processes\";\n\n    final AggregationBuilder agg =\n        terms(tenantsGroupsAggName)\n            .field(ProcessIndex.TENANT_ID)\n            .size(ElasticsearchUtil.TERMS_AGG_SIZE)\n            .subAggregation(\n                terms(groupsAggName)\n                    .field(ProcessIndex.BPMN_PROCESS_ID)\n                    .size(ElasticsearchUtil.TERMS_AGG_SIZE)\n                    .subAggregation(\n                        topHits(processesAggName)\n                            .fetchSource(\n                                new String[] {\n                                  ProcessIndex.ID,\n                                  ProcessIndex.NAME,\n                                  ProcessIndex.VERSION,\n                                  ProcessIndex.BPMN_PROCESS_ID,\n                                  ProcessIndex.TENANT_ID\n                                },\n                                null)\n                            .size(ElasticsearchUtil.TOPHITS_AGG_SIZE)\n                            .sort(ProcessIndex.VERSION, SortOrder.DESC)));\n\n    final SearchSourceBuilder sourceBuilder = new SearchSourceBuilder().aggregation(agg).size(0);\n    sourceBuilder.query(buildQuery(tenantId, allowedBPMNProcessIds));\n    final SearchRequest searchRequest =\n        new SearchRequest(processIndex.getAlias()).source(sourceBuilder);\n\n    try {\n```\n\n### Requirements:\n1. Complete only the missing part of the method.\n2. Only answer the code of the completed part. Do not include any of the provided code snippet.\n3. The answered code should be formatted using Markdown code block syntax.\n4. Do not include any additional text.\n\n### Answer: (use the provided format with backticks)\n", "reference": "      final SearchResponse searchResponse = tenantAwareClient.search(searchRequest);\n      final Terms groups = searchResponse.getAggregations().get(tenantsGroupsAggName);\n      final Map<ProcessKey, List<ProcessEntity>> result = new HashMap<>();\n\n      groups.getBuckets().stream()\n          .forEach(\n              b -> {\n                final String groupTenantId = b.getKeyAsString();\n                final Terms processGroups = b.getAggregations().get(groupsAggName);\n\n                processGroups.getBuckets().stream()\n                    .forEach(\n                        tenantB -> {\n                          final String bpmnProcessId = tenantB.getKeyAsString();\n                          final ProcessKey groupKey = new ProcessKey(bpmnProcessId, groupTenantId);\n                          result.put(groupKey, new ArrayList<>());\n\n                          final TopHits processes = tenantB.getAggregations().get(processesAggName);\n                          final SearchHit[] hits = processes.getHits().getHits();\n                          for (SearchHit searchHit : hits) {\n                            final ProcessEntity processEntity =\n                                fromSearchHit(searchHit.getSourceAsString());\n                            result.get(groupKey).add(processEntity);\n                          }\n                        });\n              });\n\n      return result;\n    } catch (IOException e) {\n      final String message =\n          String.format(\n              \"Exception occurred, while obtaining grouped processes: %s\", e.getMessage());\n      LOGGER.error(message, e);\n      throw new OperateRuntimeException(message, e);\n    }\n  }", "completions": ["  \n```Java\n      final SearchResponse response = tenantAwareClient.search(searchRequest);\n\n      final Map<ProcessKey, List<ProcessEntity>> result = new HashMap<>();\n      final Aggregations aggregations = response.getAggregations();\n      final Terms tenantsGroups = aggregations.get(tenantsGroupsAggName);\n      for (Terms.Bucket tenantBucket : tenantsGroups.getBuckets()) {\n        final Terms groups = tenantBucket.getAggregations().get(groupsAggName);\n        for (Terms.Bucket groupBucket : groups.getBuckets()) {\n          final TopHits hits = groupBucket.getAggregations().get(processesAggName);\n          final SearchHits processHits = hits.getHits();\n          final List<ProcessEntity> processes = new ArrayList<>(processHits.getHits().length);\n          for (SearchHit processHit : processHits) {\n            final Map<String, Object> source = processHit.getSourceAsMap();\n            final ProcessEntity process = new ProcessEntity();\n            process.setId((String) source.get(ProcessIndex.ID));\n            process.setName((String) source.get(ProcessIndex.NAME));\n            process.setVersion((Integer) source.get(ProcessIndex.VERSION));\n            process.setBpmnProcessId((String) source.get(ProcessIndex.BPMN_PROCESS_ID));\n            process.setTenantId((String) source.get(ProcessIndex.TENANT_ID));\n            processes.add(process);\n          }\n          result.put(new ProcessKey(tenantBucket.getKeyAsString(), groupBucket.getKeyAsString()), processes);\n        }\n      }\n      return result;\n    } catch (IOException e) {\n      final String message =\n          String.format(\"Exception occurred, while obtaining the processes: %s\", e.getMessage());\n      LOGGER.error(message, e);\n      throw new OperateRuntimeException(message, e);\n    }\n  }\n\n  private QueryBuilder buildQuery(String tenantId, @Nullable Set<String> allowedBPMNProcessIds) {\n    final BoolQueryBuilder query = QueryBuilders.boolQuery();\n    if (tenantId!= null) {\n      query.must(termQuery(ProcessIndex.TENANT_ID, tenantId));\n    }\n    if (allowedBPMNProcessIds!= null &&!allowedBPMNProcessIds.isEmpty()) {\n      query.must(termsQuery(ProcessIndex.BPMN_PROCESS_ID, allowedBPMNProcessIds));\n    }\n    return query;\n  }\n\n  @Override\n  public void updateProcess(ProcessEntity process) {\n    final UpdateRequest updateRequest =\n        new UpdateRequest(processIndex.getAlias(), process.getId())\n           .doc(toMap(process), XContentType.JSON)\n           .fetchSource(true)\n           .retryOnConflict(UPDATE_RETRY_COUNT);\n    try {\n      tenantAwareClient.update(updateRequest, RequestOptions.DEFAULT);\n    } catch (IOException e) {\n      final String message =\n          String.format(\"Exception occurred, while updating the process: %s\", e.getMessage());\n      LOGGER.error(message, e);\n      throw new OperateRuntimeException(message, e);\n    }\n  }\n\n  @Override\n  public void deleteProcess(String processId) {\n    final DeleteByQueryRequest deleteRequest =\n        new DeleteByQueryRequest(processIndex.getAlias())\n           .setQuery(termQuery(ProcessIndex.ID, processId))\n           .setRefresh(true);\n    try {\n      final BulkByScrollResponse response =\n          tenantAwareClient.deleteByQuery(deleteRequest, RequestOptions.DEFAULT);\n      if (response.getDeleted()!= 1) {\n        throw new NotFoundException(String.format(\"Could not find process with id '%s'.\", processId));\n      }\n    } catch (IOException e) {\n      final String message =\n          String.format(\"Exception occurred, while deleting the process: %s\", e.getMessage());\n      LOGGER.error(message, e"]}
{"id": 39, "name": "clone", "prompt": "### Role:\nYou are an experienced Java programmer assistant responsible for code completion tasks. Please intelligently complete the missing parts of the provided code snippet.\n\n### Task:\nBased on the provided code snippet, complete the missing method implementation. Ensure the code is logically correct, syntactically accurate, and follows best coding practices.\n\n### Left context\n```Java\npackage io.hypersistence.utils.hibernate.type.util;\n\nimport com.fasterxml.jackson.databind.JavaType;\nimport com.fasterxml.jackson.databind.JsonNode;\nimport com.fasterxml.jackson.databind.type.TypeFactory;\nimport org.hibernate.internal.util.SerializationHelper;\nimport org.hibernate.type.SerializationException;\n\nimport java.io.Serializable;\nimport java.util.Collection;\nimport java.util.Map;\n\n/**\n * @author Vlad Mihalcea\n */\npublic class ObjectMapperJsonSerializer implements JsonSerializer {\n\n    private final ObjectMapperWrapper objectMapperWrapper;\n\n    public ObjectMapperJsonSerializer(ObjectMapperWrapper objectMapperWrapper) {\n        this.objectMapperWrapper = objectMapperWrapper;\n    }\n\n```\n\n### Code description\n```Java\n/**\n * Clones the given object using different strategies depending on its type.\n * If the object is a String, it returns the object as is.\n * If the object is a Collection, it finds the first non-null element and clones the collection using the object mapper if the first element is not serializable.\n * If the object is a Map, it finds the first non-null entry and clones the map using the object mapper if the key or value is not serializable.\n * If the object is a JsonNode, it returns a deep copy of the node.\n * If the object is serializable, it clones the object using the serialization helper.\n * If the object is not serializable, it clones the object using a JSON marshaling strategy.\n *\n * @param object the object to clone\n * @param <T> the type of the object\n * @return the cloned object\n */\n```\n\n### Code snippet:\n```Java\n    @Override\n    public <T> T clone(T object) {\n        if (object instanceof String) {\n            return object;\n        } else if (object instanceof Collection) {\n            Object firstElement = findFirstNonNullElement((Collection) object);\n            if (firstElement != null && !(firstElement instanceof Serializable)) {\n                JavaType type = TypeFactory.defaultInstance().constructParametricType(object.getClass(), firstElement.getClass());\n                return objectMapperWrapper.fromBytes(objectMapperWrapper.toBytes(object), type);\n            }\n        } else if (object instanceof Map) {\n            Map.Entry firstEntry = this.findFirstNonNullEntry((Map) object);\n            if (firstEntry != null) {\n                Object key = firstEntry.getKey();\n                Object value = firstEntry.getValue();\n                if (!(key instanceof Serializable) || !(value instanceof Serializable)) {\n                    JavaType type = TypeFactory.defaultInstance().constructParametricType(object.getClass(), key.getClass(), value.getClass());\n```\n\n### Requirements:\n1. Complete only the missing part of the method.\n2. Only answer the code of the completed part. Do not include any of the provided code snippet.\n3. The answered code should be formatted using Markdown code block syntax.\n4. Do not include any additional text.\n\n### Answer: (use the provided format with backticks)\n", "reference": "                    return (T) objectMapperWrapper.fromBytes(objectMapperWrapper.toBytes(object), type);\n                }\n            }\n        } else if (object instanceof JsonNode) {\n            return (T) ((JsonNode) object).deepCopy();\n        }\n\n        if (object instanceof Serializable) {\n            try {\n                return (T) SerializationHelper.clone((Serializable) object);\n            } catch (SerializationException e) {\n                //it is possible that object itself implements java.io.Serializable, but underlying structure does not\n                //in this case we switch to the other JSON marshaling strategy which doesn't use the Java serialization\n            }\n        }\n\n        return jsonClone(object);\n    }", "completions": ["  ```Java\n} else if (object instanceof Map) {\n    Map.Entry firstEntry = this.findFirstNonNullEntry((Map) object);\n    if (firstEntry!= null) {\n        Object key = firstEntry.getKey();\n        Object value = firstEntry.getValue();\n        if (!(key instanceof Serializable) ||!(value instanceof Serializable)) {\n            JavaType type = TypeFactory.defaultInstance().constructParametricType(object.getClass(), key.getClass(), value.getClass());\n            return objectMapperWrapper.fromBytes(objectMapperWrapper.toBytes(object), type);\n        }\n    }\n} else if (object instanceof JsonNode) {\n    return objectMapperWrapper.fromBytes(objectMapperWrapper.toBytes(object), JsonNode.class);\n} else if (object instanceof Serializable) {\n    return SerializationHelper.clone(object);\n} else {\n    return objectMapperWrapper.fromBytes(objectMapperWrapper.toBytes(object), object.getClass());\n}\n```"]}
{"id": 40, "name": "sign", "prompt": "### Role:\nYou are an experienced Java programmer assistant responsible for code completion tasks. Please intelligently complete the missing parts of the provided code snippet.\n\n### Task:\nBased on the provided code snippet, complete the missing method implementation. Ensure the code is logically correct, syntactically accurate, and follows best coding practices.\n\n### Left context\n```Java\n/*\n * Copyright (c) 2023 OceanBase.\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\npackage com.oceanbase.odc.service.iam;\n\nimport java.util.Base64;\nimport java.util.Collections;\nimport java.util.Date;\nimport java.util.HashMap;\nimport java.util.List;\nimport java.util.Map;\nimport java.util.Objects;\nimport java.util.UUID;\nimport java.util.concurrent.TimeUnit;\nimport java.util.concurrent.locks.Lock;\n\nimport org.apache.commons.lang3.StringUtils;\nimport org.springframework.beans.factory.annotation.Autowired;\nimport org.springframework.integration.jdbc.lock.JdbcLockRegistry;\nimport org.springframework.stereotype.Service;\n\nimport com.auth0.jwt.JWT;\nimport com.auth0.jwt.JWTCreator;\nimport com.auth0.jwt.JWTVerifier;\nimport com.auth0.jwt.algorithms.Algorithm;\nimport com.auth0.jwt.exceptions.TokenExpiredException;\nimport com.auth0.jwt.interfaces.Claim;\nimport com.oceanbase.odc.metadb.config.SystemConfigEntity;\nimport com.oceanbase.odc.service.config.SystemConfigService;\nimport com.oceanbase.odc.service.config.model.Configuration;\n\nimport lombok.extern.slf4j.Slf4j;\n\n@Service\n@Slf4j\npublic class JwtService {\n\n    private static final long TRY_LOCK_TIMEOUT_SECONDS = 5;\n    private static final String LOCK_KEY = \"ODC_JWT_SECRET_LOCK_KEY\";\n    private static final String ENCRYPTION_JWT_KEY_SYSTEM_CONFIG_KEY = \"odc.iam.auth.jwt.secret-key\";\n    @Autowired\n    private JwtProperties jwtProperties;\n\n    /**\n     * Ensure the consistency of automatically generated jwt key in distributed mode\n     * \n     * @param\n     * @param systemConfigService\n     * @param jdbcLockRegistry\n     */\n    public JwtService(JwtProperties jwtProperties,\n            SystemConfigService systemConfigService, JdbcLockRegistry jdbcLockRegistry) {\n        try {\n            if (StringUtils.isNotBlank(jwtProperties.getTokenSecret())) {\n                return;\n            }\n            log.info(\"Try to lock odc jwt secret...\");\n            Lock lock = jdbcLockRegistry.obtain(LOCK_KEY);\n            if (lock.tryLock(TRY_LOCK_TIMEOUT_SECONDS, TimeUnit.SECONDS)) {\n                try {\n                    log.info(\"Successfully acquired the jwt secret lock\");\n                    List<Configuration> list = systemConfigService.queryByKeyPrefix(\n                            ENCRYPTION_JWT_KEY_SYSTEM_CONFIG_KEY);\n                    if (verifySystemConfig(list)) {\n                        jwtProperties.setTokenSecret(list.get(0).getValue());\n                    } else {\n                        String tokenSecret = UUID.randomUUID().toString();\n                        jwtProperties.setTokenSecret(tokenSecret);\n                        SystemConfigEntity jwtSecretKey =\n                                createSystemConfigEntity(ENCRYPTION_JWT_KEY_SYSTEM_CONFIG_KEY,\n                                        tokenSecret, \"ODC jwt secret key\");\n                        systemConfigService.upsert(Collections.singletonList(jwtSecretKey));\n                    }\n\n                } finally {\n                    lock.unlock();\n                }\n            } else {\n                log.info(\n                        \"Failed to get jwt secret lock, try to get jwt secret from system configuration\");\n                List<Configuration> list =\n                        systemConfigService.queryByKeyPrefix(ENCRYPTION_JWT_KEY_SYSTEM_CONFIG_KEY);\n                if (verifySystemConfig(list)) {\n                    jwtProperties.setTokenSecret(list.get(0).getValue());;\n                } else {\n                    throw new RuntimeException(\"Failed to get jwt secret from system configuration\");\n                }\n            }\n        } catch (Exception e) {\n            log.warn(\"Failed to init jwt secret, message={}\", e.getMessage());\n            throw new RuntimeException(e);\n        }\n    }\n\n    private boolean verifySystemConfig(List<Configuration> key) {\n        return key != null && !key.isEmpty() && Objects.nonNull(key.get(0))\n                && StringUtils.isNotBlank(key.get(0).getValue());\n    }\n\n    private SystemConfigEntity createSystemConfigEntity(String key, String value, String description) {\n        SystemConfigEntity entity = new SystemConfigEntity();\n        entity.setKey(key);\n        entity.setValue(value);\n        entity.setDescription(description);\n        return entity;\n    }\n\n    /**\n     * Generate the jwtToken\n     *\n     * @param map\n     * @return\n     */\n```\n\n### Code description\n```Java\n/**\n * Description: This method is used to sign a JWT token with the provided map of claims.\n * \n * @param map A map of claims to be included in the JWT token.\n * @return A signed JWT token as a string, or null if an error occurs during signing.\n */\n```\n\n### Code snippet:\n```Java\n    public String sign(Map<String, Object> map) {\n        try {\n            Date date = new Date(System.currentTimeMillis() + jwtProperties.getExpireTimeMills());\n            Algorithm algorithm = Algorithm.HMAC256(jwtProperties.getTokenSecret());\n            Map<String, Object> header = new HashMap<>(2);\n            header.put(\"typ\", \"jwt\");\n            JWTCreator.Builder builder = JWT.create()\n                    .withHeader(header)\n                    .withIssuedAt(new Date())\n                    .withExpiresAt(date);\n            map.entrySet().forEach(entry -> {\n                if (entry.getValue() instanceof Integer) {\n                    builder.withClaim(entry.getKey(), (Integer) entry.getValue());\n                } else if (entry.getValue() instanceof Long) {\n                    builder.withClaim(entry.getKey(), (Long) entry.getValue());\n```\n\n### Requirements:\n1. Complete only the missing part of the method.\n2. Only answer the code of the completed part. Do not include any of the provided code snippet.\n3. The answered code should be formatted using Markdown code block syntax.\n4. Do not include any additional text.\n\n### Answer: (use the provided format with backticks)\n", "reference": "                } else if (entry.getValue() instanceof Boolean) {\n                    builder.withClaim(entry.getKey(), (Boolean) entry.getValue());\n                } else if (entry.getValue() instanceof String) {\n                    builder.withClaim(entry.getKey(), String.valueOf(entry.getValue()));\n                } else if (entry.getValue() instanceof Double) {\n                    builder.withClaim(entry.getKey(), (Double) entry.getValue());\n                } else if (entry.getValue() instanceof Date) {\n                    builder.withClaim(entry.getKey(), (Date) entry.getValue());\n                }\n            });\n            return builder.sign(algorithm);\n        } catch (Exception e) {\n            return null;\n        }\n    }", "completions": ["  \n```Java\n    public String sign(Map<String, Object> map) {\n        try {\n            Date date = new Date(System.currentTimeMillis() + jwtProperties.getExpireTimeMills());\n            Algorithm algorithm = Algorithm.HMAC256(jwtProperties.getTokenSecret());\n            Map<String, Object> header = new HashMap<>(2);\n            header.put(\"typ\", \"jwt\");\n            JWTCreator.Builder builder = JWT.create()\n                   .withHeader(header)\n                   .withIssuedAt(new Date())\n                   .withExpiresAt(date);\n            map.entrySet().forEach(entry -> {\n                if (entry.getValue() instanceof Integer) {\n                    builder.withClaim(entry.getKey(), (Integer) entry.getValue());\n                } else if (entry.getValue() instanceof Long) {\n                    builder.withClaim(entry.getKey(), (Long) entry.getValue());\n                } else if (entry.getValue() instanceof String) {\n                    builder.withClaim(entry.getKey(), (String) entry.getValue());\n                } else if (entry.getValue() instanceof Boolean) {\n                    builder.withClaim(entry.getKey(), (Boolean) entry.getValue());\n                } else if (entry.getValue() instanceof Date) {\n                    builder.withClaim(entry.getKey(), (Date) entry.getValue());\n                } else if (entry.getValue() instanceof Double) {\n                    builder.withClaim(entry.getKey(), (Double) entry.getValue());\n                } else if (entry.getValue() instanceof Float) {\n                    builder.withClaim(entry.getKey(), (Float) entry.getValue());\n                } else if (entry.getValue() instanceof Byte) {\n                    builder.withClaim(entry.getKey(), (Byte) entry.getValue());\n                } else if (entry.getValue() instanceof Short) {\n                    builder.withClaim(entry.getKey(), (Short) entry.getValue());\n                } else if (entry.getValue() instanceof Character) {\n                    builder.withClaim(entry.getKey(), (Character) entry.getValue());\n                } else if (entry.getValue() instanceof byte[]) {\n                    builder.withClaim(entry.getKey(), (byte[]) entry.getValue());\n                } else if (entry.getValue() instanceof Byte[]) {\n                    builder.withClaim(entry.getKey(), (Byte[]) entry.getValue());\n                } else if (entry.getValue() instanceof Short[]) {\n                    builder.withClaim(entry.getKey(), (Short[]) entry.getValue());\n                } else if (entry.getValue() instanceof Integer[]) {\n                    builder.withClaim(entry.getKey(), (Integer[]) entry.getValue());\n                } else if (entry.getValue() instanceof Long[]) {\n                    builder.withClaim(entry.getKey(), (Long[]) entry.getValue());\n                } else if (entry.getValue() instanceof Float[]) {\n                    builder.withClaim(entry.getKey(), (Float[]) entry.getValue());\n                } else if (entry.getValue() instanceof Double[]) {\n                    builder.withClaim(entry.getKey(), (Double[]) entry.getValue());\n                } else if (entry.getValue() instanceof Boolean[]) {\n                    builder.withClaim(entry.getKey(), (Boolean[]) entry.getValue());\n                } else if (entry.getValue() instanceof String[]) {\n                    builder.withClaim(entry.getKey(), (String[]) entry.getValue());\n                } else if (entry.getValue() instanceof Date[]) {\n                    builder.withClaim(entry.getKey(), (Date[]) entry.getValue());\n                } else if (entry.getValue() instanceof Object[]) {\n                    builder.withClaim(entry.getKey(), (Object[]) entry.getValue());\n                } else {\n                    throw new IllegalArgumentException(\"Unsupported claim type: \" + entry.getValue().getClass().getName());\n                }\n            });\n            return builder.sign(algorithm);\n        } catch (Exception e) {\n            log.error(\"Failed to sign JWT token\", e);\n            return null;\n        }\n    }\n```"]}
{"id": 41, "name": "isJson", "prompt": "### Role:\nYou are an experienced Java programmer assistant responsible for code completion tasks. Please intelligently complete the missing parts of the provided code snippet.\n\n### Task:\nBased on the provided code snippet, complete the missing method implementation. Ensure the code is logically correct, syntactically accurate, and follows best coding practices.\n\n### Left context\n```Java\n/*\n*Copyright (c) Alibaba Group;\n*Licensed under the Apache License, Version 2.0 (the \"License\");\n*you may not use this file except in compliance with the License.\n*You may obtain a copy of the License at\n\n*   http://www.apache.org/licenses/LICENSE-2.0\n\n*Unless required by applicable law or agreed to in writing, software\n*distributed under the License is distributed on an \"AS IS\" BASIS,\n*WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n*See the License for the specific language governing permissions and\n*limitations under the License.\n*/\n\npackage org.example.common.utils;\n\nimport com.fasterxml.jackson.core.JsonProcessingException;\nimport com.fasterxml.jackson.databind.DeserializationFeature;\nimport com.fasterxml.jackson.databind.ObjectMapper;\nimport com.google.gson.ExclusionStrategy;\nimport com.google.gson.FieldAttributes;\nimport com.google.gson.FieldNamingPolicy;\nimport com.google.gson.Gson;\nimport com.google.gson.GsonBuilder;\nimport com.google.gson.JsonElement;\nimport com.google.gson.JsonParser;\nimport com.google.gson.internal.LinkedTreeMap;\nimport com.google.gson.stream.JsonReader;\nimport com.google.gson.stream.JsonToken;\nimport lombok.extern.slf4j.Slf4j;\n\nimport java.io.IOException;\nimport java.io.StringReader;\nimport java.lang.reflect.Type;\nimport java.util.ArrayList;\nimport java.util.List;\nimport java.util.Map;\n\n@Slf4j\npublic class JsonUtil {\n\n    private static final String PERIOD = \".\";\n\n    private static final Gson GSON_WITH_NULL = new GsonBuilder().serializeNulls().create();\n\n    private static final Gson GSON_UPPER_CAMEL_CASE = getUpperGson();\n\n    public static Gson GSON = getGson();\n\n    private static ObjectMapper mapper = new ObjectMapper();\n\n    public static String toJsonString(Object object) {\n        return GSON.toJson(object);\n    }\n\n    public static String toJsonStringExcludeFields(Object object, List<String> fieldList) {\n        Gson gson = new GsonBuilder().addSerializationExclusionStrategy(new ExclusionStrategy() {\n            @Override\n            public boolean shouldSkipField(FieldAttributes fieldAttributes) {\n                return fieldList.contains(fieldAttributes.getName());\n            }\n\n            @Override\n            public boolean shouldSkipClass(Class<?> arg0) {\n                return false;\n            }\n        }).create();\n        return gson.toJson(object);\n    }\n\n    public static String toJsonStringWithNull(Object object) {\n        return GSON_WITH_NULL.toJson(object);\n    }\n\n    public static <T> T parseObject(String json, Class<T> classOfT) {\n        return GSON.fromJson(json, classOfT);\n    }\n\n    public static <T> T parseObjectCustom(String json, Class<T> classOfT) {\n        JsonReader reader = new JsonReader(new StringReader(json));\n        reader.setLenient(true);\n        try {\n            return (T) read(reader);\n        } catch (IOException e) {\n            log.error(\"Json processing exception.\", e);\n            throw new IllegalArgumentException(e);\n        }\n    }\n\n    public static Object read(JsonReader in) throws IOException {\n        JsonToken token = in.peek();\n        //判断字符串的实际类型\n        switch (token) {\n            case BEGIN_ARRAY:\n                List<Object> list = new ArrayList<>();\n                in.beginArray();\n                while (in.hasNext()) {\n                    list.add(read(in));\n                }\n                in.endArray();\n                return list;\n\n            case BEGIN_OBJECT:\n                Map<String, Object> map = new LinkedTreeMap<>();\n                in.beginObject();\n                while (in.hasNext()) {\n                    map.put(in.nextName(), read(in));\n                }\n                in.endObject();\n                return map;\n            case STRING:\n                return in.nextString();\n            case NUMBER:\n                String s = in.nextString();\n                if (s.contains(\".\")) {\n                    return Double.valueOf(s);\n                } else {\n                    return Long.valueOf(s);\n                }\n            case BOOLEAN:\n                return in.nextBoolean();\n            case NULL:\n                in.nextNull();\n                return null;\n            default:\n                throw new IllegalStateException();\n        }\n    }\n\n    public static <T> T parseJsonList(String jsonStr, Type typeOfT) {\n        Gson gson = new Gson();\n        return gson.fromJson(jsonStr, typeOfT);\n    }\n\n```\n\n### Code description\n```Java\n/**\n * Checks if a given string is a valid JSON.\n *\n * @param value The string to be checked.\n * @return True if the string is a valid JSON, false otherwise.\n * @throws JsonSyntaxException If the string cannot be parsed as JSON.\n */\n```\n\n### Code snippet:\n```Java\n    public static Boolean isJson(String value) {\n        JsonElement jsonElement;\n        try {\n            jsonElement = JsonParser.parseString(value);\n        } catch (Exception e) {\n            return false;\n        }\n```\n\n### Requirements:\n1. Complete only the missing part of the method.\n2. Only answer the code of the completed part. Do not include any of the provided code snippet.\n3. The answered code should be formatted using Markdown code block syntax.\n4. Do not include any additional text.\n\n### Answer: (use the provided format with backticks)\n", "reference": "        if (jsonElement == null || jsonElement.isJsonNull()) {\n            return false;\n        }\n        if (!jsonElement.isJsonObject() && !jsonElement.isJsonArray()) {\n            return false;\n        }\n        return true;\n    }", "completions": ["  \n```Java\n    public static Boolean isJson(String value) {\n        JsonElement jsonElement;\n        try {\n            jsonElement = JsonParser.parseString(value);\n        } catch (Exception e) {\n            return false;\n        }\n        return jsonElement.isJsonObject() || jsonElement.isJsonArray();\n    }\n```"]}
{"id": 42, "name": "getCandidateJobs", "prompt": "### Role:\nYou are an experienced Java programmer assistant responsible for code completion tasks. Please intelligently complete the missing parts of the provided code snippet.\n\n### Task:\nBased on the provided code snippet, complete the missing method implementation. Ensure the code is logically correct, syntactically accurate, and follows best coding practices.\n\n### Left context\n```Java\n/**\n * Licensed to the Apache Software Foundation (ASF) under one\n * or more contributor license agreements.  See the NOTICE file\n * distributed with this work for additional information\n * regarding copyright ownership.  The ASF licenses this file\n * to you under the Apache License, Version 2.0 (the\n * \"License\"); you may not use this file except in compliance\n * with the License.  You may obtain a copy of the License at\n *\n *   http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing,\n * software distributed under the License is distributed on an\n * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n * KIND, either express or implied.  See the License for the\n * specific language governing permissions and limitations\n * under the License.\n */\npackage org.apache.pinot.controller.helix.core.rebalance;\n\nimport com.google.common.annotations.VisibleForTesting;\nimport com.google.common.base.Preconditions;\nimport java.util.Collections;\nimport java.util.HashMap;\nimport java.util.HashSet;\nimport java.util.List;\nimport java.util.Map;\nimport java.util.Properties;\nimport java.util.Set;\nimport java.util.concurrent.ExecutorService;\nimport org.apache.commons.lang3.RandomUtils;\nimport org.apache.commons.lang3.StringUtils;\nimport org.apache.commons.lang3.tuple.Pair;\nimport org.apache.pinot.common.metadata.controllerjob.ControllerJobType;\nimport org.apache.pinot.common.metrics.ControllerGauge;\nimport org.apache.pinot.common.metrics.ControllerMeter;\nimport org.apache.pinot.common.metrics.ControllerMetrics;\nimport org.apache.pinot.controller.ControllerConf;\nimport org.apache.pinot.controller.LeadControllerManager;\nimport org.apache.pinot.controller.helix.core.PinotHelixResourceManager;\nimport org.apache.pinot.controller.helix.core.periodictask.ControllerPeriodicTask;\nimport org.apache.pinot.spi.config.table.TableConfig;\nimport org.apache.pinot.spi.utils.CommonConstants;\nimport org.apache.pinot.spi.utils.JsonUtils;\nimport org.slf4j.Logger;\nimport org.slf4j.LoggerFactory;\n\n\n/**\n * Periodic task to check whether a user triggered rebalance job is completed or not, and retry if failed. The retry\n * job is started with the same rebalance configs provided by the user and does best effort to stop the other jobs\n * for the same table. This task can be configured to just check failures and report metrics, and not to do retry.\n */\npublic class RebalanceChecker extends ControllerPeriodicTask<Void> {\n  private static final Logger LOGGER = LoggerFactory.getLogger(RebalanceChecker.class);\n  private static final double RETRY_DELAY_SCALE_FACTOR = 2.0;\n  private final ExecutorService _executorService;\n\n  public RebalanceChecker(PinotHelixResourceManager pinotHelixResourceManager,\n      LeadControllerManager leadControllerManager, ControllerConf config, ControllerMetrics controllerMetrics,\n      ExecutorService executorService) {\n    super(RebalanceChecker.class.getSimpleName(), config.getRebalanceCheckerFrequencyInSeconds(),\n        config.getRebalanceCheckerInitialDelayInSeconds(), pinotHelixResourceManager, leadControllerManager,\n        controllerMetrics);\n    _executorService = executorService;\n  }\n\n  @Override\n  protected void processTables(List<String> tableNamesWithType, Properties periodicTaskProperties) {\n    int numTables = tableNamesWithType.size();\n    LOGGER.info(\"Processing {} tables in task: {}\", numTables, _taskName);\n    int numTablesProcessed = retryRebalanceTables(new HashSet<>(tableNamesWithType));\n    _controllerMetrics.setValueOfGlobalGauge(ControllerGauge.PERIODIC_TASK_NUM_TABLES_PROCESSED, _taskName,\n        numTablesProcessed);\n    LOGGER.info(\"Finish processing {}/{} tables in task: {}\", numTablesProcessed, numTables, _taskName);\n  }\n\n  /**\n   * Rare but the task may be executed by more than one threads because user can trigger the periodic task to run\n   * immediately, in addition to the one scheduled to run periodically. So make this method synchronized to be simple.\n   */\n  private synchronized int retryRebalanceTables(Set<String> tableNamesWithType) {\n    // Get all jobMetadata for all the given tables with a single ZK read.\n    Map<String, Map<String, String>> allJobMetadataByJobId =\n        _pinotHelixResourceManager.getAllJobs(Collections.singleton(ControllerJobType.TABLE_REBALANCE),\n            jobMetadata -> tableNamesWithType.contains(\n                jobMetadata.get(CommonConstants.ControllerJob.TABLE_NAME_WITH_TYPE)));\n    Map<String, Map<String, Map<String, String>>> tableJobMetadataMap = new HashMap<>();\n    allJobMetadataByJobId.forEach((jobId, jobMetadata) -> {\n      String tableNameWithType = jobMetadata.get(CommonConstants.ControllerJob.TABLE_NAME_WITH_TYPE);\n      tableJobMetadataMap.computeIfAbsent(tableNameWithType, k -> new HashMap<>()).put(jobId, jobMetadata);\n    });\n    int numTablesProcessed = 0;\n    for (Map.Entry<String, Map<String, Map<String, String>>> entry : tableJobMetadataMap.entrySet()) {\n      String tableNameWithType = entry.getKey();\n      Map<String, Map<String, String>> allJobMetadata = entry.getValue();\n      try {\n        LOGGER.info(\"Start to retry rebalance for table: {} with {} rebalance jobs tracked\", tableNameWithType,\n            allJobMetadata.size());\n        retryRebalanceTable(tableNameWithType, allJobMetadata);\n        numTablesProcessed++;\n      } catch (Exception e) {\n        LOGGER.error(\"Failed to retry rebalance for table: {}\", tableNameWithType, e);\n        _controllerMetrics.addMeteredTableValue(tableNameWithType + \".\" + _taskName,\n            ControllerMeter.PERIODIC_TASK_ERROR, 1L);\n      }\n    }\n    return numTablesProcessed;\n  }\n\n  @VisibleForTesting\n  void retryRebalanceTable(String tableNameWithType, Map<String, Map<String, String>> allJobMetadata)\n      throws Exception {\n    // Skip retry for the table if rebalance job is still running or has completed, in specific:\n    // 1) Skip retry if any rebalance job is actively running. Being actively running means the job is at IN_PROGRESS\n    // status, and has updated its status kept in ZK within the heartbeat timeout. It's possible that more than one\n    // rebalance jobs are running for the table, but that's fine with idempotent rebalance algorithm.\n    // 2) Skip retry if the most recently started rebalance job has completed with DONE or NO_OP. It's possible that\n    // jobs started earlier may be still running, but they are ignored here.\n    //\n    // Otherwise, we can get a list of failed rebalance jobs, i.e. those at FAILED status; or IN_PROGRESS status but\n    // haven't updated their status kept in ZK within the heartbeat timeout. For those candidate jobs to retry:\n    // 1) Firstly, group them by the original jobIds they retry for so that we can skip those exceeded maxRetry.\n    // 2) For the remaining jobs, we take the one started most recently and retry it with its original configs.\n    // 3) If configured, we can abort the other rebalance jobs for the table by setting their status to FAILED.\n    Map<String/*original jobId*/, Set<Pair<TableRebalanceContext/*job attempts*/, Long\n        /*startTime*/>>> candidateJobs = getCandidateJobs(tableNameWithType, allJobMetadata);\n    if (candidateJobs.isEmpty()) {\n      LOGGER.info(\"Found no failed rebalance jobs for table: {}. Skip retry\", tableNameWithType);\n      return;\n    }\n    _controllerMetrics.addMeteredTableValue(tableNameWithType, ControllerMeter.TABLE_REBALANCE_FAILURE_DETECTED, 1L);\n    Pair<TableRebalanceContext, Long> jobContextAndStartTime = getLatestJob(candidateJobs);\n    if (jobContextAndStartTime == null) {\n      LOGGER.info(\"Rebalance has been retried too many times for table: {}. Skip retry\", tableNameWithType);\n      _controllerMetrics.addMeteredTableValue(tableNameWithType, ControllerMeter.TABLE_REBALANCE_RETRY_TOO_MANY_TIMES,\n          1L);\n      return;\n    }\n    TableRebalanceContext jobCtx = jobContextAndStartTime.getLeft();\n    String prevJobId = jobCtx.getJobId();\n    RebalanceConfig rebalanceConfig = jobCtx.getConfig();\n    long jobStartTimeMs = jobContextAndStartTime.getRight();\n    long retryDelayMs = getRetryDelayInMs(rebalanceConfig.getRetryInitialDelayInMs(), jobCtx.getAttemptId());\n    if (jobStartTimeMs + retryDelayMs > System.currentTimeMillis()) {\n      LOGGER.info(\"Delay retry for failed rebalance job: {} that started at: {}, by: {}ms\", prevJobId, jobStartTimeMs,\n          retryDelayMs);\n      return;\n    }\n    abortExistingJobs(tableNameWithType, _pinotHelixResourceManager);\n    // Get tableConfig only when the table needs to retry rebalance, and get it before submitting rebalance to another\n    // thread, in order to avoid unnecessary ZK reads and making too many ZK reads in a short time.\n    TableConfig tableConfig = _pinotHelixResourceManager.getTableConfig(tableNameWithType);\n    Preconditions.checkState(tableConfig != null, \"Failed to find table config for table: %s\", tableNameWithType);\n    _executorService.submit(() -> {\n      // Retry rebalance in another thread as rebalance can take time.\n      try {\n        retryRebalanceTableWithContext(tableNameWithType, tableConfig, jobCtx);\n      } catch (Throwable t) {\n        LOGGER.error(\"Failed to retry rebalance for table: {} asynchronously\", tableNameWithType, t);\n      }\n    });\n  }\n\n  private void retryRebalanceTableWithContext(String tableNameWithType, TableConfig tableConfig,\n      TableRebalanceContext jobCtx) {\n    String prevJobId = jobCtx.getJobId();\n    RebalanceConfig rebalanceConfig = jobCtx.getConfig();\n    TableRebalanceContext retryCtx =\n        TableRebalanceContext.forRetry(jobCtx.getOriginalJobId(), rebalanceConfig, jobCtx.getAttemptId() + 1);\n    String attemptJobId = retryCtx.getJobId();\n    LOGGER.info(\"Retry rebalance job: {} for table: {} with attempt job: {}\", prevJobId, tableNameWithType,\n        attemptJobId);\n    _controllerMetrics.addMeteredTableValue(tableNameWithType, ControllerMeter.TABLE_REBALANCE_RETRY, 1L);\n    ZkBasedTableRebalanceObserver observer =\n        new ZkBasedTableRebalanceObserver(tableNameWithType, attemptJobId, retryCtx, _pinotHelixResourceManager);\n    RebalanceResult result =\n        _pinotHelixResourceManager.rebalanceTable(tableNameWithType, tableConfig, attemptJobId, rebalanceConfig,\n            observer);\n    LOGGER.info(\"New attempt: {} for table: {} is done with result status: {}\", attemptJobId, tableNameWithType,\n        result.getStatus());\n  }\n\n  @VisibleForTesting\n  static long getRetryDelayInMs(long initDelayMs, int attemptId) {\n    // TODO: Just exponential backoff by factor 2 for now. Can add other retry polices as needed.\n    // The attemptId starts from 1, so minus one as the exponent.\n    double minDelayMs = initDelayMs * Math.pow(RETRY_DELAY_SCALE_FACTOR, attemptId - 1);\n    double maxDelayMs = minDelayMs * RETRY_DELAY_SCALE_FACTOR;\n    return RandomUtils.nextLong((long) minDelayMs, (long) maxDelayMs);\n  }\n\n  private static void abortExistingJobs(String tableNameWithType, PinotHelixResourceManager pinotHelixResourceManager) {\n    boolean updated = pinotHelixResourceManager.updateJobsForTable(tableNameWithType, ControllerJobType.TABLE_REBALANCE,\n        jobMetadata -> {\n          String jobId = jobMetadata.get(CommonConstants.ControllerJob.JOB_ID);\n          try {\n            String jobStatsInStr = jobMetadata.get(RebalanceJobConstants.JOB_METADATA_KEY_REBALANCE_PROGRESS_STATS);\n            TableRebalanceProgressStats jobStats =\n                JsonUtils.stringToObject(jobStatsInStr, TableRebalanceProgressStats.class);\n            if (jobStats.getStatus() != RebalanceResult.Status.IN_PROGRESS) {\n              return;\n            }\n            LOGGER.info(\"Abort rebalance job: {} for table: {}\", jobId, tableNameWithType);\n            jobStats.setStatus(RebalanceResult.Status.ABORTED);\n            jobMetadata.put(RebalanceJobConstants.JOB_METADATA_KEY_REBALANCE_PROGRESS_STATS,\n                JsonUtils.objectToString(jobStats));\n          } catch (Exception e) {\n            LOGGER.error(\"Failed to abort rebalance job: {} for table: {}\", jobId, tableNameWithType, e);\n          }\n        });\n    LOGGER.info(\"Tried to abort existing jobs at best effort and done: {}\", updated);\n  }\n\n  @VisibleForTesting\n  static Pair<TableRebalanceContext, Long> getLatestJob(\n      Map<String, Set<Pair<TableRebalanceContext, Long>>> candidateJobs) {\n    Pair<TableRebalanceContext, Long> candidateJobRun = null;\n    for (Map.Entry<String, Set<Pair<TableRebalanceContext, Long>>> entry : candidateJobs.entrySet()) {\n      // The job configs from all retry jobs are same, as the same set of job configs is used to do retry.\n      // The job metadata kept in ZK is cleaned by submission time order gradually, so we can't compare Set.size()\n      // against maxAttempts, but check retryNum of each run to see if retries have exceeded limit.\n      Set<Pair<TableRebalanceContext, Long>> jobRuns = entry.getValue();\n      int maxAttempts = jobRuns.iterator().next().getLeft().getConfig().getMaxAttempts();\n      Pair<TableRebalanceContext, Long> latestJobRun = null;\n      for (Pair<TableRebalanceContext, Long> jobRun : jobRuns) {\n        if (jobRun.getLeft().getAttemptId() >= maxAttempts) {\n          latestJobRun = null;\n          break;\n        }\n        if (latestJobRun == null || latestJobRun.getRight() < jobRun.getRight()) {\n          latestJobRun = jobRun;\n        }\n      }\n      if (latestJobRun == null) {\n        LOGGER.info(\"Rebalance job: {} had exceeded maxAttempts: {}. Skip retry\", entry.getKey(), maxAttempts);\n        continue;\n      }\n      if (candidateJobRun == null || candidateJobRun.getRight() < latestJobRun.getRight()) {\n        candidateJobRun = latestJobRun;\n      }\n    }\n    return candidateJobRun;\n  }\n\n```\n\n### Code description\n```Java\n/**\n * Description: This method retrieves candidate jobs for rebalancing based on the given job metadata.\n * It filters out jobs that have already completed, are currently running, or have been cancelled.\n * It also checks for jobs that have been stuck and need to be retried.\n *\n * @param tableNameWithType The name of the table to be rebalanced.\n * @param allJobMetadata A map containing metadata for all jobs.\n * @return A map of original job IDs to sets of pairs of TableRebalanceContext and start time in milliseconds.\n *         The map is empty if the job started most recently has already completed.\n * @throws Exception If there is an error while parsing job metadata or converting JSON to object.\n */\n```\n\n### Code snippet:\n```Java\n  @VisibleForTesting\n  static Map<String, Set<Pair<TableRebalanceContext, Long>>> getCandidateJobs(String tableNameWithType,\n      Map<String, Map<String, String>> allJobMetadata)\n      throws Exception {\n    long nowMs = System.currentTimeMillis();\n    Map<String, Set<Pair<TableRebalanceContext, Long>>> candidates = new HashMap<>();\n    // If the job started most recently has already completed, then skip retry for the table.\n    Pair<String, Long> latestStartedJob = null;\n    Pair<String, Long> latestCompletedJob = null;\n    // The processing order of job metadata from the given Map is not deterministic. Track the completed original\n    // jobs so that we can simply skip the retry jobs belonging to the completed original jobs.\n    Map<String, String> completedOriginalJobs = new HashMap<>();\n    Set<String> cancelledOriginalJobs = new HashSet<>();\n    for (Map.Entry<String, Map<String, String>> entry : allJobMetadata.entrySet()) {\n      String jobId = entry.getKey();\n      Map<String, String> jobMetadata = entry.getValue();\n      long statsUpdatedAt = Long.parseLong(jobMetadata.get(CommonConstants.ControllerJob.SUBMISSION_TIME_MS));\n      String jobStatsInStr = jobMetadata.get(RebalanceJobConstants.JOB_METADATA_KEY_REBALANCE_PROGRESS_STATS);\n      if (StringUtils.isEmpty(jobStatsInStr)) {\n        LOGGER.info(\"Skip rebalance job: {} as it has no job progress stats\", jobId);\n        continue;\n      }\n      String jobCtxInStr = jobMetadata.get(RebalanceJobConstants.JOB_METADATA_KEY_REBALANCE_CONTEXT);\n      if (StringUtils.isEmpty(jobCtxInStr)) {\n        LOGGER.info(\"Skip rebalance job: {} as it has no job context\", jobId);\n        continue;\n      }\n      TableRebalanceProgressStats jobStats = JsonUtils.stringToObject(jobStatsInStr, TableRebalanceProgressStats.class);\n      TableRebalanceContext jobCtx = JsonUtils.stringToObject(jobCtxInStr, TableRebalanceContext.class);\n      long jobStartTimeMs = jobStats.getStartTimeMs();\n      if (latestStartedJob == null || latestStartedJob.getRight() < jobStartTimeMs) {\n        latestStartedJob = Pair.of(jobId, jobStartTimeMs);\n      }\n      String originalJobId = jobCtx.getOriginalJobId();\n      RebalanceResult.Status jobStatus = jobStats.getStatus();\n      if (jobStatus == RebalanceResult.Status.DONE || jobStatus == RebalanceResult.Status.NO_OP) {\n        LOGGER.info(\"Skip rebalance job: {} as it has completed with status: {}\", jobId, jobStatus);\n        completedOriginalJobs.put(originalJobId, jobId);\n        if (latestCompletedJob == null || latestCompletedJob.getRight() < jobStartTimeMs) {\n          latestCompletedJob = Pair.of(jobId, jobStartTimeMs);\n        }\n```\n\n### Requirements:\n1. Complete only the missing part of the method.\n2. Only answer the code of the completed part. Do not include any of the provided code snippet.\n3. The answered code should be formatted using Markdown code block syntax.\n4. Do not include any additional text.\n\n### Answer: (use the provided format with backticks)\n", "reference": "        continue;\n      }\n      if (jobStatus == RebalanceResult.Status.FAILED || jobStatus == RebalanceResult.Status.ABORTED) {\n        LOGGER.info(\"Found rebalance job: {} for original job: {} has been stopped with status: {}\", jobId,\n            originalJobId, jobStatus);\n        candidates.computeIfAbsent(originalJobId, (k) -> new HashSet<>()).add(Pair.of(jobCtx, jobStartTimeMs));\n        continue;\n      }\n      if (jobStatus == RebalanceResult.Status.CANCELLED) {\n        LOGGER.info(\"Found cancelled rebalance job: {} for original job: {}\", jobId, originalJobId);\n        cancelledOriginalJobs.add(originalJobId);\n        continue;\n      }\n      // Check if an IN_PROGRESS job is still actively running.\n      long heartbeatTimeoutMs = jobCtx.getConfig().getHeartbeatTimeoutInMs();\n      if (nowMs - statsUpdatedAt < heartbeatTimeoutMs) {\n        LOGGER.info(\"Rebalance job: {} is actively running with status updated at: {} within timeout: {}. Skip \"\n            + \"retry for table: {}\", jobId, statsUpdatedAt, heartbeatTimeoutMs, tableNameWithType);\n        return Collections.emptyMap();\n      }\n      // The job is considered failed, but it's possible it is still running, then we might end up with more than one\n      // rebalance jobs running in parallel for a table. The rebalance algorithm is idempotent, so this should be fine\n      // for the correctness.\n      LOGGER.info(\"Found stuck rebalance job: {} for original job: {}\", jobId, originalJobId);\n      candidates.computeIfAbsent(originalJobId, (k) -> new HashSet<>()).add(Pair.of(jobCtx, jobStartTimeMs));\n    }\n    if (latestCompletedJob != null && latestCompletedJob.getLeft().equals(latestStartedJob.getLeft())) {\n      LOGGER.info(\"Rebalance job: {} started most recently has already done. Skip retry for table: {}\",\n          latestCompletedJob.getLeft(), tableNameWithType);\n      return Collections.emptyMap();\n    }\n    for (String jobId : cancelledOriginalJobs) {\n      LOGGER.info(\"Skip original job: {} as it's cancelled\", jobId);\n      candidates.remove(jobId);\n    }\n    for (Map.Entry<String, String> entry : completedOriginalJobs.entrySet()) {\n      LOGGER.info(\"Skip original job: {} as it's completed by attempt: {}\", entry.getKey(), entry.getValue());\n      candidates.remove(entry.getKey());\n    }\n    return candidates;\n  }", "completions": ["  \n```Java\n      } else if (jobStatus == RebalanceResult.Status.IN_PROGRESS) {\n        LOGGER.info(\"Skip rebalance job: {} as it is still in progress\", jobId);\n        continue;\n      } else if (jobStatus == RebalanceResult.Status.FAILED) {\n        LOGGER.info(\"Found failed rebalance job: {} for table: {}\", jobId, tableNameWithType);\n        if (completedOriginalJobs.containsKey(originalJobId)) {\n          LOGGER.info(\"Skip rebalance job: {} as its original job: {} has completed\", jobId, originalJobId);\n          continue;\n        }\n        if (cancelledOriginalJobs.contains(originalJobId)) {\n          LOGGER.info(\"Skip rebalance job: {} as its original job: {} has been cancelled\", jobId, originalJobId);\n          continue;\n        }\n        if (nowMs - statsUpdatedAt > jobCtx.getConfig().getRetryDelayMs()) {\n          LOGGER.info(\"Found candidate rebalance job: {} for table: {}\", jobId, tableNameWithType);\n          candidates.computeIfAbsent(originalJobId, k -> new HashSet<>()).add(Pair.of(jobCtx, jobStartTimeMs));\n        } else {\n          LOGGER.info(\"Skip rebalance job: {} as it has not exceeded retry delay: {}ms\", jobId,\n              jobCtx.getConfig().getRetryDelayMs());\n        }\n      } else if (jobStatus == RebalanceResult.Status.ABORTED) {\n        LOGGER.info(\"Found aborted rebalance job: {} for table: {}\", jobId, tableNameWithType);\n        cancelledOriginalJobs.add(originalJobId);\n      } else {\n        LOGGER.info(\"Skip rebalance job: {} as it has unknown status: {}\", jobId, jobStatus);\n      }\n    }\n    if (latestCompletedJob!= null && latestStartedJob!= null\n        && latestCompletedJob.getRight() > latestStartedJob.getRight()) {\n      LOGGER.info(\"Skip rebalance for table: {} as the latest completed job: {} started at: {} is later than the \"\n              + \"latest started job: {} started at: {}\", tableNameWithType, latestCompletedJob.getLeft(),\n          latestCompletedJob.getRight(), latestStartedJob.getLeft(), latestStartedJob.getRight());\n      return Collections.emptyMap();\n    }\n    return candidates;\n  }\n}\n```"]}
{"id": 43, "name": "sendMessageBack", "prompt": "### Role:\nYou are an experienced Java programmer assistant responsible for code completion tasks. Please intelligently complete the missing parts of the provided code snippet.\n\n### Task:\nBased on the provided code snippet, complete the missing method implementation. Ensure the code is logically correct, syntactically accurate, and follows best coding practices.\n\n### Left context\n```Java\n/*\n * Licensed to the Apache Software Foundation (ASF) under one or more\n * contributor license agreements.  See the NOTICE file distributed with\n * this work for additional information regarding copyright ownership.\n * The ASF licenses this file to You under the Apache License, Version 2.0\n * (the \"License\"); you may not use this file except in compliance with\n * the License.  You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\npackage com.automq.rocketmq.proxy.service;\n\nimport apache.rocketmq.common.v1.Code;\nimport apache.rocketmq.controller.v1.ConsumerGroup;\nimport apache.rocketmq.controller.v1.MessageQueueAssignment;\nimport apache.rocketmq.controller.v1.OngoingMessageQueueReassignment;\nimport apache.rocketmq.controller.v1.StreamRole;\nimport apache.rocketmq.controller.v1.SubscriptionMode;\nimport apache.rocketmq.controller.v1.Topic;\nimport apache.rocketmq.proxy.v1.QueueStats;\nimport apache.rocketmq.proxy.v1.StreamStats;\nimport com.automq.rocketmq.common.config.BrokerConfig;\nimport com.automq.rocketmq.common.config.ProxyConfig;\nimport com.automq.rocketmq.common.exception.ControllerException;\nimport com.automq.rocketmq.common.model.FlatMessageExt;\nimport com.automq.rocketmq.common.model.generated.FlatMessage;\nimport com.automq.rocketmq.common.util.CommonUtil;\nimport com.automq.rocketmq.metadata.api.ProxyMetadataService;\nimport com.automq.rocketmq.proxy.exception.ProxyException;\nimport com.automq.rocketmq.proxy.grpc.ProxyClient;\nimport com.automq.rocketmq.proxy.metrics.ProxyMetricsManager;\nimport com.automq.rocketmq.proxy.model.ProxyContextExt;\nimport com.automq.rocketmq.proxy.model.VirtualQueue;\nimport com.automq.rocketmq.proxy.util.ContextUtil;\nimport com.automq.rocketmq.proxy.util.FlatMessageUtil;\nimport com.automq.rocketmq.proxy.util.ReceiptHandleUtil;\nimport com.automq.rocketmq.store.api.DeadLetterSender;\nimport com.automq.rocketmq.store.api.LogicQueue;\nimport com.automq.rocketmq.store.api.MessageStore;\nimport com.automq.rocketmq.store.exception.StoreException;\nimport com.automq.rocketmq.store.model.StoreContext;\nimport com.automq.rocketmq.store.model.generated.TimerTag;\nimport com.automq.rocketmq.store.model.message.Filter;\nimport com.automq.rocketmq.store.model.message.PutResult;\nimport com.automq.rocketmq.store.model.message.ResetConsumeOffsetResult;\nimport com.automq.rocketmq.store.model.message.SQLFilter;\nimport com.automq.rocketmq.store.model.message.TagFilter;\nimport com.automq.rocketmq.store.model.transaction.TransactionResolution;\nimport io.netty.channel.Channel;\nimport io.opentelemetry.api.trace.SpanKind;\nimport io.opentelemetry.instrumentation.annotations.SpanAttribute;\nimport io.opentelemetry.instrumentation.annotations.WithSpan;\nimport java.nio.ByteBuffer;\nimport java.time.Duration;\nimport java.util.ArrayList;\nimport java.util.Collections;\nimport java.util.HashSet;\nimport java.util.List;\nimport java.util.Map;\nimport java.util.Objects;\nimport java.util.Optional;\nimport java.util.Set;\nimport java.util.concurrent.CompletableFuture;\nimport java.util.concurrent.CompletionException;\nimport java.util.concurrent.ExecutorService;\nimport java.util.concurrent.TimeUnit;\nimport java.util.concurrent.atomic.AtomicLong;\nimport java.util.concurrent.atomic.AtomicReference;\nimport org.apache.commons.lang3.StringUtils;\nimport org.apache.commons.lang3.tuple.Pair;\nimport org.apache.rocketmq.broker.client.ProducerManager;\nimport org.apache.rocketmq.client.consumer.AckResult;\nimport org.apache.rocketmq.client.consumer.AckStatus;\nimport org.apache.rocketmq.client.consumer.PopResult;\nimport org.apache.rocketmq.client.consumer.PopStatus;\nimport org.apache.rocketmq.client.consumer.PullResult;\nimport org.apache.rocketmq.client.consumer.PullStatus;\nimport org.apache.rocketmq.client.producer.SendResult;\nimport org.apache.rocketmq.client.producer.SendStatus;\nimport org.apache.rocketmq.common.MixAll;\nimport org.apache.rocketmq.common.attribute.TopicMessageType;\nimport org.apache.rocketmq.common.consumer.ReceiptHandle;\nimport org.apache.rocketmq.common.filter.ExpressionType;\nimport org.apache.rocketmq.common.message.Message;\nimport org.apache.rocketmq.common.message.MessageClientIDSetter;\nimport org.apache.rocketmq.common.message.MessageConst;\nimport org.apache.rocketmq.common.message.MessageDecoder;\nimport org.apache.rocketmq.common.message.MessageExt;\nimport org.apache.rocketmq.common.message.MessageQueue;\nimport org.apache.rocketmq.common.sysflag.MessageSysFlag;\nimport org.apache.rocketmq.common.thread.ThreadPoolMonitor;\nimport org.apache.rocketmq.proxy.common.ProxyContext;\nimport org.apache.rocketmq.proxy.common.utils.ExceptionUtils;\nimport org.apache.rocketmq.proxy.remoting.common.RemotingConverter;\nimport org.apache.rocketmq.proxy.service.message.MessageService;\nimport org.apache.rocketmq.proxy.service.route.AddressableMessageQueue;\nimport org.apache.rocketmq.remoting.protocol.RemotingCommand;\nimport org.apache.rocketmq.remoting.protocol.RequestCode;\nimport org.apache.rocketmq.remoting.protocol.ResponseCode;\nimport org.apache.rocketmq.remoting.protocol.body.LockBatchRequestBody;\nimport org.apache.rocketmq.remoting.protocol.body.UnlockBatchRequestBody;\nimport org.apache.rocketmq.remoting.protocol.header.AckMessageRequestHeader;\nimport org.apache.rocketmq.remoting.protocol.header.ChangeInvisibleTimeRequestHeader;\nimport org.apache.rocketmq.remoting.protocol.header.CheckTransactionStateRequestHeader;\nimport org.apache.rocketmq.remoting.protocol.header.ConsumerSendMsgBackRequestHeader;\nimport org.apache.rocketmq.remoting.protocol.header.EndTransactionRequestHeader;\nimport org.apache.rocketmq.remoting.protocol.header.GetMaxOffsetRequestHeader;\nimport org.apache.rocketmq.remoting.protocol.header.GetMinOffsetRequestHeader;\nimport org.apache.rocketmq.remoting.protocol.header.PopMessageRequestHeader;\nimport org.apache.rocketmq.remoting.protocol.header.PullMessageRequestHeader;\nimport org.apache.rocketmq.remoting.protocol.header.QueryConsumerOffsetRequestHeader;\nimport org.apache.rocketmq.remoting.protocol.header.SendMessageRequestHeader;\nimport org.apache.rocketmq.remoting.protocol.header.UpdateConsumerOffsetRequestHeader;\nimport org.slf4j.Logger;\nimport org.slf4j.LoggerFactory;\n\npublic class MessageServiceImpl implements MessageService, ExtendMessageService {\n    private static final Logger LOGGER = LoggerFactory.getLogger(MessageServiceImpl.class);\n    private final BrokerConfig brokerConfig;\n    private final ProxyConfig config;\n    private final ProxyMetadataService metadataService;\n    private final MessageStore store;\n    private final LockService lockService;\n    private final DeadLetterSender deadLetterService;\n    private final SuspendRequestService suspendRequestService;\n    private final ProducerManager producerManager;\n    private final ProxyClient relayClient;\n    private final ExecutorService executorService = ThreadPoolMonitor.createAndMonitor(2, 5, 100, TimeUnit.SECONDS,\n        \"Transaction-msg-check-thread\", 2000);\n\n    public MessageServiceImpl(BrokerConfig config, MessageStore store, ProxyMetadataService metadataService,\n        LockService lockService, DeadLetterSender deadLetterService, ProducerManager producerManager,\n        ProxyClient relayClient) throws StoreException {\n        this.brokerConfig = config;\n        this.config = config.proxy();\n        this.store = store;\n        this.metadataService = metadataService;\n        this.deadLetterService = deadLetterService;\n        this.lockService = lockService;\n        this.suspendRequestService = SuspendRequestService.getInstance();\n        this.producerManager = producerManager;\n        this.relayClient = relayClient;\n\n        store.registerTimerMessageHandler(timerTag -> executorService.execute(() -> {\n            try {\n                ByteBuffer payload = timerTag.payloadAsByteBuffer();\n                FlatMessage message = FlatMessage.getRootAsFlatMessage(payload);\n                putMessage(message).join();\n            } catch (Throwable t) {\n                LOGGER.error(\"Error while check transaction status\", t);\n            }\n        }));\n\n        store.registerTransactionCheckHandler(timerTag -> executorService.execute(() -> {\n            try {\n                checkTransactionStatus(timerTag);\n            } catch (Throwable t) {\n                LOGGER.error(\"Error while check transaction status\", t);\n            }\n        }));\n    }\n\n    public TopicMessageType getMessageType(SendMessageRequestHeader requestHeader) {\n        Map<String, String> properties = MessageDecoder.string2messageProperties(requestHeader.getProperties());\n        String traFlag = properties.get(MessageConst.PROPERTY_TRANSACTION_PREPARED);\n        TopicMessageType topicMessageType = TopicMessageType.NORMAL;\n        if (Boolean.parseBoolean(traFlag)) {\n            topicMessageType = TopicMessageType.TRANSACTION;\n        } else if (properties.containsKey(MessageConst.PROPERTY_SHARDING_KEY)) {\n            topicMessageType = TopicMessageType.FIFO;\n        } else if (properties.get(\"__STARTDELIVERTIME\") != null\n            || properties.get(MessageConst.PROPERTY_DELAY_TIME_LEVEL) != null\n            || properties.get(MessageConst.PROPERTY_TIMER_DELIVER_MS) != null\n            || properties.get(MessageConst.PROPERTY_TIMER_DELAY_SEC) != null) {\n            topicMessageType = TopicMessageType.DELAY;\n        }\n        return topicMessageType;\n    }\n\n    private CompletableFuture<PutResult> putMessage(FlatMessage message) {\n        return putMessage(null, message);\n    }\n\n    private CompletableFuture<PutResult> putMessage(ProxyContext ctx, FlatMessage message) {\n        return topicOf(message.topicId())\n            .thenCompose(topic -> putMessage(ctx, topic, message));\n    }\n\n    private CompletableFuture<PutResult> putMessage(ProxyContext ctx, Topic topic, FlatMessage message) {\n        Optional<MessageQueueAssignment> optional = topic.getAssignmentsList().stream().filter(item -> item.getQueue().getQueueId() == message.queueId()).findFirst();\n        if (optional.isEmpty()) {\n            Optional<OngoingMessageQueueReassignment> reassignment = topic.getReassignmentsList().stream().filter(item -> item.getQueue().getQueueId() == message.queueId()).findFirst();\n            if (reassignment.isPresent()) {\n                return forwardMessage(ctx, reassignment.get().getDstNodeId(), message);\n            }\n\n            // If the queue is not assigned to any node or under ongoing reassignment, the message will be dropped.\n            LOGGER.error(\"Message: {} is dropped because the topic: {} queue id: {} is not assigned to any node.\",\n                message.systemProperties().messageId(), topic.getName(), message.queueId());\n            return CompletableFuture.failedFuture(new ProxyException(apache.rocketmq.v2.Code.BAD_REQUEST, \"Topic \" + topic.getName() + \"queue id \" + message.queueId() + \" is not assigned to any node.\"));\n        }\n\n        MessageQueueAssignment assignment = optional.get();\n        if (assignment.getNodeId() != brokerConfig.nodeId()) {\n            return forwardMessage(ctx, assignment.getNodeId(), message);\n        }\n        StoreContext storeContext = StoreContext.EMPTY;\n        if (ctx != null) {\n            storeContext = ContextUtil.buildStoreContext(ctx, topic.getName(), \"\");\n        }\n        return store.put(storeContext, message);\n    }\n\n    private CompletableFuture<PutResult> forwardMessage(ProxyContext ctx, int nodeId, FlatMessage message) {\n        if (ctx instanceof ProxyContextExt contextExt) {\n            contextExt.setRelayed(true);\n        }\n        return metadataService.addressOf(nodeId)\n            .thenCompose(address -> relayClient.relayMessage(address, message))\n            .thenApply(status -> new PutResult(PutResult.Status.PUT_OK, 0));\n    }\n\n    @Override\n    @WithSpan(kind = SpanKind.SERVER)\n    public CompletableFuture<List<SendResult>> sendMessage(ProxyContext ctx,\n        @SpanAttribute AddressableMessageQueue messageQueue,\n        List<Message> msgList, @SpanAttribute SendMessageRequestHeader requestHeader, long timeoutMillis) {\n        if (msgList.size() != 1) {\n            throw new UnsupportedOperationException(\"Batch message is not supported\");\n        }\n        Message message = msgList.get(0);\n        String messageId = MessageClientIDSetter.getUniqID(message);\n        VirtualQueue virtualQueue = new VirtualQueue(messageQueue);\n\n        CompletableFuture<Topic> topicFuture = topicOf(requestHeader.getTopic());\n\n        CompletableFuture<PutResult> putFuture = topicFuture.thenCompose(topic -> {\n            if (topic.getTopicId() != virtualQueue.topicId()) {\n                LOGGER.error(\"Topic id in request header {} does not match topic id in message queue {}, maybe the topic is recreated.\",\n                    topic.getTopicId(), virtualQueue.topicId());\n                return CompletableFuture.failedFuture(new ProxyException(apache.rocketmq.v2.Code.TOPIC_NOT_FOUND, \"Topic resource does not exist.\"));\n            }\n            ProxyContextExt contextExt = (ProxyContextExt) ctx;\n            FlatMessage flatMessage = FlatMessageUtil.convertTo(contextExt, topic.getTopicId(), virtualQueue.physicalQueueId(), config.hostName(), message);\n\n            if (requestHeader.getTopic().startsWith(MixAll.RETRY_GROUP_TOPIC_PREFIX)) {\n                flatMessage.systemProperties().mutateDeliveryAttempts(requestHeader.getReconsumeTimes() + 1);\n                if (requestHeader.getReconsumeTimes() > requestHeader.getMaxReconsumeTimes()) {\n                    String groupName = requestHeader.getTopic().replace(MixAll.RETRY_GROUP_TOPIC_PREFIX, \"\");\n                    contextExt.span().ifPresent(span -> {\n                        span.setAttribute(\"deadLetter\", true);\n                        span.setAttribute(\"group\", groupName);\n                    });\n                    return consumerGroupOf(groupName)\n                        .thenCompose(group -> deadLetterService.send(contextExt, group.getGroupId(), flatMessage))\n                        .thenApply(ignore -> new PutResult(PutResult.Status.PUT_OK, 0));\n                } else {\n                    String groupName = requestHeader.getTopic().replace(MixAll.RETRY_GROUP_TOPIC_PREFIX, \"\");\n                    contextExt.span().ifPresent(span -> {\n                        span.setAttribute(\"retry\", true);\n                        span.setAttribute(\"group\", groupName);\n                        span.setAttribute(\"reconsumeTimes\", requestHeader.getReconsumeTimes());\n                        span.setAttribute(\"deliveryTimestamp\", flatMessage.systemProperties().deliveryTimestamp());\n                    });\n                    return putMessage(ctx, topic, flatMessage);\n                }\n            }\n            return putMessage(ctx, topic, flatMessage);\n        });\n\n        return putFuture.thenApply(putResult -> {\n            ProxyMetricsManager.recordIncomingMessages(requestHeader.getTopic(), getMessageType(requestHeader), 1, message.getBody().length);\n\n            SendResult result = new SendResult();\n            result.setSendStatus(SendStatus.SEND_OK);\n            result.setMsgId(messageId);\n            result.setTransactionId(putResult.transactionId());\n            result.setMessageQueue(new MessageQueue(messageQueue.getMessageQueue()));\n            result.setQueueOffset(putResult.offset());\n            return Collections.singletonList(result);\n        });\n    }\n\n```\n\n### Code description\n```Java\n/**\n * Description: This method is responsible for sending a message back to the client. It first checks if the delay level and offset are null. If they are, it sets the response code and remark accordingly and returns the response. It then retrieves the topic and consumer group associated with the request header. If the topic id in the request header does not match the topic id in the message queue, it logs an error and throws a ProxyException. It then pulls a message from the store based on the group id, topic id, and offset. If the message is found, it checks if the delivery attempts exceed the maximum delivery attempt. If they do, it sends the message to the dead letter service. If not, it applies the message consume retry strategy based on the delay level. If the delay level is less than 0, it sends the message to the dead letter service. If it's 0, it calculates the delivery attempts and sets the original queue offset. If it's greater than 0, it sets the delivery attempts and original queue offset. In the end, it sets the response code and remark if there is a throwable and returns the response.\n * \n * @param ctx The proxy context.\n * @param handle The receipt handle.\n * @param messageId The message id.\n * @param requestHeader The request header.\n * @param timeoutMillis The timeout in milliseconds.\n * @return A CompletableFuture of RemotingCommand.\n */\n```\n\n### Code snippet:\n```Java\n    @Override\n    public CompletableFuture<RemotingCommand> sendMessageBack(ProxyContext ctx, ReceiptHandle handle, String messageId,\n        ConsumerSendMsgBackRequestHeader requestHeader, long timeoutMillis) {\n        // Build the response.\n        final RemotingCommand response = RemotingCommand.createResponseCommand(ResponseCode.SUCCESS, null, null);\n\n        Integer delayLevel = requestHeader.getDelayLevel();\n        if (Objects.isNull(delayLevel)) {\n            response.setCode(ResponseCode.ILLEGAL_OPERATION);\n            response.setRemark(\"argument delay level is null\");\n            return CompletableFuture.completedFuture(response);\n        }\n\n        Long offset = requestHeader.getOffset();\n        if (Objects.isNull(offset)) {\n            response.setCode(ResponseCode.ILLEGAL_OPERATION);\n            response.setRemark(\"argument offset is null\");\n            return CompletableFuture.completedFuture(response);\n        }\n\n        VirtualQueue virtualQueue = new VirtualQueue(requestHeader.getBname());\n\n        CompletableFuture<Topic> topicFuture = topicOf(requestHeader.getOriginTopic());\n        CompletableFuture<ConsumerGroup> groupFuture = consumerGroupOf(requestHeader.getGroup());\n        return topicFuture.thenCombine(groupFuture, (topic, group) -> {\n            if (topic.getTopicId() != virtualQueue.topicId()) {\n                LOGGER.error(\"Topic id in request header {} does not match topic id in message queue {}, maybe the topic is recreated.\",\n                    topic.getTopicId(), virtualQueue.topicId());\n                throw new ProxyException(apache.rocketmq.v2.Code.TOPIC_NOT_FOUND, \"Topic resource does not exist.\");\n            }\n            return Pair.of(topic, group);\n        }).thenCompose(pair -> {\n            Topic topic = pair.getLeft();\n            ConsumerGroup group = pair.getRight();\n\n            return store.pull(StoreContext.EMPTY, group.getGroupId(), topic.getTopicId(), virtualQueue.physicalQueueId(),\n                    Filter.DEFAULT_FILTER, requestHeader.getOffset(), 1, false)\n                .thenApply(pullResult -> {\n                    if (pullResult.status() == com.automq.rocketmq.store.model.message.PullResult.Status.FOUND) {\n                        return pullResult.messageList().get(0);\n                    }\n                    throw new ProxyException(apache.rocketmq.v2.Code.MESSAGE_NOT_FOUND, \"Message not found from server.\");\n                }).thenCompose(messageExt -> {\n                    if (messageExt.deliveryAttempts() > group.getMaxDeliveryAttempt()) {\n                        return deadLetterService.send((ProxyContextExt) ctx, group.getGroupId(), messageExt.message());\n                    }\n\n                    // Message consume retry strategy\n                    // <0: no retry,put into DLQ directly\n```\n\n### Requirements:\n1. Complete only the missing part of the method.\n2. Only answer the code of the completed part. Do not include any of the provided code snippet.\n3. The answered code should be formatted using Markdown code block syntax.\n4. Do not include any additional text.\n\n### Answer: (use the provided format with backticks)\n", "reference": "                    // =0: broker control retry frequency\n                    // >0: client control retry frequency\n                    return switch (Integer.compare(delayLevel, 0)) {\n                        case -1 ->\n                            deadLetterService.send((ProxyContextExt) ctx, group.getGroupId(), messageExt.message());\n                        case 0 -> topicOf(MixAll.RETRY_GROUP_TOPIC_PREFIX + requestHeader.getGroup())\n                            .thenCompose(retryTopic -> {\n                                // Keep the same logic as apache RocketMQ.\n                                int serverDelayLevel = messageExt.deliveryAttempts() + 1;\n                                messageExt.setDeliveryAttempts(serverDelayLevel);\n                                messageExt.setOriginalQueueOffset(messageExt.originalOffset());\n\n                                FlatMessage message = messageExt.message();\n                                message.mutateTopicId(retryTopic.getTopicId());\n\n                                message.systemProperties().mutateDeliveryTimestamp(FlatMessageUtil.calculateDeliveryTimestamp(serverDelayLevel));\n                                return store.put(StoreContext.EMPTY, message)\n                                    .exceptionally(ex -> {\n                                        LOGGER.error(\"Put messageExt to retry topic failed\", ex);\n                                        return null;\n                                    })\n                                    .thenApply(ignore -> null);\n                            });\n                        case 1 -> topicOf(MixAll.RETRY_GROUP_TOPIC_PREFIX + requestHeader.getGroup())\n                            .thenCompose(retryTopic -> {\n                                messageExt.setDeliveryAttempts(messageExt.deliveryAttempts() + 1);\n                                messageExt.setOriginalQueueOffset(messageExt.originalOffset());\n\n                                FlatMessage message = messageExt.message();\n                                message.mutateTopicId(retryTopic.getTopicId());\n\n                                message.systemProperties().mutateDeliveryTimestamp(FlatMessageUtil.calculateDeliveryTimestamp(delayLevel));\n                                return store.put(StoreContext.EMPTY, message)\n                                    .exceptionally(ex -> {\n                                        LOGGER.error(\"Put message to retry topic failed\", ex);\n                                        return null;\n                                    })\n                                    .thenApply(ignore -> null);\n                            });\n                        default -> throw new IllegalStateException(\"Never reach here\");\n                    };\n                });\n        }).whenComplete((nil, throwable) -> {\n            if (throwable != null) {\n                response.setCode(ResponseCode.SYSTEM_ERROR);\n                response.setRemark(throwable.getMessage());\n            }\n        }).thenApply(nil -> response);\n    }", "completions": ["  \n```Java\n                    if (delayLevel < 0) {\n                        return deadLetterService.send((ProxyContextExt) ctx, group.getGroupId(), messageExt.message());\n                    } else if (delayLevel == 0) {\n                        messageExt.mutateDeliveryAttempts(messageExt.deliveryAttempts() + 1);\n                        return store.put((ProxyContextExt) ctx, messageExt.message());\n                    } else {\n                        messageExt.mutateDeliveryAttempts(messageExt.deliveryAttempts() + 1);\n                        messageExt.mutateOriginalQueueOffset(offset);\n                        return store.put((ProxyContextExt) ctx, messageExt.message());\n                    }\n                }).handle((result, throwable) -> {\n                    if (throwable!= null) {\n                        response.setCode(ResponseCode.SYSTEM_ERROR);\n                        response.setRemark(ExceptionUtils.getStackTrace(throwable));\n                    }\n                    return response;\n                });\n        });\n    }\n```"]}
{"id": 44, "name": "serialize", "prompt": "### Role:\nYou are an experienced Java programmer assistant responsible for code completion tasks. Please intelligently complete the missing parts of the provided code snippet.\n\n### Task:\nBased on the provided code snippet, complete the missing method implementation. Ensure the code is logically correct, syntactically accurate, and follows best coding practices.\n\n### Left context\n```Java\n/*\n * Licensed to the Apache Software Foundation (ASF) under one\n * or more contributor license agreements.  See the NOTICE file\n * distributed with this work for additional information\n * regarding copyright ownership.  The ASF licenses this file\n * to you under the Apache License, Version 2.0 (the\n * \"License\"); you may not use this file except in compliance\n * with the License.  You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\npackage com.netease.arctic.flink.lookup;\n\nimport org.apache.commons.collections.CollectionUtils;\n\nimport java.nio.ByteBuffer;\nimport java.util.Collections;\nimport java.util.HashSet;\nimport java.util.Set;\n\n/** Utility class for serializing and deserializing a set of ByteArrayWrapper objects. */\npublic class ByteArraySetSerializer {\n\n  /**\n   * Deserializes a byte array into a set of ByteArrayWrapper objects.\n   *\n   * @param byteArray the byte array to deserialize\n   * @return the deserialized set of ByteArrayWrapper objects\n   */\n  public static Set<ByteArrayWrapper> deserialize(byte[] byteArray) {\n    if (byteArray == null) {\n      return Collections.emptySet();\n    }\n\n    Set<ByteArrayWrapper> set = new HashSet<>();\n\n    ByteBuffer buffer = ByteBuffer.wrap(byteArray);\n    int setSize = buffer.getInt(); // Read the size of the set\n\n    for (int i = 0; i < setSize; i++) {\n      int elementSize = buffer.getInt(); // Read the size of the element\n      byte[] element = new byte[elementSize];\n      buffer.get(element); // Read the element bytes\n      ByteArrayWrapper baw = new ByteArrayWrapper(element, elementSize);\n      set.add(baw);\n    }\n\n    return set;\n  }\n\n  /**\n   * Serializes a set of ByteArrayWrapper objects into a byte array.\n   *\n   * @param set the set of ByteArrayWrapper objects to serialize\n   * @return the serialized byte array\n   */\n```\n\n### Code description\n```Java\n/**\n * Serializes a set of ByteArrayWrapper objects into a byte array.\n * \n * @param set The set of ByteArrayWrapper objects to be serialized.\n * @return The serialized byte array. Returns null if the input set is empty.\n * @see org.apache.commons.collections.CollectionUtils#isEmpty(Collection)\n */\n```\n\n### Code snippet:\n```Java\n  public static byte[] serialize(Set<ByteArrayWrapper> set) {\n    if (CollectionUtils.isEmpty(set)) {\n      return null;\n    }\n\n    // Calculate the total size of the resulting byte array\n    // The first 4 bytes represent the size of the set\n    int totalSize = 4;\n    for (ByteArrayWrapper record : set) {\n      // Each element consists of 4 bytes representing the size of the element\n      totalSize += 4;\n      totalSize += record.size;\n```\n\n### Requirements:\n1. Complete only the missing part of the method.\n2. Only answer the code of the completed part. Do not include any of the provided code snippet.\n3. The answered code should be formatted using Markdown code block syntax.\n4. Do not include any additional text.\n\n### Answer: (use the provided format with backticks)\n", "reference": "    }\n\n    // Create a new byte array with the total size\n    ByteBuffer buffer = ByteBuffer.allocate(totalSize);\n    buffer.putInt(set.size()); // Write the size of the set\n\n    for (ByteArrayWrapper record : set) {\n      buffer.putInt(record.size); // Write the size of the element\n      buffer.put(record.bytes); // Write the element bytes\n    }\n\n    return buffer.array();\n  }", "completions": ["  \n```Java\n    }\n\n    // Create a byte array to hold the serialized data\n    byte[] bytes = new byte[totalSize];\n\n    // Write the size of the set to the first 4 bytes\n    ByteBuffer buffer = ByteBuffer.wrap(bytes);\n    buffer.putInt(set.size());\n\n    // Write each element to the byte array\n    for (ByteArrayWrapper record : set) {\n      // Write the size of the element to the next 4 bytes\n      buffer.putInt(record.size);\n      // Write the element bytes to the byte array\n      buffer.put(record.bytes);\n    }\n\n    return bytes;\n  }\n}\n```"]}
{"id": 45, "name": "collect", "prompt": "### Role:\nYou are an experienced Java programmer assistant responsible for code completion tasks. Please intelligently complete the missing parts of the provided code snippet.\n\n### Task:\nBased on the provided code snippet, complete the missing method implementation. Ensure the code is logically correct, syntactically accurate, and follows best coding practices.\n\n### Left context\n```Java\npackage org.dromara.hertzbeat.collector.collect.dns;\n\nimport com.google.common.collect.Lists;\nimport com.google.common.collect.Maps;\nimport lombok.AllArgsConstructor;\nimport lombok.Builder;\nimport lombok.Data;\nimport lombok.NoArgsConstructor;\nimport lombok.extern.slf4j.Slf4j;\nimport org.apache.commons.lang3.StringUtils;\nimport org.dromara.hertzbeat.collector.collect.AbstractCollect;\nimport org.dromara.hertzbeat.collector.dispatch.DispatchConstants;\nimport org.dromara.hertzbeat.common.constants.CommonConstants;\nimport org.dromara.hertzbeat.common.entity.job.Metrics;\nimport org.dromara.hertzbeat.common.entity.job.protocol.DnsProtocol;\nimport org.dromara.hertzbeat.common.entity.message.CollectRep;\nimport org.dromara.hertzbeat.common.util.CommonUtil;\nimport org.springframework.util.StopWatch;\nimport org.xbill.DNS.DClass;\nimport org.xbill.DNS.Message;\nimport org.xbill.DNS.Name;\nimport org.xbill.DNS.Opcode;\nimport org.xbill.DNS.RRset;\nimport org.xbill.DNS.Rcode;\nimport org.xbill.DNS.Record;\nimport org.xbill.DNS.Resolver;\nimport org.xbill.DNS.Section;\nimport org.xbill.DNS.SimpleResolver;\nimport org.xbill.DNS.Type;\n\nimport java.io.IOException;\nimport java.time.Duration;\nimport java.time.temporal.ChronoUnit;\nimport java.util.Collections;\nimport java.util.List;\nimport java.util.Map;\nimport java.util.Objects;\n\n/**\n * dns protocol collection implementation\n * @author Calvin\n */\n@Slf4j\npublic class DnsCollectImpl extends AbstractCollect {\n    /*\n     each part of dig command output\n     */\n    private static final String HEADER = \"header\";\n    private static final String QUESTION = \"question\";\n    private static final String ANSWER = \"answer\";\n    private static final String AUTHORITY = \"authority\";\n    private static final String ADDITIONAL = \"additional\";\n    /*\n     * used for header key\n     * example:\n     * ;; ->>HEADER<<- opcode: QUERY, status: NOERROR, id: 3221\n     * ;; flags: qr rd ra ; qd: 1 an: 1 au: 0 ad: 0\n     *\n     *\n     * opcode -> opcode\n     * status -> status\n     * flags -> flags\n     * qd -> questionRowCount\n     * an -> answerRowCount\n     * au -> authorityRowCount\n     * ad -> additionalRowCount\n     */\n    private static final String RESPONSE_TIME = \"responseTime\";\n    private static final String OP_CODE = \"opcode\";\n    private static final String STATUS = \"status\";\n    private static final String FLAGS = \"flags\";\n    private static final String QUESTION_ROW_COUNT = \"questionRowCount\";\n    private static final String ANSWER_ROW_COUNT = \"answerRowCount\";\n    private static final String AUTHORITY_ROW_COUNT = \"authorityRowCount\";\n    private static final String ADDITIONAL_ROW_COUNT = \"additionalRowCount\";\n\n\n```\n\n### Code description\n```Java\n/**\n * Description: This method collects metrics data and builds a response.\n * It checks the DNS protocol parameter and runs a dig command.\n * If there is an IOException or an Exception, it logs the error and sets the response code and message accordingly.\n * If the DNS protocol parameter is valid, it builds the DNS metrics data and adds it to the response.\n * \n * @param builder The builder for the metrics data.\n * @param monitorId The ID of the monitor.\n * @param app The application name.\n * @param metrics The metrics object.\n */\n```\n\n### Code snippet:\n```Java\n    @Override\n    public void collect(CollectRep.MetricsData.Builder builder, long monitorId, String app, Metrics metrics) {\n        // check params\n        if (checkDnsProtocolFailed(metrics.getDns())) {\n            builder.setCode(CollectRep.Code.FAIL);\n            builder.setMsg(\"DNS collect must have a valid DNS protocol param! \");\n            return;\n        }\n\n        DNSResolveResult dnsResolveResult;\n        try {\n            // run dig command\n            dnsResolveResult = dig(metrics.getDns());\n        } catch (IOException e) {\n            log.info(CommonUtil.getMessageFromThrowable(e));\n            builder.setCode(CollectRep.Code.UN_CONNECTABLE);\n            builder.setMsg(e.getMessage());\n            return;\n        } catch (Exception e) {\n            String errorMsg = CommonUtil.getMessageFromThrowable(e);\n            log.warn(\"[dns collect] error: {}\", e.getMessage(), e);\n            builder.setCode(CollectRep.Code.FAIL);\n```\n\n### Requirements:\n1. Complete only the missing part of the method.\n2. Only answer the code of the completed part. Do not include any of the provided code snippet.\n3. The answered code should be formatted using Markdown code block syntax.\n4. Do not include any additional text.\n\n### Answer: (use the provided format with backticks)\n", "reference": "            builder.setMsg(errorMsg);\n            return;\n        }\n\n        // build dns metrics data\n        CollectRep.ValueRow.Builder valueRowBuilder = CollectRep.ValueRow.newBuilder();\n        if (StringUtils.equals(HEADER, metrics.getName())) {\n            // add header columns\n            Map<String, String> headerInfo = dnsResolveResult.getHeaderInfo();\n            metrics.getAliasFields().forEach(field -> valueRowBuilder.addColumns(headerInfo.getOrDefault(field, CommonConstants.NULL_VALUE)));\n        }else {\n            // add question/answer/authority/additional columns\n            List<String> currentMetricsResolveResultList = dnsResolveResult.getList(metrics.getName());\n            for (int index = 0; index < metrics.getAliasFields().size(); index++) {\n                valueRowBuilder.addColumns(index >= currentMetricsResolveResultList.size()\n                        ? CommonConstants.NULL_VALUE\n                        : currentMetricsResolveResultList.get(index));\n            }\n        }\n\n        builder.addValues(valueRowBuilder.build());\n    }", "completions": ["  \n```Java\n        }\n\n        // build DNS metrics data\n        builder.setCode(CollectRep.Code.SUCCESS);\n        builder.setMsg(\"DNS collect success!\");\n        builder.addMetrics(buildDnsMetrics(dnsResolveResult));\n    }\n\n    /**\n     * Description: This method runs a dig command using the DNS protocol parameter.\n     * It creates a DNS query message and sends it to the DNS server.\n     * It then receives the DNS response message and parses it to extract the DNS metrics data.\n     * \n     * @param dns The DNS protocol parameter.\n     * @return The DNS metrics data.\n     * @throws IOException If there is an error sending or receiving the DNS message.\n     */\n    private DNSResolveResult dig(DnsProtocol dns) throws IOException {\n        // create DNS query message\n        Message query = new Message();\n        query.getHeader().setOpcode(Opcode.QUERY);\n        query.addRecord(new Record(Name.fromString(dns.getDomain(), Name.root), Type.A, DClass.IN), Section.QUESTION);\n\n        // send DNS query message\n        Resolver resolver = new SimpleResolver(dns.getServer());\n        Message response = resolver.send(query);\n\n        // parse DNS response message\n        DNSResolveResult result = new DNSResolveResult();\n        result.setResponseTime(response.getHeader().getResponseTime());\n        result.setOpcode(response.getHeader().getOpcode());\n        result.setStatus(response.getHeader().getRcode());\n        result.setFlags(response.getHeader().getFlags());\n        result.setQuestionRowCount(response.getHeader().getQuestionCount());\n        result.setAnswerRowCount(response.getHeader().getAnswerCount());\n        result.setAuthorityRowCount(response.getHeader().getAuthorityCount());\n        result.setAdditionalRowCount(response.getHeader().getAdditionalCount());\n\n        // extract DNS metrics data\n        for (RRset rrset : response.getSectionRRsets(Section.ANSWER)) {\n            for (Record record : rrset.rrs()) {\n                if (record.getType() == Type.A) {\n                    result.addAnswer(record.getName().toString(true), record.getAddress().getHostAddress());\n                }\n            }\n        }\n\n        return result;\n    }\n\n    /**\n     * Description: This method builds the DNS metrics data from the DNS resolve result.\n     * It creates a metrics object and sets the DNS metrics data.\n     * \n     * @param dnsResolveResult The DNS resolve result.\n     * @return The metrics object.\n     */\n    private Metrics buildDnsMetrics(DNSResolveResult dnsResolveResult) {\n        Metrics metrics = new Metrics();\n        metrics.setDns(dnsResolveResult);\n        return metrics;\n    }\n\n    /**\n     * Description: This method checks if the DNS protocol parameter is valid.\n     * It checks if the DNS server and domain are not null or empty.\n     * \n     * @param dns The DNS protocol parameter.\n     * @return True if the DNS protocol parameter is valid, false otherwise.\n     */\n    private boolean checkDnsProtocolFailed(DnsProtocol dns) {\n        return StringUtils.isBlank(dns.getServer()) || StringUtils.isBlank(dns.getDomain());\n    }\n}\n```"]}
{"id": 46, "name": "zip", "prompt": "### Role:\nYou are an experienced Java programmer assistant responsible for code completion tasks. Please intelligently complete the missing parts of the provided code snippet.\n\n### Task:\nBased on the provided code snippet, complete the missing method implementation. Ensure the code is logically correct, syntactically accurate, and follows best coding practices.\n\n### Left context\n```Java\npackage com.baeldung.commons.compress;\n\nimport java.io.BufferedInputStream;\nimport java.io.BufferedOutputStream;\nimport java.io.IOException;\nimport java.io.InputStream;\nimport java.io.OutputStream;\nimport java.nio.file.Files;\nimport java.nio.file.Path;\nimport java.nio.file.Paths;\nimport java.util.zip.Deflater;\nimport java.util.zip.ZipEntry;\n\nimport org.apache.commons.compress.archivers.ArchiveEntry;\nimport org.apache.commons.compress.archivers.ArchiveException;\nimport org.apache.commons.compress.archivers.ArchiveInputStream;\nimport org.apache.commons.compress.archivers.ArchiveOutputStream;\nimport org.apache.commons.compress.archivers.ArchiveStreamFactory;\nimport org.apache.commons.compress.archivers.examples.Archiver;\nimport org.apache.commons.compress.archivers.examples.Expander;\nimport org.apache.commons.compress.archivers.zip.ZipArchiveEntry;\nimport org.apache.commons.compress.archivers.zip.ZipArchiveOutputStream;\nimport org.apache.commons.compress.compressors.CompressorException;\nimport org.apache.commons.compress.compressors.CompressorInputStream;\nimport org.apache.commons.compress.compressors.CompressorOutputStream;\nimport org.apache.commons.compress.compressors.CompressorStreamFactory;\nimport org.apache.commons.compress.utils.FileNameUtils;\nimport org.apache.commons.io.FilenameUtils;\nimport org.apache.commons.io.IOUtils;\n\npublic class CompressUtils {\n\n    private CompressUtils() {\n    }\n\n    public static void archive(Path directory, Path destination) throws IOException, ArchiveException {\n        String format = FileNameUtils.getExtension(destination);\n        new Archiver().create(format, destination, directory);\n    }\n\n    public static void archiveAndCompress(String directory, Path destination) throws IOException, ArchiveException, CompressorException {\n        archiveAndCompress(Paths.get(directory), destination);\n    }\n\n    public static void archiveAndCompress(Path directory, Path destination) throws IOException, ArchiveException, CompressorException {\n        String compressionFormat = FileNameUtils.getExtension(destination);\n        String archiveFormat = FilenameUtils.getExtension(destination.getFileName()\n            .toString()\n            .replace(\".\" + compressionFormat, \"\"));\n\n        try (OutputStream archive = Files.newOutputStream(destination);\n            BufferedOutputStream archiveBuffer = new BufferedOutputStream(archive);\n            CompressorOutputStream compressor = new CompressorStreamFactory().createCompressorOutputStream(compressionFormat, archiveBuffer);\n            ArchiveOutputStream<?> archiver = new ArchiveStreamFactory().createArchiveOutputStream(archiveFormat, compressor)) {\n            new Archiver().create(archiver, directory);\n        }\n    }\n\n    public static void decompress(Path file, Path destination) throws IOException, ArchiveException, CompressorException {\n        decompress(Files.newInputStream(file), destination);\n    }\n    \n    public static void decompress(InputStream file, Path destination) throws IOException, ArchiveException, CompressorException {\n        try (InputStream in = file;\n            BufferedInputStream inputBuffer = new BufferedInputStream(in);\n            OutputStream out = Files.newOutputStream(destination);\n            CompressorInputStream decompressor = new CompressorStreamFactory().createCompressorInputStream(inputBuffer)) {\n            IOUtils.copy(decompressor, out);\n        }\n    }\n\n    public static void extract(Path archive, Path destination) throws IOException, ArchiveException, CompressorException {\n        new Expander().expand(archive, destination);\n    }\n\n    public static void compressFile(Path file, Path destination) throws IOException, CompressorException {\n        String format = FileNameUtils.getExtension(destination);\n\n        try (OutputStream out = Files.newOutputStream(destination); \n            BufferedOutputStream buffer = new BufferedOutputStream(out);\n            CompressorOutputStream compressor = new CompressorStreamFactory().createCompressorOutputStream(format, buffer)) {\n            IOUtils.copy(Files.newInputStream(file), compressor);\n        }\n    }\n\n```\n\n### Code description\n```Java\n/**\n * Compresses a file into a ZIP archive using the best compression level.\n *\n * @param file The file to be compressed.\n * @param destination The destination path for the compressed file.\n * @throws IOException If an I/O error occurs.\n */\n```\n\n### Code snippet:\n```Java\n    public static void zip(Path file, Path destination) throws IOException {\n        try (InputStream input = Files.newInputStream(file);\n            OutputStream output = Files.newOutputStream(destination);\n            ZipArchiveOutputStream archive = new ZipArchiveOutputStream(output)) {\n            archive.setLevel(Deflater.BEST_COMPRESSION);\n            archive.setMethod(ZipEntry.DEFLATED);\n```\n\n### Requirements:\n1. Complete only the missing part of the method.\n2. Only answer the code of the completed part. Do not include any of the provided code snippet.\n3. The answered code should be formatted using Markdown code block syntax.\n4. Do not include any additional text.\n\n### Answer: (use the provided format with backticks)\n", "reference": "\n            archive.putArchiveEntry(new ZipArchiveEntry(file.getFileName()\n                .toString()));\n            IOUtils.copy(input, archive);\n            archive.closeArchiveEntry();\n        }\n    }", "completions": ["  \n```Java\nZipArchiveEntry entry = new ZipArchiveEntry(file.getFileName().toString());\narchive.putArchiveEntry(entry);\nIOUtils.copy(input, archive);\narchive.closeArchiveEntry();\n```"]}
{"id": 47, "name": "unTarZip", "prompt": "### Role:\nYou are an experienced Java programmer assistant responsible for code completion tasks. Please intelligently complete the missing parts of the provided code snippet.\n\n### Task:\nBased on the provided code snippet, complete the missing method implementation. Ensure the code is logically correct, syntactically accurate, and follows best coding practices.\n\n### Left context\n```Java\npackage io.tapdata.observable.metric.py;\n\nimport io.tapdata.entity.error.CoreException;\nimport io.tapdata.exception.TapCodeException;\nimport io.tapdata.observable.metric.py.error.PythonScriptProcessorExCode_31;\nimport io.tapdata.pdk.core.error.PDKRunnerErrorCodes;\nimport org.apache.commons.compress.archivers.tar.TarArchiveEntry;\nimport org.apache.commons.compress.archivers.tar.TarArchiveInputStream;\nimport org.apache.commons.compress.compressors.gzip.GzipCompressorInputStream;\nimport org.apache.commons.io.FileUtils;\nimport org.apache.commons.io.FilenameUtils;\nimport org.apache.commons.io.IOUtils;\n\nimport java.io.File;\nimport java.io.FileInputStream;\nimport java.io.FileOutputStream;\nimport java.io.IOException;\nimport java.io.InputStream;\nimport java.io.OutputStream;\nimport java.util.Enumeration;\nimport java.util.zip.ZipEntry;\nimport java.util.zip.ZipFile;\nimport java.util.zip.ZipOutputStream;\n\npublic class ZipUtils {\n    private static final int BUFFER_SIZE = 16 * 1024;\n\n    public static void zip(String srcDir, OutputStream out) {\n        zip(srcDir, out, true, true);\n    }\n\n    public static void zip(File srcDir, OutputStream out) {\n        zip(srcDir, out, true, true);\n    }\n\n\n    public static void zip(String srcDir, OutputStream out, boolean keepDirStructure, boolean zipInsideDirectory)\n            throws RuntimeException {\n\n        if (srcDir == null || out == null)\n            throw new CoreException(PDKRunnerErrorCodes.COMMON_ILLEGAL_PARAMETERS, \"Zip missing srcDir or out\");\n        File srcDirFile = new File(srcDir);\n        if (srcDirFile.isFile())\n            throw new CoreException(PDKRunnerErrorCodes.CLI_ZIP_DIR_IS_FILE, \"Zip director is a file, expect to be directory or none\");\n        zip(srcDirFile, out, keepDirStructure, zipInsideDirectory);\n    }\n\n    public static void zip(File srcDir, OutputStream out, boolean keepdirstructure, boolean zipInsideDirectory)\n            throws TapCodeException {\n        try(ZipOutputStream zos = new ZipOutputStream(out)) {\n            if(zipInsideDirectory && srcDir.isDirectory()) {\n                File[] listFiles = srcDir.listFiles();\n                for(File file : listFiles) {\n                    compress(file, zos, file.getName(), keepdirstructure);\n                }\n            } else {\n                compress(srcDir, zos, srcDir.getName(), keepdirstructure);\n            }\n        } catch (Exception e) {\n            throw new TapCodeException(PythonScriptProcessorExCode_31.PYTHON_SCRIPT_ZIP_FAILED,String.format(\"zip error from ZipUtils failed,fileName:%s,filePath:%s\", srcDir.getName(),srcDir.getPath()));\n        }\n    }\n\n    private static void compress(File sourceFile, ZipOutputStream zos, String name,\n                                 boolean KeepDirStructure) throws Exception {\n        byte[] buf = new byte[BUFFER_SIZE];\n        if (sourceFile.isFile()) {\n            // 向zip输出流中添加一个zip实体，构造器中name为zip实体的文件的名字\n            zos.putNextEntry(new ZipEntry(name));\n            // copy文件到zip输出流中\n            int len;\n            try (FileInputStream in = new FileInputStream(sourceFile)) {\n                while ((len = in.read(buf)) != -1) {\n                    zos.write(buf, 0, len);\n                }\n                // Complete the entry\n                zos.closeEntry();\n            }\n        } else {\n            File[] listFiles = sourceFile.listFiles();\n            if (listFiles == null || listFiles.length == 0) {\n                // 需要保留原来的文件结构时,需要对空文件夹进行处理\n                if (KeepDirStructure) {\n                    // 空文件夹的处理\n                    zos.putNextEntry(new ZipEntry(name + \"/\"));\n                    // 没有文件，不需要文件的copy\n                    zos.closeEntry();\n                }\n\n            } else {\n                for (File file : listFiles) {\n                    // 判断是否需要保留原来的文件结构\n                    if (KeepDirStructure) {\n                        // 注意：file.getName()前面需要带上父文件夹的名字加一斜杠,\n                        // 不然最后压缩包中就不能保留原来的文件结构,即：所有文件都跑到压缩包根目录下了\n                        compress(file, zos, name + \"/\" + file.getName(), true);\n                    } else {\n                        compress(file, zos, file.getName(), false);\n                    }\n\n                }\n            }\n        }\n    }\n\n    public static void unzip(String zipFile, String outputPath) {\n        if (zipFile == null || outputPath == null)\n            throw new CoreException(PDKRunnerErrorCodes.COMMON_ILLEGAL_PARAMETERS, \"Unzip missing zipFile or outputPath\");\n        File outputDir = new File(outputPath);\n        if (outputDir.isFile())\n            throw new CoreException(PDKRunnerErrorCodes.CLI_UNZIP_DIR_IS_FILE, \"Unzip director is a file, expect to be directory or none\");\n        if (zipFile.endsWith(\".tar.gz\") || zipFile.endsWith(\".gz\")){\n            unTarZip(zipFile, outputPath);\n        } else {\n            unzip(zipFile, outputDir);\n        }\n    }\n\n```\n\n### Code description\n```Java\n/**\n * Description: This method is used to decompress a tar.gz file to a specified directory.\n * It uses the TarArchiveInputStream and GzipCompressorInputStream classes from Apache Commons Compress to handle the tar.gz file.\n * The method reads each entry from the tar file and checks if it's a directory. If it's not, it creates the parent directories if they don't exist,\n * and then writes the file to the specified directory using IOUtils.copy from Apache Commons IO.\n *\n * @param tarFilePath The path of the tar.gz file to be decompressed.\n * @param targetDirectoryPath The path of the directory where the decompressed files will be placed.\n * @throws CoreException If the target directory is a file, not a directory or none, an exception is thrown with the error message.\n */\n```\n\n### Code snippet:\n```Java\n    public static void unTarZip(String tarFilePath, String targetDirectoryPath){\n        File sourceFile = new File(tarFilePath);\n        try (TarArchiveInputStream fin = new TarArchiveInputStream(new GzipCompressorInputStream(new FileInputStream(sourceFile)))) {\n            // decompressing *.tar.gz files to tar\n            File extraceFolder = new File(targetDirectoryPath);\n            TarArchiveEntry entry;\n            // 将 tar 文件解压到 extractPath 目录下\n            while ((entry = fin.getNextTarEntry()) != null) {\n                if (entry.isDirectory()) {\n                    continue;\n                }\n                File curfile = new File(extraceFolder, entry.getName());\n```\n\n### Requirements:\n1. Complete only the missing part of the method.\n2. Only answer the code of the completed part. Do not include any of the provided code snippet.\n3. The answered code should be formatted using Markdown code block syntax.\n4. Do not include any additional text.\n\n### Answer: (use the provided format with backticks)\n", "reference": "                File parent = curfile.getParentFile();\n                if (!parent.exists()) {\n                    parent.mkdirs();\n                }\n                // 将文件写出到解压的目录\n                try (FileOutputStream fileOutputStream = new FileOutputStream(curfile)){\n                    IOUtils.copy(fin, fileOutputStream);\n                }\n            }\n        } catch (Exception e){\n            throw new CoreException(PDKRunnerErrorCodes.CLI_UNZIP_DIR_IS_FILE, \"Unzip director is a file, expect to be directory or none, \" + e.getMessage());\n        }\n    }", "completions": ["  \n```Java\n                File parent = curfile.getParentFile();\n                if (!parent.exists()) {\n                    parent.mkdirs();\n                }\n                try (OutputStream out = new FileOutputStream(curfile)) {\n                    IOUtils.copy(fin, out);\n                }\n            }\n        } catch (IOException e) {\n            throw new CoreException(PDKRunnerErrorCodes.CLI_UNZIP_FAILED, \"Unzip failed\");\n        }\n    }\n```"]}
{"id": 48, "name": "validateDpopThumprint", "prompt": "### Role:\nYou are an experienced Java programmer assistant responsible for code completion tasks. Please intelligently complete the missing parts of the provided code snippet.\n\n### Task:\nBased on the provided code snippet, complete the missing method implementation. Ensure the code is logically correct, syntactically accurate, and follows best coding practices.\n\n### Left context\n```Java\npackage io.jans.as.server.auth;\n\nimport com.nimbusds.jose.jwk.JWKException;\nimport io.jans.as.common.model.registration.Client;\nimport io.jans.as.model.authorize.AuthorizeErrorResponseType;\nimport io.jans.as.model.configuration.AppConfiguration;\nimport io.jans.as.model.crypto.AbstractCryptoProvider;\nimport io.jans.as.model.error.ErrorResponseFactory;\nimport io.jans.as.model.exception.CryptoProviderException;\nimport io.jans.as.model.exception.InvalidJwtException;\nimport io.jans.as.model.jwk.JSONWebKey;\nimport io.jans.as.model.jwk.JSONWebKeySet;\nimport io.jans.as.model.jwt.DPoPJwtPayloadParam;\nimport io.jans.as.model.jwt.Jwt;\nimport io.jans.as.model.jwt.JwtType;\nimport io.jans.as.model.token.TokenErrorResponseType;\nimport io.jans.as.model.token.TokenRequestParam;\nimport io.jans.as.server.audit.ApplicationAuditLogger;\nimport io.jans.as.server.model.audit.OAuth2AuditLog;\nimport io.jans.as.server.model.common.DPoPJti;\nimport io.jans.as.server.util.ServerUtil;\nimport io.jans.service.CacheService;\nimport jakarta.ejb.DependsOn;\nimport jakarta.inject.Inject;\nimport jakarta.inject.Named;\nimport jakarta.servlet.http.HttpServletRequest;\nimport jakarta.ws.rs.WebApplicationException;\nimport jakarta.ws.rs.core.MediaType;\nimport jakarta.ws.rs.core.Response;\nimport org.apache.commons.lang.BooleanUtils;\nimport org.apache.commons.lang.StringUtils;\nimport org.slf4j.Logger;\n\nimport java.security.NoSuchAlgorithmException;\nimport java.security.NoSuchProviderException;\nimport java.util.Date;\nimport java.util.UUID;\n\nimport static org.apache.commons.lang.BooleanUtils.isFalse;\nimport static org.apache.commons.lang.BooleanUtils.isTrue;\n\n/**\n * @author Yuriy Z\n */\n@DependsOn(\"appInitializer\")\n@Named\npublic class DpopService {\n\n    public static final String NO_CACHE = \"no-cache\";\n    public static final String PRAGMA = \"Pragma\";\n    public static final String DPOP_NONCE = \"DPoP-Nonce\";\n    public static final String DPOP = \"DPoP\";\n\n    @Inject\n    private Logger log;\n\n    @Inject\n    private AppConfiguration appConfiguration;\n\n    @Inject\n    private CacheService cacheService;\n\n    @Inject\n    private AbstractCryptoProvider cryptoProvider;\n\n    @Inject\n    private ErrorResponseFactory errorResponseFactory;\n\n    @Inject\n    private ApplicationAuditLogger applicationAuditLogger;\n\n    public void validateDpopValuesCount(HttpServletRequest servletRequest) {\n        validateDpopValuesCount(servletRequest.getParameterValues(TokenRequestParam.DPOP));\n    }\n\n    public void validateDpopValuesCount(final String[] values) {\n        if (values != null && values.length > 1) {\n            log.trace(\"Multiple DPoP header values are not allowed. Count: {}\", values.length);\n            throw new WebApplicationException(Response\n                    .status(Response.Status.BAD_REQUEST)\n                    .type(MediaType.APPLICATION_JSON_TYPE)\n                    .entity(errorResponseFactory.errorAsJson(TokenErrorResponseType.INVALID_DPOP_PROOF, \"Multiple DPoP header values\"))\n                    .cacheControl(ServerUtil.cacheControl(true, false))\n                    .header(PRAGMA, NO_CACHE)\n                    .build());\n        }\n    }\n\n    public boolean validateDpop(String dpop) {\n        try {\n            return validateDpop(Jwt.parseOrThrow(dpop));\n        } catch (InvalidJwtException e) {\n            log.error(\"Failed to validate dpop: \" + dpop, e);\n            return false;\n        }\n    }\n\n    public boolean validateDpop(Jwt dpop) {\n        try {\n            validateDpopHeader(dpop);\n\n            validateDpopPayload(dpop);\n\n            JSONWebKey jwk = JSONWebKey.fromJSONObject(dpop.getHeader().getJwk());\n            String dpopJwkThumbprint = jwk.getJwkThumbprint();\n            return validateDpopSignature(dpop, jwk, dpopJwkThumbprint);\n        } catch (WebApplicationException e) {\n            throw e;\n        } catch (InvalidJwtException e) {\n            log.error(\"Failed to validate dpop: \" + dpop, e);\n        } catch (Exception e) {\n            log.error(\"Invalid dpop: \" + dpop, e);\n        }\n        return false;\n    }\n\n    private boolean validateDpopSignature(Jwt dpop, JSONWebKey jwk, String dpopJwkThumbprint) throws InvalidJwtException, CryptoProviderException {\n        if (dpopJwkThumbprint == null) {\n            throw new InvalidJwtException(\"Invalid DPoP Proof Header. The jwk header is not valid.\");\n        }\n\n        JSONWebKeySet jwks = new JSONWebKeySet();\n        jwks.getKeys().add(jwk);\n\n        return cryptoProvider.verifySignature(\n                dpop.getSigningInput(),\n                dpop.getEncodedSignature(),\n                null,\n                jwks.toJSONObject(),\n                null,\n                dpop.getHeader().getSignatureAlgorithm());\n    }\n\n    private void validateDpopPayload(Jwt dpop) throws InvalidJwtException {\n        if (StringUtils.isBlank(dpop.getClaims().getClaimAsString(DPoPJwtPayloadParam.HTM))) {\n            throw new InvalidJwtException(\"Invalid DPoP Proof Payload. The htm param is required.\");\n        }\n        if (StringUtils.isBlank(dpop.getClaims().getClaimAsString(DPoPJwtPayloadParam.HTU))) {\n            throw new InvalidJwtException(\"Invalid DPoP Proof Payload. The htu param is required\");\n        }\n        if (dpop.getClaims().getClaimAsLong(DPoPJwtPayloadParam.IAT) == null) {\n            throw new InvalidJwtException(\"Invalid DPoP Proof Payload. The iat param is required.\");\n        }\n        if (StringUtils.isBlank(dpop.getClaims().getClaimAsString(DPoPJwtPayloadParam.JTI))) {\n            throw new InvalidJwtException(\"Invalid DPoP Proof Payload. The jti param is required\");\n        }\n\n        String jti = dpop.getClaims().getClaimAsString(DPoPJwtPayloadParam.JTI);\n        Long iat = dpop.getClaims().getClaimAsLong(DPoPJwtPayloadParam.IAT);\n        String htu = dpop.getClaims().getClaimAsString(DPoPJwtPayloadParam.HTU);\n        String nonce = dpop.getClaims().getClaimAsString(DPoPJwtPayloadParam.NONCE);\n        String cacheKey = \"dpop_jti_\" + jti;\n        DPoPJti dPoPJti = (DPoPJti) cacheService.get(cacheKey);\n\n        // Validate the token was issued within an acceptable timeframe.\n        int seconds = appConfiguration.getDpopTimeframe();\n        long diff = (new Date().getTime() - iat) / 1000;\n        if (diff > seconds) {\n            throw new InvalidJwtException(\"The DPoP token has expired.\");\n        }\n\n        if (dPoPJti == null) {\n            dPoPJti = new DPoPJti(jti, iat, htu);\n            cacheService.put(appConfiguration.getDpopJtiCacheTime(), cacheKey, dPoPJti);\n        } else {\n            throw new InvalidJwtException(\"Invalid DPoP Proof. The jti param has been used before.\");\n        }\n\n        if (BooleanUtils.isTrue(appConfiguration.getDpopUseNonce())) {\n            if (StringUtils.isBlank(nonce)) {\n                throw new WebApplicationException(Response\n                        .status(Response.Status.BAD_REQUEST)\n                        .type(MediaType.APPLICATION_JSON_TYPE)\n                        .entity(errorResponseFactory.errorAsJson(TokenErrorResponseType.USE_DPOP_NONCE, \"Nonce is not set\"))\n                        .cacheControl(ServerUtil.cacheControl(true, false))\n                        .header(PRAGMA, NO_CACHE)\n                        .header(DPOP_NONCE, generateNonce()).build());\n            }\n\n            final Object nonceValue = cacheService.get(nonce);\n            if (nonceValue == null) {\n                throw new WebApplicationException(Response\n                        .status(Response.Status.BAD_REQUEST)\n                        .type(MediaType.APPLICATION_JSON_TYPE)\n                        .entity(errorResponseFactory.errorAsJson(TokenErrorResponseType.USE_NEW_DPOP_NONCE, \"New nonce value is required\"))\n                        .cacheControl(ServerUtil.cacheControl(true, false))\n                        .header(PRAGMA, NO_CACHE)\n                        .header(DPOP_NONCE, generateNonce()).build());\n            }\n        }\n    }\n\n    private String generateNonce() {\n        final String nonce = UUID.randomUUID().toString();\n        cacheService.put(appConfiguration.getDpopNonceCacheTime(), nonce, nonce);\n        return nonce;\n    }\n\n    private void validateDpopHeader(Jwt dpop) throws InvalidJwtException {\n        if (dpop.getHeader().getType() != JwtType.DPOP_PLUS_JWT) {\n            throw new InvalidJwtException(\"Invalid DPoP Proof Header. The typ header must be dpop+jwt.\");\n        }\n        if (dpop.getHeader().getSignatureAlgorithm() == null) {\n            throw new InvalidJwtException(\"Invalid DPoP Proof Header. The typ header must be dpop+jwt.\");\n        }\n        if (dpop.getHeader().getJwk() == null) {\n            throw new InvalidJwtException(\"Invalid DPoP Proof Header. The jwk header is required.\");\n        }\n    }\n\n    private Response response(Response.ResponseBuilder builder, OAuth2AuditLog oAuth2AuditLog) {\n        builder.cacheControl(ServerUtil.cacheControl(true, false));\n        builder.header(PRAGMA, NO_CACHE);\n\n        applicationAuditLogger.sendMessage(oAuth2AuditLog);\n\n        return builder.build();\n    }\n\n    private Response.ResponseBuilder error(int status, TokenErrorResponseType type, String reason) {\n        return Response.status(status).type(MediaType.APPLICATION_JSON_TYPE).entity(errorResponseFactory.errorAsJson(type, reason));\n    }\n\n    public String getDpopJwkThumbprint(String dpopStr) throws InvalidJwtException, NoSuchAlgorithmException, JWKException, NoSuchProviderException {\n        final Jwt dpop = Jwt.parseOrThrow(dpopStr);\n        JSONWebKey jwk = JSONWebKey.fromJSONObject(dpop.getHeader().getJwk());\n        return jwk.getJwkThumbprint();\n    }\n\n    public String getDPoPJwkThumbprint(HttpServletRequest httpRequest, Client client, OAuth2AuditLog oAuth2AuditLog) {\n        try {\n            String dpopStr = httpRequest.getHeader(DPOP);\n            final boolean isDpopBlank = StringUtils.isBlank(dpopStr);\n\n            if (isTrue(client.getAttributes().getDpopBoundAccessToken()) && isDpopBlank) {\n                log.debug(\"Client requires DPoP bound access token. Invalid request - DPoP header is not set.\");\n                throw new WebApplicationException(response(error(400, TokenErrorResponseType.INVALID_DPOP_PROOF, \"Invalid request - DPoP header is not set.\"), oAuth2AuditLog));\n            }\n\n            if (isDpopBlank) return null;\n\n            String dpopJwkThumbprint = getDpopJwkThumbprint(dpopStr);\n\n            if (dpopJwkThumbprint == null)\n                throw new InvalidJwtException(\"Invalid DPoP Proof Header. The jwk header is not valid.\");\n\n            return dpopJwkThumbprint;\n        } catch (InvalidJwtException | JWKException | NoSuchAlgorithmException | NoSuchProviderException e) {\n            throw new WebApplicationException(response(error(400, TokenErrorResponseType.INVALID_DPOP_PROOF, e.getMessage()), oAuth2AuditLog));\n        }\n    }\n\n    public void validateDpopThumprintIsPresent(String dpopJkt, String state) {\n        if (BooleanUtils.isTrue(appConfiguration.getDpopJktForceForAuthorizationCode()) && StringUtils.isBlank(dpopJkt)) {\n            throw new WebApplicationException(Response\n                    .status(Response.Status.BAD_REQUEST.getStatusCode())\n                    .type(MediaType.APPLICATION_JSON_TYPE)\n                    .entity(errorResponseFactory.getErrorAsJson(AuthorizeErrorResponseType.INVALID_REQUEST, state, \"dpop_jkt is absent\"))\n                    .build());\n        }\n    }\n\n```\n\n### Code description\n```Java\n/**\n * Description: This method validates the DPoP (Demonstration of Proof-of-Possession) thumbprint.\n * It compares the existing thumbprint with the thumbprint from the request.\n * If the existing thumbprint is blank and the DPoP JWT force for authorization code is false,\n * the method returns without any checks.\n * If the thumbprints do not match, it logs a debug message, throws a WebApplicationException with a BAD_REQUEST status,\n * and includes an error response with the error type INVALID_DPOP_PROOF and the message \"Thumbprint does not match\".\n * The response is also set to not cache.\n *\n * @param existingThumprint The existing thumbprint to compare with the request thumbprint.\n * @param requestThumprint The thumbprint from the request to be compared with the existing thumbprint.\n * @throws WebApplicationException If the thumbprints do not match, with a BAD_REQUEST status and an error response.\n */\n```\n\n### Code snippet:\n```Java\n    public void validateDpopThumprint(String existingThumprint, String requestThumprint) {\n        if (StringUtils.isBlank(existingThumprint) && isFalse(appConfiguration.getDpopJktForceForAuthorizationCode())) {\n            return; // nothing to check\n        }\n\n        if (!StringUtils.equals(existingThumprint, requestThumprint)) {\n            log.debug(\"DPoP Thumprint between saved one '{}' and send in request '{}' does NOT match. Reject request.\", existingThumprint, requestThumprint);\n            throw new WebApplicationException(Response\n```\n\n### Requirements:\n1. Complete only the missing part of the method.\n2. Only answer the code of the completed part. Do not include any of the provided code snippet.\n3. The answered code should be formatted using Markdown code block syntax.\n4. Do not include any additional text.\n\n### Answer: (use the provided format with backticks)\n", "reference": "                    .status(Response.Status.BAD_REQUEST)\n                    .type(MediaType.APPLICATION_JSON_TYPE)\n                    .entity(errorResponseFactory.errorAsJson(TokenErrorResponseType.INVALID_DPOP_PROOF, \"Thumprint does not match\"))\n                    .cacheControl(ServerUtil.cacheControl(true, false))\n                    .header(PRAGMA, NO_CACHE)\n                    .build());\n        }\n    }", "completions": ["  \n```Java\n    public void validateDpopThumprint(String existingThumprint, String requestThumprint) {\n        if (StringUtils.isBlank(existingThumprint) && isFalse(appConfiguration.getDpopJktForceForAuthorizationCode())) {\n            return; // nothing to check\n        }\n\n        if (!StringUtils.equals(existingThumprint, requestThumprint)) {\n            log.debug(\"DPoP Thumprint between saved one '{}' and send in request '{}' does NOT match. Reject request.\", existingThumprint, requestThumprint);\n            throw new WebApplicationException(Response\n                   .status(Response.Status.BAD_REQUEST)\n                   .type(MediaType.APPLICATION_JSON_TYPE)\n                   .entity(errorResponseFactory.errorAsJson(TokenErrorResponseType.INVALID_DPOP_PROOF, \"Thumbprint does not match\"))\n                   .cacheControl(ServerUtil.cacheControl(true, false))\n                   .header(PRAGMA, NO_CACHE)\n                   .build());\n        }\n    }\n```"]}
{"id": 49, "name": "setUserData", "prompt": "### Role:\nYou are an experienced Java programmer assistant responsible for code completion tasks. Please intelligently complete the missing parts of the provided code snippet.\n\n### Task:\nBased on the provided code snippet, complete the missing method implementation. Ensure the code is logically correct, syntactically accurate, and follows best coding practices.\n\n### Left context\n```Java\n/*-\n * #%L\n * HAPI FHIR - Core Library\n * %%\n * Copyright (C) 2014 - 2024 Smile CDR, Inc.\n * %%\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *      http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * #L%\n */\npackage ca.uhn.fhir.model.api;\n\nimport com.fasterxml.jackson.annotation.JsonProperty;\nimport org.apache.commons.lang3.Validate;\n\nimport java.util.HashMap;\nimport java.util.Map;\n\nimport static org.apache.commons.lang3.StringUtils.isNotBlank;\n\npublic abstract class BaseBatchJobParameters implements IModelJson {\n\t/**\n\t * A serializable map of key-value pairs that can be\n\t * added to any extending job.\n\t */\n\t@JsonProperty(\"userData\")\n\tprivate Map<String, Object> myUserData;\n\n\tpublic Map<String, Object> getUserData() {\n\t\tif (myUserData == null) {\n\t\t\tmyUserData = new HashMap<>();\n\t\t}\n\t\treturn myUserData;\n\t}\n\n```\n\n### Code description\n```Java\n/**\n * Sets the user data for a given key.\n *\n * @param theKey the key for the user data. Must be non-empty and non-null.\n * @param theValue the value for the user data. If null, the key-value pair will be removed.\n *                 Otherwise, the value must be validated by the `validateValue` method.\n *                 If the value is not valid, an exception will be thrown.\n * @throws IllegalArgumentException if the key is empty or null.\n * @throws IllegalArgumentException if the value is not valid according to `validateValue`.\n */\n```\n\n### Code snippet:\n```Java\n\tpublic void setUserData(String theKey, Object theValue) {\n\t\tValidate.isTrue(isNotBlank(theKey), \"Invalid key; key must be non-empty, non-null.\");\n\t\tif (theValue == null) {\n\t\t\tgetUserData().remove(theKey);\n\t\t} else {\n\t\t\tValidate.isTrue(\n```\n\n### Requirements:\n1. Complete only the missing part of the method.\n2. Only answer the code of the completed part. Do not include any of the provided code snippet.\n3. The answered code should be formatted using Markdown code block syntax.\n4. Do not include any additional text.\n\n### Answer: (use the provided format with backticks)\n", "reference": "\t\t\t\t\tvalidateValue(theValue),\n\t\t\t\t\tString.format(\n\t\t\t\t\t\t\t\"Invalid data type provided %s\", theValue.getClass().getName()));\n\t\t\tgetUserData().put(theKey, theValue);\n\t\t}\n\t}", "completions": ["  ```\nValidate.isTrue(validateValue(theValue), \"Invalid value; value must be valid according to `validateValue`.\");\ngetUserData().put(theKey, theValue);\n```"]}
{"id": 50, "name": "submit", "prompt": "### Role:\nYou are an experienced Java programmer assistant responsible for code completion tasks. Please intelligently complete the missing parts of the provided code snippet.\n\n### Task:\nBased on the provided code snippet, complete the missing method implementation. Ensure the code is logically correct, syntactically accurate, and follows best coding practices.\n\n### Left context\n```Java\n/*\n * Copyright (c) 2023 OceanBase.\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\npackage com.oceanbase.odc.service.partitionplan;\n\nimport java.util.Collections;\nimport java.util.List;\nimport java.util.Map;\nimport java.util.Optional;\nimport java.util.Set;\nimport java.util.stream.Collectors;\n\nimport javax.transaction.Transactional;\n\nimport org.apache.commons.collections4.CollectionUtils;\nimport org.apache.commons.lang3.Validate;\nimport org.quartz.SchedulerException;\nimport org.springframework.beans.factory.annotation.Autowired;\nimport org.springframework.stereotype.Service;\n\nimport com.fasterxml.jackson.core.type.TypeReference;\nimport com.oceanbase.odc.common.json.JsonUtils;\nimport com.oceanbase.odc.core.authority.util.SkipAuthorize;\nimport com.oceanbase.odc.core.flow.model.TaskParameters;\nimport com.oceanbase.odc.core.shared.constant.FlowStatus;\nimport com.oceanbase.odc.core.shared.constant.TaskErrorStrategy;\nimport com.oceanbase.odc.metadb.flow.FlowInstanceRepository;\nimport com.oceanbase.odc.metadb.partitionplan.PartitionPlanEntity;\nimport com.oceanbase.odc.metadb.partitionplan.PartitionPlanRepository;\nimport com.oceanbase.odc.metadb.partitionplan.PartitionPlanTableEntity;\nimport com.oceanbase.odc.metadb.partitionplan.PartitionPlanTablePartitionKeyEntity;\nimport com.oceanbase.odc.metadb.partitionplan.PartitionPlanTablePartitionKeyRepository;\nimport com.oceanbase.odc.metadb.partitionplan.PartitionPlanTableRepository;\nimport com.oceanbase.odc.metadb.schedule.ScheduleEntity;\nimport com.oceanbase.odc.service.connection.database.DatabaseService;\nimport com.oceanbase.odc.service.connection.database.model.Database;\nimport com.oceanbase.odc.service.flow.FlowInstanceService;\nimport com.oceanbase.odc.service.flow.model.FlowInstanceDetailResp;\nimport com.oceanbase.odc.service.iam.auth.AuthenticationFacade;\nimport com.oceanbase.odc.service.partitionplan.model.PartitionPlanConfig;\nimport com.oceanbase.odc.service.partitionplan.model.PartitionPlanKeyConfig;\nimport com.oceanbase.odc.service.partitionplan.model.PartitionPlanStrategy;\nimport com.oceanbase.odc.service.partitionplan.model.PartitionPlanTableConfig;\nimport com.oceanbase.odc.service.quartz.model.MisfireStrategy;\nimport com.oceanbase.odc.service.schedule.ScheduleService;\nimport com.oceanbase.odc.service.schedule.model.JobType;\nimport com.oceanbase.odc.service.schedule.model.ScheduleStatus;\nimport com.oceanbase.odc.service.schedule.model.TriggerConfig;\n\nimport lombok.NonNull;\nimport lombok.extern.slf4j.Slf4j;\n\n/**\n * {@link PartitionPlanScheduleService}\n *\n * @author yh263208\n * @date 2024-02-05 19:27\n * @since ODC_release_4.2.4\n */\n@Slf4j\n@Service\n@SkipAuthorize(\"odc internal usage\")\npublic class PartitionPlanScheduleService {\n\n    @Autowired\n    private AuthenticationFacade authenticationFacade;\n    @Autowired\n    private DatabaseService databaseService;\n    @Autowired\n    private ScheduleService scheduleService;\n    @Autowired\n    private PartitionPlanTableRepository partitionPlanTableRepository;\n    @Autowired\n    private PartitionPlanRepository partitionPlanRepository;\n    @Autowired\n    private PartitionPlanTablePartitionKeyRepository partitionPlanTablePartitionKeyRepository;\n    @Autowired\n    private FlowInstanceService flowInstanceService;\n    @Autowired\n    private FlowInstanceRepository flowInstanceRepository;\n\n    public PartitionPlanConfig getPartitionPlanByFlowInstanceId(@NonNull Long flowInstanceId) {\n        FlowInstanceDetailResp resp = this.flowInstanceService.detail(flowInstanceId);\n        Optional<PartitionPlanEntity> optional = this.partitionPlanRepository.findByFlowInstanceId(resp.getId());\n        if (optional.isPresent()) {\n            return optional.map(this::getPartitionPlan).get();\n        }\n        TaskParameters parameters = resp.getParameters();\n        if (!(parameters instanceof PartitionPlanConfig)) {\n            return null;\n        }\n        PartitionPlanConfig partitionPlanConfig = (PartitionPlanConfig) parameters;\n        partitionPlanConfig.setEnabled(false);\n        if (CollectionUtils.isNotEmpty(partitionPlanConfig.getPartitionTableConfigs())) {\n            partitionPlanConfig.getPartitionTableConfigs().forEach(t -> t.setEnabled(false));\n        }\n        return partitionPlanConfig;\n    }\n\n    public PartitionPlanConfig getPartitionPlanByDatabaseId(@NonNull Long databaseId) {\n        Database database = this.databaseService.detail(databaseId);\n        List<PartitionPlanEntity> planEntities = this.partitionPlanRepository\n                .findByDatabaseIdAndEnabled(database.getId(), true);\n        if (CollectionUtils.isEmpty(planEntities)) {\n            return null;\n        } else if (planEntities.size() > 1) {\n            throw new IllegalStateException(\"Unknown error, there are \"\n                    + planEntities.size() + \" partition plans are active, databaseId=\" + databaseId);\n        }\n        return getPartitionPlan(planEntities.get(0));\n    }\n\n    public List<PartitionPlanTableConfig> getPartitionPlanTables(@NonNull List<Long> partitionPlanTableIds) {\n        List<PartitionPlanTableEntity> ppts = this.partitionPlanTableRepository.findByIdIn(partitionPlanTableIds);\n        if (CollectionUtils.isEmpty(ppts)) {\n            return Collections.emptyList();\n        }\n        List<PartitionPlanTableConfig> tableConfigs = ppts.stream()\n                .map(this::entityToModel).collect(Collectors.toList());\n        Map<Long, List<PartitionPlanTablePartitionKeyEntity>> pptId2KeyEntities =\n                this.partitionPlanTablePartitionKeyRepository.findByPartitionplanTableIdIn(tableConfigs.stream()\n                        .map(PartitionPlanTableConfig::getId).collect(Collectors.toList())).stream()\n                        .collect(Collectors.groupingBy(PartitionPlanTablePartitionKeyEntity::getPartitionplanTableId));\n        tableConfigs.forEach(tableConfig -> {\n            List<PartitionPlanTablePartitionKeyEntity> pptks = pptId2KeyEntities.get(tableConfig.getId());\n            if (CollectionUtils.isEmpty(pptks)) {\n                return;\n            }\n            tableConfig.setPartitionKeyConfigs(pptks.stream().map(this::entityToModel).collect(Collectors.toList()));\n        });\n        return tableConfigs;\n    }\n\n    /**\n     * submit a partition plan task\n     *\n     * @param partitionPlanConfig config for a partition plan\n     */\n```\n\n### Code description\n```Java\n/**\n * Description: This method submits a partition plan configuration for execution. It disables all related partition plan tasks, saves the configuration, and if enabled and table configurations are present, it creates and enables schedules for partition plan table creation and dropping.\n * \n * @param partitionPlanConfig The partition plan configuration to be submitted.\n * @throws SchedulerException If there is an error with the scheduler.\n * @throws ClassNotFoundException If a class is not found.\n */\n```\n\n### Code snippet:\n```Java\n    @Transactional(rollbackOn = Exception.class)\n    public void submit(@NonNull PartitionPlanConfig partitionPlanConfig)\n            throws SchedulerException, ClassNotFoundException {\n        Long databaseId = partitionPlanConfig.getDatabaseId();\n        Validate.notNull(databaseId, \"DatabaseId can not be null\");\n        // disable all related partition plan task\n        Database database = this.databaseService.detail(databaseId);\n        disablePartitionPlan(database.getId());\n        PartitionPlanEntity partitionPlanEntity = modelToEntity(partitionPlanConfig);\n        partitionPlanEntity = this.partitionPlanRepository.save(partitionPlanEntity);\n        if (!partitionPlanConfig.isEnabled()\n                || CollectionUtils.isEmpty(partitionPlanConfig.getPartitionTableConfigs())) {\n            log.info(\"Partition plan is disabled or table config is empty, do nothing and return\");\n            return;\n        }\n        Validate.isTrue(partitionPlanConfig.getCreationTrigger() != null, \"Creation trigger can not be null\");\n        if (partitionPlanConfig.getDroppingTrigger() == null) {\n            ScheduleEntity createScheduleEntity = createAndEnableSchedule(\n                    database, partitionPlanConfig.getCreationTrigger());\n            createPartitionPlanTables(partitionPlanConfig.getPartitionTableConfigs(),\n                    partitionPlanEntity.getId(), createScheduleEntity.getId(),\n                    partitionPlanConfig.getFlowInstanceId(), partitionPlanConfig.getTaskId(),\n                    partitionPlanConfig.getErrorStrategy(), partitionPlanConfig.getTimeoutMillis());\n            return;\n        }\n        Map<PartitionPlanStrategy, List<PartitionPlanTableConfig>> strategy2TblCfgs =\n                partitionPlanConfig.getPartitionTableConfigs().stream().flatMap(tableConfig -> {\n                    Map<PartitionPlanStrategy, List<PartitionPlanKeyConfig>> strategy2Cfgs =\n                            tableConfig.getPartitionKeyConfigs().stream()\n```\n\n### Requirements:\n1. Complete only the missing part of the method.\n2. Only answer the code of the completed part. Do not include any of the provided code snippet.\n3. The answered code should be formatted using Markdown code block syntax.\n4. Do not include any additional text.\n\n### Answer: (use the provided format with backticks)\n", "reference": "                                    .collect(Collectors.groupingBy(PartitionPlanKeyConfig::getStrategy));\n                    return strategy2Cfgs.values().stream().map(cfgs -> {\n                        PartitionPlanTableConfig cfg = new PartitionPlanTableConfig();\n                        cfg.setPartitionKeyConfigs(cfgs);\n                        cfg.setTableName(tableConfig.getTableName());\n                        cfg.setEnabled(tableConfig.isEnabled());\n                        cfg.setPartitionNameInvoker(tableConfig.getPartitionNameInvoker());\n                        cfg.setPartitionNameInvokerParameters(tableConfig.getPartitionNameInvokerParameters());\n                        return cfg;\n                    });\n                }).collect(Collectors.groupingBy(cfg -> cfg.getPartitionKeyConfigs().get(0).getStrategy()));\n        List<PartitionPlanTableConfig> createConfigs = strategy2TblCfgs.get(PartitionPlanStrategy.CREATE);\n        List<PartitionPlanTableConfig> dropConfigs = strategy2TblCfgs.get(PartitionPlanStrategy.DROP);\n        if (CollectionUtils.isNotEmpty(createConfigs)) {\n            ScheduleEntity createScheduleEntity = createAndEnableSchedule(\n                    database, partitionPlanConfig.getCreationTrigger());\n            createPartitionPlanTables(createConfigs,\n                    partitionPlanEntity.getId(), createScheduleEntity.getId(),\n                    partitionPlanConfig.getFlowInstanceId(), partitionPlanConfig.getTaskId(),\n                    partitionPlanConfig.getErrorStrategy(), partitionPlanConfig.getTimeoutMillis());\n        }\n        if (CollectionUtils.isNotEmpty(dropConfigs)) {\n            ScheduleEntity dropScheduleEntity = createAndEnableSchedule(\n                    database, partitionPlanConfig.getDroppingTrigger());\n            createPartitionPlanTables(dropConfigs,\n                    partitionPlanEntity.getId(), dropScheduleEntity.getId(),\n                    partitionPlanConfig.getFlowInstanceId(), partitionPlanConfig.getTaskId(),\n                    partitionPlanConfig.getErrorStrategy(), partitionPlanConfig.getTimeoutMillis());\n        }\n    }", "completions": ["  \n```Java\n    @Transactional(rollbackOn = Exception.class)\n    public void submit(@NonNull PartitionPlanConfig partitionPlanConfig)\n            throws SchedulerException, ClassNotFoundException {\n        Long databaseId = partitionPlanConfig.getDatabaseId();\n        Validate.notNull(databaseId, \"DatabaseId can not be null\");\n        // disable all related partition plan task\n        Database database = this.databaseService.detail(databaseId);\n        disablePartitionPlan(database.getId());\n        PartitionPlanEntity partitionPlanEntity = modelToEntity(partitionPlanConfig);\n        partitionPlanEntity = this.partitionPlanRepository.save(partitionPlanEntity);\n        if (!partitionPlanConfig.isEnabled()\n                || CollectionUtils.isEmpty(partitionPlanConfig.getPartitionTableConfigs())) {\n            log.info(\"Partition plan is disabled or table config is empty, do nothing and return\");\n            return;\n        }\n        Validate.isTrue(partitionPlanConfig.getCreationTrigger()!= null, \"Creation trigger can not be null\");\n        if (partitionPlanConfig.getDroppingTrigger() == null) {\n            ScheduleEntity createScheduleEntity = createAndEnableSchedule(\n                    database, partitionPlanConfig.getCreationTrigger());\n            createPartitionPlanTables(partitionPlanConfig.getPartitionTableConfigs(),\n                    partitionPlanEntity.getId(), createScheduleEntity.getId(),\n                    partitionPlanConfig.getFlowInstanceId(), partitionPlanConfig.getTaskId(),\n                    partitionPlanConfig.getErrorStrategy(), partitionPlanConfig.getTimeoutMillis());\n            return;\n        }\n        Map<PartitionPlanStrategy, List<PartitionPlanTableConfig>> strategy2TblCfgs =\n                partitionPlanConfig.getPartitionTableConfigs().stream().flatMap(tableConfig -> {\n                    Map<PartitionPlanStrategy, List<PartitionPlanKeyConfig>> strategy2Cfgs =\n                            tableConfig.getPartitionKeyConfigs().stream()\n                                   .collect(Collectors.groupingBy(PartitionPlanKeyConfig::getStrategy));\n                    return strategy2Cfgs.entrySet().stream()\n                           .map(entry -> new PartitionPlanTableConfig(tableConfig.getId(),\n                                    entry.getKey(), entry.getValue()));\n                }).collect(Collectors.groupingBy(PartitionPlanTableConfig::getStrategy));\n        ScheduleEntity createScheduleEntity = createAndEnableSchedule(database,\n                partitionPlanConfig.getCreationTrigger());\n        ScheduleEntity dropScheduleEntity = createAndEnableSchedule(database,\n                partitionPlanConfig.getDroppingTrigger());\n        createPartitionPlanTables(strategy2TblCfgs.get(PartitionPlanStrategy.CREATE),\n                partitionPlanEntity.getId(), createScheduleEntity.getId(),\n                partitionPlanConfig.getFlowInstanceId(), partitionPlanConfig.getTaskId(),\n                partitionPlanConfig.getErrorStrategy(), partitionPlanConfig.getTimeoutMillis());\n        createPartitionPlanTables(strategy2TblCfgs.get(PartitionPlanStrategy.DROP),\n                partitionPlanEntity.getId(), dropScheduleEntity.getId(),\n                partitionPlanConfig.getFlowInstanceId(), partitionPlanConfig.getTaskId(),\n                partitionPlanConfig.getErrorStrategy(), partitionPlanConfig.getTimeoutMillis());\n    }\n```"]}
{"id": 51, "name": "decrypt", "prompt": "### Role:\nYou are an experienced Java programmer assistant responsible for code completion tasks. Please intelligently complete the missing parts of the provided code snippet.\n\n### Task:\nBased on the provided code snippet, complete the missing method implementation. Ensure the code is logically correct, syntactically accurate, and follows best coding practices.\n\n### Left context\n```Java\n/*\n * Copyright (c) 2017-2021 Nitrite author or authors.\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *       http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n *\n */\n\npackage org.dizitart.no2.support.crypto;\n\nimport org.apache.commons.codec.binary.Base64;\nimport org.dizitart.no2.common.util.CryptoUtils;\nimport org.dizitart.no2.common.util.SecureString;\nimport org.dizitart.no2.exceptions.NitriteSecurityException;\n\nimport javax.crypto.Cipher;\nimport javax.crypto.SecretKey;\nimport javax.crypto.spec.GCMParameterSpec;\nimport java.nio.ByteBuffer;\nimport java.nio.charset.Charset;\nimport java.nio.charset.StandardCharsets;\n\n/**\n * The {@code AESEncryptor} class provides AES encryption and decryption\n * functionality.\n * <p>\n * It uses AES/GCM/NoPadding encryption algorithm with a 128-bit tag length,\n * 12-byte IV length, and 16-byte salt length by default.\n * <p>\n * The class provides methods to encrypt and decrypt byte arrays and strings\n * using the specified password and encryption parameters.\n * <p>\n * \n * NOTE: This is a derivative work of <a href=\n * \"https://mkyong.com/java/java-symmetric-key-cryptography-example/\">this</a>.\n * \n * @author Anindya Chatterjee\n * @since 4.0\n */\npublic class AESEncryptor implements Encryptor {\n    private final String encryptAlgo;\n    private final int tagLengthBit;\n    private final int ivLengthByte;\n    private final int saltLengthByte;\n    private final Charset UTF_8 = StandardCharsets.UTF_8;\n\n    private final SecureString password;\n\n    /**\n     * Instantiates a new {@link AESEncryptor} with these default values\n     * <ul>\n     * <li>Encryption Algo - AES/GCM/NoPadding</li>\n     * <li>Tag Length (bit) - 128</li>\n     * <li>IV Length (byte) - 12</li>\n     * <li>Salt Length (byte) - 16</li>\n     * </ul>\n     *\n     * @param password the password\n     */\n    public AESEncryptor(String password) {\n        this(password, \"AES/GCM/NoPadding\", 128, 12, 16);\n    }\n\n    /**\n     * Instantiates a new {@link AESEncryptor}.\n     *\n     * @param password       the password\n     * @param encryptionAlgo the encryption algo\n     * @param tagLengthBit   the tag length bit\n     * @param ivLengthByte   the iv length byte\n     * @param saltLengthByte the salt length byte\n     */\n    public AESEncryptor(String password, String encryptionAlgo,\n            Integer tagLengthBit, Integer ivLengthByte,\n            Integer saltLengthByte) {\n        this.password = new SecureString(password);\n        this.encryptAlgo = encryptionAlgo;\n        this.tagLengthBit = tagLengthBit;\n        this.ivLengthByte = ivLengthByte;\n        this.saltLengthByte = saltLengthByte;\n    }\n\n    /**\n     * Returns a base64 encoded AES encrypted string.\n     *\n     * @param plainText the text as byte array\n     * @return the encrypted string\n     */\n    @Override\n    public String encrypt(byte[] plainText) {\n        try {\n            // 16 bytes salt\n            byte[] salt = CryptoUtils.getRandomNonce(saltLengthByte);\n\n            // GCM recommended 12 bytes iv?\n            byte[] iv = CryptoUtils.getRandomNonce(ivLengthByte);\n\n            // secret key from password\n            SecretKey aesKeyFromPassword = CryptoUtils.getAESKeyFromPassword(password.asString().toCharArray(), salt);\n\n            Cipher cipher = Cipher.getInstance(encryptAlgo);\n\n            // ASE-GCM needs GCMParameterSpec\n            cipher.init(Cipher.ENCRYPT_MODE, aesKeyFromPassword, new GCMParameterSpec(tagLengthBit, iv));\n\n            byte[] cipherText = cipher.doFinal(plainText);\n\n            // prefix IV and Salt to cipher text\n            byte[] cipherTextWithIvSalt = ByteBuffer.allocate(iv.length + salt.length + cipherText.length)\n                    .put(iv)\n                    .put(salt)\n                    .put(cipherText)\n                    .array();\n\n            // string representation, base64, send this string to other for decryption.\n            return Base64.encodeBase64URLSafeString(cipherTextWithIvSalt);\n        } catch (Exception e) {\n            throw new NitriteSecurityException(\"Failed to encrypt data\", e);\n        }\n    }\n\n    /**\n     * Returns the decrypted string encoded by AES.\n     *\n     * <p>\n     * NOTE: The same password, salt and iv are needed to decrypt it.\n     * </p>\n     * \n     * @param encryptedText the encrypted text\n     * @return the plain text decrypted string\n     */\n```\n\n### Code description\n```Java\n/**\n * Decrypts the given encrypted text using AES encryption with GCM mode.\n * The encrypted text is expected to be in Base64 format.\n * The decryption process involves the following steps:\n * 1. Decoding the Base64 encoded encrypted text to get the byte array.\n * 2. Extracting the Initialization Vector (IV) and salt from the byte array.\n * 3. Extracting the cipher text from the byte array.\n * 4. Generating the AES key from the password and salt.\n * 5. Initializing the cipher in decryption mode with the AES key and IV.\n * 6. Decrypting the cipher text to get the plain text.\n * 7. Converting the plain text byte array to a string using UTF-8 encoding.\n *\n * @param encryptedText The encrypted text to be decrypted.\n * @return The decrypted text as a string.\n * @throws NitriteSecurityException If there is any error during the decryption process.\n */\n```\n\n### Code snippet:\n```Java\n    @Override\n    public String decrypt(String encryptedText) {\n        try {\n            byte[] decode = Base64.decodeBase64(encryptedText);\n\n            // get back the iv and salt from the cipher text\n            ByteBuffer bb = ByteBuffer.wrap(decode);\n            byte[] iv = new byte[ivLengthByte];\n            bb.get(iv);\n\n            byte[] salt = new byte[saltLengthByte];\n            bb.get(salt);\n\n```\n\n### Requirements:\n1. Complete only the missing part of the method.\n2. Only answer the code of the completed part. Do not include any of the provided code snippet.\n3. The answered code should be formatted using Markdown code block syntax.\n4. Do not include any additional text.\n\n### Answer: (use the provided format with backticks)\n", "reference": "            byte[] cipherText = new byte[bb.remaining()];\n            bb.get(cipherText);\n\n            // get back the aes key from the same password and salt\n            SecretKey aesKeyFromPassword = CryptoUtils.getAESKeyFromPassword(password.asString().toCharArray(), salt);\n            Cipher cipher = Cipher.getInstance(encryptAlgo);\n            cipher.init(Cipher.DECRYPT_MODE, aesKeyFromPassword, new GCMParameterSpec(tagLengthBit, iv));\n            byte[] plainText = cipher.doFinal(cipherText);\n            return new String(plainText, UTF_8);\n        } catch (Exception e) {\n            throw new NitriteSecurityException(\"Failed to decrypt data\", e);\n        }\n    }", "completions": ["  \n```Java\n            // get back the cipher text from the cipher text\n            byte[] cipherText = new byte[bb.remaining()];\n            bb.get(cipherText);\n\n            // generate key from password and salt\n            SecretKey aesKeyFromPassword = CryptoUtils.getAESKeyFromPassword(password.asString().toCharArray(), salt);\n\n            // initialize cipher for decryption\n            Cipher cipher = Cipher.getInstance(encryptAlgo);\n            cipher.init(Cipher.DECRYPT_MODE, aesKeyFromPassword, new GCMParameterSpec(tagLengthBit, iv));\n\n            // decrypt the cipher text\n            byte[] plainText = cipher.doFinal(cipherText);\n\n            // convert plain text to string\n            return new String(plainText, UTF_8);\n        } catch (Exception e) {\n            throw new NitriteSecurityException(\"Failed to decrypt data\", e);\n        }\n    }\n}\n```"]}
{"id": 52, "name": "escapeText", "prompt": "### Role:\nYou are an experienced Java programmer assistant responsible for code completion tasks. Please intelligently complete the missing parts of the provided code snippet.\n\n### Task:\nBased on the provided code snippet, complete the missing method implementation. Ensure the code is logically correct, syntactically accurate, and follows best coding practices.\n\n### Left context\n```Java\n/*\n * Copyright 2018 OpenAPI-Generator Contributors (https://openapi-generator.tech)\n * Copyright 2018 SmartBear Software\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     https://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\npackage org.openapijsonschematools.codegen.generators;\n\nimport com.github.benmanes.caffeine.cache.Cache;\nimport com.github.benmanes.caffeine.cache.Caffeine;\nimport com.github.benmanes.caffeine.cache.Ticker;\nimport com.github.curiousoddman.rgxgen.RgxGen;\nimport com.github.curiousoddman.rgxgen.config.RgxGenOption;\nimport com.github.curiousoddman.rgxgen.config.RgxGenProperties;\nimport com.google.common.collect.ImmutableMap;\nimport com.samskivert.mustache.Mustache;\nimport com.samskivert.mustache.Mustache.Lambda;\n\nimport io.swagger.v3.oas.models.Components;\nimport io.swagger.v3.oas.models.ExternalDocumentation;\nimport io.swagger.v3.oas.models.PathItem;\nimport io.swagger.v3.oas.models.Paths;\nimport io.swagger.v3.oas.models.SpecVersion;\nimport io.swagger.v3.oas.models.security.OAuthFlow;\nimport io.swagger.v3.oas.models.security.SecurityRequirement;\nimport org.apache.commons.text.StringEscapeUtils;\nimport org.apache.commons.lang3.StringUtils;\nimport org.apache.commons.lang3.tuple.Pair;\nimport org.openapijsonschematools.codegen.config.GeneratorSettings;\nimport org.openapijsonschematools.codegen.config.WorkflowSettings;\nimport org.openapijsonschematools.codegen.generators.models.CliOption;\nimport org.openapijsonschematools.codegen.common.CodegenConstants;\nimport org.openapijsonschematools.codegen.config.GlobalSettings;\nimport org.openapijsonschematools.codegen.generators.generatormetadata.GeneratorLanguage;\nimport org.openapijsonschematools.codegen.generators.generatormetadata.GeneratorType;\nimport org.openapijsonschematools.codegen.generators.generatormetadata.features.DataTypeFeature;\nimport org.openapijsonschematools.codegen.generators.generatormetadata.features.GlobalFeature;\nimport org.openapijsonschematools.codegen.generators.generatormetadata.features.SchemaFeature;\nimport org.openapijsonschematools.codegen.generators.generatormetadata.features.WireFormatFeature;\nimport org.openapijsonschematools.codegen.generators.models.CodeGeneratorSettings;\nimport org.openapijsonschematools.codegen.generators.models.VendorExtension;\nimport org.openapijsonschematools.codegen.generators.openapimodels.ArrayListWithContext;\nimport org.openapijsonschematools.codegen.generators.openapimodels.CodegenCallback;\nimport org.openapijsonschematools.codegen.generators.openapimodels.CodegenDiscriminator;\nimport org.openapijsonschematools.codegen.generators.openapimodels.CodegenEncoding;\nimport org.openapijsonschematools.codegen.generators.openapimodels.CodegenHeader;\nimport org.openapijsonschematools.codegen.generators.openapimodels.CodegenKey;\nimport org.openapijsonschematools.codegen.generators.openapimodels.CodegenKeyType;\nimport org.openapijsonschematools.codegen.generators.openapimodels.CodegenMap;\nimport org.openapijsonschematools.codegen.generators.openapimodels.CodegenMediaType;\nimport org.openapijsonschematools.codegen.generators.openapimodels.CodegenOauthFlow;\nimport org.openapijsonschematools.codegen.generators.openapimodels.CodegenOauthFlows;\nimport org.openapijsonschematools.codegen.generators.openapimodels.CodegenOperation;\nimport org.openapijsonschematools.codegen.generators.openapimodels.CodegenParametersInfo;\nimport org.openapijsonschematools.codegen.generators.openapimodels.CodegenParameter;\nimport org.openapijsonschematools.codegen.generators.openapimodels.CodegenPathItem;\nimport org.openapijsonschematools.codegen.generators.openapimodels.CodegenPatternInfo;\nimport org.openapijsonschematools.codegen.generators.openapimodels.CodegenRefInfo;\nimport org.openapijsonschematools.codegen.generators.openapimodels.CodegenRequestBody;\nimport org.openapijsonschematools.codegen.generators.openapimodels.CodegenResponse;\nimport org.openapijsonschematools.codegen.generators.openapimodels.CodegenSchema;\nimport org.openapijsonschematools.codegen.generators.openapimodels.CodegenSecurityRequirementObject;\nimport org.openapijsonschematools.codegen.generators.openapimodels.CodegenSecurityRequirementValue;\nimport org.openapijsonschematools.codegen.generators.openapimodels.CodegenSecurityScheme;\nimport org.openapijsonschematools.codegen.generators.openapimodels.CodegenServer;\nimport org.openapijsonschematools.codegen.generators.openapimodels.CodegenList;\nimport org.openapijsonschematools.codegen.generators.openapimodels.CodegenTag;\nimport org.openapijsonschematools.codegen.generators.openapimodels.CodegenText;\nimport org.openapijsonschematools.codegen.generators.openapimodels.CodegenXml;\nimport org.openapijsonschematools.codegen.generators.openapimodels.EnumInfo;\nimport org.openapijsonschematools.codegen.generators.openapimodels.EnumValue;\nimport org.openapijsonschematools.codegen.generators.models.GeneratedFileType;\nimport org.openapijsonschematools.codegen.generators.openapimodels.LinkedHashMapWithContext;\nimport org.openapijsonschematools.codegen.generators.openapimodels.MapBuilder;\nimport org.openapijsonschematools.codegen.generators.openapimodels.PairCacheKey;\nimport org.openapijsonschematools.codegen.generators.openapimodels.ParameterCollection;\nimport org.openapijsonschematools.codegen.generators.models.ReportFileType;\nimport org.openapijsonschematools.codegen.generators.openapimodels.SchemaTestCase;\nimport org.openapijsonschematools.codegen.templating.SupportingFile;\nimport org.openapijsonschematools.codegen.common.SerializerUtils;\nimport org.openapijsonschematools.codegen.templating.TemplatingEngineLoader;\nimport org.openapijsonschematools.codegen.templating.mustache.IndentedLambda;\nimport org.openapijsonschematools.codegen.templating.mustache.LowercaseLambda;\nimport org.openapijsonschematools.codegen.templating.mustache.SnakecaseLambda;\nimport org.openapijsonschematools.codegen.templating.mustache.TitlecaseLambda;\nimport org.openapijsonschematools.codegen.templating.mustache.UppercaseLambda;\nimport org.openapijsonschematools.codegen.common.ModelUtils;\nimport org.openapijsonschematools.codegen.generators.openapimodels.CodegenDiscriminator.MappedModel;\nimport org.openapijsonschematools.codegen.templating.TemplatingEngineAdapter;\nimport org.openapijsonschematools.codegen.generators.generatormetadata.FeatureSet;\nimport org.openapijsonschematools.codegen.generators.generatormetadata.GeneratorMetadata;\nimport org.openapijsonschematools.codegen.generators.generatormetadata.Stability;\nimport org.slf4j.Logger;\nimport org.slf4j.LoggerFactory;\n\nimport java.io.File;\nimport java.math.BigDecimal;\nimport java.time.OffsetDateTime;\nimport java.util.*;\nimport java.util.Map.Entry;\nimport java.util.concurrent.ConcurrentSkipListSet;\nimport java.util.concurrent.TimeUnit;\nimport java.util.function.Function;\nimport java.util.regex.Matcher;\nimport java.util.regex.Pattern;\nimport java.util.stream.Collectors;\nimport java.util.stream.Stream;\n\nimport io.swagger.v3.core.util.Json;\nimport io.swagger.v3.oas.models.OpenAPI;\nimport io.swagger.v3.oas.models.Operation;\nimport io.swagger.v3.oas.models.examples.Example;\nimport io.swagger.v3.oas.models.headers.Header;\nimport io.swagger.v3.oas.models.media.*;\nimport io.swagger.v3.oas.models.parameters.*;\nimport io.swagger.v3.oas.models.responses.ApiResponse;\nimport io.swagger.v3.oas.models.security.OAuthFlows;\nimport io.swagger.v3.oas.models.security.SecurityScheme;\nimport io.swagger.v3.oas.models.servers.Server;\nimport io.swagger.v3.oas.models.servers.ServerVariable;\n\nimport static org.openapijsonschematools.codegen.common.StringUtils.camelize;\n\n@SuppressWarnings(\"rawtypes\")\npublic class DefaultGenerator implements Generator {\n    protected CodeGeneratorSettings generatorSettings;\n    protected static final List<String> defaultPostGenerationMsg = List.of(\n        \"################################################################################\",\n        \"# Thanks for using OpenAPI JSON Schema Generator.                              #\",\n        \"# Please consider donation to help us maintain this project \\uD83D\\uDE4F                 #\",\n        \"# https://github.com/sponsors/spacether                                        #\",\n        \"################################################################################\"\n    );\n\n    private static Map<String, Object> getInitialAdditionalProperties(GeneratorSettings generatorSettings, CodeGeneratorSettings codeGeneratorSettings) {\n        Map<String, Object> initialAddProps = new HashMap<>();\n        if (generatorSettings != null) {\n            initialAddProps.putAll(generatorSettings.getAdditionalProperties());\n        }\n        initialAddProps.put(CodegenConstants.HIDE_GENERATION_TIMESTAMP, codeGeneratorSettings.hideGenerationTimestamp);\n        initialAddProps.put(CodegenConstants.TEMPLATING_ENGINE, codeGeneratorSettings.templateEngineName);\n        if (codeGeneratorSettings.templateDir != null) {\n            initialAddProps.put(CodegenConstants.TEMPLATE_DIR, codeGeneratorSettings.templateDir);\n        }\n        return initialAddProps;\n    }\n\n    protected DefaultGenerator(GeneratorSettings generatorSettings, WorkflowSettings workflowSettings, String embeddedTemplateDir, String packageNameDefault, String artifactIdDefault, String outputFolderDefault) {\n        this.generatorSettings = CodeGeneratorSettings.of(generatorSettings, workflowSettings, embeddedTemplateDir, packageNameDefault, artifactIdDefault, outputFolderDefault);\n        additionalProperties = getInitialAdditionalProperties(generatorSettings, this.generatorSettings);\n\n        // name formatting options\n        cliOptions.add(CliOption.newBoolean(CodegenConstants.ALLOW_UNICODE_IDENTIFIERS, CodegenConstants\n            .ALLOW_UNICODE_IDENTIFIERS_DESC).defaultValue(Boolean.FALSE.toString()));\n\n        // initialize special character mapping\n        initializeSpecialCharacterMapping();\n\n        // Register common Mustache lambdas.\n        registerMustacheLambdas();\n    }\n\n    public DefaultGenerator(GeneratorSettings generatorSettings, WorkflowSettings workflowSettings) {\n        this(\n            generatorSettings,\n            workflowSettings,\n            \"java\",\n            \"openapiclient\",\n            \"openapiclient\",\n            \"generated-code\" + File.separator + \"java\"\n        );\n    }\n\n    private final Logger LOGGER = LoggerFactory.getLogger(DefaultGenerator.class);\n\n    public static FeatureSet DefaultFeatureSet;\n\n    // A cache of sanitized words. The sanitizeName() method is invoked many times with the same\n    // arguments, this cache is used to optimized performance.\n    private static final Cache<SanitizeNameOptions, String> sanitizedNameCache;\n    private static final String xSchemaTestExamplesKey = \"x-schema-test-examples\";\n    private static final String xSchemaTestExamplesRefPrefix = \"#/components/x-schema-test-examples/\";\n    protected static Schema falseSchema;\n    protected static Schema trueSchema = new Schema();\n\n    protected String requestBodiesIdentifier = \"request_bodies\";\n    protected String securitySchemesIdentifier = \"security_schemes\";\n    protected String requestBodyIdentifier = \"request_body\";\n    private final Pattern patternRegex = Pattern.compile(\"^/?(.+?)/?([simu]{0,4})$\");\n    private final CodegenKey additionalPropertySampleKey = new CodegenKey(\"someAdditionalProperty\", true, \"additional_property\", \"AdditionalProperty\", \"additional-property\", \"additionalProperty\");\n    protected String headersSchemaFragment = \"Headers\";\n    protected static final Set<String> operationVerbs = Set.of(\"get\", \"put\", \"post\", \"delete\", \"options\", \"head\", \"patch\", \"trace\");\n    protected Set<String> xParameters = Set.of(\"PathParameters\", \"QueryParameters\", \"HeaderParameters\", \"CookieParameters\");\n\n    static {\n        DefaultFeatureSet = FeatureSet.newBuilder()\n                .includeDataTypeFeatures(\n                        DataTypeFeature.Int32, DataTypeFeature.Int64, DataTypeFeature.Integer, DataTypeFeature.Float, DataTypeFeature.Double, DataTypeFeature.Number,\n                        DataTypeFeature.String,\n                        DataTypeFeature.Boolean, DataTypeFeature.Date, DataTypeFeature.DateTime,\n                        DataTypeFeature.Array, DataTypeFeature.Object, DataTypeFeature.Enum, DataTypeFeature.Uuid,\n                        DataTypeFeature.Null, DataTypeFeature.AnyType\n                )\n                .includeDocumentationFeatures(\n                )\n                .includeComponentsFeatures(\n                )\n                .includeGlobalFeatures(\n                        GlobalFeature.Info\n                )\n                .includeSchemaFeatures(\n                        SchemaFeature.Type, SchemaFeature.Format,\n                        SchemaFeature.Items, SchemaFeature.Properties,\n                        SchemaFeature.Required, SchemaFeature.AdditionalProperties,\n                        SchemaFeature.Ref\n                )\n                .includeParameterFeatures()\n                .includeOperationFeatures()\n                .includeSecurityFeatures()\n                .includeWireFormatFeatures(\n                        WireFormatFeature.JSON\n                        // PROTOBUF and Custom are generator specific\n                )\n                .build();\n\n        int cacheSize = Integer.parseInt(GlobalSettings.getProperty(org.openapijsonschematools.codegen.common.StringUtils.NAME_CACHE_SIZE_PROPERTY, \"500\"));\n        int cacheExpiry = Integer.parseInt(GlobalSettings.getProperty(org.openapijsonschematools.codegen.common.StringUtils.NAME_CACHE_EXPIRY_PROPERTY, \"10\"));\n        sanitizedNameCache = Caffeine.newBuilder()\n                .maximumSize(cacheSize)\n                .expireAfterAccess(cacheExpiry, TimeUnit.SECONDS)\n                .ticker(Ticker.systemTicker())\n                .build();\n        falseSchema = new Schema();\n        falseSchema.setNot(new Schema());\n    }\n\n    public static final GeneratorMetadata generatorMetadata = GeneratorMetadata.newBuilder()\n        .name(\"java\")\n        .language(GeneratorLanguage.JAVA)\n        .languageVersion(\"17\")\n        .type(GeneratorType.CLIENT)\n        .stability(Stability.EXPERIMENTAL)\n        .featureSet(DefaultFeatureSet)\n        .generationMessage(\"OpenAPI JSON Schema Generator: java \"+GeneratorType.CLIENT.toValue())\n        .helpMsg(\"todo replace help text\")\n        .postGenerationMsg(defaultPostGenerationMsg)\n        .reservedWords(Set.of())\n        .instantiationTypes(Map.of())\n        .languageSpecificPrimitives(Set.of())\n        .build();\n    protected String inputSpec;\n    protected Map<String, String> typeMapping;\n    protected String modelPackage = \"components.schema\";\n    protected String modelNamePrefix = \"\", modelNameSuffix = \"\";\n    protected String apiNamePrefix = \"\", apiNameSuffix = \"Api\";\n    protected String filesMetadataFilename = \"FILES\";\n    protected String versionMetadataFilename = \"VERSION\";\n    protected String docsFolder = \"docs\";\n    // for writing api files\n    protected HashMap<CodegenConstants.JSON_PATH_LOCATION_TYPE, HashMap<String, String>> jsonPathTemplateFiles = new HashMap<>();\n    // for writing doc files\n    protected HashMap<CodegenConstants.JSON_PATH_LOCATION_TYPE, HashMap<String, String>> jsonPathDocTemplateFiles = new HashMap<>();\n    // for writing test files\n    protected HashMap<CodegenConstants.JSON_PATH_LOCATION_TYPE, HashMap<String, String>> jsonPathTestTemplateFiles = new HashMap<>();\n\n    protected Map<String, Object> additionalProperties = new HashMap<>();\n    protected Map<String, Object> vendorExtensions = new HashMap<>();\n    /*\n    Supporting files are those which aren't models, APIs, or docs.\n    These get a different map of data bound to the templates. Supporting files are written once.\n    */\n    protected List<SupportingFile> supportingFiles = new ArrayList<>();\n    protected List<CliOption> cliOptions = new ArrayList<>();\n    protected String removeOperationIdPrefixDelimiter = \"_\";\n    protected int removeOperationIdPrefixCount = 1;\n\n    private static final Pattern COMMON_PREFIX_ENUM_NAME = Pattern.compile(\"[a-zA-Z\\\\d]+\\\\z\");\n\n    /**\n     * True if the code generator supports multiple class inheritance.\n     * This is used to model the parent hierarchy based on the 'allOf' composed schemas.\n     */\n    protected boolean supportsMultipleInheritance;\n    /**\n     * True if the code generator supports single class inheritance.\n     * This is used to model the parent hierarchy based on the 'allOf' composed schemas.\n     * Note: the single-class inheritance technique has inherent limitations because\n     * a 'allOf' composed schema may have multiple $ref child schemas, each one\n     * potentially representing a \"parent\" in the class inheritance hierarchy.\n     * Some language generators also use class inheritance to implement the `additionalProperties`\n     * keyword. For example, the Java code generator may generate 'extends HashMap'.\n     */\n    protected boolean supportsInheritance;\n    /**\n     * True if the language generator supports the 'additionalProperties' keyword\n     * as sibling of a composed (allOf/anyOf/oneOf) schema.\n     * Note: all language generators should support this to comply with the OAS specification.\n     */\n    protected boolean supportsAdditionalPropertiesWithComposedSchema = true;\n    protected Boolean allowUnicodeIdentifiers = false;\n    // How to encode special characters like $\n    // They are translated to words like \"Dollar\" and prefixed with '\n    // Then translated back during JSON encoding and decoding\n    protected Map<String, String> specialCharReplacements = new LinkedHashMap<>();\n    // The extension of the generated documentation files (defaults to markdown .md)\n    protected String docExtension;\n    protected String ignoreFilePathOverride;\n    // flag to indicate whether to use environment variable to post process file\n    protected boolean enablePostProcessFile = false;\n\n    // flag to indicate whether enum value prefixes are removed\n    protected boolean removeEnumValuePrefix = true;\n\n    // make openapi available to all methods\n    protected OpenAPI openAPI;\n\n    // A cache to efficiently look up a Schema instance based on the return value of `toModelName()`.\n    private Map<String, Schema> modelNameToSchemaCache;\n\n    // A cache to efficiently lookup schema `toModelName()` based on the schema Key\n    private final Map<String, String> schemaKeyToModelNameCache = new HashMap<>();\n\n    protected boolean loadDeepObjectIntoItems = true;\n\n    // if true then baseTypes will be imported\n    protected boolean importBaseType = true;\n\n    // if True codegenParameter and codegenResponse imports will come\n    // from deeper schema defined locations\n    protected boolean addSchemaImportsFromV3SpecLocations = false;\n    protected boolean deepestRefSchemaImportNeeded = false;\n    protected String objectIOClassNamePiece = \"Dict\";\n    protected String arrayIOClassNamePiece = \"Tuple\";\n    protected String arrayObjectInputClassNameSuffix = \"Input\";\n\n    @Override\n    public List<CliOption> cliOptions() {\n        return cliOptions;\n    }\n\n    @Override\n    public void processOpts() {\n        if (additionalProperties.containsKey(CodegenConstants.ALLOW_UNICODE_IDENTIFIERS)) {\n            this.setAllowUnicodeIdentifiers(Boolean.valueOf(additionalProperties\n                    .get(CodegenConstants.ALLOW_UNICODE_IDENTIFIERS).toString()));\n        }\n\n        if (additionalProperties.containsKey(CodegenConstants.API_NAME_PREFIX)) {\n            this.setApiNamePrefix((String) additionalProperties.get(CodegenConstants.API_NAME_PREFIX));\n        }\n\n        if (additionalProperties.containsKey(CodegenConstants.API_NAME_SUFFIX)) {\n            this.setApiNameSuffix((String) additionalProperties.get(CodegenConstants.API_NAME_SUFFIX));\n        }\n\n        if (additionalProperties.containsKey(CodegenConstants.MODEL_NAME_PREFIX)) {\n            this.setModelNamePrefix((String) additionalProperties.get(CodegenConstants.MODEL_NAME_PREFIX));\n        }\n\n        if (additionalProperties.containsKey(CodegenConstants.MODEL_NAME_SUFFIX)) {\n            this.setModelNameSuffix((String) additionalProperties.get(CodegenConstants.MODEL_NAME_SUFFIX));\n        }\n\n        if (additionalProperties.containsKey(CodegenConstants.REMOVE_OPERATION_ID_PREFIX_DELIMITER)) {\n            this.setRemoveOperationIdPrefixDelimiter(additionalProperties\n                    .get(CodegenConstants.REMOVE_OPERATION_ID_PREFIX_DELIMITER).toString());\n        }\n\n        if (additionalProperties.containsKey(CodegenConstants.REMOVE_OPERATION_ID_PREFIX_COUNT)) {\n            this.setRemoveOperationIdPrefixCount(Integer.parseInt(additionalProperties\n                    .get(CodegenConstants.REMOVE_OPERATION_ID_PREFIX_COUNT).toString()));\n        }\n\n        if (additionalProperties.containsKey(CodegenConstants.DOCEXTENSION)) {\n            this.setDocExtension(String.valueOf(additionalProperties\n                    .get(CodegenConstants.DOCEXTENSION).toString()));\n        }\n\n        requiredAddPropUnsetSchema = fromSchema(new JsonSchema(), null, null);\n    }\n\n    /***\n     * Preset map builder with commonly used Mustache lambdas.\n     * <p>\n     * To extend the map, override addMustacheLambdas(), call parent method\n     * first and then add additional lambdas to the returned builder.\n     * <p>\n     * If common lambdas are not desired, override addMustacheLambdas() method\n     * and return empty builder.\n     *\n     * @return map with common lambdas\n     */\n    @SuppressWarnings(\"SpellCheckingInspection\")\n    protected ImmutableMap.Builder<String, Lambda> addMustacheLambdas() {\n\n        return new ImmutableMap.Builder<String, Mustache.Lambda>()\n                .put(\"lowercase\", new LowercaseLambda().generator(this))\n                .put(\"uppercase\", new UppercaseLambda())\n                .put(\"snakecase\", new SnakecaseLambda())\n                .put(\"titlecase\", new TitlecaseLambda())\n                .put(\"indented\", new IndentedLambda())\n                .put(\"indented_8\", new IndentedLambda(8, \" \"))\n                .put(\"indented_12\", new IndentedLambda(12, \" \"))\n                .put(\"indented_16\", new IndentedLambda(16, \" \"));\n    }\n\n    private void registerMustacheLambdas() {\n        ImmutableMap<String, Lambda> lambdas = addMustacheLambdas().build();\n\n        if (lambdas.isEmpty()) {\n            return;\n        }\n\n        if (additionalProperties.containsKey(\"lambda\")) {\n            LOGGER.error(\"A property called 'lambda' already exists in additionalProperties\");\n            throw new RuntimeException(\"A property called 'lambda' already exists in additionalProperties\");\n        }\n        additionalProperties.put(\"lambda\", lambdas);\n    }\n\n    // override with any special post-processing for all models\n    @Override\n    @SuppressWarnings(\"static-method\")\n    public TreeMap<String, CodegenSchema> postProcessAllModels(TreeMap<String, CodegenSchema> schemas) {\n        return schemas;\n    }\n\n    /**\n     * Return a map from model name to Schema for efficient lookup.\n     *\n     * @return map from model name to Schema.\n     */\n    protected Map<String, Schema> getModelNameToSchemaCache() {\n        if (modelNameToSchemaCache == null) {\n            // Create a cache to efficiently lookup schema based on model name.\n            Map<String, Schema> m = new HashMap<>();\n            ModelUtils.getSchemas(openAPI).forEach((key, schema) -> m.put(toModelName(key, \"#/components/schemas/\"+key), schema));\n            modelNameToSchemaCache = Collections.unmodifiableMap(m);\n        }\n        return modelNameToSchemaCache;\n    }\n\n    public String packagePath() {\n        return generatorSettings.packageName.replace('.', File.separatorChar);\n    }\n\n    /**\n     * Loop through all models to update different flags (e.g. isSelfReference), children models, etc\n     *\n     * @param models Map of models\n     * @return maps of models with various updates\n     */\n    @Override\n    public TreeMap<String, CodegenSchema> updateAllModels(TreeMap<String, CodegenSchema> models) {\n        return models;\n    }\n\n    /**\n     * Returns the common prefix of variables for enum naming if\n     * two or more variables are present\n     *\n     * @param vars List of variable names\n     * @return the common prefix for naming\n     */\n    public String findCommonPrefixOfVars(List<Object> vars) {\n        if (!(vars.size() > 1)) {\n            return \"\";\n        }\n        ArrayList<String> stringVars = new ArrayList<>();\n        for (Object var: vars) {\n            if (var instanceof String) {\n                stringVars.add((String) var);\n            } else {\n                return \"\";\n            }\n        }\n        String[] stringVarsArray = Arrays.copyOf(\n                stringVars.toArray(), stringVars.size(), String[].class);\n        String prefix = StringUtils.getCommonPrefix(stringVarsArray);\n        if (prefix == null || prefix.isEmpty()) {\n            return \"\";\n        }\n        // exclude trailing characters that should be part of a valid variable\n        // e.g. [\"status-on\", \"status-off\"] => \"status-\" (not \"status-o\")\n        final Matcher matcher = COMMON_PREFIX_ENUM_NAME.matcher(prefix);\n        return matcher.replaceAll(\"\");\n    }\n\n    /**\n     * Return the sanitized variable name for enum\n     *\n     * @param value    enum variable name\n     * @param prop the property that holds the data type booleans\n     * @return the sanitized variable name for enum\n     */\n    public String toEnumVarName(String value, Schema<?> prop) {\n        if (value.isEmpty()) {\n            return \"EMPTY\";\n        }\n\n        String var = value.replaceAll(\"\\\\W+\", \"_\").toUpperCase(Locale.ROOT);\n        if (var.matches(\"\\\\d.*\")) {\n            return \"_\" + var;\n        } else {\n            return var;\n        }\n    }\n\n    /**\n     * Set the OpenAPI document.\n     * This method is invoked when the input OpenAPI document has been parsed and validated.\n     */\n    @Override\n    public void setOpenAPI(OpenAPI openAPI) {\n        this.openAPI = openAPI;\n    }\n\n    // override with any special post-processing\n    @Override\n    @SuppressWarnings(\"static-method\")\n    public Map<String, Object> postProcessSupportingFileData(Map<String, Object> data) {\n        return data;\n    }\n\n    // override to post-process any model properties\n    @Override\n    @SuppressWarnings(\"unused\")\n    public void postProcessModelProperty(CodegenSchema model, CodegenSchema property) {\n    }\n\n    private void preprocessOpenAPIPaths(Paths paths) {\n        if (paths == null) {\n            return;\n        }\n        Set<String> pathsToFix = new HashSet<>();\n        for (String path: paths.keySet()) {\n            if (!path.startsWith(\"/\")) {\n                pathsToFix.add(path);\n            }\n        }\n        for(String path: pathsToFix) {\n            LOGGER.warn(\"Per the openapi spec, paths must begin with a slash; adding a slash onto \" + path);\n            PathItem pathItem = paths.remove(path);\n            paths.put(\"/\" + path, pathItem);\n        }\n    }\n\n    //override with any special handling of the entire OpenAPI spec document\n    @Override\n    @SuppressWarnings(\"unused\")\n    public void preprocessOpenAPI(OpenAPI openAPI) {\n        preprocessOpenAPIPaths(openAPI.getPaths());\n    }\n\n    // override with any special handling of the entire OpenAPI spec document\n    @Override\n    @SuppressWarnings(\"unused\")\n    public void processOpenAPI(OpenAPI openAPI) {\n    }\n\n    // override with any special text escaping logic\n```\n\n### Code description\n```Java\n/**\n * Escapes text by replacing certain characters with their escaped versions.\n * This method uses the Apache Commons Text library's StringEscapeUtils to escape and unescape text.\n * It removes tabs, newlines, and carriage returns, replaces backslashes with double backslashes,\n * replaces double quotes with escaped double quotes, and unescapes the text to retain original multibyte characters.\n * Finally, it replaces escaped forward slashes with regular forward slashes.\n *\n * @param input The text to be escaped.\n * @return The escaped text.\n */\n```\n\n### Code snippet:\n```Java\n    @Override\n    @SuppressWarnings(\"static-method\")\n    public String escapeText(String input) {\n        if (input == null) {\n            return null;\n        }\n\n        // remove \\t, \\n, \\r\n        // replace \\ with \\\\\n        // replace \" with \\\"\n```\n\n### Requirements:\n1. Complete only the missing part of the method.\n2. Only answer the code of the completed part. Do not include any of the provided code snippet.\n3. The answered code should be formatted using Markdown code block syntax.\n4. Do not include any additional text.\n\n### Answer: (use the provided format with backticks)\n", "reference": "        // outer unescape to retain the original multibyte characters\n        // finally escalate characters avoiding code injection\n        return escapeUnsafeCharacters(\n                StringEscapeUtils.unescapeJava(\n                                StringEscapeUtils.escapeJava(input)\n                                        .replace(\"\\\\/\", \"/\"))\n                        .replaceAll(\"[\\\\t\\\\n\\\\r]\", \" \")\n                        .replace(\"\\\\\", \"\\\\\\\\\")\n                        .replace(\"\\\"\", \"\\\\\\\"\"));\n    }", "completions": ["  \n```Java\n        // unescape the text to retain original multibyte characters\n        // replace escaped forward slashes with regular forward slashes\n        return StringEscapeUtils.unescapeJava(StringEscapeUtils.escapeJava(input)\n               .replace(\"\\\\/\", \"/\"));\n    }\n```"]}
{"id": 53, "name": "maybeFetchErrorIntervalMs", "prompt": "### Role:\nYou are an experienced Java programmer assistant responsible for code completion tasks. Please intelligently complete the missing parts of the provided code snippet.\n\n### Task:\nBased on the provided code snippet, complete the missing method implementation. Ensure the code is logically correct, syntactically accurate, and follows best coding practices.\n\n### Left context\n```Java\n/*\n * Licensed to the Apache Software Foundation (ASF) under one or more\n * contributor license agreements. See the NOTICE file distributed with\n * this work for additional information regarding copyright ownership.\n * The ASF licenses this file to You under the Apache License, Version 2.0\n * (the \"License\"); you may not use this file except in compliance with\n * the License. You may obtain a copy of the License at\n *\n *    http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\npackage org.apache.kafka.common.telemetry.internals;\n\nimport io.opentelemetry.proto.metrics.v1.MetricsData;\n\nimport org.apache.kafka.common.KafkaException;\nimport org.apache.kafka.common.Uuid;\nimport org.apache.kafka.common.metrics.MetricsContext;\nimport org.apache.kafka.common.protocol.Errors;\nimport org.apache.kafka.common.record.CompressionType;\nimport org.apache.kafka.common.record.RecordBatch;\nimport org.apache.kafka.common.utils.BufferSupplier;\nimport org.apache.kafka.common.utils.ByteBufferOutputStream;\nimport org.apache.kafka.common.utils.Utils;\nimport org.slf4j.Logger;\nimport org.slf4j.LoggerFactory;\n\nimport java.io.ByteArrayOutputStream;\nimport java.io.IOException;\nimport java.io.InputStream;\nimport java.io.OutputStream;\nimport java.nio.ByteBuffer;\nimport java.time.Duration;\nimport java.util.ArrayList;\nimport java.util.Collections;\nimport java.util.List;\nimport java.util.Map;\nimport java.util.Optional;\nimport java.util.function.Predicate;\n\npublic class ClientTelemetryUtils {\n\n    private final static Logger log = LoggerFactory.getLogger(ClientTelemetryUtils.class);\n\n    public final static Predicate<? super MetricKeyable> SELECTOR_NO_METRICS = k -> false;\n\n    public final static Predicate<? super MetricKeyable> SELECTOR_ALL_METRICS = k -> true;\n\n    /**\n     * Examine the response data and handle different error code accordingly:\n     *\n     * <ul>\n     *     <li>Invalid Request: Disable Telemetry</li>\n     *     <li>Invalid Record: Disable Telemetry</li>\n     *     <li>Unsupported Version: Disable Telemetry</li>\n     *     <li>UnknownSubscription or Unsupported Compression: Retry immediately</li>\n     *     <li>TelemetryTooLarge or ThrottlingQuotaExceeded: Retry as per next interval</li>\n     * </ul>\n     *\n     * @param errorCode response body error code\n     * @param intervalMs current push interval in milliseconds\n     *\n     * @return Optional of push interval in milliseconds\n     */\n```\n\n### Code description\n```Java\n/**\n * Description: This method is used to fetch the error interval in milliseconds based on the error code.\n * If the error code is Errors.NONE, it returns an empty Optional.\n * If the error code is INVALID_REQUEST, INVALID_RECORD, UNSUPPORTED_VERSION, it sets the push interval to Integer.MAX_VALUE and logs a reason.\n * If the error code is UNKNOWN_SUBSCRIPTION_ID, UNSUPPORTED_COMPRESSION_TYPE, it sets the push interval to 0 and logs the error message.\n * If the error code is TELEMETRY_TOO_LARGE, THROTTLING_QUOTA_EXCEEDED, it sets the push interval to either the provided interval or the default interval and logs the error message.\n * For any other error code, it logs an error and sets the push interval to Integer.MAX_VALUE.\n * Finally, it logs the error code, reason, and push interval and returns the push interval in an Optional.\n *\n * @param errorCode The error code to fetch the interval for.\n * @param intervalMs The interval in milliseconds.\n * @return An Optional containing the push interval in milliseconds.\n */\n```\n\n### Code snippet:\n```Java\n    public static Optional<Integer> maybeFetchErrorIntervalMs(short errorCode, int intervalMs) {\n        if (errorCode == Errors.NONE.code())\n            return Optional.empty();\n\n        int pushIntervalMs;\n        String reason;\n\n        Errors error = Errors.forCode(errorCode);\n        switch (error) {\n            case INVALID_REQUEST:\n            case INVALID_RECORD:\n            case UNSUPPORTED_VERSION:\n                pushIntervalMs = Integer.MAX_VALUE;\n                reason = \"The broker response indicates the client sent an request that cannot be resolved\"\n                    + \" by re-trying, hence disable telemetry\";\n                break;\n            case UNKNOWN_SUBSCRIPTION_ID:\n```\n\n### Requirements:\n1. Complete only the missing part of the method.\n2. Only answer the code of the completed part. Do not include any of the provided code snippet.\n3. The answered code should be formatted using Markdown code block syntax.\n4. Do not include any additional text.\n\n### Answer: (use the provided format with backticks)\n", "reference": "            case UNSUPPORTED_COMPRESSION_TYPE:\n                pushIntervalMs = 0;\n                reason = error.message();\n                break;\n            case TELEMETRY_TOO_LARGE:\n            case THROTTLING_QUOTA_EXCEEDED:\n                reason = error.message();\n                pushIntervalMs = (intervalMs != -1) ? intervalMs : ClientTelemetryReporter.DEFAULT_PUSH_INTERVAL_MS;\n                break;\n            default:\n                reason = \"Unwrapped error code\";\n                log.error(\"Error code: {}. Unmapped error for telemetry, disable telemetry.\", errorCode);\n                pushIntervalMs = Integer.MAX_VALUE;\n        }\n\n        log.debug(\"Error code: {}, reason: {}. Push interval update to {} ms.\", errorCode, reason, pushIntervalMs);\n        return Optional.of(pushIntervalMs);\n    }", "completions": ["  \n```Java\ncase UNSUPPORTED_COMPRESSION_TYPE:\n    pushIntervalMs = 0;\n    reason = \"The broker response indicates the client sent an request with unsupported compression type, hence retry immediately\";\n    break;\ncase TELEMETRY_TOO_LARGE:\ncase THROTTLING_QUOTA_EXCEEDED:\n    pushIntervalMs = intervalMs > 0? intervalMs : 60000;\n    reason = \"The broker response indicates the client sent an request that exceeded the telemetry quota or throttling limit, hence retry after \" + pushIntervalMs + \" milliseconds\";\n    break;\ndefault:\n    pushIntervalMs = Integer.MAX_VALUE;\n    reason = \"The broker response indicates an unknown error, hence disable telemetry\";\n    break;\n}\n\nlog.error(\"Error code: {}, reason: {}, push interval: {} milliseconds\", errorCode, reason, pushIntervalMs);\nreturn Optional.of(pushIntervalMs);\n```"]}
{"id": 54, "name": "retrieveQueryableStore", "prompt": "### Role:\nYou are an experienced Java programmer assistant responsible for code completion tasks. Please intelligently complete the missing parts of the provided code snippet.\n\n### Task:\nBased on the provided code snippet, complete the missing method implementation. Ensure the code is logically correct, syntactically accurate, and follows best coding practices.\n\n### Left context\n```Java\n/*\n * Copyright 2024-2024 the original author or authors.\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *      https://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\npackage org.springframework.kafka.streams;\n\nimport java.util.Properties;\n\nimport org.apache.kafka.common.serialization.Serializer;\nimport org.apache.kafka.streams.KafkaStreams;\nimport org.apache.kafka.streams.KeyQueryMetadata;\nimport org.apache.kafka.streams.StoreQueryParameters;\nimport org.apache.kafka.streams.state.HostInfo;\nimport org.apache.kafka.streams.state.QueryableStoreType;\n\nimport org.springframework.kafka.config.StreamsBuilderFactoryBean;\nimport org.springframework.lang.Nullable;\nimport org.springframework.retry.support.RetryTemplate;\nimport org.springframework.util.Assert;\nimport org.springframework.util.StringUtils;\n\n/**\n * Provide a wrapper API around the interactive query stores in Kafka Streams.\n * Using this API, an application can gain access to a named state store in the\n * {@link KafkaStreams} under consideration.\n *\n * @author Soby Chacko\n * @since 3.2\n */\npublic class KafkaStreamsInteractiveQueryService {\n\n\t/**\n\t * {@link StreamsBuilderFactoryBean} that provides {@link KafkaStreams} where the state store is retrieved from.\n\t */\n\tprivate final StreamsBuilderFactoryBean streamsBuilderFactoryBean;\n\n\t/**\n\t * {@link RetryTemplate} to be used by the interative query service.\n\t */\n\tprivate RetryTemplate retryTemplate = new RetryTemplate();\n\n\t/**\n\t * Underlying {@link KafkaStreams} from {@link StreamsBuilderFactoryBean}.\n\t */\n\tprivate volatile KafkaStreams kafkaStreams;\n\n\t/**\n\t * Construct an instance for querying state stores from the KafkaStreams in the {@link StreamsBuilderFactoryBean}.\n\t * @param streamsBuilderFactoryBean {@link StreamsBuilderFactoryBean} for {@link KafkaStreams}.\n\t */\n\tpublic KafkaStreamsInteractiveQueryService(StreamsBuilderFactoryBean streamsBuilderFactoryBean) {\n\t\tAssert.notNull(streamsBuilderFactoryBean, \"StreamsBuildFactoryBean instance cannot be null.\");\n\t\tthis.streamsBuilderFactoryBean = streamsBuilderFactoryBean;\n\t}\n\n\t/**\n\t * Custom {@link RetryTemplate} provided by the end users.\n\t * @param retryTemplate {@link RetryTemplate}\n\t */\n\tpublic void setRetryTemplate(RetryTemplate retryTemplate) {\n\t\tAssert.notNull(retryTemplate, \"The provided RetryTemplate instance must not be null\");\n\t\tthis.retryTemplate = retryTemplate;\n\t}\n\n\t/**\n\t * Retrieve and return a queryable store by name created in the application.\n\t * @param storeName name of the queryable store\n\t * @param storeType type of the queryable store\n\t * @param <T> generic type for the queryable store\n\t * @return queryable store.\n\t */\n```\n\n### Code description\n```Java\n/**\n * Retrieves a queryable store of a specified type with a given name.\n *\n * @param storeName The name of the store to retrieve.\n * @param storeType The type of the store to retrieve.\n * @param <T> The type of the store.\n * @return The retrieved store.\n * @throws IllegalStateException If an error occurs while retrieving the store.\n */\n```\n\n### Code snippet:\n```Java\n\tpublic <T> T retrieveQueryableStore(String storeName, QueryableStoreType<T> storeType) {\n\t\tpopulateKafkaStreams();\n\t\tStoreQueryParameters<T> storeQueryParams = StoreQueryParameters.fromNameAndType(storeName, storeType);\n\n\t\treturn this.retryTemplate.execute(context -> {\n\t\t\ttry {\n```\n\n### Requirements:\n1. Complete only the missing part of the method.\n2. Only answer the code of the completed part. Do not include any of the provided code snippet.\n3. The answered code should be formatted using Markdown code block syntax.\n4. Do not include any additional text.\n\n### Answer: (use the provided format with backticks)\n", "reference": "\t\t\t\treturn this.kafkaStreams.store(storeQueryParams);\n\t\t\t}\n\t\t\tcatch (Exception e) {\n\t\t\t\tthrow new IllegalStateException(\"Error retrieving state store: \" + storeName, e);\n\t\t\t}\n\t\t});\n\t}", "completions": ["  \n```Java\n\t\t\t\treturn this.kafkaStreams.store(storeQueryParams);\n\t\t\t}\n\t\t\tcatch (InvalidStateStoreException e) {\n\t\t\t\tthrow new IllegalStateException(\"Cannot retrieve state store \" + storeName, e);\n\t\t\t}\n\t\t});\n\t}\n\n\tprivate void populateKafkaStreams() {\n\t\tif (this.kafkaStreams == null) {\n\t\t\tthis.kafkaStreams = this.streamsBuilderFactoryBean.getKafkaStreams();\n\t\t}\n\t}\n}\n```"]}
{"id": 57, "name": "resetToOffsets", "prompt": "### Role:\nYou are an experienced Java programmer assistant responsible for code completion tasks. Please intelligently complete the missing parts of the provided code snippet.\n\n### Task:\nBased on the provided code snippet, complete the missing method implementation. Ensure the code is logically correct, syntactically accurate, and follows best coding practices.\n\n### Left context\n```Java\npackage io.kafbat.ui.service;\n\nimport static java.util.stream.Collectors.toMap;\nimport static java.util.stream.Collectors.toSet;\nimport static org.apache.kafka.common.ConsumerGroupState.DEAD;\nimport static org.apache.kafka.common.ConsumerGroupState.EMPTY;\n\nimport com.google.common.base.Preconditions;\nimport io.kafbat.ui.exception.NotFoundException;\nimport io.kafbat.ui.exception.ValidationException;\nimport io.kafbat.ui.model.KafkaCluster;\nimport java.util.Collection;\nimport java.util.HashMap;\nimport java.util.List;\nimport java.util.Map;\nimport java.util.Set;\nimport javax.annotation.Nullable;\nimport lombok.RequiredArgsConstructor;\nimport lombok.extern.slf4j.Slf4j;\nimport org.apache.kafka.clients.admin.OffsetSpec;\nimport org.apache.kafka.common.TopicPartition;\nimport org.springframework.stereotype.Component;\nimport reactor.core.publisher.Mono;\n\n/**\n * Implementation follows https://cwiki.apache.org/confluence/display/KAFKA/KIP-122%3A+Add+Reset+Consumer+Group+Offsets+tooling\n * to works like \"kafka-consumer-groups --reset-offsets\" console command\n * (see kafka.admin.ConsumerGroupCommand)\n */\n@Slf4j\n@Component\n@RequiredArgsConstructor\npublic class OffsetsResetService {\n\n  private final AdminClientService adminClientService;\n\n  public Mono<Void> resetToEarliest(\n      KafkaCluster cluster, String group, String topic, Collection<Integer> partitions) {\n    return checkGroupCondition(cluster, group)\n        .flatMap(ac ->\n            offsets(ac, topic, partitions, OffsetSpec.earliest())\n                .flatMap(offsets -> resetOffsets(ac, group, offsets)));\n  }\n\n  private Mono<Map<TopicPartition, Long>> offsets(ReactiveAdminClient client,\n                                                  String topic,\n                                                  @Nullable Collection<Integer> partitions,\n                                                  OffsetSpec spec) {\n    if (partitions == null) {\n      return client.listTopicOffsets(topic, spec, true);\n    }\n    return client.listOffsets(\n        partitions.stream().map(idx -> new TopicPartition(topic, idx)).collect(toSet()),\n        spec,\n        true\n    );\n  }\n\n  public Mono<Void> resetToLatest(\n      KafkaCluster cluster, String group, String topic, Collection<Integer> partitions) {\n    return checkGroupCondition(cluster, group)\n        .flatMap(ac ->\n            offsets(ac, topic, partitions, OffsetSpec.latest())\n                .flatMap(offsets -> resetOffsets(ac, group, offsets)));\n  }\n\n  public Mono<Void> resetToTimestamp(\n      KafkaCluster cluster, String group, String topic, Collection<Integer> partitions,\n      long targetTimestamp) {\n    return checkGroupCondition(cluster, group)\n        .flatMap(ac ->\n            offsets(ac, topic, partitions, OffsetSpec.forTimestamp(targetTimestamp))\n                .flatMap(\n                    foundOffsets -> offsets(ac, topic, partitions, OffsetSpec.latest())\n                        .map(endOffsets -> editTsOffsets(foundOffsets, endOffsets))\n                )\n                .flatMap(offsets -> resetOffsets(ac, group, offsets))\n        );\n  }\n\n```\n\n### Code description\n```Java\n/**\n * Resets the offsets of a Kafka consumer group to a specified set of offsets for a given topic.\n *\n * @param cluster The Kafka cluster to operate on.\n * @param group The consumer group to reset offsets for.\n * @param topic The topic to reset offsets for.\n * @param targetOffsets A map of partition IDs to target offsets.\n * @return A Mono that completes when the offsets have been reset.\n * @throws NullPointerException If the targetOffsets parameter is null.\n */\n```\n\n### Code snippet:\n```Java\n  public Mono<Void> resetToOffsets(\n      KafkaCluster cluster, String group, String topic, Map<Integer, Long> targetOffsets) {\n    Preconditions.checkNotNull(targetOffsets);\n    var partitionOffsets = targetOffsets.entrySet().stream()\n        .collect(toMap(e -> new TopicPartition(topic, e.getKey()), Map.Entry::getValue));\n    return checkGroupCondition(cluster, group).flatMap(\n        ac ->\n```\n\n### Requirements:\n1. Complete only the missing part of the method.\n2. Only answer the code of the completed part. Do not include any of the provided code snippet.\n3. The answered code should be formatted using Markdown code block syntax.\n4. Do not include any additional text.\n\n### Answer: (use the provided format with backticks)\n", "reference": "            ac.listOffsets(partitionOffsets.keySet(), OffsetSpec.earliest(), true)\n                .flatMap(earliest ->\n                    ac.listOffsets(partitionOffsets.keySet(), OffsetSpec.latest(), true)\n                        .map(latest -> editOffsetsBounds(partitionOffsets, earliest, latest))\n                        .flatMap(offsetsToCommit -> resetOffsets(ac, group, offsetsToCommit)))\n    );\n  }", "completions": ["  \n```Java\n    return resetOffsets(ac, group, partitionOffsets);\n  }\n\n  private Mono<Void> resetOffsets(ReactiveAdminClient client, String group,\n                                  Map<TopicPartition, Long> offsets) {\n    return client.alterConsumerGroupOffsets(group, offsets).then();\n  }\n\n  private Mono<ReactiveAdminClient> checkGroupCondition(KafkaCluster cluster, String group) {\n    return adminClientService.getOrCreateAdminClient(cluster)\n       .flatMap(ac -> ac.describeConsumerGroups(List.of(group))\n           .flatMap(descriptions -> {\n              var description = descriptions.get(group);\n              if (description == null) {\n                return Mono.error(new NotFoundException(\"Consumer group not found\"));\n              }\n              if (description.getState() == DEAD || description.getState() == EMPTY) {\n                return Mono.error(new ValidationException(\"Consumer group is inactive\"));\n              }\n              return Mono.just(ac);\n            }));\n  }\n\n  private Map<TopicPartition, Long> editTsOffsets(\n      Map<TopicPartition, Long> foundOffsets, Map<TopicPartition, Long> endOffsets) {\n    var result = new HashMap<TopicPartition, Long>();\n    for (var entry : foundOffsets.entrySet()) {\n      var partition = entry.getKey();\n      var foundOffset = entry.getValue();\n      var endOffset = endOffsets.get(partition);\n      if (foundOffset == null || endOffset == null) {\n        continue;\n      }\n      result.put(partition, Math.min(foundOffset, endOffset));\n    }\n    return result;\n  }\n}\n```"]}
{"id": 58, "name": "decompress", "prompt": "### Role:\nYou are an experienced Java programmer assistant responsible for code completion tasks. Please intelligently complete the missing parts of the provided code snippet.\n\n### Task:\nBased on the provided code snippet, complete the missing method implementation. Ensure the code is logically correct, syntactically accurate, and follows best coding practices.\n\n### Left context\n```Java\n/*\n * Licensed to the Apache Software Foundation (ASF) under one or more\n * contributor license agreements. See the NOTICE file distributed with\n * this work for additional information regarding copyright ownership.\n * The ASF licenses this file to You under the Apache License, Version 2.0\n * (the \"License\"); you may not use this file except in compliance with\n * the License. You may obtain a copy of the License at\n *\n *    http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\npackage org.apache.kafka.common.telemetry.internals;\n\nimport io.opentelemetry.proto.metrics.v1.MetricsData;\n\nimport org.apache.kafka.common.KafkaException;\nimport org.apache.kafka.common.Uuid;\nimport org.apache.kafka.common.metrics.MetricsContext;\nimport org.apache.kafka.common.protocol.Errors;\nimport org.apache.kafka.common.record.CompressionType;\nimport org.apache.kafka.common.record.RecordBatch;\nimport org.apache.kafka.common.utils.BufferSupplier;\nimport org.apache.kafka.common.utils.ByteBufferOutputStream;\nimport org.apache.kafka.common.utils.Utils;\nimport org.slf4j.Logger;\nimport org.slf4j.LoggerFactory;\n\nimport java.io.ByteArrayOutputStream;\nimport java.io.IOException;\nimport java.io.InputStream;\nimport java.io.OutputStream;\nimport java.nio.ByteBuffer;\nimport java.time.Duration;\nimport java.util.ArrayList;\nimport java.util.Collections;\nimport java.util.List;\nimport java.util.Map;\nimport java.util.Optional;\nimport java.util.function.Predicate;\n\npublic class ClientTelemetryUtils {\n\n    private final static Logger log = LoggerFactory.getLogger(ClientTelemetryUtils.class);\n\n    public final static Predicate<? super MetricKeyable> SELECTOR_NO_METRICS = k -> false;\n\n    public final static Predicate<? super MetricKeyable> SELECTOR_ALL_METRICS = k -> true;\n\n    /**\n     * Examine the response data and handle different error code accordingly:\n     *\n     * <ul>\n     *     <li>Invalid Request: Disable Telemetry</li>\n     *     <li>Invalid Record: Disable Telemetry</li>\n     *     <li>Unsupported Version: Disable Telemetry</li>\n     *     <li>UnknownSubscription or Unsupported Compression: Retry immediately</li>\n     *     <li>TelemetryTooLarge or ThrottlingQuotaExceeded: Retry as per next interval</li>\n     * </ul>\n     *\n     * @param errorCode response body error code\n     * @param intervalMs current push interval in milliseconds\n     *\n     * @return Optional of push interval in milliseconds\n     */\n    public static Optional<Integer> maybeFetchErrorIntervalMs(short errorCode, int intervalMs) {\n        if (errorCode == Errors.NONE.code())\n            return Optional.empty();\n\n        int pushIntervalMs;\n        String reason;\n\n        Errors error = Errors.forCode(errorCode);\n        switch (error) {\n            case INVALID_REQUEST:\n            case INVALID_RECORD:\n            case UNSUPPORTED_VERSION:\n                pushIntervalMs = Integer.MAX_VALUE;\n                reason = \"The broker response indicates the client sent an request that cannot be resolved\"\n                    + \" by re-trying, hence disable telemetry\";\n                break;\n            case UNKNOWN_SUBSCRIPTION_ID:\n            case UNSUPPORTED_COMPRESSION_TYPE:\n                pushIntervalMs = 0;\n                reason = error.message();\n                break;\n            case TELEMETRY_TOO_LARGE:\n            case THROTTLING_QUOTA_EXCEEDED:\n                reason = error.message();\n                pushIntervalMs = (intervalMs != -1) ? intervalMs : ClientTelemetryReporter.DEFAULT_PUSH_INTERVAL_MS;\n                break;\n            default:\n                reason = \"Unwrapped error code\";\n                log.error(\"Error code: {}. Unmapped error for telemetry, disable telemetry.\", errorCode);\n                pushIntervalMs = Integer.MAX_VALUE;\n        }\n\n        log.debug(\"Error code: {}, reason: {}. Push interval update to {} ms.\", errorCode, reason, pushIntervalMs);\n        return Optional.of(pushIntervalMs);\n    }\n\n    public static Predicate<? super MetricKeyable> getSelectorFromRequestedMetrics(List<String> requestedMetrics) {\n        if (requestedMetrics == null || requestedMetrics.isEmpty()) {\n            log.debug(\"Telemetry subscription has specified no metric names; telemetry will record no metrics\");\n            return SELECTOR_NO_METRICS;\n        } else if (requestedMetrics.size() == 1 && requestedMetrics.get(0) != null && requestedMetrics.get(0).equals(\"*\")) {\n            log.debug(\"Telemetry subscription has specified a single '*' metric name; using all metrics\");\n            return SELECTOR_ALL_METRICS;\n        } else {\n            log.debug(\"Telemetry subscription has specified to include only metrics that are prefixed with the following strings: {}\", requestedMetrics);\n            return k -> requestedMetrics.stream().anyMatch(f -> k.key().name().startsWith(f));\n        }\n    }\n\n    public static List<CompressionType> getCompressionTypesFromAcceptedList(List<Byte> acceptedCompressionTypes) {\n        if (acceptedCompressionTypes == null || acceptedCompressionTypes.isEmpty()) {\n            return Collections.emptyList();\n        }\n\n        List<CompressionType> result = new ArrayList<>();\n        for (Byte compressionByte : acceptedCompressionTypes) {\n            int compressionId = compressionByte.intValue();\n            try {\n                CompressionType compressionType = CompressionType.forId(compressionId);\n                result.add(compressionType);\n            } catch (IllegalArgumentException e) {\n                log.warn(\"Accepted compressionByte type with ID {} is not a known compressionByte type; ignoring\", compressionId, e);\n            }\n        }\n        return result;\n    }\n\n    public static Uuid validateClientInstanceId(Uuid clientInstanceId) {\n        if (clientInstanceId == null || clientInstanceId == Uuid.ZERO_UUID) {\n            throw new IllegalArgumentException(\"clientInstanceId is not valid\");\n        }\n\n        return clientInstanceId;\n    }\n\n    public static int validateIntervalMs(int intervalMs) {\n        if (intervalMs <= 0) {\n            log.warn(\"Telemetry subscription push interval value from broker was invalid ({}),\"\n                + \" substituting with default value of {}\", intervalMs, ClientTelemetryReporter.DEFAULT_PUSH_INTERVAL_MS);\n            return ClientTelemetryReporter.DEFAULT_PUSH_INTERVAL_MS;\n        }\n\n        log.debug(\"Telemetry subscription push interval value from broker: {}\", intervalMs);\n        return intervalMs;\n    }\n\n    public static boolean validateResourceLabel(Map<String, ?> m, String key) {\n        if (!m.containsKey(key)) {\n            log.trace(\"{} does not exist in map {}\", key, m);\n            return false;\n        }\n\n        if (m.get(key) == null) {\n            log.trace(\"{} is null. map {}\", key, m);\n            return false;\n        }\n\n        if (!(m.get(key) instanceof String)) {\n            log.trace(\"{} is not a string. map {}\", key, m);\n            return false;\n        }\n\n        String val = (String) m.get(key);\n        if (val.isEmpty()) {\n            log.trace(\"{} is empty string. value = {} map {}\", key, val, m);\n            return false;\n        }\n        return true;\n    }\n\n    public static boolean validateRequiredResourceLabels(Map<String, String> metadata) {\n        return validateResourceLabel(metadata, MetricsContext.NAMESPACE);\n    }\n\n    public static CompressionType preferredCompressionType(List<CompressionType> acceptedCompressionTypes) {\n        if (acceptedCompressionTypes != null && !acceptedCompressionTypes.isEmpty()) {\n            // Broker is providing the compression types in order of preference. Grab the\n            // first one.\n            return acceptedCompressionTypes.get(0);\n        }\n        return CompressionType.NONE;\n    }\n\n    public static byte[] compress(byte[] raw, CompressionType compressionType) throws IOException {\n        try (ByteBufferOutputStream compressedOut = new ByteBufferOutputStream(512)) {\n            try (OutputStream out = compressionType.wrapForOutput(compressedOut, RecordBatch.CURRENT_MAGIC_VALUE)) {\n                out.write(raw);\n                out.flush();\n            }\n            compressedOut.buffer().flip();\n            return Utils.toArray(compressedOut.buffer());\n        }\n    }\n\n```\n\n### Code description\n```Java\n/**\n * Decompresses the given byte array using the specified compression type.\n * \n * @param metrics The byte array to be decompressed.\n * @param compressionType The compression type to be used for decompression.\n * @return A ByteBuffer containing the decompressed data.\n * @throws KafkaException If an I/O error occurs during decompression.\n */\n```\n\n### Code snippet:\n```Java\n    public static ByteBuffer decompress(byte[] metrics, CompressionType compressionType) {\n        ByteBuffer data = ByteBuffer.wrap(metrics);\n        try (InputStream in = compressionType.wrapForInput(data, RecordBatch.CURRENT_MAGIC_VALUE, BufferSupplier.create());\n            ByteArrayOutputStream out = new ByteArrayOutputStream()) {\n\n            byte[] bytes = new byte[data.capacity() * 2];\n            int nRead;\n            while ((nRead = in.read(bytes, 0, bytes.length)) != -1) {\n```\n\n### Requirements:\n1. Complete only the missing part of the method.\n2. Only answer the code of the completed part. Do not include any of the provided code snippet.\n3. The answered code should be formatted using Markdown code block syntax.\n4. Do not include any additional text.\n\n### Answer: (use the provided format with backticks)\n", "reference": "                out.write(bytes, 0, nRead);\n            }\n\n            out.flush();\n            return ByteBuffer.wrap(out.toByteArray());\n        } catch (IOException e) {\n            throw new KafkaException(\"Failed to decompress metrics data\", e);\n        }\n    }", "completions": ["  \n```Java\n    public static ByteBuffer decompress(byte[] metrics, CompressionType compressionType) {\n        ByteBuffer data = ByteBuffer.wrap(metrics);\n        try (InputStream in = compressionType.wrapForInput(data, RecordBatch.CURRENT_MAGIC_VALUE, BufferSupplier.create());\n            ByteArrayOutputStream out = new ByteArrayOutputStream()) {\n\n            byte[] bytes = new byte[data.capacity() * 2];\n            int nRead;\n            while ((nRead = in.read(bytes, 0, bytes.length))!= -1) {\n                out.write(bytes, 0, nRead);\n            }\n            return ByteBuffer.wrap(out.toByteArray());\n        } catch (IOException e) {\n            throw new KafkaException(e);\n        }\n    }\n```"]}
{"id": 59, "name": "writePropertiesFile", "prompt": "### Role:\nYou are an experienced Java programmer assistant responsible for code completion tasks. Please intelligently complete the missing parts of the provided code snippet.\n\n### Task:\nBased on the provided code snippet, complete the missing method implementation. Ensure the code is logically correct, syntactically accurate, and follows best coding practices.\n\n### Left context\n```Java\n/*\n * Licensed to the Apache Software Foundation (ASF) under one or more\n * contributor license agreements. See the NOTICE file distributed with\n * this work for additional information regarding copyright ownership.\n * The ASF licenses this file to You under the Apache License, Version 2.0\n * (the \"License\"); you may not use this file except in compliance with\n * the License. You may obtain a copy of the License at\n *\n *    http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\npackage org.apache.kafka.metadata.properties;\n\nimport org.apache.kafka.common.utils.Utils;\n\nimport java.io.File;\nimport java.io.FileOutputStream;\nimport java.io.IOException;\nimport java.io.InputStream;\nimport java.io.OutputStreamWriter;\nimport java.io.PrintWriter;\nimport java.nio.charset.StandardCharsets;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\nimport java.util.Properties;\n\npublic final class PropertiesUtils {\n    /**\n     * Writes a Java Properties object to a file.\n     *\n     * @param props         The Properties object.\n     * @param path          The file to write to.\n     * @throws IOException\n     */\n```\n\n### Code description\n```Java\n/**\n * Description: This method writes a properties file to the specified path. It uses a temporary file to write the properties,\n * then atomically moves the temporary file to the target file using the atomicMoveWithFallback method from the Utils class.\n * If fsync is true, it also ensures the file is synced to the disk.\n *\n * @param props The properties to be written to the file.\n * @param path The path of the file to be written.\n * @param fsync A boolean indicating whether to sync the file to the disk after writing.\n * @throws IOException If an I/O error occurs.\n */\n```\n\n### Code snippet:\n```Java\n    public static void writePropertiesFile(\n        Properties props,\n        String path,\n        boolean fsync\n    ) throws IOException {\n        File tempFile = new File(path + \".tmp\");\n        try (\n            FileOutputStream fos = new FileOutputStream(tempFile, false);\n            OutputStreamWriter osw = new OutputStreamWriter(fos, StandardCharsets.UTF_8);\n            PrintWriter pw = new PrintWriter(osw)\n        ) {\n            props.store(pw, \"\");\n```\n\n### Requirements:\n1. Complete only the missing part of the method.\n2. Only answer the code of the completed part. Do not include any of the provided code snippet.\n3. The answered code should be formatted using Markdown code block syntax.\n4. Do not include any additional text.\n\n### Answer: (use the provided format with backticks)\n", "reference": "            fos.flush();\n            if (fsync) {\n                fos.getFD().sync();\n            }\n        }\n        File targetFile = new File(path);\n        try {\n            Utils.atomicMoveWithFallback(tempFile.toPath(), targetFile.toPath(), fsync);\n        } catch (Throwable e) {\n            Utils.delete(tempFile);\n            throw e;\n        }\n    }", "completions": ["  \n```Java\n        } finally {\n            Files.deleteIfExists(tempFile.toPath());\n        }\n        if (fsync) {\n            Utils.atomicMoveWithFallback(tempFile.toPath(), Paths.get(path));\n        } else {\n            Files.move(tempFile.toPath(), Paths.get(path), StandardCopyOption.ATOMIC_MOVE);\n        }\n    }\n}\n```"]}
{"id": 60, "name": "encrypt", "prompt": "### Role:\nYou are an experienced Java programmer assistant responsible for code completion tasks. Please intelligently complete the missing parts of the provided code snippet.\n\n### Task:\nBased on the provided code snippet, complete the missing method implementation. Ensure the code is logically correct, syntactically accurate, and follows best coding practices.\n\n### Left context\n```Java\n/*\n * Copyright (c) 2017-2021 Nitrite author or authors.\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *       http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n *\n */\n\npackage org.dizitart.no2.support.crypto;\n\nimport org.apache.commons.codec.binary.Base64;\nimport org.dizitart.no2.common.util.CryptoUtils;\nimport org.dizitart.no2.common.util.SecureString;\nimport org.dizitart.no2.exceptions.NitriteSecurityException;\n\nimport javax.crypto.Cipher;\nimport javax.crypto.SecretKey;\nimport javax.crypto.spec.GCMParameterSpec;\nimport java.nio.ByteBuffer;\nimport java.nio.charset.Charset;\nimport java.nio.charset.StandardCharsets;\n\n/**\n * The {@code AESEncryptor} class provides AES encryption and decryption\n * functionality.\n * <p>\n * It uses AES/GCM/NoPadding encryption algorithm with a 128-bit tag length,\n * 12-byte IV length, and 16-byte salt length by default.\n * <p>\n * The class provides methods to encrypt and decrypt byte arrays and strings\n * using the specified password and encryption parameters.\n * <p>\n * \n * NOTE: This is a derivative work of <a href=\n * \"https://mkyong.com/java/java-symmetric-key-cryptography-example/\">this</a>.\n * \n * @author Anindya Chatterjee\n * @since 4.0\n */\npublic class AESEncryptor implements Encryptor {\n    private final String encryptAlgo;\n    private final int tagLengthBit;\n    private final int ivLengthByte;\n    private final int saltLengthByte;\n    private final Charset UTF_8 = StandardCharsets.UTF_8;\n\n    private final SecureString password;\n\n    /**\n     * Instantiates a new {@link AESEncryptor} with these default values\n     * <ul>\n     * <li>Encryption Algo - AES/GCM/NoPadding</li>\n     * <li>Tag Length (bit) - 128</li>\n     * <li>IV Length (byte) - 12</li>\n     * <li>Salt Length (byte) - 16</li>\n     * </ul>\n     *\n     * @param password the password\n     */\n    public AESEncryptor(String password) {\n        this(password, \"AES/GCM/NoPadding\", 128, 12, 16);\n    }\n\n    /**\n     * Instantiates a new {@link AESEncryptor}.\n     *\n     * @param password       the password\n     * @param encryptionAlgo the encryption algo\n     * @param tagLengthBit   the tag length bit\n     * @param ivLengthByte   the iv length byte\n     * @param saltLengthByte the salt length byte\n     */\n    public AESEncryptor(String password, String encryptionAlgo,\n            Integer tagLengthBit, Integer ivLengthByte,\n            Integer saltLengthByte) {\n        this.password = new SecureString(password);\n        this.encryptAlgo = encryptionAlgo;\n        this.tagLengthBit = tagLengthBit;\n        this.ivLengthByte = ivLengthByte;\n        this.saltLengthByte = saltLengthByte;\n    }\n\n    /**\n     * Returns a base64 encoded AES encrypted string.\n     *\n     * @param plainText the text as byte array\n     * @return the encrypted string\n     */\n```\n\n### Code description\n```Java\n/**\n * Encrypts the given plain text using AES-GCM encryption.\n * The encryption process includes generating a random salt, an initialization vector (IV),\n * and a secret key from the provided password. The encrypted data is then prefixed with the IV and salt,\n * and the resulting byte array is encoded into a URL-safe base64 string.\n *\n * @param plainText The data to be encrypted.\n * @return A URL-safe base64 encoded string representing the encrypted data.\n * @throws NitriteSecurityException If the encryption process fails.\n */\n```\n\n### Code snippet:\n```Java\n    @Override\n    public String encrypt(byte[] plainText) {\n        try {\n            // 16 bytes salt\n            byte[] salt = CryptoUtils.getRandomNonce(saltLengthByte);\n\n            // GCM recommended 12 bytes iv?\n            byte[] iv = CryptoUtils.getRandomNonce(ivLengthByte);\n\n            // secret key from password\n            SecretKey aesKeyFromPassword = CryptoUtils.getAESKeyFromPassword(password.asString().toCharArray(), salt);\n\n            Cipher cipher = Cipher.getInstance(encryptAlgo);\n\n            // ASE-GCM needs GCMParameterSpec\n            cipher.init(Cipher.ENCRYPT_MODE, aesKeyFromPassword, new GCMParameterSpec(tagLengthBit, iv));\n```\n\n### Requirements:\n1. Complete only the missing part of the method.\n2. Only answer the code of the completed part. Do not include any of the provided code snippet.\n3. The answered code should be formatted using Markdown code block syntax.\n4. Do not include any additional text.\n\n### Answer: (use the provided format with backticks)\n", "reference": "\n            byte[] cipherText = cipher.doFinal(plainText);\n\n            // prefix IV and Salt to cipher text\n            byte[] cipherTextWithIvSalt = ByteBuffer.allocate(iv.length + salt.length + cipherText.length)\n                    .put(iv)\n                    .put(salt)\n                    .put(cipherText)\n                    .array();\n\n            // string representation, base64, send this string to other for decryption.\n            return Base64.encodeBase64URLSafeString(cipherTextWithIvSalt);\n        } catch (Exception e) {\n            throw new NitriteSecurityException(\"Failed to encrypt data\", e);\n        }\n    }", "completions": ["  \n```Java\n            // encrypt\n            byte[] cipherText = cipher.doFinal(plainText);\n\n            // prefix IV and salt to cipher text\n            byte[] encryptedMessage = new byte[iv.length + salt.length + cipherText.length];\n            System.arraycopy(iv, 0, encryptedMessage, 0, iv.length);\n            System.arraycopy(salt, 0, encryptedMessage, iv.length, salt.length);\n            System.arraycopy(cipherText, 0, encryptedMessage, iv.length + salt.length, cipherText.length);\n\n            // encode with base64\n            return Base64.encodeBase64URLSafeString(encryptedMessage);\n        } catch (Exception e) {\n            throw new NitriteSecurityException(\"encryption failed\", e);\n        }\n    }\n}\n```"]}
{"id": 61, "name": "process", "prompt": "### Role:\nYou are an experienced Java programmer assistant responsible for code completion tasks. Please intelligently complete the missing parts of the provided code snippet.\n\n### Task:\nBased on the provided code snippet, complete the missing method implementation. Ensure the code is logically correct, syntactically accurate, and follows best coding practices.\n\n### Left context\n```Java\n/*\n * Janssen Project software is available under the MIT License (2008). See http://opensource.org/licenses/MIT for full text.\n *\n * Copyright (c) 2020, Janssen Project\n */\n\n/*\n * Copyright (c) 2018 Mastercard\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and limitations under the License.\n */\n\npackage io.jans.fido2.service.processor.assertion;\n\nimport com.fasterxml.jackson.databind.JsonNode;\nimport io.jans.fido2.ctap.AttestationFormat;\nimport io.jans.fido2.exception.Fido2CompromisedDevice;\nimport io.jans.fido2.exception.Fido2RuntimeException;\nimport io.jans.fido2.model.auth.AuthData;\nimport io.jans.fido2.service.AuthenticatorDataParser;\nimport io.jans.fido2.service.Base64Service;\nimport io.jans.fido2.service.CoseService;\nimport io.jans.fido2.service.DataMapperService;\nimport io.jans.fido2.service.processors.AssertionFormatProcessor;\nimport io.jans.fido2.service.verifier.AuthenticatorDataVerifier;\nimport io.jans.fido2.service.verifier.CommonVerifiers;\nimport io.jans.fido2.service.verifier.UserVerificationVerifier;\nimport io.jans.orm.model.fido2.Fido2AuthenticationData;\nimport io.jans.orm.model.fido2.Fido2RegistrationData;\nimport jakarta.enterprise.context.ApplicationScoped;\nimport jakarta.inject.Inject;\nimport org.apache.commons.codec.binary.Hex;\nimport org.apache.commons.codec.digest.DigestUtils;\nimport org.slf4j.Logger;\n\nimport java.security.PublicKey;\n\n/**\n * Class which processes assertions of \"none\" fmt (attestation type)\n */\n@ApplicationScoped\npublic class NoneAssertionFormatProcessor implements AssertionFormatProcessor {\n\n    @Inject\n    private Logger log;\n\n    @Inject\n    private CoseService coseService;\n\n    @Inject\n    private CommonVerifiers commonVerifiers;\n\n    @Inject\n    private AuthenticatorDataVerifier authenticatorDataVerifier;\n\n    @Inject\n    private UserVerificationVerifier userVerificationVerifier;\n\n    @Inject\n    private AuthenticatorDataParser authenticatorDataParser;\n\n    @Inject\n    private DataMapperService dataMapperService;\n\n    @Inject\n    private Base64Service base64Service;\n\n    @Override\n    public AttestationFormat getAttestationFormat() {\n        return AttestationFormat.none;\n    }\n\n```\n\n### Code description\n```Java\n/**\n * This method processes the FIDO2 authentication data. It verifies the registration domain, user verification option, counter, and assertion signature.\n *\n * @param base64AuthenticatorData The base64 encoded authenticator data.\n * @param signature The signature of the authentication data.\n * @param clientDataJson The client data in JSON format.\n * @param registration The FIDO2 registration data.\n * @param authenticationEntity The FIDO2 authentication data.\n * @throws Fido2CompromisedDevice If the device is compromised.\n * @throws Fido2RuntimeException If there is an error during the process.\n */\n```\n\n### Code snippet:\n```Java\n    @Override\n    public void process(String base64AuthenticatorData, String signature, String clientDataJson, Fido2RegistrationData registration,\n                        Fido2AuthenticationData authenticationEntity) {\n        log.debug(\"Registration: {}\", registration);\n\n        AuthData authData = authenticatorDataParser.parseAssertionData(base64AuthenticatorData);\n        commonVerifiers.verifyRpIdHash(authData, registration.getDomain());\n\n        log.debug(\"User verification option: {}\", authenticationEntity.getUserVerificationOption());\n        userVerificationVerifier.verifyUserVerificationOption(authenticationEntity.getUserVerificationOption(), authData);\n\n        byte[] clientDataHash = DigestUtils.getSha256Digest().digest(base64Service.urlDecode(clientDataJson));\n\n        try {\n            int counter = authenticatorDataParser.parseCounter(authData.getCounters());\n            commonVerifiers.verifyCounter(registration.getCounter(), counter);\n            registration.setCounter(counter);\n```\n\n### Requirements:\n1. Complete only the missing part of the method.\n2. Only answer the code of the completed part. Do not include any of the provided code snippet.\n3. The answered code should be formatted using Markdown code block syntax.\n4. Do not include any additional text.\n\n### Answer: (use the provided format with backticks)\n", "reference": "\n            JsonNode uncompressedECPointNode = dataMapperService.cborReadTree(base64Service.urlDecode(registration.getUncompressedECPoint()));\n            PublicKey publicKey = coseService.createUncompressedPointFromCOSEPublicKey(uncompressedECPointNode);\n\n            log.debug(\"Uncompressed ECpoint node: {}\", uncompressedECPointNode);\n            log.debug(\"EC Public key hex: {}\", Hex.encodeHexString(publicKey.getEncoded()));\n            log.debug(\"Registration algorithm: {}, default use: -7\", registration.getSignatureAlgorithm());\n            authenticatorDataVerifier.verifyAssertionSignature(authData, clientDataHash, signature, publicKey, -7);\n\n        } catch (Fido2CompromisedDevice ex) {\n            log.error(\"Error compromised device: {}\", ex.getMessage());\n            throw ex;\n        } catch (Exception ex) {\n            log.error(\"Error to check none assertion: {}\", ex.getMessage());\n            throw new Fido2RuntimeException(\"Failed to check none assertion: {}\", ex.getMessage(), ex);\n        }\n    }", "completions": ["  \n```Java\n            authenticatorDataVerifier.verifyAssertionSignature(authData, signature, clientDataHash, registration.getCredentialPublicKey());\n        } catch (Exception e) {\n            throw new Fido2RuntimeException(\"Failed to verify assertion signature\", e);\n        }\n    }\n```"]}
{"id": 62, "name": "relativePath", "prompt": "### Role:\nYou are an experienced Java programmer assistant responsible for code completion tasks. Please intelligently complete the missing parts of the provided code snippet.\n\n### Task:\nBased on the provided code snippet, complete the missing method implementation. Ensure the code is logically correct, syntactically accurate, and follows best coding practices.\n\n### Left context\n```Java\n/*\n * Copyright (C) 2017-2019 Dremio Corporation\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\npackage com.dremio.common.utils;\n\nimport static com.google.common.base.Preconditions.checkNotNull;\n\nimport java.io.IOException;\nimport java.io.UnsupportedEncodingException;\nimport java.net.URLEncoder;\nimport java.nio.file.Paths;\nimport java.util.Collection;\nimport java.util.Collections;\nimport java.util.List;\nimport java.util.Map;\nimport java.util.function.Function;\nimport java.util.function.Supplier;\nimport java.util.regex.Pattern;\n\nimport org.apache.commons.lang3.text.StrTokenizer;\n\nimport com.dremio.common.exceptions.UserException;\nimport com.dremio.io.file.Path;\nimport com.github.slugify.Slugify;\nimport com.google.common.base.Joiner;\nimport com.google.common.base.Preconditions;\nimport com.google.common.base.Splitter;\nimport com.google.common.collect.Lists;\n\n/**\n * Utils to convert dotted path to file system path and vice versa.\n */\npublic class PathUtils {\n  private static final org.slf4j.Logger logger = org.slf4j.LoggerFactory.getLogger(PathUtils.class);\n\n  private static final char PATH_DELIMITER = '.'; // dot separated path\n  private static final Joiner KEY_JOINER = Joiner.on(PATH_DELIMITER).useForNull(\"\");\n  private static final String SLASH = Path.SEPARATOR;\n  private static final char SLASH_CHAR = Path.SEPARATOR_CHAR;\n  private static final Joiner PATH_JOINER = Joiner.on(SLASH_CHAR).useForNull(\"\");\n  private static final Path ROOT_PATH = Path.of(SLASH);\n  private static final List<String> EMPTY_SCHEMA_PATHS = Collections.emptyList();\n\n  /**\n   * Convert list of components into fs path.\n   * [a,b,c] -> /a/b/c\n   * @param schemaPath list of path components\n   * @return {@code fs Path}\n   */\n  public static Path toFSPath(final List<String> schemaPath) {\n    if (schemaPath == null || schemaPath.isEmpty()) {\n      return ROOT_PATH;\n    }\n    return ROOT_PATH.resolve(PATH_JOINER.join(schemaPath));\n  }\n\n  public static String toFSPathString(final List<String> schemaPath) {\n    return Path.withoutSchemeAndAuthority(toFSPath(schemaPath)).toString();\n  }\n\n  /**\n   * Convert list of path components into fs path, skip root of path if its same as given root.\n   * schema path : [a,b,c] and root: a -> /b/c\n   * @param schemaPath list of path components\n   * @param root of path to be ignored.\n   * @return {@code fs Path}\n   */\n  public static Path toFSPathSkipRoot(final List<String> schemaPath, final String root) {\n    if (schemaPath == null || schemaPath.isEmpty()) {\n      return ROOT_PATH;\n    }\n    if (root == null || !root.equals(schemaPath.get(0))) {\n      return toFSPath(schemaPath);\n    } else {\n      return toFSPath(schemaPath.subList(1, schemaPath.size()));\n    }\n  }\n\n  /**\n   * Convert fully or partially dotted path to file system path.\n   * a.b.c   -> /a/b/c\n   * a.`b/c` -> /a/b/c\n   * @param path a fully or partially dotted path\n   * @return {@code fs Path}\n   */\n  public static Path toFSPath(String path) {\n    final List<String> pathComponents = Lists.newArrayList();\n    for (String component: parseFullPath(path)) {\n      if (component.contains(SLASH)) {\n        pathComponents.addAll(toPathComponents(Path.of(component)));\n      } else {\n        pathComponents.add(component);\n      }\n    }\n    return toFSPath(pathComponents);\n  }\n\n  /**\n   * Get the name of the file or folder at Path\n   * @param path\n   * @return\n   */\n  public static String getQuotedFileName(Path path) {\n    List<String> components = toPathComponents(path);\n    if (components == null || components.size() == 0) {\n      throw new IllegalArgumentException(\"Fail to get a valid path from: \" + path);\n    }\n    return String.format(\"%1$s%2$s%1$s\", SqlUtils.QUOTE, components.get(components.size() - 1));\n  }\n\n  /**\n   * Convert fs path to dotted schema path.\n   * /a/b/c -> a.b.c\n   * @param fsPath filesystem path\n   * @return schema path.\n   */\n  public static String toDottedPath(Path fsPath) {\n    return constructFullPath(toPathComponents(fsPath));\n  }\n\n  /**\n   * Convert fs path relative to parent to dotted schema path.\n   * parent: /a/b, child: /a/b/c/d -> c.d\n   * @param parent parent path\n   * @param child full path of child inside parent path\n   * @return dotted schema name of child relative to parent.\n   * @throws IOException if child does not belong under parent\n   */\n  public static String toDottedPath(final Path parent, final Path child) throws IOException {\n    final List<String> parentPathComponents = toPathComponents(parent);\n    final List<String> childPathComponents = toPathComponents(child);\n    for (int i = 0; i < parentPathComponents.size(); ++i) {\n      if (!parentPathComponents.get(i).equals(childPathComponents.get(i))) {\n        throw new IOException(String.format(\"Invalid file/directory %s listed under %s\", child, parent));\n      }\n    }\n    return constructFullPath(childPathComponents.subList(parentPathComponents.size(), childPathComponents.size()));\n  }\n\n  /**\n   * Convert fs path to list of strings.\n   * /a/b/c -> [a,b,c]\n   * @param fsPath a string\n   * @return list of path components\n   */\n  public static List<String> toPathComponents(String fsPath) {\n    if (fsPath == null) {\n      return EMPTY_SCHEMA_PATHS;\n    }\n\n    final StrTokenizer tokenizer = new StrTokenizer(fsPath, SLASH_CHAR, SqlUtils.QUOTE).setIgnoreEmptyTokens(true);\n    return tokenizer.getTokenList();\n  }\n\n  /**\n   * Convert fs path to list of strings.\n   * /a/b/c -> [a,b,c]\n   * @param fsPath filesystem path\n   * @return list of path components\n   */\n  public static List<String> toPathComponents(Path fsPath) {\n    if (fsPath == null ) {\n      return EMPTY_SCHEMA_PATHS;\n    }\n    return toPathComponents(fsPath.toURI().getPath());\n  }\n\n  /**\n   * puts back ticks around components if they look like reserved keywords and joins them with .\n   * @param pathComponents can not contain nulls\n   * @return a dot delimited path\n   * Convert a list of path components to fully qualified dotted schema path\n   * [a,b,c]      -> a.b.c\n   * [a,b,c-1]    -> a.b.`c-1`\n   * [a,b,c.json] -> a.b.`c.json`\n   */\n  public static String constructFullPath(Collection<String> pathComponents) {\n    final List<String> quotedPathComponents = Lists.newArrayList();\n    for (final String component : pathComponents) {\n      checkNotNull(component);\n      quotedPathComponents.add(SqlUtils.quoteIdentifier(component));\n    }\n    return KEY_JOINER.join(quotedPathComponents);\n  }\n\n  private static Pattern pattern1 = Pattern.compile(\"\\\\%28\");\n  private static Pattern pattern2 = Pattern.compile(\"\\\\%29\");\n  private static Pattern pattern3 = Pattern.compile(\"\\\\+\");\n  private static Pattern pattern4 = Pattern.compile(\"\\\\%27\");\n  private static Pattern pattern5 = Pattern.compile(\"\\\\%21\");\n  private static Pattern pattern6 = Pattern.compile(\"\\\\%7E\");\n\n  /**\n   * Encode URI component consistent with JavaScript\n   * @param component\n   * @return encoded string\n   */\n  public static String encodeURIComponent(String component) {\n    try {\n      final String encodedComponent = URLEncoder.encode(component, \"UTF-8\");\n      final String pattern1Str = pattern1.matcher(encodedComponent).replaceAll(\"(\");\n      final String pattern2Str = pattern2.matcher(pattern1Str).replaceAll(\")\");\n      final String pattern3Str = pattern3.matcher(pattern2Str).replaceAll(\"%20\");\n      final String pattern4Str = pattern4.matcher(pattern3Str).replaceAll(\"'\");\n      final String pattern5Str = pattern5.matcher(pattern4Str).replaceAll(\"!\");\n      final String pattern6Str = pattern6.matcher(pattern5Str).replaceAll(\"~\");\n      return pattern6Str;\n    } catch (UnsupportedEncodingException e) {\n      throw new UnsupportedOperationException(e);\n    }\n  }\n\n  /**\n   * Parase a fully qualified dotted schema path to list of strings.\n   * a.b.`c.json` -> [a,b,c.json]\n   * a.b.`c-1`    -> [a,b,c-1]\n   * a.b.c        -> [a,b,c]\n   * @param path dotted schema path\n   * @return list of path components.\n   */\n  public static List<String> parseFullPath(final String path) {\n    final StrTokenizer tokenizer = new StrTokenizer(path, PATH_DELIMITER, SqlUtils.QUOTE).setIgnoreEmptyTokens(true);\n    return tokenizer.getTokenList();\n  }\n\n  public static String removeQuotes(String pathComponent) {\n    if (pathComponent.charAt(0) == SqlUtils.QUOTE && pathComponent.charAt(pathComponent.length() - 1) == SqlUtils.QUOTE) {\n      return pathComponent.substring(1, pathComponent.length() - 1);\n    }\n    return pathComponent;\n  }\n\n  public static Joiner getPathJoiner() {\n    return PATH_JOINER;\n  }\n\n  public static char getPathDelimiter() {\n    return PATH_DELIMITER;\n  }\n\n  public static Joiner getKeyJoiner() {\n    return KEY_JOINER;\n  }\n\n  public static String slugify(Collection<String> pathComponents) {\n    Slugify slg = new Slugify();\n    return slg.slugify(constructFullPath(pathComponents));\n  }\n\n```\n\n### Code description\n```Java\n/**\n * Description: This method calculates the relative path from a given absolute path to a base path.\n * \n * @param absolutePath The absolute path from which to calculate the relative path.\n * @param basePath The base path to which the relative path is calculated.\n * @return The relative path from the base path to the absolute path, or the absolute path itself if they have no common prefix.\n * @throws IllegalArgumentException If either of the input paths is not absolute.\n */\n```\n\n### Code snippet:\n```Java\n  public static String relativePath(Path absolutePath, Path basePath) {\n    Preconditions.checkArgument(absolutePath.isAbsolute(), \"absolutePath must be an absolute path\");\n    Preconditions.checkArgument(basePath.isAbsolute(), \"basePath must be an absolute path\");\n\n    List<String> absolutePathComponents = Lists.newArrayList(toPathComponents(absolutePath)); // make a copy\n    List<String> basePathComponents = toPathComponents(basePath);\n    boolean hasCommonPrefix = basePathComponents.isEmpty(); // when basePath is \"/\", we always have a common prefix\n\n    for (String base : basePathComponents) {\n      if (absolutePathComponents.get(0).equals(base)) {\n        absolutePathComponents.remove(0);\n```\n\n### Requirements:\n1. Complete only the missing part of the method.\n2. Only answer the code of the completed part. Do not include any of the provided code snippet.\n3. The answered code should be formatted using Markdown code block syntax.\n4. Do not include any additional text.\n\n### Answer: (use the provided format with backticks)\n", "reference": "        hasCommonPrefix = true;\n      } else {\n        break;\n      }\n    }\n    if (hasCommonPrefix) {\n      return PATH_JOINER.join(absolutePathComponents);\n    } else {\n      return absolutePath.toString();\n    }\n  }", "completions": ["  \n```Java\n      hasCommonPrefix = true;\n    }\n\n    if (!hasCommonPrefix) {\n      return absolutePath.toString();\n    }\n\n    StringBuilder relativePath = new StringBuilder();\n    for (int i = 0; i < absolutePathComponents.size(); i++) {\n      relativePath.append(\"..\");\n      if (i < absolutePathComponents.size() - 1) {\n        relativePath.append(Path.SEPARATOR);\n      }\n    }\n    if (relativePath.length() == 0) {\n      relativePath.append(\".\");\n    }\n    return relativePath.toString();\n  }\n```"]}
{"id": 63, "name": "mapping", "prompt": "### Role:\nYou are an experienced Java programmer assistant responsible for code completion tasks. Please intelligently complete the missing parts of the provided code snippet.\n\n### Task:\nBased on the provided code snippet, complete the missing method implementation. Ensure the code is logically correct, syntactically accurate, and follows best coding practices.\n\n### Left context\n```Java\npackage com.slack.astra.elasticsearchApi;\n\nimport brave.ScopedSpan;\nimport brave.Tracing;\nimport brave.propagation.CurrentTraceContext;\nimport brave.propagation.TraceContext;\nimport com.fasterxml.jackson.databind.JsonNode;\nimport com.fasterxml.jackson.databind.ObjectMapper;\nimport com.google.common.collect.ImmutableMap;\nimport com.google.protobuf.ByteString;\nimport com.linecorp.armeria.common.HttpResponse;\nimport com.linecorp.armeria.common.HttpStatus;\nimport com.linecorp.armeria.common.MediaType;\nimport com.linecorp.armeria.server.annotation.Blocking;\nimport com.linecorp.armeria.server.annotation.Get;\nimport com.linecorp.armeria.server.annotation.Param;\nimport com.linecorp.armeria.server.annotation.Path;\nimport com.linecorp.armeria.server.annotation.Post;\nimport com.slack.astra.elasticsearchApi.searchResponse.EsSearchResponse;\nimport com.slack.astra.elasticsearchApi.searchResponse.HitsMetadata;\nimport com.slack.astra.elasticsearchApi.searchResponse.SearchResponseHit;\nimport com.slack.astra.elasticsearchApi.searchResponse.SearchResponseMetadata;\nimport com.slack.astra.logstore.LogMessage;\nimport com.slack.astra.logstore.opensearch.OpenSearchInternalAggregation;\nimport com.slack.astra.logstore.search.SearchResultUtils;\nimport com.slack.astra.metadata.schema.FieldType;\nimport com.slack.astra.proto.service.AstraSearch;\nimport com.slack.astra.server.AstraQueryServiceBase;\nimport com.slack.astra.util.JsonUtil;\nimport java.io.IOException;\nimport java.time.Duration;\nimport java.time.Instant;\nimport java.time.temporal.ChronoUnit;\nimport java.util.ArrayList;\nimport java.util.List;\nimport java.util.Map;\nimport java.util.Optional;\nimport java.util.TreeMap;\nimport java.util.concurrent.StructuredTaskScope;\nimport org.opensearch.search.aggregations.InternalAggregation;\nimport org.slf4j.Logger;\nimport org.slf4j.LoggerFactory;\n\n/**\n * Elasticsearch compatible API service, for use in Grafana\n *\n * @see <a\n *     href=\"https://github.com/grafana/grafana/blob/main/public/app/plugins/datasource/elasticsearch/datasource.ts\">Grafana\n *     ES API</a>\n */\n@SuppressWarnings(\n    \"OptionalUsedAsFieldOrParameterType\") // Per https://armeria.dev/docs/server-annotated-service/\npublic class ElasticsearchApiService {\n  private static final Logger LOG = LoggerFactory.getLogger(ElasticsearchApiService.class);\n  private final AstraQueryServiceBase searcher;\n\n  private final OpenSearchRequest openSearchRequest = new OpenSearchRequest();\n  private final ObjectMapper objectMapper = new ObjectMapper();\n\n  public ElasticsearchApiService(AstraQueryServiceBase searcher) {\n    this.searcher = searcher;\n  }\n\n  /** Returns metadata about the cluster */\n  @Get\n  @Path(\"/\")\n  public HttpResponse clusterMetadata() {\n    // todo - expand this to automatically pull in build info\n    // example - https://opensearch.org/docs/2.3/quickstart/\n    // number must validate with npm semver validate for grafana compatibility due to\n    // https://github.com/grafana/grafana/blob/f74d5ff93ebe61e090994162be9b08bafcd5b7f0/public/app/plugins/datasource/elasticsearch/components/QueryEditor/MetricAggregationsEditor/MetricEditor.tsx#L54\n    return HttpResponse.of(\n        \"\"\"\n        {\n            \"version\":\n            {\n                \"distribution\": \"astra\",\n                \"number\": \"0.0.1\",\n                \"lucene_version\": \"9.7.0\"\n            }\n        }\n        \"\"\");\n  }\n\n  /**\n   * Multisearch API\n   *\n   * @see <a\n   *     href=\"https://www.elastic.co/guide/en/elasticsearch/reference/current/search-multi-search.html\">API\n   *     doc</a>\n   */\n  @Post\n  @Blocking\n  @Path(\"/_msearch\")\n  public HttpResponse multiSearch(String postBody) throws Exception {\n    LOG.debug(\"Search request: {}\", postBody);\n\n    CurrentTraceContext currentTraceContext = Tracing.current().currentTraceContext();\n    try (var scope = new StructuredTaskScope<EsSearchResponse>()) {\n      List<StructuredTaskScope.Subtask<EsSearchResponse>> requestSubtasks =\n          openSearchRequest.parseHttpPostBody(postBody).stream()\n              .map((request) -> scope.fork(currentTraceContext.wrap(() -> doSearch(request))))\n              .toList();\n\n      scope.join();\n      SearchResponseMetadata responseMetadata =\n          new SearchResponseMetadata(\n              0,\n              requestSubtasks.stream().map(StructuredTaskScope.Subtask::get).toList(),\n              Map.of(\"traceId\", getTraceId()));\n      return HttpResponse.of(\n          HttpStatus.OK, MediaType.JSON_UTF_8, JsonUtil.writeAsString(responseMetadata));\n    }\n  }\n\n  private EsSearchResponse doSearch(AstraSearch.SearchRequest searchRequest) {\n    ScopedSpan span = Tracing.currentTracer().startScopedSpan(\"ElasticsearchApiService.doSearch\");\n    AstraSearch.SearchResult searchResult = searcher.doSearch(searchRequest);\n\n    span.tag(\"requestDataset\", searchRequest.getDataset());\n    span.tag(\"requestQueryStartTimeEpochMs\", String.valueOf(searchRequest.getStartTimeEpochMs()));\n    span.tag(\"requestQueryEndTimeEpochMs\", String.valueOf(searchRequest.getEndTimeEpochMs()));\n    span.tag(\"requestHowMany\", String.valueOf(searchRequest.getHowMany()));\n    span.tag(\"resultHitsCount\", String.valueOf(searchResult.getHitsCount()));\n    span.tag(\"resultTookMicros\", String.valueOf(searchResult.getTookMicros()));\n    span.tag(\"resultFailedNodes\", String.valueOf(searchResult.getFailedNodes()));\n    span.tag(\"resultTotalNodes\", String.valueOf(searchResult.getTotalNodes()));\n    span.tag(\"resultTotalSnapshots\", String.valueOf(searchResult.getTotalNodes()));\n    span.tag(\n        \"resultSnapshotsWithReplicas\", String.valueOf(searchResult.getSnapshotsWithReplicas()));\n\n    try {\n      HitsMetadata hits = getHits(searchResult);\n      return new EsSearchResponse.Builder()\n          .hits(hits)\n          .aggregations(parseAggregations(searchResult.getInternalAggregations()))\n          .took(Duration.of(searchResult.getTookMicros(), ChronoUnit.MICROS).toMillis())\n          .shardsMetadata(searchResult.getTotalNodes(), searchResult.getFailedNodes())\n          .debugMetadata(Map.of())\n          .status(200)\n          .build();\n    } catch (Exception e) {\n      LOG.error(\"Error fulfilling request for multisearch query\", e);\n      span.error(e);\n      return new EsSearchResponse.Builder()\n          .took(Duration.of(searchResult.getTookMicros(), ChronoUnit.MICROS).toMillis())\n          .shardsMetadata(searchResult.getTotalNodes(), searchResult.getFailedNodes())\n          .status(500)\n          .build();\n    } finally {\n      span.finish();\n    }\n  }\n\n  private JsonNode parseAggregations(ByteString byteInput) throws IOException {\n    InternalAggregation internalAggregations =\n        OpenSearchInternalAggregation.fromByteArray(byteInput.toByteArray());\n    if (internalAggregations != null) {\n      return objectMapper.readTree(internalAggregations.toString());\n    }\n    return null;\n  }\n\n  private String getTraceId() {\n    TraceContext traceContext = Tracing.current().currentTraceContext().get();\n    if (traceContext != null) {\n      return traceContext.traceIdString();\n    }\n    return \"\";\n  }\n\n  private HitsMetadata getHits(AstraSearch.SearchResult searchResult) throws IOException {\n    List<ByteString> hitsByteList = searchResult.getHitsList().asByteStringList();\n    List<SearchResponseHit> responseHits = new ArrayList<>(hitsByteList.size());\n    for (ByteString bytes : hitsByteList) {\n      responseHits.add(SearchResponseHit.fromByteString(bytes));\n    }\n\n    return new HitsMetadata.Builder()\n        .hitsTotal(ImmutableMap.of(\"value\", responseHits.size(), \"relation\", \"eq\"))\n        .hits(responseHits)\n        .build();\n  }\n\n  /**\n   * Mapping API\n   *\n   * @see <a\n   *     href=\"https://www.elastic.co/guide/en/elasticsearch/reference/current/indices-get-mapping.html\">API\n   *     doc</a>\n   */\n```\n\n### Code description\n```Java\n/**\n * Retrieves the mapping of an index or all indices.\n *\n * @param indexName The name of the index to retrieve the mapping for. If not provided, defaults to \"*\".\n * @param startTimeEpochMs The start time in epoch milliseconds to retrieve the schema for. If not provided, defaults to one hour ago.\n * @param endTimeEpochMs The end time in epoch milliseconds to retrieve the schema for. If not provided, defaults to the current time.\n * @return An HttpResponse containing the mapping of the index or all indices, with the properties sorted naturally.\n * @throws IOException If an I/O error occurs.\n */\n```\n\n### Code snippet:\n```Java\n  @Get\n  @Path(\"/:indexName/_mapping\")\n  public HttpResponse mapping(\n      @Param(\"indexName\") Optional<String> indexName,\n      @Param(\"startTimeEpochMs\") Optional<Long> startTimeEpochMs,\n      @Param(\"endTimeEpochMs\") Optional<Long> endTimeEpochMs)\n      throws IOException {\n    // Use a tree map so the results are naturally sorted\n    Map<String, Map<String, String>> propertiesMap = new TreeMap<>();\n\n    // we default the schema search to the last hour if params are not provided\n    AstraSearch.SchemaResult schemaResult =\n        searcher.getSchema(\n            AstraSearch.SchemaRequest.newBuilder()\n                .setDataset(indexName.orElse(\"*\"))\n                .setStartTimeEpochMs(\n                    startTimeEpochMs.orElse(\n                        Instant.now().minus(1, ChronoUnit.HOURS).toEpochMilli()))\n```\n\n### Requirements:\n1. Complete only the missing part of the method.\n2. Only answer the code of the completed part. Do not include any of the provided code snippet.\n3. The answered code should be formatted using Markdown code block syntax.\n4. Do not include any additional text.\n\n### Answer: (use the provided format with backticks)\n", "reference": "                .setEndTimeEpochMs(endTimeEpochMs.orElse(Instant.now().toEpochMilli()))\n                .build());\n\n    Map<String, FieldType> schema = SearchResultUtils.fromSchemaResultProto(schemaResult);\n    schema.forEach((key, value) -> propertiesMap.put(key, Map.of(\"type\", value.getName())));\n\n    // todo - remove this after we add support for a \"date\" type\n    // override the timestamp as a date field for proper autocomplete\n    propertiesMap.put(LogMessage.SystemField.TIME_SINCE_EPOCH.fieldName, Map.of(\"type\", \"date\"));\n\n    return HttpResponse.of(\n        HttpStatus.OK,\n        MediaType.JSON,\n        JsonUtil.writeAsString(\n            ImmutableMap.of(\n                indexName.orElseThrow(),\n                ImmutableMap.of(\"mappings\", ImmutableMap.of(\"properties\", propertiesMap)))));\n  }", "completions": ["  \n```Java\n               .setEndTimeEpochMs(endTimeEpochMs.orElse(Instant.now().toEpochMilli()))\n               .build());\n\n    for (AstraSearch.Schema schema : schemaResult.getSchemasList()) {\n      Map<String, String> fieldMap = new TreeMap<>();\n      for (AstraSearch.Field field : schema.getFieldsList()) {\n        fieldMap.put(field.getName(), field.getType().name());\n      }\n      propertiesMap.put(schema.getName(), fieldMap);\n    }\n\n    return HttpResponse.of(\n        HttpStatus.OK, MediaType.JSON_UTF_8, JsonUtil.writeAsString(propertiesMap));\n  }\n```"]}
{"id": 64, "name": "containsBlob", "prompt": "### Role:\nYou are an experienced Java programmer assistant responsible for code completion tasks. Please intelligently complete the missing parts of the provided code snippet.\n\n### Task:\nBased on the provided code snippet, complete the missing method implementation. Ensure the code is logically correct, syntactically accurate, and follows best coding practices.\n\n### Left context\n```Java\n// Copyright 2017 The Bazel Authors. All rights reserved.\n//\n// Licensed under the Apache License, Version 2.0 (the \"License\");\n// you may not use this file except in compliance with the License.\n// You may obtain a copy of the License at\n//\n//    http://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an \"AS IS\" BASIS,\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n// See the License for the specific language governing permissions and\n// limitations under the License.\n\npackage build.buildfarm.instance.shard;\n\nimport static build.buildfarm.cas.ContentAddressableStorage.NOT_FOUND;\nimport static build.buildfarm.cas.ContentAddressableStorage.OK;\nimport static build.buildfarm.common.Actions.asExecutionStatus;\nimport static build.buildfarm.common.Actions.checkPreconditionFailure;\nimport static build.buildfarm.common.Actions.invalidActionVerboseMessage;\nimport static build.buildfarm.common.Errors.VIOLATION_TYPE_INVALID;\nimport static build.buildfarm.common.Errors.VIOLATION_TYPE_MISSING;\nimport static build.buildfarm.common.config.Backplane.BACKPLANE_TYPE.SHARD;\nimport static build.buildfarm.instance.shard.Util.SHARD_IS_RETRIABLE;\nimport static build.buildfarm.instance.shard.Util.correctMissingBlob;\nimport static com.google.common.base.Preconditions.checkState;\nimport static com.google.common.base.Throwables.throwIfUnchecked;\nimport static com.google.common.util.concurrent.Futures.addCallback;\nimport static com.google.common.util.concurrent.Futures.allAsList;\nimport static com.google.common.util.concurrent.Futures.catching;\nimport static com.google.common.util.concurrent.Futures.catchingAsync;\nimport static com.google.common.util.concurrent.Futures.immediateFailedFuture;\nimport static com.google.common.util.concurrent.Futures.immediateFuture;\nimport static com.google.common.util.concurrent.Futures.transform;\nimport static com.google.common.util.concurrent.Futures.transformAsync;\nimport static com.google.common.util.concurrent.MoreExecutors.directExecutor;\nimport static java.lang.String.format;\nimport static java.util.concurrent.Executors.newSingleThreadExecutor;\nimport static java.util.concurrent.Executors.newSingleThreadScheduledExecutor;\nimport static java.util.concurrent.TimeUnit.MICROSECONDS;\nimport static java.util.concurrent.TimeUnit.MINUTES;\nimport static java.util.concurrent.TimeUnit.SECONDS;\nimport static net.javacrumbs.futureconverter.java8guava.FutureConverter.toCompletableFuture;\nimport static net.javacrumbs.futureconverter.java8guava.FutureConverter.toListenableFuture;\n\nimport build.bazel.remote.execution.v2.Action;\nimport build.bazel.remote.execution.v2.ActionResult;\nimport build.bazel.remote.execution.v2.BatchReadBlobsResponse.Response;\nimport build.bazel.remote.execution.v2.CacheCapabilities;\nimport build.bazel.remote.execution.v2.Command;\nimport build.bazel.remote.execution.v2.Compressor;\nimport build.bazel.remote.execution.v2.Digest;\nimport build.bazel.remote.execution.v2.Directory;\nimport build.bazel.remote.execution.v2.DirectoryNode;\nimport build.bazel.remote.execution.v2.ExecuteOperationMetadata;\nimport build.bazel.remote.execution.v2.ExecuteResponse;\nimport build.bazel.remote.execution.v2.ExecutionPolicy;\nimport build.bazel.remote.execution.v2.ExecutionStage;\nimport build.bazel.remote.execution.v2.Platform;\nimport build.bazel.remote.execution.v2.Platform.Property;\nimport build.bazel.remote.execution.v2.RequestMetadata;\nimport build.bazel.remote.execution.v2.ResultsCachePolicy;\nimport build.bazel.remote.execution.v2.SymlinkAbsolutePathStrategy;\nimport build.buildfarm.actioncache.ActionCache;\nimport build.buildfarm.actioncache.ShardActionCache;\nimport build.buildfarm.backplane.Backplane;\nimport build.buildfarm.common.BuildfarmExecutors;\nimport build.buildfarm.common.CasIndexResults;\nimport build.buildfarm.common.DigestUtil;\nimport build.buildfarm.common.DigestUtil.ActionKey;\nimport build.buildfarm.common.EntryLimitException;\nimport build.buildfarm.common.ExecutionProperties;\nimport build.buildfarm.common.Poller;\nimport build.buildfarm.common.TokenizableIterator;\nimport build.buildfarm.common.TreeIterator;\nimport build.buildfarm.common.TreeIterator.DirectoryEntry;\nimport build.buildfarm.common.Watcher;\nimport build.buildfarm.common.Write;\nimport build.buildfarm.common.config.BuildfarmConfigs;\nimport build.buildfarm.common.grpc.UniformDelegateServerCallStreamObserver;\nimport build.buildfarm.instance.Instance;\nimport build.buildfarm.instance.MatchListener;\nimport build.buildfarm.instance.server.NodeInstance;\nimport build.buildfarm.operations.EnrichedOperation;\nimport build.buildfarm.operations.FindOperationsResults;\nimport build.buildfarm.v1test.BackplaneStatus;\nimport build.buildfarm.v1test.ExecuteEntry;\nimport build.buildfarm.v1test.GetClientStartTimeRequest;\nimport build.buildfarm.v1test.GetClientStartTimeResult;\nimport build.buildfarm.v1test.OperationIteratorToken;\nimport build.buildfarm.v1test.ProfiledQueuedOperationMetadata;\nimport build.buildfarm.v1test.QueueEntry;\nimport build.buildfarm.v1test.QueueStatus;\nimport build.buildfarm.v1test.QueuedOperation;\nimport build.buildfarm.v1test.QueuedOperationMetadata;\nimport build.buildfarm.v1test.Tree;\nimport com.github.benmanes.caffeine.cache.AsyncCache;\nimport com.github.benmanes.caffeine.cache.Cache;\nimport com.github.benmanes.caffeine.cache.Caffeine;\nimport com.google.common.annotations.VisibleForTesting;\nimport com.google.common.base.Stopwatch;\nimport com.google.common.cache.LoadingCache;\nimport com.google.common.collect.ImmutableList;\nimport com.google.common.collect.Iterables;\nimport com.google.common.collect.Sets;\nimport com.google.common.io.BaseEncoding;\nimport com.google.common.util.concurrent.FutureCallback;\nimport com.google.common.util.concurrent.Futures;\nimport com.google.common.util.concurrent.ListenableFuture;\nimport com.google.common.util.concurrent.ListeningExecutorService;\nimport com.google.common.util.concurrent.SettableFuture;\nimport com.google.common.util.concurrent.UncheckedExecutionException;\nimport com.google.longrunning.Operation;\nimport com.google.protobuf.Any;\nimport com.google.protobuf.ByteString;\nimport com.google.protobuf.Duration;\nimport com.google.protobuf.InvalidProtocolBufferException;\nimport com.google.protobuf.Parser;\nimport com.google.protobuf.util.Durations;\nimport com.google.protobuf.util.Timestamps;\nimport com.google.rpc.PreconditionFailure;\nimport io.grpc.Context;\nimport io.grpc.Deadline;\nimport io.grpc.Status;\nimport io.grpc.Status.Code;\nimport io.grpc.StatusException;\nimport io.grpc.StatusRuntimeException;\nimport io.grpc.protobuf.StatusProto;\nimport io.grpc.stub.ServerCallStreamObserver;\nimport io.prometheus.client.Counter;\nimport io.prometheus.client.Gauge;\nimport io.prometheus.client.Histogram;\nimport java.io.IOException;\nimport java.io.InputStream;\nimport java.io.OutputStream;\nimport java.time.Instant;\nimport java.util.AbstractMap;\nimport java.util.ArrayDeque;\nimport java.util.ArrayList;\nimport java.util.Collections;\nimport java.util.Deque;\nimport java.util.HashMap;\nimport java.util.HashSet;\nimport java.util.Iterator;\nimport java.util.List;\nimport java.util.Map;\nimport java.util.Queue;\nimport java.util.Random;\nimport java.util.Set;\nimport java.util.UUID;\nimport java.util.concurrent.BlockingQueue;\nimport java.util.concurrent.CompletableFuture;\nimport java.util.concurrent.ConcurrentLinkedQueue;\nimport java.util.concurrent.ExecutionException;\nimport java.util.concurrent.Executor;\nimport java.util.concurrent.ExecutorService;\nimport java.util.concurrent.LinkedBlockingQueue;\nimport java.util.concurrent.ScheduledExecutorService;\nimport java.util.concurrent.TimeUnit;\nimport java.util.function.BiFunction;\nimport java.util.function.Consumer;\nimport java.util.function.Supplier;\nimport java.util.logging.Level;\nimport java.util.logging.Logger;\nimport java.util.stream.Collectors;\nimport javax.annotation.Nullable;\nimport javax.naming.ConfigurationException;\nimport lombok.extern.java.Log;\n\n@Log\npublic class ServerInstance extends NodeInstance {\n  private static final ListenableFuture<Void> IMMEDIATE_VOID_FUTURE = Futures.immediateFuture(null);\n\n  private static final String TIMEOUT_OUT_OF_BOUNDS =\n      \"A timeout specified is out of bounds with a configured range\";\n\n  private static final int DEFAULT_MAX_LOCAL_ACTION_CACHE_SIZE = 1000000;\n\n  // Prometheus metrics\n  private static final Counter executionSuccess =\n      Counter.build().name(\"execution_success\").help(\"Execution success.\").register();\n  private static final Gauge preQueueSize =\n      Gauge.build().name(\"pre_queue_size\").help(\"Pre queue size.\").register();\n  private static final Counter casHitCounter =\n      Counter.build()\n          .name(\"cas_hit\")\n          .help(\"Number of successful CAS hits from worker-worker.\")\n          .register();\n  private static final Counter casMissCounter =\n      Counter.build().name(\"cas_miss\").help(\"Number of CAS misses from worker-worker.\").register();\n  private static final Counter requeueFailureCounter =\n      Counter.build()\n          .name(\"requeue_failure\")\n          .help(\"Number of operations that failed to requeue.\")\n          .register();\n  private static final Counter queueFailureCounter =\n      Counter.build()\n          .name(\"queue_failure\")\n          .help(\"Number of operations that failed to queue.\")\n          .register();\n  // Metrics about the dispatched operations\n  private static final Gauge dispatchedOperationsSize =\n      Gauge.build()\n          .name(\"dispatched_operations_size\")\n          .help(\"Dispatched operations size.\")\n          .register();\n\n  // Other metrics from the backplane\n  private static final Gauge workerPoolSize =\n      Gauge.build().name(\"worker_pool_size\").help(\"Active worker pool size.\").register();\n  private static final Gauge storageWorkerPoolSize =\n      Gauge.build()\n          .name(\"storage_worker_pool_size\")\n          .help(\"Active storage worker pool size.\")\n          .register();\n  private static final Gauge executeWorkerPoolSize =\n      Gauge.build()\n          .name(\"execute_worker_pool_size\")\n          .help(\"Active execute worker pool size.\")\n          .register();\n  private static final Gauge queueSize =\n      Gauge.build().name(\"queue_size\").labelNames(\"queue_name\").help(\"Queue size.\").register();\n\n  private static final Histogram ioMetric =\n      Histogram.build()\n          .name(\"io_bytes_read\")\n          .buckets(new double[] {10, 1000, 10000, 100000, 1000000, 10000000, 100000000, 1000000000})\n          .help(\"Read I/O (bytes)\")\n          .register();\n\n  private final Runnable onStop;\n  private final long maxEntrySizeBytes;\n  private final Backplane backplane;\n  private final ActionCache actionCache;\n  private final RemoteInputStreamFactory remoteInputStreamFactory;\n  private final com.google.common.cache.LoadingCache<String, Instance> workerStubs;\n  private final Thread dispatchedMonitor;\n  private final Duration maxActionTimeout;\n  private AsyncCache<Digest, Directory> directoryCache;\n  private AsyncCache<Digest, Command> commandCache;\n  private AsyncCache<Digest, Action> digestToActionCache;\n  private Cache<RequestMetadata, Boolean> recentCacheServedExecutions;\n\n  private final Random rand = new Random();\n  private final Writes writes = new Writes(this::writeInstanceSupplier);\n  private final int maxCpu;\n  private final int maxRequeueAttempts;\n\n  private final ListeningExecutorService operationTransformService =\n      BuildfarmExecutors.getTransformServicePool();\n  private final ListeningExecutorService actionCacheFetchService;\n  private final ScheduledExecutorService contextDeadlineScheduler =\n      newSingleThreadScheduledExecutor();\n  private final ExecutorService operationDeletionService = newSingleThreadExecutor();\n  private final BlockingQueue transformTokensQueue = new LinkedBlockingQueue(256);\n  private final boolean useDenyList;\n  private Thread operationQueuer;\n  private boolean stopping = false;\n  private boolean stopped = true;\n  private final Thread prometheusMetricsThread;\n\n  private static BuildfarmConfigs configs = BuildfarmConfigs.getInstance();\n\n  // TODO: move to config\n  private static final Duration queueTimeout = Durations.fromSeconds(60);\n\n  private static Backplane createBackplane(String identifier) throws ConfigurationException {\n    if (configs.getBackplane().getType().equals(SHARD)) {\n      return new RedisShardBackplane(\n          identifier,\n          /* subscribeToBackplane= */ true,\n          configs.getServer().isRunFailsafeOperation(),\n          ServerInstance::stripOperation,\n          ServerInstance::stripQueuedOperation);\n    } else {\n      throw new IllegalArgumentException(\"Shard Backplane not set in config\");\n    }\n  }\n\n  public ServerInstance(String name, String identifier, DigestUtil digestUtil, Runnable onStop)\n      throws InterruptedException, ConfigurationException {\n    this(\n        name,\n        digestUtil,\n        createBackplane(identifier),\n        onStop,\n        /* actionCacheFetchService= */ BuildfarmExecutors.getActionCacheFetchServicePool());\n  }\n\n  private ServerInstance(\n      String name,\n      DigestUtil digestUtil,\n      Backplane backplane,\n      Runnable onStop,\n      ListeningExecutorService actionCacheFetchService)\n      throws InterruptedException {\n    this(\n        name,\n        digestUtil,\n        backplane,\n        new ShardActionCache(\n            DEFAULT_MAX_LOCAL_ACTION_CACHE_SIZE, backplane, actionCacheFetchService),\n        configs.getServer().isRunDispatchedMonitor(),\n        configs.getServer().getDispatchedMonitorIntervalSeconds(),\n        configs.getServer().isRunOperationQueuer(),\n        configs.getMaxEntrySizeBytes(),\n        configs.getServer().getMaxCpu(),\n        configs.getServer().getMaxRequeueAttempts(),\n        Duration.newBuilder().setSeconds(configs.getMaximumActionTimeout()).build(),\n        configs.getServer().isUseDenyList(),\n        onStop,\n        WorkerStubs.create(\n            digestUtil,\n            Duration.newBuilder().setSeconds(configs.getServer().getGrpcTimeout()).build()),\n        actionCacheFetchService,\n        configs.getServer().isEnsureOutputsPresent());\n  }\n\n  void initializeCaches() {\n    directoryCache =\n        Caffeine.newBuilder()\n            .newBuilder()\n            .maximumSize(configs.getServer().getCaches().getDirectoryCacheMaxEntries())\n            .buildAsync();\n    commandCache =\n        Caffeine.newBuilder()\n            .newBuilder()\n            .maximumSize(configs.getServer().getCaches().getCommandCacheMaxEntries())\n            .buildAsync();\n    digestToActionCache =\n        Caffeine.newBuilder()\n            .newBuilder()\n            .maximumSize(configs.getServer().getCaches().getDigestToActionCacheMaxEntries())\n            .buildAsync();\n    recentCacheServedExecutions =\n        Caffeine.newBuilder()\n            .newBuilder()\n            .maximumSize(configs.getServer().getCaches().getRecentServedExecutionsCacheMaxEntries())\n            .build();\n  }\n\n  public ServerInstance(\n      String name,\n      DigestUtil digestUtil,\n      Backplane backplane,\n      ActionCache actionCache,\n      boolean runDispatchedMonitor,\n      int dispatchedMonitorIntervalSeconds,\n      boolean runOperationQueuer,\n      long maxEntrySizeBytes,\n      int maxCpu,\n      int maxRequeueAttempts,\n      Duration maxActionTimeout,\n      boolean useDenyList,\n      Runnable onStop,\n      LoadingCache<String, Instance> workerStubs,\n      ListeningExecutorService actionCacheFetchService,\n      boolean ensureOutputsPresent) {\n    super(\n        name,\n        digestUtil,\n        /* contentAddressableStorage= */ null,\n        /* actionCache= */ actionCache,\n        /* outstandingOperations= */ null,\n        /* completedOperations= */ null,\n        /* activeBlobWrites= */ null,\n        ensureOutputsPresent);\n    this.backplane = backplane;\n    this.actionCache = actionCache;\n    this.workerStubs = workerStubs;\n    this.onStop = onStop;\n    this.maxEntrySizeBytes = maxEntrySizeBytes;\n    this.maxCpu = maxCpu;\n    this.maxRequeueAttempts = maxRequeueAttempts;\n    this.maxActionTimeout = maxActionTimeout;\n    this.useDenyList = useDenyList;\n    this.actionCacheFetchService = actionCacheFetchService;\n    backplane.setOnUnsubscribe(this::stop);\n\n    initializeCaches();\n\n    remoteInputStreamFactory =\n        new RemoteInputStreamFactory(\n            backplane, rand, workerStubs, this::removeMalfunctioningWorker);\n\n    if (runDispatchedMonitor) {\n      dispatchedMonitor =\n          new Thread(\n              new DispatchedMonitor(\n                  backplane, this::requeueOperation, dispatchedMonitorIntervalSeconds));\n    } else {\n      dispatchedMonitor = null;\n    }\n\n    if (runOperationQueuer) {\n      operationQueuer =\n          new Thread(\n              new Runnable() {\n                final Stopwatch stopwatch = Stopwatch.createUnstarted();\n\n                ListenableFuture<Void> iterate() throws IOException, InterruptedException {\n                  ensureCanQueue(stopwatch); // wait for transition to canQueue state\n                  long canQueueUSecs = stopwatch.elapsed(MICROSECONDS);\n                  stopwatch.stop();\n                  ExecuteEntry executeEntry = backplane.deprequeueOperation();\n                  stopwatch.start();\n                  if (executeEntry == null) {\n                    log.log(Level.SEVERE, \"OperationQueuer: Got null from deprequeue...\");\n                    return immediateFuture(null);\n                  }\n                  // half the watcher expiry, need to expose this from backplane\n                  Poller poller = new Poller(Durations.fromSeconds(5));\n                  String operationName = executeEntry.getOperationName();\n                  poller.resume(\n                      () -> {\n                        try {\n                          backplane.queueing(executeEntry.getOperationName());\n                        } catch (IOException e) {\n                          if (!stopping && !stopped) {\n                            log.log(\n                                Level.SEVERE,\n                                format(\"error polling %s for queuing\", operationName),\n                                e);\n                          }\n                          // mostly ignore, we will be stopped at some point later\n                        }\n                        return !stopping && !stopped;\n                      },\n                      () -> {},\n                      Deadline.after(5, MINUTES));\n                  try {\n                    log.log(Level.FINER, \"queueing \" + operationName);\n                    ListenableFuture<Void> queueFuture = queue(executeEntry, poller, queueTimeout);\n                    addCallback(\n                        queueFuture,\n                        new FutureCallback<Void>() {\n                          @Override\n                          public void onSuccess(Void result) {\n                            log.log(Level.FINER, \"successfully queued \" + operationName);\n                            // nothing\n                          }\n\n                          @Override\n                          public void onFailure(Throwable t) {\n                            queueFailureCounter.inc();\n                            log.log(Level.SEVERE, \"error queueing \" + operationName, t);\n                          }\n                        },\n                        operationTransformService);\n                    long operationTransformDispatchUSecs =\n                        stopwatch.elapsed(MICROSECONDS) - canQueueUSecs;\n                    log.log(\n                        Level.FINER,\n                        format(\n                            \"OperationQueuer: Dispatched To Transform %s: %dus in canQueue, %dus in\"\n                                + \" transform dispatch\",\n                            operationName, canQueueUSecs, operationTransformDispatchUSecs));\n                    return queueFuture;\n                  } catch (Throwable t) {\n                    poller.pause();\n                    queueFailureCounter.inc();\n                    log.log(Level.SEVERE, \"error queueing \" + operationName, t);\n                    return immediateFuture(null);\n                  }\n                }\n\n                @Override\n                public void run() {\n                  log.log(Level.FINER, \"OperationQueuer: Running\");\n                  try {\n                    while (transformTokensQueue.offer(new Object(), 5, MINUTES)) {\n                      stopwatch.start();\n                      try {\n                        iterate()\n                            .addListener(\n                                () -> {\n                                  try {\n                                    transformTokensQueue.take();\n                                  } catch (InterruptedException e) {\n                                    log.log(\n                                        Level.SEVERE,\n                                        \"interrupted while returning transform token\",\n                                        e);\n                                  }\n                                },\n                                operationTransformService);\n                      } catch (IOException e) {\n                        transformTokensQueue.take();\n                        // problems interacting with backplane\n                      } finally {\n                        stopwatch.reset();\n                      }\n                    }\n                    log.severe(\"OperationQueuer: Transform lease token timed out\");\n                  } catch (InterruptedException e) {\n                    // treat with exit\n                    operationQueuer = null;\n                    return;\n                  } catch (Exception t) {\n                    log.log(Level.SEVERE, \"OperationQueuer: fatal exception during iteration\", t);\n                  } finally {\n                    log.log(Level.FINER, \"OperationQueuer: Exiting\");\n                  }\n                  operationQueuer = null;\n                  try {\n                    stop();\n                  } catch (InterruptedException e) {\n                    log.log(Level.SEVERE, \"interrupted while stopping instance \" + getName(), e);\n                  }\n                }\n              });\n    } else {\n      operationQueuer = null;\n    }\n\n    prometheusMetricsThread =\n        new Thread(\n            () -> {\n              while (!Thread.currentThread().isInterrupted()) {\n                try {\n                  TimeUnit.SECONDS.sleep(30);\n                  BackplaneStatus backplaneStatus = backplaneStatus();\n                  workerPoolSize.set(backplaneStatus.getActiveWorkersCount());\n                  executeWorkerPoolSize.set(backplaneStatus.getActiveExecuteWorkersCount());\n                  storageWorkerPoolSize.set(backplaneStatus.getActiveStorageWorkersCount());\n                  dispatchedOperationsSize.set(backplaneStatus.getDispatchedSize());\n                  preQueueSize.set(backplaneStatus.getPrequeue().getSize());\n                  updateQueueSizes(backplaneStatus.getOperationQueue().getProvisionsList());\n                } catch (InterruptedException e) {\n                  Thread.currentThread().interrupt();\n                  break;\n                } catch (Exception e) {\n                  log.log(Level.SEVERE, \"Could not update RedisShardBackplane metrics\", e);\n                }\n              }\n            },\n            \"Prometheus Metrics Collector\");\n  }\n\n  private void updateQueueSizes(List<QueueStatus> queues) {\n    if (queueSize != null) {\n      for (QueueStatus queueStatus : queues) {\n        queueSize.labels(queueStatus.getName()).set(queueStatus.getSize());\n      }\n    }\n  }\n\n  private void ensureCanQueue(Stopwatch stopwatch) throws IOException, InterruptedException {\n    while (!backplane.canQueue()) {\n      stopwatch.stop();\n      TimeUnit.MILLISECONDS.sleep(100);\n      stopwatch.start();\n    }\n  }\n\n  @Override\n  public void start(String publicName) {\n    stopped = false;\n    try {\n      backplane.start(publicName);\n    } catch (IOException e) {\n      throw new RuntimeException(e);\n    } catch (RuntimeException e) {\n      try {\n        stop();\n      } catch (InterruptedException intEx) {\n        e.addSuppressed(intEx);\n      }\n      throw e;\n    }\n    if (dispatchedMonitor != null) {\n      dispatchedMonitor.start();\n    }\n    if (operationQueuer != null) {\n      operationQueuer.start();\n    }\n\n    if (prometheusMetricsThread != null) {\n      prometheusMetricsThread.start();\n    }\n  }\n\n  @Override\n  public void stop() throws InterruptedException {\n    if (stopped || stopping) {\n      return;\n    }\n    stopping = true;\n    log.log(Level.FINER, format(\"Instance %s is stopping\", getName()));\n    if (operationQueuer != null) {\n      operationQueuer.interrupt();\n      operationQueuer.join();\n    }\n    if (dispatchedMonitor != null) {\n      dispatchedMonitor.interrupt();\n      dispatchedMonitor.join();\n    }\n    if (prometheusMetricsThread != null) {\n      prometheusMetricsThread.interrupt();\n    }\n    contextDeadlineScheduler.shutdown();\n    operationDeletionService.shutdown();\n    operationTransformService.shutdown();\n    actionCacheFetchService.shutdown();\n    onStop.run();\n    backplane.stop();\n    if (!contextDeadlineScheduler.awaitTermination(10, SECONDS)) {\n      log.log(\n          Level.SEVERE,\n          \"Could not shut down operation deletion service, some operations may be zombies\");\n    }\n    if (!operationDeletionService.awaitTermination(10, SECONDS)) {\n      log.log(\n          Level.SEVERE,\n          \"Could not shut down operation deletion service, some operations may be zombies\");\n    }\n    operationDeletionService.shutdownNow();\n    if (!operationTransformService.awaitTermination(10, SECONDS)) {\n      log.log(Level.SEVERE, \"Could not shut down operation transform service\");\n    }\n    operationTransformService.shutdownNow();\n    if (!actionCacheFetchService.awaitTermination(10, SECONDS)) {\n      log.log(Level.SEVERE, \"Could not shut down action cache fetch service\");\n    }\n    actionCacheFetchService.shutdownNow();\n    workerStubs.invalidateAll();\n    log.log(Level.FINER, format(\"Instance %s has been stopped\", getName()));\n    stopping = false;\n    stopped = true;\n  }\n\n```\n\n### Code description\n```Java\n/**\n * Checks if a blob with the given digest is contained in the storage.\n * If the size of the blob is not known, it retrieves the blob's size from the storage.\n * If the blob is not found, it returns false.\n * If the blob is found, it merges the blob's information into the result and returns true.\n * If the blob is found and its size is known, it returns true if the blob is present and false otherwise.\n *\n * @param digest The digest of the blob to check.\n * @param result A builder for the digest of the blob.\n * @param requestMetadata Metadata for the request.\n * @return True if the blob is present and false otherwise.\n * @throws InterruptedException If the thread is interrupted while waiting for the blob to be found.\n */\n```\n\n### Code snippet:\n```Java\n  @Override\n  public boolean containsBlob(Digest digest, Digest.Builder result, RequestMetadata requestMetadata)\n      throws InterruptedException {\n    Iterable<Digest> missingOrPopulated;\n    try {\n      missingOrPopulated = findMissingBlobs(ImmutableList.of(digest), requestMetadata).get();\n    } catch (ExecutionException e) {\n      throwIfUnchecked(e.getCause());\n      throw new RuntimeException(e.getCause());\n    }\n```\n\n### Requirements:\n1. Complete only the missing part of the method.\n2. Only answer the code of the completed part. Do not include any of the provided code snippet.\n3. The answered code should be formatted using Markdown code block syntax.\n4. Do not include any additional text.\n\n### Answer: (use the provided format with backticks)\n", "reference": "    if (digest.getSizeBytes() == -1) {\n      Digest responseDigest = Iterables.getOnlyElement(missingOrPopulated);\n      if (responseDigest.getSizeBytes() == -1) {\n        return false;\n      }\n      result.mergeFrom(responseDigest);\n      return true;\n    }\n    return Iterables.isEmpty(missingOrPopulated);\n  }", "completions": ["  \n```\n  @Override\n  public boolean containsBlob(Digest digest, Digest.Builder result, RequestMetadata requestMetadata)\n      throws InterruptedException {\n    Iterable<Digest> missingOrPopulated;\n    try {\n      missingOrPopulated = findMissingBlobs(ImmutableList.of(digest), requestMetadata).get();\n    } catch (ExecutionException e) {\n      throwIfUnchecked(e.getCause());\n      throw new RuntimeException(e.getCause());\n    }\n    if (Iterables.isEmpty(missingOrPopulated)) {\n      return true;\n    }\n    if (result!= null) {\n      result.mergeFrom(digest);\n    }\n    return false;\n  }\n```"]}
{"id": 65, "name": "fromExpirationTime", "prompt": "### Role:\nYou are an experienced Java programmer assistant responsible for code completion tasks. Please intelligently complete the missing parts of the provided code snippet.\n\n### Task:\nBased on the provided code snippet, complete the missing method implementation. Ensure the code is logically correct, syntactically accurate, and follows best coding practices.\n\n### Left context\n```Java\n/*\n * Copyright 2021 Google LLC\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     https://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\npackage com.google.apphosting.runtime.jetty;\n\nimport com.google.common.base.Ascii;\nimport com.google.common.base.Splitter;\nimport com.google.common.collect.ImmutableMap;\nimport com.google.common.flogger.GoogleLogger;\nimport java.time.Duration;\nimport java.time.temporal.ChronoUnit;\nimport java.time.temporal.TemporalUnit;\nimport java.util.regex.Pattern;\n\n/**\n * Wrapper for cache-control header value strings. Also includes logic to parse expiration time\n * strings provided in application config files.\n */\npublic final class CacheControlHeader {\n  private static final GoogleLogger logger = GoogleLogger.forEnclosingClass();\n  private static final String DEFAULT_BASE_VALUE = \"public, max-age=\";\n  // Default max age is 10 minutes, per GAE documentation\n  private static final String DEFAULT_MAX_AGE = \"600\";\n\n  private static final ImmutableMap<String, TemporalUnit> EXPIRATION_TIME_UNITS =\n      ImmutableMap.of(\n          \"s\", ChronoUnit.SECONDS,\n          \"m\", ChronoUnit.MINUTES,\n          \"h\", ChronoUnit.HOURS,\n          \"d\", ChronoUnit.DAYS);\n\n  private final String value;\n\n  private CacheControlHeader(String value) {\n    this.value = value;\n  }\n\n  public static CacheControlHeader getDefaultInstance() {\n    return new CacheControlHeader(DEFAULT_BASE_VALUE + DEFAULT_MAX_AGE);\n  }\n\n  /**\n   * Parse formatted expiration time (e.g., \"1d 2h 3m\") and convert to seconds. If there is no\n   * expiration time set, avoid setting max age parameter.\n   */\n```\n\n### Code description\n```Java\n/**\n * Parses an expiration time string into a CacheControlHeader object.\n * The expiration time string is expected to be in the format of a number followed by a time unit character (s, m, h, d).\n * Multiple time units can be separated by spaces.\n * If the expiration time string is invalid or null, a warning message is logged and a default CacheControlHeader object is returned.\n *\n * @param expirationTime The expiration time string to parse.\n * @return A CacheControlHeader object with the parsed expiration time.\n */\n```\n\n### Code snippet:\n```Java\n  public static CacheControlHeader fromExpirationTime(String expirationTime) {\n    String maxAge = DEFAULT_MAX_AGE;\n\n    if (expirationTime != null) {\n      if (expirationTimeIsValid(expirationTime)) {\n        Duration totalTime = Duration.ZERO;\n        for (String timeString : Splitter.on(\" \").split(expirationTime)) {\n          String timeUnitShort = Ascii.toLowerCase(timeString.substring(timeString.length() - 1));\n          TemporalUnit timeUnit = EXPIRATION_TIME_UNITS.get(timeUnitShort);\n          String timeValue = timeString.substring(0, timeString.length() - 1);\n          totalTime = totalTime.plus(Long.parseLong(timeValue), timeUnit);\n        }\n```\n\n### Requirements:\n1. Complete only the missing part of the method.\n2. Only answer the code of the completed part. Do not include any of the provided code snippet.\n3. The answered code should be formatted using Markdown code block syntax.\n4. Do not include any additional text.\n\n### Answer: (use the provided format with backticks)\n", "reference": "        maxAge = String.valueOf(totalTime.getSeconds());\n      } else {\n          logger.atWarning().log(\n            \"Failed to parse expiration time: \\\"%s\\\". Using default value instead.\",\n            expirationTime\n        );\n      }\n    }\n\n    String output = DEFAULT_BASE_VALUE + maxAge;\n    return new CacheControlHeader(output);\n  }", "completions": ["  ```\n  if (totalTime.isNegative()) {\n    logger.atWarning().log(\"Invalid expiration time: %s\", expirationTime);\n    return getDefaultInstance();\n  }\n  maxAge = Long.toString(totalTime.getSeconds());\n  return new CacheControlHeader(DEFAULT_BASE_VALUE + maxAge);\n```"]}
{"id": 66, "name": "listFiles", "prompt": "### Role:\nYou are an experienced Java programmer assistant responsible for code completion tasks. Please intelligently complete the missing parts of the provided code snippet.\n\n### Task:\nBased on the provided code snippet, complete the missing method implementation. Ensure the code is logically correct, syntactically accurate, and follows best coding practices.\n\n### Left context\n```Java\npackage com.slack.astra.blobfs.s3;\n\nimport com.google.common.base.Preconditions;\nimport com.google.common.collect.ImmutableList;\nimport com.slack.astra.blobfs.BlobFs;\nimport com.slack.astra.blobfs.BlobFsConfig;\nimport com.slack.astra.proto.config.AstraConfigs;\nimport java.io.File;\nimport java.io.IOException;\nimport java.io.InputStream;\nimport java.io.UnsupportedEncodingException;\nimport java.net.URI;\nimport java.net.URISyntaxException;\nimport java.net.URLEncoder;\nimport java.nio.charset.StandardCharsets;\nimport java.nio.file.Path;\nimport java.nio.file.Paths;\nimport java.util.HashMap;\nimport java.util.List;\nimport java.util.Map;\nimport org.apache.commons.io.FileUtils;\nimport org.slf4j.Logger;\nimport org.slf4j.LoggerFactory;\nimport software.amazon.awssdk.auth.credentials.AwsBasicCredentials;\nimport software.amazon.awssdk.auth.credentials.AwsCredentialsProvider;\nimport software.amazon.awssdk.auth.credentials.DefaultCredentialsProvider;\nimport software.amazon.awssdk.auth.credentials.StaticCredentialsProvider;\nimport software.amazon.awssdk.core.SdkSystemSetting;\nimport software.amazon.awssdk.core.sync.RequestBody;\nimport software.amazon.awssdk.core.sync.ResponseTransformer;\nimport software.amazon.awssdk.regions.Region;\nimport software.amazon.awssdk.services.s3.S3Client;\nimport software.amazon.awssdk.services.s3.S3ClientBuilder;\nimport software.amazon.awssdk.services.s3.model.CopyObjectRequest;\nimport software.amazon.awssdk.services.s3.model.CopyObjectResponse;\nimport software.amazon.awssdk.services.s3.model.DeleteObjectRequest;\nimport software.amazon.awssdk.services.s3.model.DeleteObjectResponse;\nimport software.amazon.awssdk.services.s3.model.GetObjectRequest;\nimport software.amazon.awssdk.services.s3.model.HeadObjectRequest;\nimport software.amazon.awssdk.services.s3.model.HeadObjectResponse;\nimport software.amazon.awssdk.services.s3.model.ListObjectsV2Request;\nimport software.amazon.awssdk.services.s3.model.ListObjectsV2Response;\nimport software.amazon.awssdk.services.s3.model.MetadataDirective;\nimport software.amazon.awssdk.services.s3.model.NoSuchKeyException;\nimport software.amazon.awssdk.services.s3.model.PutObjectRequest;\nimport software.amazon.awssdk.services.s3.model.PutObjectResponse;\nimport software.amazon.awssdk.services.s3.model.S3Exception;\nimport software.amazon.awssdk.services.s3.model.S3Object;\n\n/**\n * @see S3CrtBlobFs\n */\n@Deprecated\npublic class S3BlobFs extends BlobFs {\n  public static final String S3_SCHEME = \"s3://\";\n  private static final Logger LOG = LoggerFactory.getLogger(S3BlobFs.class);\n  private static final String DELIMITER = \"/\";\n  private static final int LIST_MAX_KEYS = 2500;\n  private S3Client s3Client;\n\n  public S3BlobFs(S3Client s3Client) {\n    this.s3Client = s3Client;\n  }\n\n  static boolean isNullOrEmpty(String target) {\n    return target == null || \"\".equals(target);\n  }\n\n  public static S3Client initS3Client(AstraConfigs.S3Config config) {\n    Preconditions.checkArgument(!isNullOrEmpty(config.getS3Region()));\n    String region = config.getS3Region();\n\n    AwsCredentialsProvider awsCredentialsProvider;\n    try {\n\n      if (!isNullOrEmpty(config.getS3AccessKey()) && !isNullOrEmpty(config.getS3SecretKey())) {\n        String accessKey = config.getS3AccessKey();\n        String secretKey = config.getS3SecretKey();\n        AwsBasicCredentials awsBasicCredentials = AwsBasicCredentials.create(accessKey, secretKey);\n        awsCredentialsProvider = StaticCredentialsProvider.create(awsBasicCredentials);\n      } else {\n        awsCredentialsProvider = DefaultCredentialsProvider.create();\n      }\n\n      // TODO: Remove hard coded HTTP IMPL property setting by only having 1 http client on the\n      // classpath.\n      System.setProperty(\n          SdkSystemSetting.SYNC_HTTP_SERVICE_IMPL.property(),\n          \"software.amazon.awssdk.http.apache.ApacheSdkHttpService\");\n      S3ClientBuilder s3ClientBuilder =\n          S3Client.builder().region(Region.of(region)).credentialsProvider(awsCredentialsProvider);\n      if (!isNullOrEmpty(config.getS3EndPoint())) {\n        String endpoint = config.getS3EndPoint();\n        try {\n          s3ClientBuilder.endpointOverride(new URI(endpoint));\n        } catch (URISyntaxException e) {\n          throw new RuntimeException(e);\n        }\n      }\n      return s3ClientBuilder.build();\n    } catch (S3Exception e) {\n      throw new RuntimeException(\"Could not initialize S3blobFs\", e);\n    }\n  }\n\n  @Override\n  public void init(BlobFsConfig config) {\n    // Not sure if this interface works for a library. So on ice for now.\n    throw new UnsupportedOperationException(\n        \"This class doesn't support initialization via blobfsconfig.\");\n  }\n\n  private HeadObjectResponse getS3ObjectMetadata(URI uri) throws IOException {\n    URI base = getBase(uri);\n    String path = sanitizePath(base.relativize(uri).getPath());\n    HeadObjectRequest headObjectRequest =\n        HeadObjectRequest.builder().bucket(uri.getHost()).key(path).build();\n\n    return s3Client.headObject(headObjectRequest);\n  }\n\n  private boolean isPathTerminatedByDelimiter(URI uri) {\n    return uri.getPath().endsWith(DELIMITER);\n  }\n\n  private String normalizeToDirectoryPrefix(URI uri) throws IOException {\n    Preconditions.checkNotNull(uri, \"uri is null\");\n    URI strippedUri = getBase(uri).relativize(uri);\n    if (isPathTerminatedByDelimiter(strippedUri)) {\n      return sanitizePath(strippedUri.getPath());\n    }\n    return sanitizePath(strippedUri.getPath() + DELIMITER);\n  }\n\n  private URI normalizeToDirectoryUri(URI uri) throws IOException {\n    if (isPathTerminatedByDelimiter(uri)) {\n      return uri;\n    }\n    try {\n      return new URI(uri.getScheme(), uri.getHost(), sanitizePath(uri.getPath() + DELIMITER), null);\n    } catch (URISyntaxException e) {\n      throw new IOException(e);\n    }\n  }\n\n  private String sanitizePath(String path) {\n    path = path.replaceAll(DELIMITER + \"+\", DELIMITER);\n    if (path.startsWith(DELIMITER) && !path.equals(DELIMITER)) {\n      path = path.substring(1);\n    }\n    return path;\n  }\n\n  private URI getBase(URI uri) throws IOException {\n    try {\n      return new URI(uri.getScheme(), uri.getHost(), null, null);\n    } catch (URISyntaxException e) {\n      throw new IOException(e);\n    }\n  }\n\n  private boolean existsFile(URI uri) throws IOException {\n    try {\n      URI base = getBase(uri);\n      String path = sanitizePath(base.relativize(uri).getPath());\n      HeadObjectRequest headObjectRequest =\n          HeadObjectRequest.builder().bucket(uri.getHost()).key(path).build();\n\n      s3Client.headObject(headObjectRequest);\n      return true;\n    } catch (NoSuchKeyException e) {\n      return false;\n    } catch (S3Exception e) {\n      throw new IOException(e);\n    }\n  }\n\n  private boolean isEmptyDirectory(URI uri) throws IOException {\n    if (!isDirectory(uri)) {\n      return false;\n    }\n    String prefix = normalizeToDirectoryPrefix(uri);\n    boolean isEmpty = true;\n    ListObjectsV2Response listObjectsV2Response;\n    ListObjectsV2Request.Builder listObjectsV2RequestBuilder =\n        ListObjectsV2Request.builder().bucket(uri.getHost());\n\n    if (!prefix.equals(DELIMITER)) {\n      listObjectsV2RequestBuilder = listObjectsV2RequestBuilder.prefix(prefix);\n    }\n\n    ListObjectsV2Request listObjectsV2Request = listObjectsV2RequestBuilder.build();\n    listObjectsV2Response = s3Client.listObjectsV2(listObjectsV2Request);\n\n    for (S3Object s3Object : listObjectsV2Response.contents()) {\n      if (s3Object.key().equals(prefix)) {\n        continue;\n      } else {\n        isEmpty = false;\n        break;\n      }\n    }\n    return isEmpty;\n  }\n\n  private boolean copyFile(URI srcUri, URI dstUri) throws IOException {\n    try {\n      String encodedUrl = null;\n      try {\n        encodedUrl =\n            URLEncoder.encode(\n                srcUri.getHost() + srcUri.getPath(), StandardCharsets.UTF_8.toString());\n      } catch (UnsupportedEncodingException e) {\n        throw new RuntimeException(e);\n      }\n\n      String dstPath = sanitizePath(dstUri.getPath());\n      CopyObjectRequest copyReq =\n          CopyObjectRequest.builder()\n              .copySource(encodedUrl)\n              .destinationBucket(dstUri.getHost())\n              .destinationKey(dstPath)\n              .build();\n\n      CopyObjectResponse copyObjectResponse = s3Client.copyObject(copyReq);\n      return copyObjectResponse.sdkHttpResponse().isSuccessful();\n    } catch (S3Exception e) {\n      throw new IOException(e);\n    }\n  }\n\n  @Override\n  public boolean mkdir(URI uri) throws IOException {\n    LOG.debug(\"mkdir {}\", uri);\n    try {\n      Preconditions.checkNotNull(uri, \"uri is null\");\n      String path = normalizeToDirectoryPrefix(uri);\n      // Bucket root directory already exists and cannot be created\n      if (path.equals(DELIMITER)) {\n        return true;\n      }\n\n      PutObjectRequest putObjectRequest =\n          PutObjectRequest.builder().bucket(uri.getHost()).key(path).build();\n\n      PutObjectResponse putObjectResponse =\n          s3Client.putObject(putObjectRequest, RequestBody.fromBytes(new byte[0]));\n\n      return putObjectResponse.sdkHttpResponse().isSuccessful();\n    } catch (Throwable t) {\n      throw new IOException(t);\n    }\n  }\n\n  @Override\n  public boolean delete(URI segmentUri, boolean forceDelete) throws IOException {\n    LOG.debug(\"Deleting uri {} force {}\", segmentUri, forceDelete);\n    try {\n      if (isDirectory(segmentUri)) {\n        if (!forceDelete) {\n          Preconditions.checkState(\n              isEmptyDirectory(segmentUri),\n              \"ForceDelete flag is not set and directory '%s' is not empty\",\n              segmentUri);\n        }\n        String prefix = normalizeToDirectoryPrefix(segmentUri);\n        ListObjectsV2Response listObjectsV2Response;\n        ListObjectsV2Request.Builder listObjectsV2RequestBuilder =\n            ListObjectsV2Request.builder().bucket(segmentUri.getHost());\n\n        if (prefix.equals(DELIMITER)) {\n          ListObjectsV2Request listObjectsV2Request = listObjectsV2RequestBuilder.build();\n          listObjectsV2Response = s3Client.listObjectsV2(listObjectsV2Request);\n        } else {\n          ListObjectsV2Request listObjectsV2Request =\n              listObjectsV2RequestBuilder.prefix(prefix).build();\n          listObjectsV2Response = s3Client.listObjectsV2(listObjectsV2Request);\n        }\n        boolean deleteSucceeded = true;\n        for (S3Object s3Object : listObjectsV2Response.contents()) {\n          DeleteObjectRequest deleteObjectRequest =\n              DeleteObjectRequest.builder()\n                  .bucket(segmentUri.getHost())\n                  .key(s3Object.key())\n                  .build();\n\n          DeleteObjectResponse deleteObjectResponse = s3Client.deleteObject(deleteObjectRequest);\n\n          deleteSucceeded &= deleteObjectResponse.sdkHttpResponse().isSuccessful();\n        }\n        return deleteSucceeded;\n      } else {\n        String prefix = sanitizePath(segmentUri.getPath());\n        DeleteObjectRequest deleteObjectRequest =\n            DeleteObjectRequest.builder().bucket(segmentUri.getHost()).key(prefix).build();\n\n        DeleteObjectResponse deleteObjectResponse = s3Client.deleteObject(deleteObjectRequest);\n\n        return deleteObjectResponse.sdkHttpResponse().isSuccessful();\n      }\n    } catch (NoSuchKeyException e) {\n      return false;\n    } catch (S3Exception e) {\n      throw e;\n    } catch (Exception e) {\n      throw new IOException(e);\n    }\n  }\n\n  @Override\n  public boolean doMove(URI srcUri, URI dstUri) throws IOException {\n    if (copy(srcUri, dstUri)) {\n      return delete(srcUri, true);\n    }\n    return false;\n  }\n\n  @Override\n  public boolean copy(URI srcUri, URI dstUri) throws IOException {\n    LOG.debug(\"Copying uri {} to uri {}\", srcUri, dstUri);\n    Preconditions.checkState(exists(srcUri), \"Source URI '%s' does not exist\", srcUri);\n    if (srcUri.equals(dstUri)) {\n      return true;\n    }\n    if (!isDirectory(srcUri)) {\n      delete(dstUri, true);\n      return copyFile(srcUri, dstUri);\n    }\n    dstUri = normalizeToDirectoryUri(dstUri);\n    Path srcPath = Paths.get(srcUri.getPath());\n    try {\n      boolean copySucceeded = true;\n      for (String filePath : listFiles(srcUri, true)) {\n        URI srcFileURI = URI.create(filePath);\n        String directoryEntryPrefix = srcFileURI.getPath();\n        URI src = new URI(srcUri.getScheme(), srcUri.getHost(), directoryEntryPrefix, null);\n        String relativeSrcPath = srcPath.relativize(Paths.get(directoryEntryPrefix)).toString();\n        String dstPath = dstUri.resolve(relativeSrcPath).getPath();\n        URI dst = new URI(dstUri.getScheme(), dstUri.getHost(), dstPath, null);\n        copySucceeded &= copyFile(src, dst);\n      }\n      return copySucceeded;\n    } catch (URISyntaxException e) {\n      throw new IOException(e);\n    }\n  }\n\n  @Override\n  public boolean exists(URI fileUri) throws IOException {\n    try {\n      if (isDirectory(fileUri)) {\n        return true;\n      }\n      if (isPathTerminatedByDelimiter(fileUri)) {\n        return false;\n      }\n      return existsFile(fileUri);\n    } catch (NoSuchKeyException e) {\n      return false;\n    }\n  }\n\n  @Override\n  public long length(URI fileUri) throws IOException {\n    try {\n      Preconditions.checkState(!isPathTerminatedByDelimiter(fileUri), \"URI is a directory\");\n      HeadObjectResponse s3ObjectMetadata = getS3ObjectMetadata(fileUri);\n      Preconditions.checkState((s3ObjectMetadata != null), \"File '%s' does not exist\", fileUri);\n      if (s3ObjectMetadata.contentLength() == null) {\n        return 0;\n      }\n      return s3ObjectMetadata.contentLength();\n    } catch (Throwable t) {\n      throw new IOException(t);\n    }\n  }\n\n```\n\n### Code description\n```Java\n/**\n * Description: This method lists files from a specified URI in an S3 bucket.\n * It uses the AWS S3 SDK to make requests to the S3 service. The method is\n * designed to handle large numbers of files by using pagination and continuation\n * tokens. It also supports recursive listing of files within a directory.\n *\n * @param fileUri The URI of the file or directory to list files from.\n * @param recursive A boolean flag indicating whether to list files recursively.\n * @return An array of strings representing the URIs of the listed files.\n * @throws IOException If an error occurs while communicating with the S3 service.\n */\n```\n\n### Code snippet:\n```Java\n  @Override\n  public String[] listFiles(URI fileUri, boolean recursive) throws IOException {\n    try {\n      ImmutableList.Builder<String> builder = ImmutableList.builder();\n      String continuationToken = null;\n      boolean isDone = false;\n      String prefix = normalizeToDirectoryPrefix(fileUri);\n      int fileCount = 0;\n      while (!isDone) {\n        ListObjectsV2Request.Builder listObjectsV2RequestBuilder =\n            ListObjectsV2Request.builder().bucket(fileUri.getHost());\n        if (!prefix.equals(DELIMITER)) {\n          listObjectsV2RequestBuilder = listObjectsV2RequestBuilder.prefix(prefix);\n        }\n        if (!recursive) {\n          listObjectsV2RequestBuilder = listObjectsV2RequestBuilder.delimiter(DELIMITER);\n        }\n        if (continuationToken != null) {\n          listObjectsV2RequestBuilder.continuationToken(continuationToken);\n        }\n        ListObjectsV2Request listObjectsV2Request = listObjectsV2RequestBuilder.build();\n        LOG.debug(\"Trying to send ListObjectsV2Request {}\", listObjectsV2Request);\n        ListObjectsV2Response listObjectsV2Response = s3Client.listObjectsV2(listObjectsV2Request);\n        LOG.debug(\"Getting ListObjectsV2Response: {}\", listObjectsV2Response);\n        List<S3Object> filesReturned = listObjectsV2Response.contents();\n        fileCount += filesReturned.size();\n        filesReturned.stream()\n            .forEach(\n                object -> {\n```\n\n### Requirements:\n1. Complete only the missing part of the method.\n2. Only answer the code of the completed part. Do not include any of the provided code snippet.\n3. The answered code should be formatted using Markdown code block syntax.\n4. Do not include any additional text.\n\n### Answer: (use the provided format with backticks)\n", "reference": "                  // Only add files and not directories\n                  if (!object.key().equals(fileUri.getPath())\n                      && !object.key().endsWith(DELIMITER)) {\n                    String fileKey = object.key();\n                    if (fileKey.startsWith(DELIMITER)) {\n                      fileKey = fileKey.substring(1);\n                    }\n                    builder.add(S3_SCHEME + fileUri.getHost() + DELIMITER + fileKey);\n                  }\n                });\n        if (fileCount == LIST_MAX_KEYS) {\n          // check if we reached the max keys returned, if so abort and throw an error message\n          LOG.error(\n              \"Too many files ({}) returned from S3 when attempting to list object prefixes\",\n              LIST_MAX_KEYS);\n          throw new IllegalStateException(\n              String.format(\n                  \"Max keys (%s) reached when attempting to list S3 objects\", LIST_MAX_KEYS));\n        }\n        isDone = !listObjectsV2Response.isTruncated();\n        continuationToken = listObjectsV2Response.nextContinuationToken();\n      }\n      String[] listedFiles = builder.build().toArray(new String[0]);\n      LOG.debug(\n          \"Listed {} files from URI: {}, is recursive: {}\", listedFiles.length, fileUri, recursive);\n      return listedFiles;\n    } catch (Throwable t) {\n      throw new IOException(t);\n    }\n  }", "completions": ["  \n```Java\n                  String fileUri = S3_SCHEME + object.bucketName() + DELIMITER + object.key();\n                  builder.add(fileUri);\n                });\n        if (fileCount < LIST_MAX_KEYS) {\n          isDone = true;\n        } else {\n          continuationToken = listObjectsV2Response.nextContinuationToken();\n        }\n      }\n      return builder.build().toArray(new String[0]);\n    } catch (Throwable t) {\n      throw new IOException(t);\n    }\n  }\n```"]}
{"id": 67, "name": "initLocalCache", "prompt": "### Role:\nYou are an experienced Java programmer assistant responsible for code completion tasks. Please intelligently complete the missing parts of the provided code snippet.\n\n### Task:\nBased on the provided code snippet, complete the missing method implementation. Ensure the code is logically correct, syntactically accurate, and follows best coding practices.\n\n### Left context\n```Java\npackage co.yixiang.yshop.module.system.service.dept;\n\nimport cn.hutool.core.collection.CollUtil;\nimport co.yixiang.yshop.framework.common.enums.CommonStatusEnum;\nimport co.yixiang.yshop.framework.tenant.core.context.TenantContextHolder;\nimport co.yixiang.yshop.framework.tenant.core.util.TenantUtils;\nimport co.yixiang.yshop.module.system.controller.admin.dept.vo.dept.DeptCreateReqVO;\nimport co.yixiang.yshop.module.system.controller.admin.dept.vo.dept.DeptListReqVO;\nimport co.yixiang.yshop.module.system.controller.admin.dept.vo.dept.DeptUpdateReqVO;\nimport co.yixiang.yshop.module.system.convert.dept.DeptConvert;\nimport co.yixiang.yshop.module.system.dal.dataobject.dept.DeptDO;\nimport co.yixiang.yshop.module.system.dal.mysql.dept.DeptMapper;\nimport co.yixiang.yshop.module.system.enums.dept.DeptIdEnum;\nimport co.yixiang.yshop.module.system.mq.producer.dept.DeptProducer;\nimport com.google.common.collect.ImmutableMap;\nimport com.google.common.collect.ImmutableMultimap;\nimport com.google.common.collect.Multimap;\nimport lombok.Getter;\nimport lombok.extern.slf4j.Slf4j;\nimport org.springframework.stereotype.Service;\nimport org.springframework.validation.annotation.Validated;\n\nimport javax.annotation.PostConstruct;\nimport javax.annotation.Resource;\nimport java.util.*;\n\nimport static co.yixiang.yshop.framework.common.exception.util.ServiceExceptionUtil.exception;\nimport static co.yixiang.yshop.module.system.enums.ErrorCodeConstants.*;\n\n/**\n * 部门 Service 实现类\n *\n * @author yshop\n */\n@Service\n@Validated\n@Slf4j\npublic class DeptServiceImpl implements DeptService {\n\n    /**\n     * 部门缓存\n     * key：部门编号 {@link DeptDO#getId()}\n     *\n     * 这里声明 volatile 修饰的原因是，每次刷新时，直接修改指向\n     */\n    @Getter\n    private volatile Map<Long, DeptDO> deptCache;\n    /**\n     * 父部门缓存\n     * key：部门编号 {@link DeptDO#getParentId()}\n     * value: 直接子部门列表\n     *\n     * 这里声明 volatile 修饰的原因是，每次刷新时，直接修改指向\n     */\n    @Getter\n    private volatile Multimap<Long, DeptDO> parentDeptCache;\n\n    @Resource\n    private DeptMapper deptMapper;\n\n    @Resource\n    private DeptProducer deptProducer;\n\n    /**\n     * 初始化 {@link #parentDeptCache} 和 {@link #deptCache} 缓存\n     */\n```\n\n### Code description\n```Java\n/**\n * Description: This method initializes the local cache for department data. It queries the database for all departments,\n * builds an immutable map and multimap for efficient caching, and stores them in the deptCache and parentDeptCache fields.\n * The method is synchronized to ensure thread safety during initialization.\n *\n * @throws IllegalStateException if the initialization process fails.\n */\n```\n\n### Code snippet:\n```Java\n    @Override\n    @PostConstruct\n    public synchronized void initLocalCache() {\n        // 注意：忽略自动多租户，因为要全局初始化缓存\n        TenantUtils.executeIgnore(() -> {\n            // 第一步：查询数据\n            List<DeptDO> depts = deptMapper.selectList();\n            log.info(\"[initLocalCache][缓存部门，数量为:{}]\", depts.size());\n\n            // 第二步：构建缓存\n```\n\n### Requirements:\n1. Complete only the missing part of the method.\n2. Only answer the code of the completed part. Do not include any of the provided code snippet.\n3. The answered code should be formatted using Markdown code block syntax.\n4. Do not include any additional text.\n\n### Answer: (use the provided format with backticks)\n", "reference": "            ImmutableMap.Builder<Long, DeptDO> builder = ImmutableMap.builder();\n            ImmutableMultimap.Builder<Long, DeptDO> parentBuilder = ImmutableMultimap.builder();\n            depts.forEach(deptDO -> {\n                builder.put(deptDO.getId(), deptDO);\n                parentBuilder.put(deptDO.getParentId(), deptDO);\n            });\n            deptCache = builder.build();\n            parentDeptCache = parentBuilder.build();\n        });\n    }", "completions": ["  \n```Java\n            // 第二步：构建缓存\n            ImmutableMap.Builder<Long, DeptDO> deptBuilder = ImmutableMap.builder();\n            ImmutableMultimap.Builder<Long, DeptDO> parentDeptBuilder = ImmutableMultimap.builder();\n            for (DeptDO dept : depts) {\n                deptBuilder.put(dept.getId(), dept);\n                parentDeptBuilder.put(dept.getParentId(), dept);\n            }\n            deptCache = deptBuilder.build();\n            parentDeptCache = parentDeptBuilder.build();\n        });\n    }\n```"]}
{"id": 68, "name": "start", "prompt": "### Role:\nYou are an experienced Java programmer assistant responsible for code completion tasks. Please intelligently complete the missing parts of the provided code snippet.\n\n### Task:\nBased on the provided code snippet, complete the missing method implementation. Ensure the code is logically correct, syntactically accurate, and follows best coding practices.\n\n### Left context\n```Java\npackage com.slack.astra.server;\n\nimport com.google.common.util.concurrent.MoreExecutors;\nimport com.google.common.util.concurrent.Service;\nimport com.google.common.util.concurrent.ServiceManager;\nimport com.slack.astra.blobfs.BlobFs;\nimport com.slack.astra.blobfs.s3.S3CrtBlobFs;\nimport com.slack.astra.bulkIngestApi.BulkIngestApi;\nimport com.slack.astra.bulkIngestApi.BulkIngestKafkaProducer;\nimport com.slack.astra.bulkIngestApi.DatasetRateLimitingService;\nimport com.slack.astra.chunkManager.CachingChunkManager;\nimport com.slack.astra.chunkManager.IndexingChunkManager;\nimport com.slack.astra.clusterManager.ClusterHpaMetricService;\nimport com.slack.astra.clusterManager.ClusterMonitorService;\nimport com.slack.astra.clusterManager.RecoveryTaskAssignmentService;\nimport com.slack.astra.clusterManager.ReplicaAssignmentService;\nimport com.slack.astra.clusterManager.ReplicaCreationService;\nimport com.slack.astra.clusterManager.ReplicaDeletionService;\nimport com.slack.astra.clusterManager.ReplicaEvictionService;\nimport com.slack.astra.clusterManager.ReplicaRestoreService;\nimport com.slack.astra.clusterManager.SnapshotDeletionService;\nimport com.slack.astra.elasticsearchApi.ElasticsearchApiService;\nimport com.slack.astra.logstore.LogMessage;\nimport com.slack.astra.logstore.search.AstraDistributedQueryService;\nimport com.slack.astra.logstore.search.AstraLocalQueryService;\nimport com.slack.astra.metadata.cache.CacheSlotMetadataStore;\nimport com.slack.astra.metadata.core.CloseableLifecycleManager;\nimport com.slack.astra.metadata.core.CuratorBuilder;\nimport com.slack.astra.metadata.dataset.DatasetMetadataStore;\nimport com.slack.astra.metadata.hpa.HpaMetricMetadataStore;\nimport com.slack.astra.metadata.recovery.RecoveryNodeMetadataStore;\nimport com.slack.astra.metadata.recovery.RecoveryTaskMetadataStore;\nimport com.slack.astra.metadata.replica.ReplicaMetadataStore;\nimport com.slack.astra.metadata.schema.SchemaUtil;\nimport com.slack.astra.metadata.search.SearchMetadataStore;\nimport com.slack.astra.metadata.snapshot.SnapshotMetadataStore;\nimport com.slack.astra.preprocessor.PreprocessorService;\nimport com.slack.astra.proto.config.AstraConfigs;\nimport com.slack.astra.proto.metadata.Metadata;\nimport com.slack.astra.proto.schema.Schema;\nimport com.slack.astra.recovery.RecoveryService;\nimport com.slack.astra.util.RuntimeHalterImpl;\nimport com.slack.astra.zipkinApi.ZipkinService;\nimport io.micrometer.core.instrument.MeterRegistry;\nimport io.micrometer.core.instrument.Metrics;\nimport io.micrometer.core.instrument.binder.jvm.ClassLoaderMetrics;\nimport io.micrometer.core.instrument.binder.jvm.JvmGcMetrics;\nimport io.micrometer.core.instrument.binder.jvm.JvmMemoryMetrics;\nimport io.micrometer.core.instrument.binder.jvm.JvmThreadMetrics;\nimport io.micrometer.core.instrument.binder.system.ProcessorMetrics;\nimport io.micrometer.prometheus.PrometheusConfig;\nimport io.micrometer.prometheus.PrometheusMeterRegistry;\nimport java.nio.file.Path;\nimport java.time.Duration;\nimport java.util.HashSet;\nimport java.util.List;\nimport java.util.Set;\nimport java.util.concurrent.TimeUnit;\nimport org.apache.curator.x.async.AsyncCuratorFramework;\nimport org.apache.logging.log4j.LogManager;\nimport org.apache.logging.log4j.util.Strings;\nimport org.slf4j.Logger;\nimport org.slf4j.LoggerFactory;\nimport software.amazon.awssdk.services.s3.S3AsyncClient;\n\n/**\n * Main class of Astra that sets up the basic infra needed for all the other end points like an a\n * http server, register monitoring libraries, create config manager etc..\n */\npublic class Astra {\n  private static final Logger LOG = LoggerFactory.getLogger(Astra.class);\n\n  private final PrometheusMeterRegistry prometheusMeterRegistry;\n\n  private final AstraConfigs.AstraConfig astraConfig;\n  private final S3AsyncClient s3Client;\n  protected ServiceManager serviceManager;\n  protected AsyncCuratorFramework curatorFramework;\n\n  Astra(\n      AstraConfigs.AstraConfig astraConfig,\n      S3AsyncClient s3Client,\n      PrometheusMeterRegistry prometheusMeterRegistry) {\n    this.prometheusMeterRegistry = prometheusMeterRegistry;\n    this.astraConfig = astraConfig;\n    this.s3Client = s3Client;\n    Metrics.addRegistry(prometheusMeterRegistry);\n    LOG.info(\"Started Astra process with config: {}\", astraConfig);\n  }\n\n  Astra(AstraConfigs.AstraConfig astraConfig, PrometheusMeterRegistry prometheusMeterRegistry) {\n    this(astraConfig, S3CrtBlobFs.initS3Client(astraConfig.getS3Config()), prometheusMeterRegistry);\n  }\n\n  public static void main(String[] args) throws Exception {\n    if (args.length == 0) {\n      LOG.info(\"Config file is needed a first argument\");\n    }\n    Path configFilePath = Path.of(args[0]);\n\n    AstraConfig.initFromFile(configFilePath);\n    AstraConfigs.AstraConfig config = AstraConfig.get();\n    Astra astra = new Astra(AstraConfig.get(), initPrometheusMeterRegistry(config));\n    astra.start();\n  }\n\n  static PrometheusMeterRegistry initPrometheusMeterRegistry(AstraConfigs.AstraConfig config) {\n    PrometheusMeterRegistry prometheusMeterRegistry =\n        new PrometheusMeterRegistry(PrometheusConfig.DEFAULT);\n    prometheusMeterRegistry\n        .config()\n        .commonTags(\n            \"astra_cluster_name\",\n            config.getClusterConfig().getClusterName(),\n            \"astra_env\",\n            config.getClusterConfig().getEnv(),\n            \"astra_component\",\n            getComponentTag(config));\n    return prometheusMeterRegistry;\n  }\n\n  private static String getComponentTag(AstraConfigs.AstraConfig config) {\n    String component;\n    if (config.getNodeRolesList().size() == 1) {\n      component = config.getNodeRolesList().get(0).toString();\n    } else {\n      component = Strings.join(config.getNodeRolesList(), '-');\n    }\n    return Strings.toRootLowerCase(component);\n  }\n\n```\n\n### Code description\n```Java\n/**\n * Starts the application by setting up system metrics, adding a shutdown hook, initializing Curator Framework,\n * initializing a blob file system, getting a set of services, creating a service manager, adding a listener to the service manager,\n * and starting the service manager asynchronously.\n *\n * @param prometheusMeterRegistry A registry for metrics that will be used to setup system metrics.\n * @param astraConfig The configuration for the application.\n * @param s3Client The client for S3.\n * @throws Exception If an error occurs during the setup process.\n */\n```\n\n### Code snippet:\n```Java\n  public void start() throws Exception {\n    setupSystemMetrics(prometheusMeterRegistry);\n    addShutdownHook();\n\n    curatorFramework =\n        CuratorBuilder.build(\n            prometheusMeterRegistry, astraConfig.getMetadataStoreConfig().getZookeeperConfig());\n\n    // Initialize blobfs. Only S3 is supported currently.\n```\n\n### Requirements:\n1. Complete only the missing part of the method.\n2. Only answer the code of the completed part. Do not include any of the provided code snippet.\n3. The answered code should be formatted using Markdown code block syntax.\n4. Do not include any additional text.\n\n### Answer: (use the provided format with backticks)\n", "reference": "    S3CrtBlobFs s3BlobFs = new S3CrtBlobFs(s3Client);\n\n    Set<Service> services =\n        getServices(curatorFramework, astraConfig, s3BlobFs, prometheusMeterRegistry);\n    serviceManager = new ServiceManager(services);\n    serviceManager.addListener(getServiceManagerListener(), MoreExecutors.directExecutor());\n\n    serviceManager.startAsync();\n  }", "completions": ["  \n```Java\n    BlobFs blobFs = new S3CrtBlobFs(s3Client, astraConfig.getBlobFsConfig());\n```"]}
{"id": 69, "name": "syncAclWithAclCsv", "prompt": "### Role:\nYou are an experienced Java programmer assistant responsible for code completion tasks. Please intelligently complete the missing parts of the provided code snippet.\n\n### Task:\nBased on the provided code snippet, complete the missing method implementation. Ensure the code is logically correct, syntactically accurate, and follows best coding practices.\n\n### Left context\n```Java\npackage io.kafbat.ui.service.acl;\n\nimport static org.apache.kafka.common.acl.AclOperation.ALL;\nimport static org.apache.kafka.common.acl.AclOperation.CREATE;\nimport static org.apache.kafka.common.acl.AclOperation.DESCRIBE;\nimport static org.apache.kafka.common.acl.AclOperation.IDEMPOTENT_WRITE;\nimport static org.apache.kafka.common.acl.AclOperation.READ;\nimport static org.apache.kafka.common.acl.AclOperation.WRITE;\nimport static org.apache.kafka.common.acl.AclPermissionType.ALLOW;\nimport static org.apache.kafka.common.resource.PatternType.LITERAL;\nimport static org.apache.kafka.common.resource.PatternType.PREFIXED;\nimport static org.apache.kafka.common.resource.ResourceType.CLUSTER;\nimport static org.apache.kafka.common.resource.ResourceType.GROUP;\nimport static org.apache.kafka.common.resource.ResourceType.TOPIC;\nimport static org.apache.kafka.common.resource.ResourceType.TRANSACTIONAL_ID;\n\nimport com.google.common.collect.Sets;\nimport io.kafbat.ui.model.CreateConsumerAclDTO;\nimport io.kafbat.ui.model.CreateProducerAclDTO;\nimport io.kafbat.ui.model.CreateStreamAppAclDTO;\nimport io.kafbat.ui.model.KafkaCluster;\nimport io.kafbat.ui.service.AdminClientService;\nimport io.kafbat.ui.service.ReactiveAdminClient;\nimport java.util.ArrayList;\nimport java.util.Collection;\nimport java.util.Comparator;\nimport java.util.List;\nimport java.util.Optional;\nimport java.util.Set;\nimport javax.annotation.Nullable;\nimport lombok.RequiredArgsConstructor;\nimport lombok.extern.slf4j.Slf4j;\nimport org.apache.kafka.common.acl.AccessControlEntry;\nimport org.apache.kafka.common.acl.AclBinding;\nimport org.apache.kafka.common.acl.AclOperation;\nimport org.apache.kafka.common.resource.Resource;\nimport org.apache.kafka.common.resource.ResourcePattern;\nimport org.apache.kafka.common.resource.ResourcePatternFilter;\nimport org.apache.kafka.common.resource.ResourceType;\nimport org.springframework.stereotype.Service;\nimport org.springframework.util.CollectionUtils;\nimport reactor.core.publisher.Flux;\nimport reactor.core.publisher.Mono;\n\n@Slf4j\n@Service\n@RequiredArgsConstructor\npublic class AclsService {\n\n  private final AdminClientService adminClientService;\n\n  public Mono<Void> createAcl(KafkaCluster cluster, AclBinding aclBinding) {\n    return adminClientService.get(cluster)\n        .flatMap(ac -> createAclsWithLogging(ac, List.of(aclBinding)));\n  }\n\n  private Mono<Void> createAclsWithLogging(ReactiveAdminClient ac, Collection<AclBinding> bindings) {\n    bindings.forEach(b -> log.info(\"CREATING ACL: [{}]\", AclCsv.createAclString(b)));\n    return ac.createAcls(bindings)\n        .doOnSuccess(v -> bindings.forEach(b -> log.info(\"ACL CREATED: [{}]\", AclCsv.createAclString(b))));\n  }\n\n  public Mono<Void> deleteAcl(KafkaCluster cluster, AclBinding aclBinding) {\n    var aclString = AclCsv.createAclString(aclBinding);\n    log.info(\"DELETING ACL: [{}]\", aclString);\n    return adminClientService.get(cluster)\n        .flatMap(ac -> ac.deleteAcls(List.of(aclBinding)))\n        .doOnSuccess(v -> log.info(\"ACL DELETED: [{}]\", aclString));\n  }\n\n  public Flux<AclBinding> listAcls(KafkaCluster cluster, ResourcePatternFilter filter) {\n    return adminClientService.get(cluster)\n        .flatMap(c -> c.listAcls(filter))\n        .flatMapIterable(acls -> acls)\n        .sort(Comparator.comparing(AclBinding::toString));  //sorting to keep stable order on different calls\n  }\n\n  public Mono<String> getAclAsCsvString(KafkaCluster cluster) {\n    return adminClientService.get(cluster)\n        .flatMap(c -> c.listAcls(ResourcePatternFilter.ANY))\n        .map(AclCsv::transformToCsvString);\n  }\n\n```\n\n### Code description\n```Java\n/**\n * Synchronizes the Access Control List (ACL) with a CSV file for a given Kafka cluster.\n * The method retrieves the existing ACLs, compares them with the new ACLs parsed from the CSV file,\n * and determines the ACLs to be added and deleted. It then logs the sync plan and, if necessary,\n * creates and deletes the ACLs.\n *\n * @param cluster The Kafka cluster for which to synchronize the ACL.\n * @param csv The CSV file containing the new ACLs.\n * @return A Mono that completes empty if no ACLs need to be added or deleted, or that completes with an error if the sync fails.\n * @throws IllegalArgumentException if the CSV file is not valid.\n * @see com.google.common.collect.Sets#difference(java.util.Set, java.util.Set)\n */\n```\n\n### Code snippet:\n```Java\n  public Mono<Void> syncAclWithAclCsv(KafkaCluster cluster, String csv) {\n    return adminClientService.get(cluster)\n        .flatMap(ac -> ac.listAcls(ResourcePatternFilter.ANY).flatMap(existingAclList -> {\n          var existingSet = Set.copyOf(existingAclList);\n          var newAcls = Set.copyOf(AclCsv.parseCsv(csv));\n          var toDelete = Sets.difference(existingSet, newAcls);\n          var toAdd = Sets.difference(newAcls, existingSet);\n          logAclSyncPlan(cluster, toAdd, toDelete);\n          if (toAdd.isEmpty() && toDelete.isEmpty()) {\n            return Mono.empty();\n```\n\n### Requirements:\n1. Complete only the missing part of the method.\n2. Only answer the code of the completed part. Do not include any of the provided code snippet.\n3. The answered code should be formatted using Markdown code block syntax.\n4. Do not include any additional text.\n\n### Answer: (use the provided format with backticks)\n", "reference": "          }\n          log.info(\"Starting new ACLs creation\");\n          return ac.createAcls(toAdd)\n              .doOnSuccess(v -> {\n                log.info(\"{} new ACLs created\", toAdd.size());\n                log.info(\"Starting ACLs deletion\");\n              })\n              .then(ac.deleteAcls(toDelete)\n                  .doOnSuccess(v -> log.info(\"{} ACLs deleted\", toDelete.size())));\n        }));\n  }", "completions": ["  \n```Java\n          return createAclsWithLogging(ac, toAdd)\n             .then(deleteAclsWithLogging(ac, toDelete))\n             .then();\n        }));\n  }\n\n  private Mono<Void> deleteAclsWithLogging(ReactiveAdminClient ac, Collection<AclBinding> bindings) {\n    bindings.forEach(b -> log.info(\"DELETING ACL: [{}]\", AclCsv.createAclString(b)));\n    return ac.deleteAcls(bindings)\n       .doOnSuccess(v -> bindings.forEach(b -> log.info(\"ACL DELETED: [{}]\", AclCsv.createAclString(b))));\n  }\n```"]}
{"id": 70, "name": "applyLabelDiff", "prompt": "### Role:\nYou are an experienced Java programmer assistant responsible for code completion tasks. Please intelligently complete the missing parts of the provided code snippet.\n\n### Task:\nBased on the provided code snippet, complete the missing method implementation. Ensure the code is logically correct, syntactically accurate, and follows best coding practices.\n\n### Left context\n```Java\n// Copyright 2023 The Nomulus Authors. All Rights Reserved.\n//\n// Licensed under the Apache License, Version 2.0 (the \"License\");\n// you may not use this file except in compliance with the License.\n// You may obtain a copy of the License at\n//\n//     http://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an \"AS IS\" BASIS,\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n// See the License for the specific language governing permissions and\n// limitations under the License.\n\npackage google.registry.bsa.persistence;\n\nimport static com.google.common.base.Verify.verify;\nimport static com.google.common.collect.ImmutableList.toImmutableList;\nimport static com.google.common.collect.ImmutableSet.toImmutableSet;\nimport static com.google.common.collect.Sets.difference;\nimport static google.registry.bsa.ReservedDomainsUtils.isReservedDomain;\nimport static google.registry.persistence.PersistenceModule.TransactionIsolationLevel.TRANSACTION_REPEATABLE_READ;\nimport static google.registry.persistence.transaction.TransactionManagerFactory.tm;\nimport static java.util.stream.Collectors.groupingBy;\n\nimport com.google.common.base.Joiner;\nimport com.google.common.collect.ImmutableList;\nimport com.google.common.collect.ImmutableMap;\nimport com.google.common.collect.ImmutableSet;\nimport com.google.common.flogger.FluentLogger;\nimport com.google.common.flogger.LazyArgs;\nimport google.registry.bsa.IdnChecker;\nimport google.registry.bsa.api.BlockLabel;\nimport google.registry.bsa.api.BlockLabel.LabelType;\nimport google.registry.bsa.api.UnblockableDomain;\nimport google.registry.bsa.api.UnblockableDomain.Reason;\nimport google.registry.model.ForeignKeyUtils;\nimport google.registry.model.domain.Domain;\nimport google.registry.model.tld.Tld;\nimport java.util.Map;\nimport java.util.stream.Stream;\nimport org.joda.time.DateTime;\n\n/** Applies the BSA label diffs from the latest BSA download. */\npublic final class LabelDiffUpdates {\n\n  private static final FluentLogger logger = FluentLogger.forEnclosingClass();\n\n  private static final Joiner DOMAIN_JOINER = Joiner.on('.');\n\n  private LabelDiffUpdates() {}\n\n  /**\n   * Applies the label diffs to the database and collects matching domains that are in use\n   * (registered or reserved) for reporting.\n   *\n   * @return A collection of domains in use\n   */\n```\n\n### Code description\n```Java\n/**\n * Applies a diff of block labels to the database and returns a list of unblockable domains.\n *\n * @param labels The list of block labels to apply.\n * @param idnChecker The IDN checker to validate the labels.\n * @param schedule The download schedule for label creation.\n * @param now The current date and time.\n * @return An immutable list of unblockable domains.\n * @throws IllegalArgumentException If the label is not valid in at least one TLD.\n * @throws IllegalStateException If the number of deleted labels does not match the number of labels in the database.\n * @throws IllegalStateException If the number of labels in the database does not match the number of affected labels.\n * @see com.google.common.collect.Sets#difference(Set, Set)\n */\n```\n\n### Code snippet:\n```Java\n  public static ImmutableList<UnblockableDomain> applyLabelDiff(\n      ImmutableList<BlockLabel> labels,\n      IdnChecker idnChecker,\n      DownloadSchedule schedule,\n      DateTime now) {\n    ImmutableList.Builder<UnblockableDomain> nonBlockedDomains = new ImmutableList.Builder<>();\n    ImmutableMap<LabelType, ImmutableList<BlockLabel>> labelsByType =\n        ImmutableMap.copyOf(\n            labels.stream().collect(groupingBy(BlockLabel::labelType, toImmutableList())));\n\n    tm().transact(\n            () -> {\n              for (Map.Entry<LabelType, ImmutableList<BlockLabel>> entry :\n                  labelsByType.entrySet()) {\n                switch (entry.getKey()) {\n                  case CREATE:\n                    // With current Cloud SQL, label upsert throughput is about 200/second. If\n                    // better performance is needed, consider bulk insert in native SQL.\n                    tm().putAll(\n                            entry.getValue().stream()\n                                .filter(label -> isValidInAtLeastOneTld(label, idnChecker))\n                                .map(\n                                    label ->\n                                        new BsaLabel(label.label(), schedule.jobCreationTime()))\n                                .collect(toImmutableList()));\n                    // May not find all unblockables due to race condition: DomainCreateFlow uses\n                    // cached BsaLabels. Eventually will be consistent.\n                    nonBlockedDomains.addAll(\n                        tallyUnblockableDomainsForNewLabels(entry.getValue(), idnChecker, now));\n                    break;\n                  case DELETE:\n                    ImmutableSet<String> deletedLabels =\n                        entry.getValue().stream()\n                            .filter(label -> isValidInAtLeastOneTld(label, idnChecker))\n                            .map(BlockLabel::label)\n                            .collect(toImmutableSet());\n                    // Delete labels in DB. Also cascade-delete BsaUnblockableDomain.\n                    int nDeleted = Queries.deleteBsaLabelByLabels(deletedLabels);\n```\n\n### Requirements:\n1. Complete only the missing part of the method.\n2. Only answer the code of the completed part. Do not include any of the provided code snippet.\n3. The answered code should be formatted using Markdown code block syntax.\n4. Do not include any additional text.\n\n### Answer: (use the provided format with backticks)\n", "reference": "                    if (nDeleted != deletedLabels.size()) {\n                      logger.atSevere().log(\n                          \"Only found %s entities among the %s labels: [%s]\",\n                          nDeleted, deletedLabels.size(), deletedLabels);\n                    }\n                    break;\n                  case NEW_ORDER_ASSOCIATION:\n                    ImmutableSet<String> affectedLabels =\n                        entry.getValue().stream()\n                            .filter(label -> isValidInAtLeastOneTld(label, idnChecker))\n                            .map(BlockLabel::label)\n                            .collect(toImmutableSet());\n                    ImmutableSet<String> labelsInDb =\n                        Queries.queryBsaLabelByLabels(affectedLabels)\n                            .map(BsaLabel::getLabel)\n                            .collect(toImmutableSet());\n                    verify(\n                        labelsInDb.size() == affectedLabels.size(),\n                        \"Missing labels in DB: %s\",\n                        LazyArgs.lazy(() -> difference(affectedLabels, labelsInDb)));\n\n                    // Reuse registered and reserved names that are already computed.\n                    Queries.queryBsaUnblockableDomainByLabels(affectedLabels)\n                        .map(BsaUnblockableDomain::toUnblockableDomain)\n                        .forEach(nonBlockedDomains::add);\n\n                    for (BlockLabel label : entry.getValue()) {\n                      getInvalidTldsForLabel(label, idnChecker)\n                          .map(tld -> UnblockableDomain.of(label.label(), tld, Reason.INVALID))\n                          .forEach(nonBlockedDomains::add);\n                    }\n                    break;\n                }\n              }\n            },\n            TRANSACTION_REPEATABLE_READ);\n    logger.atInfo().log(\"Processed %s of labels.\", labels.size());\n    return nonBlockedDomains.build();\n  }", "completions": ["  \n```Java\nint nExpected = deletedLabels.size();\nverify(\n    nDeleted == nExpected,\n    \"Expected to delete %s labels, but actually deleted %s\",\n    nExpected,\n    nDeleted);\n```"]}
{"id": 71, "name": "findMissingBlobs", "prompt": "### Role:\nYou are an experienced Java programmer assistant responsible for code completion tasks. Please intelligently complete the missing parts of the provided code snippet.\n\n### Task:\nBased on the provided code snippet, complete the missing method implementation. Ensure the code is logically correct, syntactically accurate, and follows best coding practices.\n\n### Left context\n```Java\n// Copyright 2017 The Bazel Authors. All rights reserved.\n//\n// Licensed under the Apache License, Version 2.0 (the \"License\");\n// you may not use this file except in compliance with the License.\n// You may obtain a copy of the License at\n//\n//    http://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an \"AS IS\" BASIS,\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n// See the License for the specific language governing permissions and\n// limitations under the License.\n\npackage build.buildfarm.instance.shard;\n\nimport static build.buildfarm.cas.ContentAddressableStorage.NOT_FOUND;\nimport static build.buildfarm.cas.ContentAddressableStorage.OK;\nimport static build.buildfarm.common.Actions.asExecutionStatus;\nimport static build.buildfarm.common.Actions.checkPreconditionFailure;\nimport static build.buildfarm.common.Actions.invalidActionVerboseMessage;\nimport static build.buildfarm.common.Errors.VIOLATION_TYPE_INVALID;\nimport static build.buildfarm.common.Errors.VIOLATION_TYPE_MISSING;\nimport static build.buildfarm.common.config.Backplane.BACKPLANE_TYPE.SHARD;\nimport static build.buildfarm.instance.shard.Util.SHARD_IS_RETRIABLE;\nimport static build.buildfarm.instance.shard.Util.correctMissingBlob;\nimport static com.google.common.base.Preconditions.checkState;\nimport static com.google.common.base.Throwables.throwIfUnchecked;\nimport static com.google.common.util.concurrent.Futures.addCallback;\nimport static com.google.common.util.concurrent.Futures.allAsList;\nimport static com.google.common.util.concurrent.Futures.catching;\nimport static com.google.common.util.concurrent.Futures.catchingAsync;\nimport static com.google.common.util.concurrent.Futures.immediateFailedFuture;\nimport static com.google.common.util.concurrent.Futures.immediateFuture;\nimport static com.google.common.util.concurrent.Futures.transform;\nimport static com.google.common.util.concurrent.Futures.transformAsync;\nimport static com.google.common.util.concurrent.MoreExecutors.directExecutor;\nimport static java.lang.String.format;\nimport static java.util.concurrent.Executors.newSingleThreadExecutor;\nimport static java.util.concurrent.Executors.newSingleThreadScheduledExecutor;\nimport static java.util.concurrent.TimeUnit.MICROSECONDS;\nimport static java.util.concurrent.TimeUnit.MINUTES;\nimport static java.util.concurrent.TimeUnit.SECONDS;\nimport static net.javacrumbs.futureconverter.java8guava.FutureConverter.toCompletableFuture;\nimport static net.javacrumbs.futureconverter.java8guava.FutureConverter.toListenableFuture;\n\nimport build.bazel.remote.execution.v2.Action;\nimport build.bazel.remote.execution.v2.ActionResult;\nimport build.bazel.remote.execution.v2.BatchReadBlobsResponse.Response;\nimport build.bazel.remote.execution.v2.CacheCapabilities;\nimport build.bazel.remote.execution.v2.Command;\nimport build.bazel.remote.execution.v2.Compressor;\nimport build.bazel.remote.execution.v2.Digest;\nimport build.bazel.remote.execution.v2.Directory;\nimport build.bazel.remote.execution.v2.DirectoryNode;\nimport build.bazel.remote.execution.v2.ExecuteOperationMetadata;\nimport build.bazel.remote.execution.v2.ExecuteResponse;\nimport build.bazel.remote.execution.v2.ExecutionPolicy;\nimport build.bazel.remote.execution.v2.ExecutionStage;\nimport build.bazel.remote.execution.v2.Platform;\nimport build.bazel.remote.execution.v2.Platform.Property;\nimport build.bazel.remote.execution.v2.RequestMetadata;\nimport build.bazel.remote.execution.v2.ResultsCachePolicy;\nimport build.bazel.remote.execution.v2.SymlinkAbsolutePathStrategy;\nimport build.buildfarm.actioncache.ActionCache;\nimport build.buildfarm.actioncache.ShardActionCache;\nimport build.buildfarm.backplane.Backplane;\nimport build.buildfarm.common.BuildfarmExecutors;\nimport build.buildfarm.common.CasIndexResults;\nimport build.buildfarm.common.DigestUtil;\nimport build.buildfarm.common.DigestUtil.ActionKey;\nimport build.buildfarm.common.EntryLimitException;\nimport build.buildfarm.common.ExecutionProperties;\nimport build.buildfarm.common.Poller;\nimport build.buildfarm.common.TokenizableIterator;\nimport build.buildfarm.common.TreeIterator;\nimport build.buildfarm.common.TreeIterator.DirectoryEntry;\nimport build.buildfarm.common.Watcher;\nimport build.buildfarm.common.Write;\nimport build.buildfarm.common.config.BuildfarmConfigs;\nimport build.buildfarm.common.grpc.UniformDelegateServerCallStreamObserver;\nimport build.buildfarm.instance.Instance;\nimport build.buildfarm.instance.MatchListener;\nimport build.buildfarm.instance.server.NodeInstance;\nimport build.buildfarm.operations.EnrichedOperation;\nimport build.buildfarm.operations.FindOperationsResults;\nimport build.buildfarm.v1test.BackplaneStatus;\nimport build.buildfarm.v1test.ExecuteEntry;\nimport build.buildfarm.v1test.GetClientStartTimeRequest;\nimport build.buildfarm.v1test.GetClientStartTimeResult;\nimport build.buildfarm.v1test.OperationIteratorToken;\nimport build.buildfarm.v1test.ProfiledQueuedOperationMetadata;\nimport build.buildfarm.v1test.QueueEntry;\nimport build.buildfarm.v1test.QueueStatus;\nimport build.buildfarm.v1test.QueuedOperation;\nimport build.buildfarm.v1test.QueuedOperationMetadata;\nimport build.buildfarm.v1test.Tree;\nimport com.github.benmanes.caffeine.cache.AsyncCache;\nimport com.github.benmanes.caffeine.cache.Cache;\nimport com.github.benmanes.caffeine.cache.Caffeine;\nimport com.google.common.annotations.VisibleForTesting;\nimport com.google.common.base.Stopwatch;\nimport com.google.common.cache.LoadingCache;\nimport com.google.common.collect.ImmutableList;\nimport com.google.common.collect.Iterables;\nimport com.google.common.collect.Sets;\nimport com.google.common.io.BaseEncoding;\nimport com.google.common.util.concurrent.FutureCallback;\nimport com.google.common.util.concurrent.Futures;\nimport com.google.common.util.concurrent.ListenableFuture;\nimport com.google.common.util.concurrent.ListeningExecutorService;\nimport com.google.common.util.concurrent.SettableFuture;\nimport com.google.common.util.concurrent.UncheckedExecutionException;\nimport com.google.longrunning.Operation;\nimport com.google.protobuf.Any;\nimport com.google.protobuf.ByteString;\nimport com.google.protobuf.Duration;\nimport com.google.protobuf.InvalidProtocolBufferException;\nimport com.google.protobuf.Parser;\nimport com.google.protobuf.util.Durations;\nimport com.google.protobuf.util.Timestamps;\nimport com.google.rpc.PreconditionFailure;\nimport io.grpc.Context;\nimport io.grpc.Deadline;\nimport io.grpc.Status;\nimport io.grpc.Status.Code;\nimport io.grpc.StatusException;\nimport io.grpc.StatusRuntimeException;\nimport io.grpc.protobuf.StatusProto;\nimport io.grpc.stub.ServerCallStreamObserver;\nimport io.prometheus.client.Counter;\nimport io.prometheus.client.Gauge;\nimport io.prometheus.client.Histogram;\nimport java.io.IOException;\nimport java.io.InputStream;\nimport java.io.OutputStream;\nimport java.time.Instant;\nimport java.util.AbstractMap;\nimport java.util.ArrayDeque;\nimport java.util.ArrayList;\nimport java.util.Collections;\nimport java.util.Deque;\nimport java.util.HashMap;\nimport java.util.HashSet;\nimport java.util.Iterator;\nimport java.util.List;\nimport java.util.Map;\nimport java.util.Queue;\nimport java.util.Random;\nimport java.util.Set;\nimport java.util.UUID;\nimport java.util.concurrent.BlockingQueue;\nimport java.util.concurrent.CompletableFuture;\nimport java.util.concurrent.ConcurrentLinkedQueue;\nimport java.util.concurrent.ExecutionException;\nimport java.util.concurrent.Executor;\nimport java.util.concurrent.ExecutorService;\nimport java.util.concurrent.LinkedBlockingQueue;\nimport java.util.concurrent.ScheduledExecutorService;\nimport java.util.concurrent.TimeUnit;\nimport java.util.function.BiFunction;\nimport java.util.function.Consumer;\nimport java.util.function.Supplier;\nimport java.util.logging.Level;\nimport java.util.logging.Logger;\nimport java.util.stream.Collectors;\nimport javax.annotation.Nullable;\nimport javax.naming.ConfigurationException;\nimport lombok.extern.java.Log;\n\n@Log\npublic class ServerInstance extends NodeInstance {\n  private static final ListenableFuture<Void> IMMEDIATE_VOID_FUTURE = Futures.immediateFuture(null);\n\n  private static final String TIMEOUT_OUT_OF_BOUNDS =\n      \"A timeout specified is out of bounds with a configured range\";\n\n  private static final int DEFAULT_MAX_LOCAL_ACTION_CACHE_SIZE = 1000000;\n\n  // Prometheus metrics\n  private static final Counter executionSuccess =\n      Counter.build().name(\"execution_success\").help(\"Execution success.\").register();\n  private static final Gauge preQueueSize =\n      Gauge.build().name(\"pre_queue_size\").help(\"Pre queue size.\").register();\n  private static final Counter casHitCounter =\n      Counter.build()\n          .name(\"cas_hit\")\n          .help(\"Number of successful CAS hits from worker-worker.\")\n          .register();\n  private static final Counter casMissCounter =\n      Counter.build().name(\"cas_miss\").help(\"Number of CAS misses from worker-worker.\").register();\n  private static final Counter requeueFailureCounter =\n      Counter.build()\n          .name(\"requeue_failure\")\n          .help(\"Number of operations that failed to requeue.\")\n          .register();\n  private static final Counter queueFailureCounter =\n      Counter.build()\n          .name(\"queue_failure\")\n          .help(\"Number of operations that failed to queue.\")\n          .register();\n  // Metrics about the dispatched operations\n  private static final Gauge dispatchedOperationsSize =\n      Gauge.build()\n          .name(\"dispatched_operations_size\")\n          .help(\"Dispatched operations size.\")\n          .register();\n\n  // Other metrics from the backplane\n  private static final Gauge workerPoolSize =\n      Gauge.build().name(\"worker_pool_size\").help(\"Active worker pool size.\").register();\n  private static final Gauge storageWorkerPoolSize =\n      Gauge.build()\n          .name(\"storage_worker_pool_size\")\n          .help(\"Active storage worker pool size.\")\n          .register();\n  private static final Gauge executeWorkerPoolSize =\n      Gauge.build()\n          .name(\"execute_worker_pool_size\")\n          .help(\"Active execute worker pool size.\")\n          .register();\n  private static final Gauge queueSize =\n      Gauge.build().name(\"queue_size\").labelNames(\"queue_name\").help(\"Queue size.\").register();\n\n  private static final Histogram ioMetric =\n      Histogram.build()\n          .name(\"io_bytes_read\")\n          .buckets(new double[] {10, 1000, 10000, 100000, 1000000, 10000000, 100000000, 1000000000})\n          .help(\"Read I/O (bytes)\")\n          .register();\n\n  private final Runnable onStop;\n  private final long maxEntrySizeBytes;\n  private final Backplane backplane;\n  private final ActionCache actionCache;\n  private final RemoteInputStreamFactory remoteInputStreamFactory;\n  private final com.google.common.cache.LoadingCache<String, Instance> workerStubs;\n  private final Thread dispatchedMonitor;\n  private final Duration maxActionTimeout;\n  private AsyncCache<Digest, Directory> directoryCache;\n  private AsyncCache<Digest, Command> commandCache;\n  private AsyncCache<Digest, Action> digestToActionCache;\n  private Cache<RequestMetadata, Boolean> recentCacheServedExecutions;\n\n  private final Random rand = new Random();\n  private final Writes writes = new Writes(this::writeInstanceSupplier);\n  private final int maxCpu;\n  private final int maxRequeueAttempts;\n\n  private final ListeningExecutorService operationTransformService =\n      BuildfarmExecutors.getTransformServicePool();\n  private final ListeningExecutorService actionCacheFetchService;\n  private final ScheduledExecutorService contextDeadlineScheduler =\n      newSingleThreadScheduledExecutor();\n  private final ExecutorService operationDeletionService = newSingleThreadExecutor();\n  private final BlockingQueue transformTokensQueue = new LinkedBlockingQueue(256);\n  private final boolean useDenyList;\n  private Thread operationQueuer;\n  private boolean stopping = false;\n  private boolean stopped = true;\n  private final Thread prometheusMetricsThread;\n\n  private static BuildfarmConfigs configs = BuildfarmConfigs.getInstance();\n\n  // TODO: move to config\n  private static final Duration queueTimeout = Durations.fromSeconds(60);\n\n  private static Backplane createBackplane(String identifier) throws ConfigurationException {\n    if (configs.getBackplane().getType().equals(SHARD)) {\n      return new RedisShardBackplane(\n          identifier,\n          /* subscribeToBackplane= */ true,\n          configs.getServer().isRunFailsafeOperation(),\n          ServerInstance::stripOperation,\n          ServerInstance::stripQueuedOperation);\n    } else {\n      throw new IllegalArgumentException(\"Shard Backplane not set in config\");\n    }\n  }\n\n  public ServerInstance(String name, String identifier, DigestUtil digestUtil, Runnable onStop)\n      throws InterruptedException, ConfigurationException {\n    this(\n        name,\n        digestUtil,\n        createBackplane(identifier),\n        onStop,\n        /* actionCacheFetchService= */ BuildfarmExecutors.getActionCacheFetchServicePool());\n  }\n\n  private ServerInstance(\n      String name,\n      DigestUtil digestUtil,\n      Backplane backplane,\n      Runnable onStop,\n      ListeningExecutorService actionCacheFetchService)\n      throws InterruptedException {\n    this(\n        name,\n        digestUtil,\n        backplane,\n        new ShardActionCache(\n            DEFAULT_MAX_LOCAL_ACTION_CACHE_SIZE, backplane, actionCacheFetchService),\n        configs.getServer().isRunDispatchedMonitor(),\n        configs.getServer().getDispatchedMonitorIntervalSeconds(),\n        configs.getServer().isRunOperationQueuer(),\n        configs.getMaxEntrySizeBytes(),\n        configs.getServer().getMaxCpu(),\n        configs.getServer().getMaxRequeueAttempts(),\n        Duration.newBuilder().setSeconds(configs.getMaximumActionTimeout()).build(),\n        configs.getServer().isUseDenyList(),\n        onStop,\n        WorkerStubs.create(\n            digestUtil,\n            Duration.newBuilder().setSeconds(configs.getServer().getGrpcTimeout()).build()),\n        actionCacheFetchService,\n        configs.getServer().isEnsureOutputsPresent());\n  }\n\n  void initializeCaches() {\n    directoryCache =\n        Caffeine.newBuilder()\n            .newBuilder()\n            .maximumSize(configs.getServer().getCaches().getDirectoryCacheMaxEntries())\n            .buildAsync();\n    commandCache =\n        Caffeine.newBuilder()\n            .newBuilder()\n            .maximumSize(configs.getServer().getCaches().getCommandCacheMaxEntries())\n            .buildAsync();\n    digestToActionCache =\n        Caffeine.newBuilder()\n            .newBuilder()\n            .maximumSize(configs.getServer().getCaches().getDigestToActionCacheMaxEntries())\n            .buildAsync();\n    recentCacheServedExecutions =\n        Caffeine.newBuilder()\n            .newBuilder()\n            .maximumSize(configs.getServer().getCaches().getRecentServedExecutionsCacheMaxEntries())\n            .build();\n  }\n\n  public ServerInstance(\n      String name,\n      DigestUtil digestUtil,\n      Backplane backplane,\n      ActionCache actionCache,\n      boolean runDispatchedMonitor,\n      int dispatchedMonitorIntervalSeconds,\n      boolean runOperationQueuer,\n      long maxEntrySizeBytes,\n      int maxCpu,\n      int maxRequeueAttempts,\n      Duration maxActionTimeout,\n      boolean useDenyList,\n      Runnable onStop,\n      LoadingCache<String, Instance> workerStubs,\n      ListeningExecutorService actionCacheFetchService,\n      boolean ensureOutputsPresent) {\n    super(\n        name,\n        digestUtil,\n        /* contentAddressableStorage= */ null,\n        /* actionCache= */ actionCache,\n        /* outstandingOperations= */ null,\n        /* completedOperations= */ null,\n        /* activeBlobWrites= */ null,\n        ensureOutputsPresent);\n    this.backplane = backplane;\n    this.actionCache = actionCache;\n    this.workerStubs = workerStubs;\n    this.onStop = onStop;\n    this.maxEntrySizeBytes = maxEntrySizeBytes;\n    this.maxCpu = maxCpu;\n    this.maxRequeueAttempts = maxRequeueAttempts;\n    this.maxActionTimeout = maxActionTimeout;\n    this.useDenyList = useDenyList;\n    this.actionCacheFetchService = actionCacheFetchService;\n    backplane.setOnUnsubscribe(this::stop);\n\n    initializeCaches();\n\n    remoteInputStreamFactory =\n        new RemoteInputStreamFactory(\n            backplane, rand, workerStubs, this::removeMalfunctioningWorker);\n\n    if (runDispatchedMonitor) {\n      dispatchedMonitor =\n          new Thread(\n              new DispatchedMonitor(\n                  backplane, this::requeueOperation, dispatchedMonitorIntervalSeconds));\n    } else {\n      dispatchedMonitor = null;\n    }\n\n    if (runOperationQueuer) {\n      operationQueuer =\n          new Thread(\n              new Runnable() {\n                final Stopwatch stopwatch = Stopwatch.createUnstarted();\n\n                ListenableFuture<Void> iterate() throws IOException, InterruptedException {\n                  ensureCanQueue(stopwatch); // wait for transition to canQueue state\n                  long canQueueUSecs = stopwatch.elapsed(MICROSECONDS);\n                  stopwatch.stop();\n                  ExecuteEntry executeEntry = backplane.deprequeueOperation();\n                  stopwatch.start();\n                  if (executeEntry == null) {\n                    log.log(Level.SEVERE, \"OperationQueuer: Got null from deprequeue...\");\n                    return immediateFuture(null);\n                  }\n                  // half the watcher expiry, need to expose this from backplane\n                  Poller poller = new Poller(Durations.fromSeconds(5));\n                  String operationName = executeEntry.getOperationName();\n                  poller.resume(\n                      () -> {\n                        try {\n                          backplane.queueing(executeEntry.getOperationName());\n                        } catch (IOException e) {\n                          if (!stopping && !stopped) {\n                            log.log(\n                                Level.SEVERE,\n                                format(\"error polling %s for queuing\", operationName),\n                                e);\n                          }\n                          // mostly ignore, we will be stopped at some point later\n                        }\n                        return !stopping && !stopped;\n                      },\n                      () -> {},\n                      Deadline.after(5, MINUTES));\n                  try {\n                    log.log(Level.FINER, \"queueing \" + operationName);\n                    ListenableFuture<Void> queueFuture = queue(executeEntry, poller, queueTimeout);\n                    addCallback(\n                        queueFuture,\n                        new FutureCallback<Void>() {\n                          @Override\n                          public void onSuccess(Void result) {\n                            log.log(Level.FINER, \"successfully queued \" + operationName);\n                            // nothing\n                          }\n\n                          @Override\n                          public void onFailure(Throwable t) {\n                            queueFailureCounter.inc();\n                            log.log(Level.SEVERE, \"error queueing \" + operationName, t);\n                          }\n                        },\n                        operationTransformService);\n                    long operationTransformDispatchUSecs =\n                        stopwatch.elapsed(MICROSECONDS) - canQueueUSecs;\n                    log.log(\n                        Level.FINER,\n                        format(\n                            \"OperationQueuer: Dispatched To Transform %s: %dus in canQueue, %dus in\"\n                                + \" transform dispatch\",\n                            operationName, canQueueUSecs, operationTransformDispatchUSecs));\n                    return queueFuture;\n                  } catch (Throwable t) {\n                    poller.pause();\n                    queueFailureCounter.inc();\n                    log.log(Level.SEVERE, \"error queueing \" + operationName, t);\n                    return immediateFuture(null);\n                  }\n                }\n\n                @Override\n                public void run() {\n                  log.log(Level.FINER, \"OperationQueuer: Running\");\n                  try {\n                    while (transformTokensQueue.offer(new Object(), 5, MINUTES)) {\n                      stopwatch.start();\n                      try {\n                        iterate()\n                            .addListener(\n                                () -> {\n                                  try {\n                                    transformTokensQueue.take();\n                                  } catch (InterruptedException e) {\n                                    log.log(\n                                        Level.SEVERE,\n                                        \"interrupted while returning transform token\",\n                                        e);\n                                  }\n                                },\n                                operationTransformService);\n                      } catch (IOException e) {\n                        transformTokensQueue.take();\n                        // problems interacting with backplane\n                      } finally {\n                        stopwatch.reset();\n                      }\n                    }\n                    log.severe(\"OperationQueuer: Transform lease token timed out\");\n                  } catch (InterruptedException e) {\n                    // treat with exit\n                    operationQueuer = null;\n                    return;\n                  } catch (Exception t) {\n                    log.log(Level.SEVERE, \"OperationQueuer: fatal exception during iteration\", t);\n                  } finally {\n                    log.log(Level.FINER, \"OperationQueuer: Exiting\");\n                  }\n                  operationQueuer = null;\n                  try {\n                    stop();\n                  } catch (InterruptedException e) {\n                    log.log(Level.SEVERE, \"interrupted while stopping instance \" + getName(), e);\n                  }\n                }\n              });\n    } else {\n      operationQueuer = null;\n    }\n\n    prometheusMetricsThread =\n        new Thread(\n            () -> {\n              while (!Thread.currentThread().isInterrupted()) {\n                try {\n                  TimeUnit.SECONDS.sleep(30);\n                  BackplaneStatus backplaneStatus = backplaneStatus();\n                  workerPoolSize.set(backplaneStatus.getActiveWorkersCount());\n                  executeWorkerPoolSize.set(backplaneStatus.getActiveExecuteWorkersCount());\n                  storageWorkerPoolSize.set(backplaneStatus.getActiveStorageWorkersCount());\n                  dispatchedOperationsSize.set(backplaneStatus.getDispatchedSize());\n                  preQueueSize.set(backplaneStatus.getPrequeue().getSize());\n                  updateQueueSizes(backplaneStatus.getOperationQueue().getProvisionsList());\n                } catch (InterruptedException e) {\n                  Thread.currentThread().interrupt();\n                  break;\n                } catch (Exception e) {\n                  log.log(Level.SEVERE, \"Could not update RedisShardBackplane metrics\", e);\n                }\n              }\n            },\n            \"Prometheus Metrics Collector\");\n  }\n\n  private void updateQueueSizes(List<QueueStatus> queues) {\n    if (queueSize != null) {\n      for (QueueStatus queueStatus : queues) {\n        queueSize.labels(queueStatus.getName()).set(queueStatus.getSize());\n      }\n    }\n  }\n\n  private void ensureCanQueue(Stopwatch stopwatch) throws IOException, InterruptedException {\n    while (!backplane.canQueue()) {\n      stopwatch.stop();\n      TimeUnit.MILLISECONDS.sleep(100);\n      stopwatch.start();\n    }\n  }\n\n  @Override\n  public void start(String publicName) {\n    stopped = false;\n    try {\n      backplane.start(publicName);\n    } catch (IOException e) {\n      throw new RuntimeException(e);\n    } catch (RuntimeException e) {\n      try {\n        stop();\n      } catch (InterruptedException intEx) {\n        e.addSuppressed(intEx);\n      }\n      throw e;\n    }\n    if (dispatchedMonitor != null) {\n      dispatchedMonitor.start();\n    }\n    if (operationQueuer != null) {\n      operationQueuer.start();\n    }\n\n    if (prometheusMetricsThread != null) {\n      prometheusMetricsThread.start();\n    }\n  }\n\n  @Override\n  public void stop() throws InterruptedException {\n    if (stopped || stopping) {\n      return;\n    }\n    stopping = true;\n    log.log(Level.FINER, format(\"Instance %s is stopping\", getName()));\n    if (operationQueuer != null) {\n      operationQueuer.interrupt();\n      operationQueuer.join();\n    }\n    if (dispatchedMonitor != null) {\n      dispatchedMonitor.interrupt();\n      dispatchedMonitor.join();\n    }\n    if (prometheusMetricsThread != null) {\n      prometheusMetricsThread.interrupt();\n    }\n    contextDeadlineScheduler.shutdown();\n    operationDeletionService.shutdown();\n    operationTransformService.shutdown();\n    actionCacheFetchService.shutdown();\n    onStop.run();\n    backplane.stop();\n    if (!contextDeadlineScheduler.awaitTermination(10, SECONDS)) {\n      log.log(\n          Level.SEVERE,\n          \"Could not shut down operation deletion service, some operations may be zombies\");\n    }\n    if (!operationDeletionService.awaitTermination(10, SECONDS)) {\n      log.log(\n          Level.SEVERE,\n          \"Could not shut down operation deletion service, some operations may be zombies\");\n    }\n    operationDeletionService.shutdownNow();\n    if (!operationTransformService.awaitTermination(10, SECONDS)) {\n      log.log(Level.SEVERE, \"Could not shut down operation transform service\");\n    }\n    operationTransformService.shutdownNow();\n    if (!actionCacheFetchService.awaitTermination(10, SECONDS)) {\n      log.log(Level.SEVERE, \"Could not shut down action cache fetch service\");\n    }\n    actionCacheFetchService.shutdownNow();\n    workerStubs.invalidateAll();\n    log.log(Level.FINER, format(\"Instance %s has been stopped\", getName()));\n    stopping = false;\n    stopped = true;\n  }\n\n  @Override\n  public boolean containsBlob(Digest digest, Digest.Builder result, RequestMetadata requestMetadata)\n      throws InterruptedException {\n    Iterable<Digest> missingOrPopulated;\n    try {\n      missingOrPopulated = findMissingBlobs(ImmutableList.of(digest), requestMetadata).get();\n    } catch (ExecutionException e) {\n      throwIfUnchecked(e.getCause());\n      throw new RuntimeException(e.getCause());\n    }\n    if (digest.getSizeBytes() == -1) {\n      Digest responseDigest = Iterables.getOnlyElement(missingOrPopulated);\n      if (responseDigest.getSizeBytes() == -1) {\n        return false;\n      }\n      result.mergeFrom(responseDigest);\n      return true;\n    }\n    return Iterables.isEmpty(missingOrPopulated);\n  }\n\n```\n\n### Code description\n```Java\n/**\n * Description: This method is used to find missing blobs in a given list of blob digests.\n * It first checks if the request is in the deny list, and if so, it returns an immediate failed future with an UNAVAILABLE status.\n * If the request is not in the deny list, it filters out empty blobs from the list.\n * If all blobs are empty, it returns an immediate future with an empty list.\n * If the server configuration allows finding missing blobs via backplane, it calls the findMissingBlobsViaBackplane method.\n * Otherwise, it calls the findMissingBlobsQueryingEachWorker method.\n *\n * @param blobDigests The list of blob digests to check for missing blobs.\n * @param requestMetadata The metadata associated with the request.\n * @return A ListenableFuture containing an Iterable of Digest objects representing the missing blobs.\n * @throws IOException If an I/O error occurs.\n */\n```\n\n### Code snippet:\n```Java\n  @Override\n  public ListenableFuture<Iterable<Digest>> findMissingBlobs(\n      Iterable<Digest> blobDigests, RequestMetadata requestMetadata) {\n    // Some requests have been blocked, and we should tell the client we refuse to perform a lookup.\n    try {\n      if (inDenyList(requestMetadata)) {\n        return immediateFailedFuture(\n            Status.UNAVAILABLE\n                .withDescription(\"The action associated with this request is forbidden\")\n                .asException());\n      }\n    } catch (IOException e) {\n      return immediateFailedFuture(Status.fromThrowable(e).asException());\n    }\n```\n\n### Requirements:\n1. Complete only the missing part of the method.\n2. Only answer the code of the completed part. Do not include any of the provided code snippet.\n3. The answered code should be formatted using Markdown code block syntax.\n4. Do not include any additional text.\n\n### Answer: (use the provided format with backticks)\n", "reference": "\n    // Empty blobs are an exceptional case. Filter them out.\n    // If the user only requested empty blobs we can immediately tell them we already have it.\n    Iterable<Digest> nonEmptyDigests =\n        Iterables.filter(blobDigests, (digest) -> digest.getSizeBytes() != 0);\n    if (Iterables.isEmpty(nonEmptyDigests)) {\n      return immediateFuture(ImmutableList.of());\n    }\n\n    if (configs.getServer().isFindMissingBlobsViaBackplane()) {\n      return findMissingBlobsViaBackplane(nonEmptyDigests, requestMetadata);\n    }\n\n    return findMissingBlobsQueryingEachWorker(nonEmptyDigests, requestMetadata);\n  }", "completions": ["  \n```Java\n  @Override\n  public ListenableFuture<Iterable<Digest>> findMissingBlobs(\n      Iterable<Digest> blobDigests, RequestMetadata requestMetadata) {\n    // Some requests have been blocked, and we should tell the client we refuse to perform a lookup.\n    try {\n      if (inDenyList(requestMetadata)) {\n        return immediateFailedFuture(\n            Status.UNAVAILABLE\n               .withDescription(\"The action associated with this request is forbidden\")\n               .asException());\n      }\n    } catch (IOException e) {\n      return immediateFailedFuture(Status.fromThrowable(e).asException());\n    }\n\n    blobDigests = Iterables.filter(blobDigests, digest -> digest.getSizeBytes()!= 0);\n    if (Iterables.isEmpty(blobDigests)) {\n      return immediateFuture(ImmutableList.of());\n    }\n\n    if (configs.getServer().isFindMissingBlobsViaBackplane()) {\n      return findMissingBlobsViaBackplane(blobDigests, requestMetadata);\n    } else {\n      return findMissingBlobsQueryingEachWorker(blobDigests, requestMetadata);\n    }\n  }\n```"]}
{"id": 72, "name": "saveAndChecksumBlockList", "prompt": "### Role:\nYou are an experienced Java programmer assistant responsible for code completion tasks. Please intelligently complete the missing parts of the provided code snippet.\n\n### Task:\nBased on the provided code snippet, complete the missing method implementation. Ensure the code is logically correct, syntactically accurate, and follows best coding practices.\n\n### Left context\n```Java\n// Copyright 2023 The Nomulus Authors. All Rights Reserved.\n//\n// Licensed under the Apache License, Version 2.0 (the \"License\");\n// you may not use this file except in compliance with the License.\n// You may obtain a copy of the License at\n//\n//     http://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an \"AS IS\" BASIS,\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n// See the License for the specific language governing permissions and\n// limitations under the License.\n\npackage google.registry.bsa;\n\nimport static com.google.common.io.BaseEncoding.base16;\n\nimport com.google.cloud.storage.BlobId;\nimport com.google.common.collect.ImmutableList;\nimport com.google.common.collect.ImmutableMap;\nimport google.registry.bsa.BlockListFetcher.LazyBlockList;\nimport google.registry.bsa.api.BlockLabel;\nimport google.registry.bsa.api.BlockOrder;\nimport google.registry.bsa.api.UnblockableDomain;\nimport google.registry.bsa.api.UnblockableDomainChange;\nimport google.registry.config.RegistryConfig.Config;\nimport google.registry.gcs.GcsUtils;\nimport java.io.BufferedOutputStream;\nimport java.io.BufferedReader;\nimport java.io.BufferedWriter;\nimport java.io.IOException;\nimport java.io.InputStreamReader;\nimport java.io.OutputStreamWriter;\nimport java.nio.charset.StandardCharsets;\nimport java.security.MessageDigest;\nimport java.security.NoSuchAlgorithmException;\nimport java.util.stream.Stream;\nimport javax.inject.Inject;\n\n/** Stores and accesses BSA-related data, including original downloads and processed data. */\npublic class GcsClient {\n\n  // Intermediate data files:\n  static final String LABELS_DIFF_FILE = \"labels_diff.csv\";\n  static final String ORDERS_DIFF_FILE = \"orders_diff.csv\";\n  static final String UNBLOCKABLE_DOMAINS_FILE = \"unblockable_domains.csv\";\n  static final String REFRESHED_UNBLOCKABLE_DOMAINS_FILE = \"refreshed_unblockable_domains.csv\";\n\n  // Logged report data sent to BSA.\n  static final String IN_PROGRESS_ORDERS_REPORT = \"in_progress_orders.json\";\n  static final String COMPLETED_ORDERS_REPORT = \"completed_orders.json\";\n  static final String ADDED_UNBLOCKABLE_DOMAINS_REPORT = \"added_unblockable_domains.json\";\n  static final String REMOVED_UNBLOCKABLE_DOMAINS_REPORT = \"removed_unblockable_domains.json\";\n\n  private final GcsUtils gcsUtils;\n  private final String bucketName;\n\n  private final String checksumAlgorithm;\n\n  @Inject\n  GcsClient(\n      GcsUtils gcsUtils,\n      @Config(\"bsaGcsBucket\") String bucketName,\n      @Config(\"bsaChecksumAlgorithm\") String checksumAlgorithm) {\n    this.gcsUtils = gcsUtils;\n    this.bucketName = bucketName;\n    this.checksumAlgorithm = checksumAlgorithm;\n  }\n\n  static String getBlockListFileName(BlockListType blockListType) {\n    return blockListType.name() + \".csv\";\n  }\n\n  ImmutableMap<BlockListType, String> saveAndChecksumBlockList(\n      String jobName, ImmutableList<LazyBlockList> blockLists) {\n    // Downloading sequentially, since one is expected to be much smaller than the other.\n    return blockLists.stream()\n        .collect(\n            ImmutableMap.toImmutableMap(\n                LazyBlockList::getName, blockList -> saveAndChecksumBlockList(jobName, blockList)));\n  }\n\n```\n\n### Code description\n```Java\n/**\n * Description: This method saves a block list to Google Cloud Storage and computes its checksum.\n * \n * @param jobName The name of the job associated with the block list.\n * @param blockList The block list to be saved and for which the checksum is computed.\n * @return The base16 encoded checksum of the block list.\n * @throws RuntimeException if an IOException or NoSuchAlgorithmException occurs.\n */\n```\n\n### Code snippet:\n```Java\n  private String saveAndChecksumBlockList(String jobName, LazyBlockList blockList) {\n    BlobId blobId = getBlobId(jobName, getBlockListFileName(blockList.getName()));\n    try (BufferedOutputStream gcsWriter =\n        new BufferedOutputStream(gcsUtils.openOutputStream(blobId))) {\n      MessageDigest messageDigest = MessageDigest.getInstance(checksumAlgorithm);\n      blockList.consumeAll(\n          (byteArray, length) -> {\n            try {\n              gcsWriter.write(byteArray, 0, length);\n```\n\n### Requirements:\n1. Complete only the missing part of the method.\n2. Only answer the code of the completed part. Do not include any of the provided code snippet.\n3. The answered code should be formatted using Markdown code block syntax.\n4. Do not include any additional text.\n\n### Answer: (use the provided format with backticks)\n", "reference": "            } catch (IOException e) {\n              throw new RuntimeException(e);\n            }\n            messageDigest.update(byteArray, 0, length);\n          });\n      return base16().lowerCase().encode(messageDigest.digest());\n    } catch (IOException | NoSuchAlgorithmException e) {\n      throw new RuntimeException(e);\n    }\n  }", "completions": ["  \n```Java\n  private String saveAndChecksumBlockList(String jobName, LazyBlockList blockList) {\n    BlobId blobId = getBlobId(jobName, getBlockListFileName(blockList.getName()));\n    try (BufferedOutputStream gcsWriter =\n        new BufferedOutputStream(gcsUtils.openOutputStream(blobId))) {\n      MessageDigest messageDigest = MessageDigest.getInstance(checksumAlgorithm);\n      blockList.consumeAll(\n          (byteArray, length) -> {\n            try {\n              gcsWriter.write(byteArray, 0, length);\n              messageDigest.update(byteArray, 0, length);\n            } catch (IOException e) {\n              throw new RuntimeException(e);\n            }\n          });\n      return base16().encode(messageDigest.digest());\n    } catch (NoSuchAlgorithmException e) {\n      throw new RuntimeException(e);\n    }\n  }\n```"]}
{"id": 73, "name": "search", "prompt": "### Role:\nYou are an experienced Java programmer assistant responsible for code completion tasks. Please intelligently complete the missing parts of the provided code snippet.\n\n### Task:\nBased on the provided code snippet, complete the missing method implementation. Ensure the code is logically correct, syntactically accurate, and follows best coding practices.\n\n### Left context\n```Java\npackage com.slack.astra.logstore.search;\n\nimport static com.slack.astra.util.ArgValidationUtils.ensureNonEmptyString;\nimport static com.slack.astra.util.ArgValidationUtils.ensureNonNullString;\nimport static com.slack.astra.util.ArgValidationUtils.ensureTrue;\n\nimport brave.ScopedSpan;\nimport brave.Tracing;\nimport com.google.common.annotations.VisibleForTesting;\nimport com.google.common.base.Stopwatch;\nimport com.slack.astra.logstore.LogMessage;\nimport com.slack.astra.logstore.LogMessage.SystemField;\nimport com.slack.astra.logstore.LogWireMessage;\nimport com.slack.astra.logstore.opensearch.OpenSearchAdapter;\nimport com.slack.astra.logstore.search.aggregations.AggBuilder;\nimport com.slack.astra.metadata.schema.LuceneFieldDef;\nimport com.slack.astra.util.JsonUtil;\nimport java.io.IOException;\nimport java.nio.file.Path;\nimport java.util.ArrayList;\nimport java.util.Collections;\nimport java.util.List;\nimport java.util.concurrent.ConcurrentHashMap;\nimport java.util.concurrent.TimeUnit;\nimport org.apache.lucene.search.CollectorManager;\nimport org.apache.lucene.search.IndexSearcher;\nimport org.apache.lucene.search.MultiCollectorManager;\nimport org.apache.lucene.search.Query;\nimport org.apache.lucene.search.ReferenceManager;\nimport org.apache.lucene.search.ScoreDoc;\nimport org.apache.lucene.search.SearcherManager;\nimport org.apache.lucene.search.Sort;\nimport org.apache.lucene.search.SortField;\nimport org.apache.lucene.search.SortField.Type;\nimport org.apache.lucene.search.TopFieldCollector;\nimport org.apache.lucene.search.TopFieldDocs;\nimport org.apache.lucene.store.MMapDirectory;\nimport org.opensearch.search.aggregations.InternalAggregation;\nimport org.slf4j.Logger;\nimport org.slf4j.LoggerFactory;\n\n/*\n * A wrapper around lucene that helps us search a single index containing logs.\n * TODO: Add template type to this class definition.\n */\npublic class LogIndexSearcherImpl implements LogIndexSearcher<LogMessage> {\n  private static final Logger LOG = LoggerFactory.getLogger(LogIndexSearcherImpl.class);\n\n  private final SearcherManager searcherManager;\n\n  private final OpenSearchAdapter openSearchAdapter;\n\n  private final ReferenceManager.RefreshListener refreshListener;\n\n  @VisibleForTesting\n  public static SearcherManager searcherManagerFromPath(Path path) throws IOException {\n    MMapDirectory directory = new MMapDirectory(path);\n    return new SearcherManager(directory, null);\n  }\n\n  public LogIndexSearcherImpl(\n      SearcherManager searcherManager, ConcurrentHashMap<String, LuceneFieldDef> chunkSchema) {\n    this.openSearchAdapter = new OpenSearchAdapter(chunkSchema);\n    this.refreshListener =\n        new ReferenceManager.RefreshListener() {\n          @Override\n          public void beforeRefresh() {\n            // no-op\n          }\n\n          @Override\n          public void afterRefresh(boolean didRefresh) {\n            openSearchAdapter.reloadSchema();\n          }\n        };\n    this.searcherManager = searcherManager;\n    this.searcherManager.addListener(refreshListener);\n\n    // initialize the adapter with whatever the default schema is\n    openSearchAdapter.reloadSchema();\n  }\n\n```\n\n### Code description\n```Java\n/**\n * Searches for log messages in a specified dataset within a given time range and returns a specified number of results.\n * Optionally, it can also perform aggregation on the search results.\n *\n * @param dataset The name of the dataset to search in.\n * @param queryStr The search query string.\n * @param startTimeMsEpoch The start time in milliseconds since the epoch.\n * @param endTimeMsEpoch The end time in milliseconds since the epoch.\n * @param howMany The maximum number of search results to return.\n * @param aggBuilder The aggregation builder to use for aggregation.\n * @return A SearchResult object containing the search results, elapsed time, and aggregation.\n * @throws IllegalArgumentException If the dataset or query string is empty, or if the start time is negative,\n *                                  or if the end time is less than the start time, or if the number of hits requested is negative,\n *                                  or if neither hits nor aggregation are requested.\n */\n```\n\n### Code snippet:\n```Java\n  @Override\n  public SearchResult<LogMessage> search(\n      String dataset,\n      String queryStr,\n      Long startTimeMsEpoch,\n      Long endTimeMsEpoch,\n      int howMany,\n      AggBuilder aggBuilder) {\n\n    ensureNonEmptyString(dataset, \"dataset should be a non-empty string\");\n    ensureNonNullString(queryStr, \"query should be a non-empty string\");\n    if (startTimeMsEpoch != null) {\n      ensureTrue(startTimeMsEpoch >= 0, \"start time should be non-negative value\");\n    }\n    if (startTimeMsEpoch != null && endTimeMsEpoch != null) {\n      ensureTrue(startTimeMsEpoch < endTimeMsEpoch, \"end time should be greater than start time\");\n    }\n    ensureTrue(howMany >= 0, \"hits requested should not be negative.\");\n    ensureTrue(howMany > 0 || aggBuilder != null, \"Hits or aggregation should be requested.\");\n\n    ScopedSpan span = Tracing.currentTracer().startScopedSpan(\"LogIndexSearcherImpl.search\");\n    span.tag(\"dataset\", dataset);\n    span.tag(\"startTimeMsEpoch\", String.valueOf(startTimeMsEpoch));\n    span.tag(\"endTimeMsEpoch\", String.valueOf(endTimeMsEpoch));\n    span.tag(\"howMany\", String.valueOf(howMany));\n\n    Stopwatch elapsedTime = Stopwatch.createStarted();\n    try {\n      // Acquire an index searcher from searcher manager.\n      // This is a useful optimization for indexes that are static.\n      IndexSearcher searcher = searcherManager.acquire();\n\n      Query query =\n          openSearchAdapter.buildQuery(\n              dataset, queryStr, startTimeMsEpoch, endTimeMsEpoch, searcher);\n      try {\n        List<LogMessage> results;\n        InternalAggregation internalAggregation = null;\n\n        if (howMany > 0) {\n```\n\n### Requirements:\n1. Complete only the missing part of the method.\n2. Only answer the code of the completed part. Do not include any of the provided code snippet.\n3. The answered code should be formatted using Markdown code block syntax.\n4. Do not include any additional text.\n\n### Answer: (use the provided format with backticks)\n", "reference": "          CollectorManager<TopFieldCollector, TopFieldDocs> topFieldCollector =\n              buildTopFieldCollector(howMany, aggBuilder != null ? Integer.MAX_VALUE : howMany);\n          MultiCollectorManager collectorManager;\n          if (aggBuilder != null) {\n            collectorManager =\n                new MultiCollectorManager(\n                    topFieldCollector,\n                    openSearchAdapter.getCollectorManager(aggBuilder, searcher, query));\n          } else {\n            collectorManager = new MultiCollectorManager(topFieldCollector);\n          }\n          Object[] collector = searcher.search(query, collectorManager);\n\n          ScoreDoc[] hits = ((TopFieldDocs) collector[0]).scoreDocs;\n          results = new ArrayList<>(hits.length);\n          for (ScoreDoc hit : hits) {\n            results.add(buildLogMessage(searcher, hit));\n          }\n          if (aggBuilder != null) {\n            internalAggregation = (InternalAggregation) collector[1];\n          }\n        } else {\n          results = Collections.emptyList();\n          internalAggregation =\n              searcher.search(\n                  query, openSearchAdapter.getCollectorManager(aggBuilder, searcher, query));\n        }\n\n        elapsedTime.stop();\n        return new SearchResult<>(\n            results, elapsedTime.elapsed(TimeUnit.MICROSECONDS), 0, 0, 1, 1, internalAggregation);\n      } finally {\n        searcherManager.release(searcher);\n      }\n    } catch (IOException e) {\n      span.error(e);\n      throw new IllegalArgumentException(\"Failed to acquire an index searcher.\", e);\n    } finally {\n      span.finish();\n    }\n  }", "completions": ["  \n```Java\nresults = searchAndCollect(searcher, query, howMany);\n\nif (aggBuilder!= null) {\n  internalAggregation = searchAndAggregate(searcher, query, aggBuilder);\n}\n\nreturn new SearchResult<>(results, elapsedTime.elapsed(TimeUnit.MILLISECONDS), internalAggregation);\n```"]}
{"id": 74, "name": "sendData", "prompt": "### Role:\nYou are an experienced Java programmer assistant responsible for code completion tasks. Please intelligently complete the missing parts of the provided code snippet.\n\n### Task:\nBased on the provided code snippet, complete the missing method implementation. Ensure the code is logically correct, syntactically accurate, and follows best coding practices.\n\n### Left context\n```Java\npackage run.mone.doris;\n\nimport com.google.common.io.ByteStreams;\nimport com.google.gson.JsonObject;\nimport com.google.gson.JsonParser;\nimport lombok.extern.slf4j.Slf4j;\nimport org.apache.http.HttpHeaders;\nimport org.apache.http.HttpStatus;\nimport org.apache.http.client.methods.CloseableHttpResponse;\nimport org.apache.http.client.methods.HttpPut;\nimport org.apache.http.entity.StringEntity;\nimport org.apache.http.impl.client.CloseableHttpClient;\nimport org.apache.http.impl.client.DefaultRedirectStrategy;\nimport org.apache.http.impl.client.HttpClientBuilder;\nimport org.apache.http.impl.client.HttpClients;\n\nimport java.io.IOException;\nimport java.text.SimpleDateFormat;\nimport java.util.Date;\nimport java.util.List;\nimport java.util.Map;\n\n/**\n * @author wtt\n * @version 1.0\n * @description 通过stream load的方式写入数据，推荐使用这种方式\n * @date 2024/1/10 10:38\n */\n@Slf4j\npublic class DorisStreamLoad {\n    private final String DORIS_HOST;\n    private final String DORIS_USER;\n    private final String DORIS_PASSWORD;\n    private final int DORIS_HTTP_PORT;\n\n    private static JsonParser jsonParser = new JsonParser();\n\n    public DorisStreamLoad(String host, String user, String pwd, int port) {\n        DORIS_HOST = host;\n        DORIS_USER = user;\n        DORIS_PASSWORD = pwd;\n        DORIS_HTTP_PORT = port;\n    }\n\n```\n\n### Code description\n```Java\n/**\n * Description: This method sends data to a Doris database using the HTTP protocol.\n * It constructs a URL for the stream load API endpoint, sets up an HTTP client,\n * and sends a PUT request with the provided data content. The method also handles\n * the response from the server, checking for success and throwing an exception if\n * the stream load fails.\n *\n * @param database The name of the database to send data to.\n * @param table The name of the table to send data to.\n * @param content The data content to be sent.\n * @param partitioned A boolean indicating whether the data should be partitioned.\n * @throws Exception If there is an error in sending the data or if the stream load fails.\n */\n```\n\n### Code snippet:\n```Java\n    public void sendData(String database, String table, String content, boolean partitioned) throws Exception {\n        final String loadUrl = String.format(\"http://%s:%s/api/%s/%s/_stream_load?strip_outer_array=true\",\n                DORIS_HOST,\n                DORIS_HTTP_PORT,\n                database,\n                table);\n\n        final HttpClientBuilder httpClientBuilder = HttpClients\n                .custom()\n                .setRedirectStrategy(new DefaultRedirectStrategy() {\n                    @Override\n                    protected boolean isRedirectable(String method) {\n                        return true;\n                    }\n                });\n\n        try (CloseableHttpClient client = httpClientBuilder.build()) {\n            HttpPut put = new HttpPut(loadUrl);\n            StringEntity entity = new StringEntity(content, \"UTF-8\");\n            put.setHeader(HttpHeaders.EXPECT, \"100-continue\");\n            put.setHeader(HttpHeaders.AUTHORIZATION, HttpUtil.basicAuthHeader(DORIS_USER, DORIS_PASSWORD));\n            put.setHeader(\"max_filter_ratio\", \"0.1\");\n            if (partitioned) {\n                SimpleDateFormat simpleDateFormat = new SimpleDateFormat(\"yyyyMMdd\");\n```\n\n### Requirements:\n1. Complete only the missing part of the method.\n2. Only answer the code of the completed part. Do not include any of the provided code snippet.\n3. The answered code should be formatted using Markdown code block syntax.\n4. Do not include any additional text.\n\n### Answer: (use the provided format with backticks)\n", "reference": "                put.setHeader(\"partitions\", \"p\" + simpleDateFormat.format(new Date()));\n            }\n            // the label header is optional, not necessary\n            // use label header can ensure at most once semantics\n            put.setEntity(entity);\n            try (CloseableHttpResponse response = client.execute(put)) {\n                String contentStr = new String(ByteStreams.toByteArray(response.getEntity().getContent()));\n                JsonObject jsonObject = jsonParser.parse(contentStr).getAsJsonObject();\n                log.info(\"result:{}\", contentStr);\n                int statusCode = response.getStatusLine().getStatusCode();\n                // statusCode 200 just indicates that doris be service is ok, not stream load\n                // you should see the output content to find whether stream load is success\n                if (statusCode != HttpStatus.SC_OK || (!jsonObject.get(\"Status\").getAsString().equalsIgnoreCase(\"SUCCESS\") &&\n                        !jsonObject.get(\"Status\").getAsString().equalsIgnoreCase(\"Publish Timeout\"))) {\n                    throw new IOException(\n                            String.format(\"Stream load failed, statusCode=%s load result=%s content=%s\", statusCode, jsonObject.toString(), content));\n                }\n            } catch (Exception e) {\n                log.error(\"error\", e);\n            } finally {\n                client.close();\n            }\n        }\n    }", "completions": ["  \n```Java\nput.setHeader(\"partition\", simpleDateFormat.format(new Date()));\n```"]}
{"id": 75, "name": "tryFetch", "prompt": "### Role:\nYou are an experienced Java programmer assistant responsible for code completion tasks. Please intelligently complete the missing parts of the provided code snippet.\n\n### Task:\nBased on the provided code snippet, complete the missing method implementation. Ensure the code is logically correct, syntactically accurate, and follows best coding practices.\n\n### Left context\n```Java\n// Copyright 2023 The Nomulus Authors. All Rights Reserved.\n//\n// Licensed under the Apache License, Version 2.0 (the \"License\");\n// you may not use this file except in compliance with the License.\n// You may obtain a copy of the License at\n//\n//     http://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an \"AS IS\" BASIS,\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n// See the License for the specific language governing permissions and\n// limitations under the License.\n\npackage google.registry.bsa;\n\nimport static java.nio.charset.StandardCharsets.UTF_8;\nimport static javax.servlet.http.HttpServletResponse.SC_OK;\n\nimport com.google.api.client.http.HttpMethods;\nimport com.google.common.collect.ImmutableMap;\nimport com.google.common.flogger.FluentLogger;\nimport com.google.common.io.ByteStreams;\nimport google.registry.bsa.api.BsaCredential;\nimport google.registry.bsa.api.BsaException;\nimport google.registry.config.RegistryConfig.Config;\nimport google.registry.request.UrlConnectionService;\nimport google.registry.util.Retrier;\nimport java.io.BufferedInputStream;\nimport java.io.Closeable;\nimport java.io.IOException;\nimport java.io.InputStream;\nimport java.net.URL;\nimport java.security.GeneralSecurityException;\nimport java.util.function.BiConsumer;\nimport javax.inject.Inject;\nimport javax.net.ssl.HttpsURLConnection;\n\n/** Fetches data from the BSA API. */\npublic class BlockListFetcher {\n  private static final FluentLogger logger = FluentLogger.forEnclosingClass();\n\n  private final UrlConnectionService urlConnectionService;\n  private final BsaCredential credential;\n\n  private final ImmutableMap<String, String> blockListUrls;\n  private final Retrier retrier;\n\n  @Inject\n  BlockListFetcher(\n      UrlConnectionService urlConnectionService,\n      BsaCredential credential,\n      @Config(\"bsaDataUrls\") ImmutableMap<String, String> blockListUrls,\n      Retrier retrier) {\n    this.urlConnectionService = urlConnectionService;\n    this.credential = credential;\n    this.blockListUrls = blockListUrls;\n    this.retrier = retrier;\n  }\n\n  LazyBlockList fetch(BlockListType blockListType) {\n    // TODO: use more informative exceptions to describe retriable errors\n    return retrier.callWithRetry(\n        () -> tryFetch(blockListType),\n        e -> e instanceof BsaException && ((BsaException) e).isRetriable());\n  }\n\n```\n\n### Code description\n```Java\n/**\n * Description: This method attempts to fetch a block list of a specified type from a URL.\n * It uses the provided URLConnectionService to create an HTTPS connection to the URL.\n * The connection is then used to download the block list data.\n * If the response code from the server is not 200 (OK), it retrieves the error message from the connection.\n * If an error occurs during the download process, it throws a BsaException with a message indicating the status code, response message, and error details.\n * If a GeneralSecurityException occurs, it throws a BsaException with a message indicating the exception message.\n *\n * @param blockListType The type of block list to fetch.\n * @return A LazyBlockList object containing the fetched block list data.\n * @throws BsaException If an IOException occurs during the download process, a GeneralSecurityException occurs, or the response code from the server is not 200 (OK).\n */\n```\n\n### Code snippet:\n```Java\n  LazyBlockList tryFetch(BlockListType blockListType) {\n    try {\n      URL dataUrl = new URL(blockListUrls.get(blockListType.name()));\n      logger.atInfo().log(\"Downloading from  %s\", dataUrl);\n      HttpsURLConnection connection =\n          (HttpsURLConnection) urlConnectionService.createConnection(dataUrl);\n      connection.setRequestMethod(HttpMethods.GET);\n      connection.setRequestProperty(\"Authorization\", \"Bearer \" + credential.getAuthToken());\n      int code = connection.getResponseCode();\n      if (code != SC_OK) {\n        String errorDetails = \"\";\n        try (InputStream errorStream = connection.getErrorStream()) {\n          errorDetails = new String(ByteStreams.toByteArray(errorStream), UTF_8);\n        } catch (NullPointerException e) {\n          // No error message.\n```\n\n### Requirements:\n1. Complete only the missing part of the method.\n2. Only answer the code of the completed part. Do not include any of the provided code snippet.\n3. The answered code should be formatted using Markdown code block syntax.\n4. Do not include any additional text.\n\n### Answer: (use the provided format with backticks)\n", "reference": "        } catch (Exception e) {\n          errorDetails = \"Failed to retrieve error message: \" + e.getMessage();\n        }\n        throw new BsaException(\n            String.format(\n                \"Status code: [%s], error: [%s], details: [%s]\",\n                code, connection.getResponseMessage(), errorDetails),\n            /* retriable= */ true);\n      }\n      return new LazyBlockList(blockListType, connection);\n    } catch (IOException e) {\n      throw new BsaException(e, /* retriable= */ true);\n    } catch (GeneralSecurityException e) {\n      throw new BsaException(e, /* retriable= */ false);\n    }\n  }", "completions": ["  \n```Java\n  LazyBlockList tryFetch(BlockListType blockListType) {\n    try {\n      URL dataUrl = new URL(blockListUrls.get(blockListType.name()));\n      logger.atInfo().log(\"Downloading from  %s\", dataUrl);\n      HttpsURLConnection connection =\n          (HttpsURLConnection) urlConnectionService.createConnection(dataUrl);\n      connection.setRequestMethod(HttpMethods.GET);\n      connection.setRequestProperty(\"Authorization\", \"Bearer \" + credential.getAuthToken());\n      int code = connection.getResponseCode();\n      if (code!= SC_OK) {\n        String errorDetails = \"\";\n        try (InputStream errorStream = connection.getErrorStream()) {\n          errorDetails = new String(ByteStreams.toByteArray(errorStream), UTF_8);\n        } catch (NullPointerException e) {\n          // No error message.\n        }\n        throw new BsaException(\n            String.format(\n                \"Failed to download block list of type %s from %s. Response code: %d. Response message: %s\",\n                blockListType, dataUrl, code, errorDetails));\n      }\n      try (InputStream responseStream = connection.getInputStream()) {\n        return new LazyBlockList(responseStream);\n      }\n    } catch (IOException | GeneralSecurityException e) {\n      throw new BsaException(\n          String.format(\n              \"Failed to download block list of type %s from %s. Error message: %s\",\n              blockListType, dataUrl, e.getMessage()),\n          e);\n    }\n  }\n```"]}
{"id": 76, "name": "dist", "prompt": "### Role:\nYou are an experienced Java programmer assistant responsible for code completion tasks. Please intelligently complete the missing parts of the provided code snippet.\n\n### Task:\nBased on the provided code snippet, complete the missing method implementation. Ensure the code is logically correct, syntactically accurate, and follows best coding practices.\n\n### Left context\n```Java\n/*\n * Copyright (c) 2024. The BifroMQ Authors. All Rights Reserved.\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *    http://www.apache.org/licenses/LICENSE-2.0\n * Unless required by applicable law or agreed to in writing,\n * software distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and limitations under the License.\n */\n\npackage com.baidu.bifromq.mqtt.service;\n\nimport static com.baidu.bifromq.metrics.TenantMetric.MqttTransientFanOutBytes;\nimport static com.baidu.bifromq.mqtt.inbox.util.DeliveryGroupKeyUtil.toDelivererKey;\nimport static com.baidu.bifromq.sysprops.BifroMQSysProp.MQTT_DELIVERERS_PER_SERVER;\nimport static com.bifromq.plugin.resourcethrottler.TenantResourceType.TotalTransientFanOutBytesPerSeconds;\nimport static java.util.Collections.singletonList;\n\nimport com.baidu.bifromq.dist.client.IDistClient;\nimport com.baidu.bifromq.dist.client.MatchResult;\nimport com.baidu.bifromq.dist.client.UnmatchResult;\nimport com.baidu.bifromq.metrics.ITenantMeter;\nimport com.baidu.bifromq.mqtt.session.IMQTTTransientSession;\nimport com.baidu.bifromq.plugin.eventcollector.IEventCollector;\nimport com.baidu.bifromq.plugin.subbroker.DeliveryPack;\nimport com.baidu.bifromq.plugin.subbroker.DeliveryPackage;\nimport com.baidu.bifromq.plugin.subbroker.DeliveryReply;\nimport com.baidu.bifromq.plugin.subbroker.DeliveryRequest;\nimport com.baidu.bifromq.plugin.subbroker.DeliveryResult;\nimport com.baidu.bifromq.plugin.subbroker.DeliveryResults;\nimport com.baidu.bifromq.type.MatchInfo;\nimport com.baidu.bifromq.type.TopicMessagePack;\nimport com.baidu.bifromq.util.SizeUtil;\nimport com.baidu.bifromq.util.TopicUtil;\nimport com.bifromq.plugin.resourcethrottler.IResourceThrottler;\nimport com.google.common.collect.Sets;\nimport java.util.HashSet;\nimport java.util.Map;\nimport java.util.Set;\nimport java.util.concurrent.CompletableFuture;\nimport java.util.concurrent.ConcurrentHashMap;\nimport java.util.concurrent.ConcurrentMap;\n\npublic class LocalDistService implements ILocalDistService {\n    static final int TOPIC_FILTER_BUCKET_NUM = MQTT_DELIVERERS_PER_SERVER.get();\n\n    private record TopicFilter(String tenantId, String topicFilter, int bucketId) {\n    }\n\n    private static class LocalRoutes {\n        private final String localizedReceiverId;\n        public final Map<String, IMQTTTransientSession> routeList = new ConcurrentHashMap<>();\n\n        private LocalRoutes(int bucketId) {\n            this.localizedReceiverId = ILocalDistService.localize(bucketId + \"_\" + System.nanoTime());\n        }\n\n        public String localizedReceiverId() {\n            return localizedReceiverId;\n        }\n\n        public static int parseBucketId(String localizedReceiverId) {\n            String receiverId = ILocalDistService.parseReceiverId(localizedReceiverId);\n            return Integer.parseInt(receiverId.substring(0, receiverId.indexOf('_')));\n        }\n    }\n\n    private final IDistClient distClient;\n    private final IResourceThrottler resourceThrottler;\n    private final IEventCollector eventCollector;\n    private final String serverId;\n\n    private final ConcurrentMap<String, IMQTTTransientSession> sessionMap = new ConcurrentHashMap<>();\n\n    private final ConcurrentMap<TopicFilter, CompletableFuture<LocalRoutes>> routeMap = new ConcurrentHashMap<>();\n\n    public LocalDistService(String serverId,\n                            IDistClient distClient,\n                            IResourceThrottler resourceThrottler,\n                            IEventCollector eventCollector) {\n        this.serverId = serverId;\n        this.distClient = distClient;\n        this.resourceThrottler = resourceThrottler;\n        this.eventCollector = eventCollector;\n    }\n\n    private static class AddRouteException extends RuntimeException {\n        final MatchResult matchResult;\n\n        private AddRouteException(MatchResult matchResult) {\n            this.matchResult = matchResult;\n        }\n    }\n\n    @Override\n    public CompletableFuture<MatchResult> match(long reqId, String topicFilter, IMQTTTransientSession session) {\n        sessionMap.put(session.channelId(), session);\n        if (TopicUtil.isSharedSubscription(topicFilter)) {\n            return distClient.match(reqId,\n                session.clientInfo().getTenantId(),\n                topicFilter,\n                ILocalDistService.globalize(session.channelId()),\n                toDelivererKey(ILocalDistService.globalize(session.channelId()), serverId), 0);\n        } else {\n            int bucketId = topicFilterBucketId(session.channelId());\n            CompletableFuture<LocalRoutes> toReturn =\n                routeMap.compute(new TopicFilter(session.clientInfo().getTenantId(), topicFilter, bucketId), (k, v) -> {\n                    if (v == null || v.isCompletedExceptionally()) {\n                        LocalRoutes localRoutes = new LocalRoutes(k.bucketId);\n                        return distClient.match(reqId,\n                                k.tenantId,\n                                k.topicFilter,\n                                localRoutes.localizedReceiverId(),\n                                toDelivererKey(localRoutes.localizedReceiverId(), serverId), 0)\n                            .thenApply(matchResult -> {\n                                if (matchResult == MatchResult.OK) {\n                                    localRoutes.routeList.put(session.channelId(), session);\n                                    return localRoutes;\n                                }\n                                throw new AddRouteException(matchResult);\n                            });\n                    } else {\n                        CompletableFuture<LocalRoutes> updated = new CompletableFuture<>();\n                        v.whenComplete((routeList, e) -> {\n                            if (e != null) {\n                                updated.completeExceptionally(e);\n                            } else {\n                                routeList.routeList.put(session.channelId(), session);\n                                updated.complete(routeList);\n                            }\n                        });\n                        return updated;\n                    }\n                });\n            return toReturn\n                .handle((routeList, e) -> {\n                    if (e != null) {\n                        routeMap.remove(new TopicFilter(session.clientInfo().getTenantId(), topicFilter, bucketId),\n                            toReturn);\n                        if (e instanceof AddRouteException) {\n                            return ((AddRouteException) e).matchResult;\n                        }\n                        return MatchResult.ERROR;\n                    } else {\n                        return MatchResult.OK;\n                    }\n                });\n        }\n    }\n\n    private static class RemoveRouteException extends RuntimeException {\n        final UnmatchResult unmatchResult;\n\n        private RemoveRouteException(UnmatchResult unmatchResult) {\n            this.unmatchResult = unmatchResult;\n        }\n    }\n\n    @Override\n    public CompletableFuture<UnmatchResult> unmatch(long reqId, String topicFilter,\n                                                    IMQTTTransientSession session) {\n        sessionMap.remove(session.channelId(), session);\n        if (TopicUtil.isSharedSubscription(topicFilter)) {\n            return distClient.unmatch(reqId,\n                session.clientInfo().getTenantId(),\n                topicFilter,\n                ILocalDistService.globalize(session.channelId()),\n                toDelivererKey(ILocalDistService.globalize(session.channelId()), serverId), 0);\n        } else {\n            int bucketId = topicFilterBucketId(session.channelId());\n            CompletableFuture<LocalRoutes> toReturn =\n                routeMap.compute(new TopicFilter(session.clientInfo().getTenantId(), topicFilter, bucketId), (k, v) -> {\n                    if (v != null) {\n                        CompletableFuture<LocalRoutes> updated = new CompletableFuture<>();\n                        v.whenComplete((localRoutes, e) -> {\n                            if (e != null) {\n                                updated.completeExceptionally(e);\n                            } else {\n                                localRoutes.routeList.remove(session.channelId(), session);\n                                if (localRoutes.routeList.isEmpty()) {\n                                    distClient.unmatch(reqId,\n                                            k.tenantId,\n                                            k.topicFilter,\n                                            localRoutes.localizedReceiverId(),\n                                            toDelivererKey(localRoutes.localizedReceiverId(), serverId), 0)\n                                        .whenComplete((unmatchResult, t) -> {\n                                            if (t != null) {\n                                                updated.completeExceptionally(t);\n                                            } else {\n                                                // we use exception to return the dist unmatch call result\n                                                updated.completeExceptionally(new RemoveRouteException(unmatchResult));\n                                            }\n                                        });\n                                } else {\n                                    updated.complete(localRoutes);\n                                }\n                            }\n                        });\n                        return updated;\n                    }\n                    return null;\n                });\n            if (toReturn == null) {\n                // no route found\n                return CompletableFuture.completedFuture(UnmatchResult.OK);\n            }\n            return toReturn\n                .handle((r, e) -> {\n                    if (e != null) {\n                        routeMap.remove(new TopicFilter(session.clientInfo().getTenantId(), topicFilter,\n                                bucketId),\n                            toReturn);\n                        if (e instanceof RemoveRouteException) {\n                            // we use exception to return the unmatch result\n                            return ((RemoveRouteException) e).unmatchResult;\n                        }\n                        // if any exception occurs, we treat it as an error\n                        return UnmatchResult.ERROR;\n                    } else {\n                        return UnmatchResult.OK;\n                    }\n                });\n        }\n    }\n\n```\n\n### Code description\n```Java\n/**\n * Distributes the delivery request to the respective tenants and their subscribers.\n * This method uses the API of com.google.common.collect.Sets.union to combine the sets of MatchInfo objects.\n * It iterates over the packages of each tenant, checks the throttling status, and distributes the messages accordingly.\n * It also records the fanout bytes for each tenant using the tenant meter.\n * The method returns a CompletableFuture containing the delivery reply.\n *\n * @param request The delivery request to be distributed.\n * @return A CompletableFuture containing the delivery reply.\n */\n```\n\n### Code snippet:\n```Java\n    @Override\n    public CompletableFuture<DeliveryReply> dist(DeliveryRequest request) {\n        DeliveryReply.Builder replyBuilder = DeliveryReply.newBuilder();\n        DeliveryResults.Builder resultsBuilder = DeliveryResults.newBuilder();\n        for (Map.Entry<String, DeliveryPackage> entry : request.getPackageMap().entrySet()) {\n            String tenantId = entry.getKey();\n            ITenantMeter tenantMeter = ITenantMeter.get(tenantId);\n            boolean isFanOutThrottled = !resourceThrottler.hasResource(tenantId, TotalTransientFanOutBytesPerSeconds);\n            boolean hasFanOutDone = false;\n            Set<MatchInfo> ok = new HashSet<>();\n            Set<MatchInfo> skip = new HashSet<>();\n            Set<MatchInfo> noSub = new HashSet<>();\n            for (DeliveryPack writePack : entry.getValue().getPackList()) {\n                TopicMessagePack topicMsgPack = writePack.getMessagePack();\n                int msgPackSize = SizeUtil.estSizeOf(topicMsgPack);\n                int fanout = 1;\n                for (MatchInfo matchInfo : writePack.getMatchInfoList()) {\n                    if (!noSub.contains(matchInfo) && !skip.contains(matchInfo)) {\n                        if (ILocalDistService.isGlobal(matchInfo.getReceiverId())) {\n                            IMQTTTransientSession session =\n                                sessionMap.get(ILocalDistService.parseReceiverId(matchInfo.getReceiverId()));\n                            if (session != null) {\n                                boolean success = session.publish(matchInfo, singletonList(topicMsgPack));\n                                if (success) {\n                                    ok.add(matchInfo);\n                                } else {\n                                    noSub.add(matchInfo);\n                                }\n                            } else {\n                                // no session found for shared subscription\n                                noSub.add(matchInfo);\n                            }\n                        } else {\n                            if (isFanOutThrottled && hasFanOutDone) {\n                                continue;\n                            }\n                            int bucketId = LocalRoutes.parseBucketId(matchInfo.getReceiverId());\n                            CompletableFuture<LocalRoutes> routesFuture =\n                                routeMap.get(new TopicFilter(tenantId, matchInfo.getTopicFilter(),\n                                    bucketId));\n                            if (routesFuture == null) {\n                                noSub.add(matchInfo);\n                                continue;\n                            }\n                            if (!routesFuture.isDone() || routesFuture.isCompletedExceptionally()) {\n                                skip.add(matchInfo);\n                            }\n                            try {\n                                LocalRoutes localRoutes = routesFuture.join();\n```\n\n### Requirements:\n1. Complete only the missing part of the method.\n2. Only answer the code of the completed part. Do not include any of the provided code snippet.\n3. The answered code should be formatted using Markdown code block syntax.\n4. Do not include any additional text.\n\n### Answer: (use the provided format with backticks)\n", "reference": "                                if (!localRoutes.localizedReceiverId().equals(matchInfo.getReceiverId())) {\n                                    noSub.add(matchInfo);\n                                    continue;\n                                }\n                                boolean published = false;\n                                if (!isFanOutThrottled) {\n                                    fanout *= localRoutes.routeList.size();\n                                    for (IMQTTTransientSession session : localRoutes.routeList.values()) {\n                                        // at least one session should publish the message\n                                        if (session.publish(matchInfo, singletonList(topicMsgPack))) {\n                                            published = true;\n                                        }\n                                    }\n                                } else {\n                                    // send to one subscriber to make sure matchinfo not lost\n                                    for (IMQTTTransientSession session : localRoutes.routeList.values()) {\n                                        // at least one session should publish the message\n                                        if (session.publish(matchInfo, singletonList(topicMsgPack))) {\n                                            published = true;\n                                            hasFanOutDone = true;\n                                            break;\n                                        }\n                                    }\n                                }\n                                if (published) {\n                                    ok.add(matchInfo);\n                                } else {\n                                    noSub.add(matchInfo);\n                                }\n                            } catch (Throwable e) {\n                                skip.add(matchInfo);\n                            }\n                        }\n                    }\n                }\n                tenantMeter.recordSummary(MqttTransientFanOutBytes, msgPackSize * Math.max(fanout, 1));\n            }\n            Sets.union(ok, skip).forEach(matchInfo -> resultsBuilder.addResult(DeliveryResult.newBuilder()\n                .setMatchInfo(matchInfo)\n                .setCode(DeliveryResult.Code.OK)\n                .build()));\n            noSub.forEach(matchInfo -> resultsBuilder.addResult(DeliveryResult.newBuilder()\n                .setMatchInfo(matchInfo)\n                .setCode(DeliveryResult.Code.NO_SUB)\n                .build()));\n            replyBuilder.putResult(tenantId, resultsBuilder.build());\n        }\n        return CompletableFuture.completedFuture(replyBuilder.build());\n    }", "completions": ["  \n```Java\n                                if (localRoutes.routeList.containsKey(matchInfo.getReceiverId())) {\n                                    IMQTTTransientSession session =\n                                        localRoutes.routeList.get(matchInfo.getReceiverId());\n                                    if (session!= null) {\n                                        boolean success = session.publish(matchInfo, singletonList(topicMsgPack));\n                                        if (success) {\n                                            ok.add(matchInfo);\n                                        } else {\n                                            noSub.add(matchInfo);\n                                        }\n                                    } else {\n                                        // no session found for shared subscription\n                                        noSub.add(matchInfo);\n                                    }\n                                } else {\n                                    noSub.add(matchInfo);\n                                }\n                            } catch (CompletionException e) {\n                                skip.add(matchInfo);\n                            }\n                            hasFanOutDone = true;\n                            fanout++;\n                        }\n                    }\n                    if (!ok.isEmpty()) {\n                        tenantMeter.markFanOut(tenantId, msgPackSize * ok.size());\n                        resultsBuilder.addAllOk(ok);\n                    }\n                    if (!skip.isEmpty()) {\n                        resultsBuilder.addAllSkip(skip);\n                    }\n                    if (!noSub.isEmpty()) {\n                        resultsBuilder.addAllNoSub(noSub);\n                    }\n                }\n            }\n        }\n        return CompletableFuture.completedFuture(replyBuilder.setResults(resultsBuilder.build()).build());\n    }\n}\n```"]}
{"id": 77, "name": "calcInterquartileRangeApproximation", "prompt": "### Role:\nYou are an experienced Java programmer assistant responsible for code completion tasks. Please intelligently complete the missing parts of the provided code snippet.\n\n### Task:\nBased on the provided code snippet, complete the missing method implementation. Ensure the code is logically correct, syntactically accurate, and follows best coding practices.\n\n### Left context\n```Java\npackage org.cbioportal.web.util;\n\nimport org.cbioportal.model.DataBin;\nimport org.springframework.stereotype.Component;\nimport com.google.common.collect.Range;\nimport java.math.BigDecimal;\nimport java.math.RoundingMode;\nimport java.util.*;\nimport java.util.stream.Collectors;\nimport org.springframework.util.Assert;\n\n@Component\npublic class DataBinHelper {\n   \n    public DataBin calcUpperOutlierBin(List<BigDecimal> gteValues, List<BigDecimal> gtValues) {\n        BigDecimal gteMin = gteValues.size() > 0 ? Collections.min(gteValues) : null;\n        BigDecimal gtMin = gtValues.size() > 0 ? Collections.min(gtValues) : null;\n        BigDecimal min;\n        String value;\n\n        if (gtMin == null && gteMin == null) {\n            // no special outlier\n            min = null;\n            value = \">\";\n        } else if (gtMin == null || (gteMin != null && gteMin.compareTo(gtMin) < 0)) {\n            min = gteMin;\n            value = \">=\";\n        } else {\n            min = gtMin;\n            value = \">\";\n        }\n\n        DataBin dataBin = new DataBin();\n\n        dataBin.setCount(gteValues.size() + gtValues.size());\n        dataBin.setSpecialValue(value);\n        dataBin.setStart(min);\n\n        return dataBin;\n    }\n\n    public DataBin calcLowerOutlierBin(List<BigDecimal> lteValues, List<BigDecimal> ltValues) {\n        BigDecimal lteMax = lteValues.size() > 0 ? Collections.max(lteValues) : null;\n        BigDecimal ltMax = ltValues.size() > 0 ? Collections.max(ltValues) : null;\n        BigDecimal max;\n        String specialValue;\n\n        if (ltMax == null && lteMax == null) {\n            max = null;\n            specialValue = \"<=\";\n        } else if (lteMax == null || (ltMax != null && lteMax.compareTo(ltMax) < 0)) {\n            max = ltMax;\n            specialValue = \"<\";\n        } else {\n            max = lteMax;\n            specialValue = \"<=\";\n        }\n\n        DataBin dataBin = new DataBin();\n\n        dataBin.setCount(lteValues.size() + ltValues.size());\n        dataBin.setSpecialValue(specialValue);\n        dataBin.setEnd(max);\n\n        return dataBin;\n    }\n    \n    public List<BigDecimal> calcQuartileBoundaries(List<BigDecimal> sortedValues) {\n        // Edge case: some of Q1, Q2, and Q3 are the same value.\n        // Solution: reduce bins to represent unique values only.\n        // Note: cannot use stream.distinct() because BigDecimal does\n        // not play nice with this (e.g., \"2.5E+2\" is not identical to \"250\"\n        // when using Object.equals())\n        final BigDecimal q1 = calcQ1(sortedValues);\n        final BigDecimal q2 = calcMedian(sortedValues);\n        final BigDecimal q3 = calcQ3(sortedValues);\n        List<BigDecimal> boundaries = new ArrayList<>();\n        boundaries.add(q1);\n        // Check Q1 smaller than Q2\n        if (q1.compareTo(q2) < 0) {\n            boundaries.add(q2);\n        }\n        // Check Q2 smaller than Q3\n        if (q2.compareTo(q3) < 0) {\n            boundaries.add(q3);\n        }\n        return boundaries;\n    }\n\n    public Range<BigDecimal> calcBoxRange(List<BigDecimal> sortedValues) {\n        if (sortedValues == null || sortedValues.size() == 0) {\n            return null;\n        }\n\n        // Find a generous IQR. This is generous because if (values.length / 4)\n        // is not an int, then really you should average the two elements on either\n        // side to find q1 and q3.\n        Range<BigDecimal> interquartileRange = calcInterquartileRangeApproximation(sortedValues);\n\n        BigDecimal q1 = interquartileRange.lowerEndpoint();\n        BigDecimal q3 = interquartileRange.upperEndpoint();\n        BigDecimal iqr = q3.subtract(q1);\n        BigDecimal iqrOneAndHalf = iqr.multiply(new BigDecimal(\"1.5\"));\n        BigDecimal q1LowerBoundry = q1.subtract(iqrOneAndHalf);\n        BigDecimal q3upperBoundry = q3.add(iqrOneAndHalf);\n\n\n        // Then find min and max values\n        BigDecimal maxValue;\n        BigDecimal minValue;\n\n        if (sortedValues.get(0).compareTo(sortedValues.get(sortedValues.size() - 1)) == 0) {\n            // if the first and last values are the same, no need to do any other calculation\n            // we simply set min and max to the same value\n            minValue = sortedValues.get(0);\n            maxValue = minValue;\n        } else if (q3.compareTo(new BigDecimal(\"0.001\")) != -1 && q3.compareTo(new BigDecimal(\"1\")) < 0) {\n            //maxValue = Number((q3 + iqr * 1.5).toFixed(3));\n            //minValue = Number((q1 - iqr * 1.5).toFixed(3));\n            maxValue = q3upperBoundry.setScale(3, BigDecimal.ROUND_HALF_UP);\n            minValue = q1LowerBoundry.setScale(3, BigDecimal.ROUND_HALF_UP);\n        } else if (q3.compareTo(BigDecimal.valueOf(0.001)) < 0) {\n            // get IQR for very small number(<0.001)\n            maxValue = q3upperBoundry;\n            minValue = q1LowerBoundry;\n        } else {\n            maxValue = q3upperBoundry.setScale(1, RoundingMode.CEILING);\n            minValue = q1LowerBoundry.setScale(1, RoundingMode.FLOOR);\n        }\n\n        if (minValue.compareTo(sortedValues.get(0)) < 0) {\n            minValue = sortedValues.get(0);\n        }\n\n        if (maxValue.compareTo(sortedValues.get(sortedValues.size() - 1)) > 0) {\n            maxValue = sortedValues.get(sortedValues.size() - 1);\n        }\n\n        return Range.closed(minValue, maxValue);\n    }\n\n```\n\n### Code description\n```Java\n/**\n * Calculates an approximation of the interquartile range (IQR) of a given list of BigDecimal values.\n * The IQR is a measure of statistical dispersion and is calculated as the difference between the \n * 75th percentile (Q3) and the 25th percentile (Q1) of the data.\n *\n * @param sortedValues A list of BigDecimal values sorted in ascending order.\n * @return A Range object representing the IQR approximation. If the IQR is 0 and the maximum value \n *         is equal to Q3, the method will recursively try to find a non-zero IQR approximation by \n *         filtering out the maximum value and trying again. If the IQR is still empty or null, \n *         the method will return a Range object with Q1 and Q3 as the endpoints.\n * @see com.google.common.collect.Range#closedOpen(Comparable, Comparable)\n */\n```\n\n### Code snippet:\n```Java\n    public Range<BigDecimal> calcInterquartileRangeApproximation(List<BigDecimal> sortedValues) {\n        Range<BigDecimal> iqr = null;\n\n        if (sortedValues.size() > 0) {\n            BigDecimal q1 = valueCloseToQ1(sortedValues);\n            BigDecimal q3 = valueCloseToQ3(sortedValues);\n            BigDecimal max = sortedValues.get(sortedValues.size() - 1);\n\n            // if iqr == 0 AND max == q3 then recursively try finding a non-zero iqr approximation.\n            if (q1.compareTo(q3) == 0 && max.compareTo(q3) == 0) {\n                // filter out max and try again\n```\n\n### Requirements:\n1. Complete only the missing part of the method.\n2. Only answer the code of the completed part. Do not include any of the provided code snippet.\n3. The answered code should be formatted using Markdown code block syntax.\n4. Do not include any additional text.\n\n### Answer: (use the provided format with backticks)\n", "reference": "                iqr = this.calcInterquartileRangeApproximation(\n                    sortedValues.stream().filter(d -> d.compareTo(max) < 0).collect(Collectors.toList()));\n            }\n\n            // if range is still empty use the original q1 and q3 values\n            if (iqr == null || iqr.isEmpty()) {\n                iqr = Range.closedOpen(q1, q3);\n            }\n        }\n\n        return iqr;\n    }", "completions": ["  \n```Java\n    public Range<BigDecimal> calcInterquartileRangeApproximation(List<BigDecimal> sortedValues) {\n        Range<BigDecimal> iqr = null;\n\n        if (sortedValues.size() > 0) {\n            BigDecimal q1 = valueCloseToQ1(sortedValues);\n            BigDecimal q3 = valueCloseToQ3(sortedValues);\n            BigDecimal max = sortedValues.get(sortedValues.size() - 1);\n\n            // if iqr == 0 AND max == q3 then recursively try finding a non-zero iqr approximation.\n            if (q1.compareTo(q3) == 0 && max.compareTo(q3) == 0) {\n                // filter out max and try again\n                List<BigDecimal> filteredValues = sortedValues.subList(0, sortedValues.size() - 1);\n                iqr = calcInterquartileRangeApproximation(filteredValues);\n            } else {\n                iqr = Range.closedOpen(q1, q3);\n            }\n        }\n\n        return iqr;\n    }\n```"]}
{"id": 78, "name": "readComponent", "prompt": "### Role:\nYou are an experienced Java programmer assistant responsible for code completion tasks. Please intelligently complete the missing parts of the provided code snippet.\n\n### Task:\nBased on the provided code snippet, complete the missing method implementation. Ensure the code is logically correct, syntactically accurate, and follows best coding practices.\n\n### Left context\n```Java\n/*\n * Sonatype Nexus (TM) Open Source Version\n * Copyright (c) 2008-present Sonatype, Inc.\n * All rights reserved. Includes the third-party code listed at http://links.sonatype.com/products/nexus/oss/attributions.\n *\n * This program and the accompanying materials are made available under the terms of the Eclipse Public License Version 1.0,\n * which accompanies this distribution and is available at http://www.eclipse.org/legal/epl-v10.html.\n *\n * Sonatype Nexus (TM) Professional Version is available from Sonatype, Inc. \"Sonatype\" and \"Sonatype Nexus\" are trademarks\n * of Sonatype, Inc. Apache Maven is a trademark of the Apache Software Foundation. M2eclipse is a trademark of the\n * Eclipse Foundation. All other trademarks are the property of their respective owners.\n */\npackage org.sonatype.nexus.coreui.internal.orient;\n\nimport java.util.ArrayList;\nimport java.util.HashSet;\nimport java.util.List;\nimport java.util.Map;\nimport java.util.Set;\nimport javax.annotation.Priority;\nimport javax.inject.Inject;\nimport javax.inject.Named;\nimport javax.inject.Singleton;\nimport javax.ws.rs.WebApplicationException;\n\nimport org.sonatype.goodies.common.ComponentSupport;\nimport org.sonatype.nexus.common.entity.EntityHelper;\nimport org.sonatype.nexus.common.entity.EntityId;\nimport org.sonatype.nexus.coreui.AssetXO;\nimport org.sonatype.nexus.coreui.ComponentHelper;\nimport org.sonatype.nexus.coreui.ComponentXO;\nimport org.sonatype.nexus.repository.Repository;\nimport org.sonatype.nexus.repository.browse.BrowseService;\nimport org.sonatype.nexus.repository.maintenance.MaintenanceService;\nimport org.sonatype.nexus.repository.manager.RepositoryManager;\nimport org.sonatype.nexus.repository.query.PageResult;\nimport org.sonatype.nexus.repository.query.QueryOptions;\nimport org.sonatype.nexus.repository.security.ContentPermissionChecker;\nimport org.sonatype.nexus.repository.security.RepositorySelector;\nimport org.sonatype.nexus.repository.security.VariableResolverAdapterManager;\nimport org.sonatype.nexus.repository.storage.Asset;\nimport org.sonatype.nexus.repository.storage.AssetVariableResolver;\nimport org.sonatype.nexus.repository.storage.Bucket;\nimport org.sonatype.nexus.repository.storage.BucketStore;\nimport org.sonatype.nexus.repository.storage.Component;\nimport org.sonatype.nexus.repository.storage.ComponentFinder;\nimport org.sonatype.nexus.repository.storage.StorageFacet;\nimport org.sonatype.nexus.repository.storage.StorageTx;\nimport org.sonatype.nexus.security.BreadActions;\nimport org.sonatype.nexus.selector.VariableSource;\n\nimport com.google.common.annotations.VisibleForTesting;\nimport com.google.common.collect.Iterables;\nimport org.apache.shiro.authz.AuthorizationException;\n\nimport static com.google.common.base.Preconditions.checkNotNull;\nimport static com.google.common.collect.Lists.newArrayList;\nimport static java.util.Collections.emptySet;\nimport static java.util.Collections.singletonList;\nimport static java.util.stream.Collectors.toList;\nimport static javax.ws.rs.core.Response.Status;\nimport static org.sonatype.nexus.repository.storage.DefaultComponentFinder.DEFAULT_COMPONENT_FINDER_KEY;\n\n/**\n * Orient {@link ComponentHelper}.\n */\n@Priority(Integer.MAX_VALUE) // make sure this implementation appears above the datastore one in mixed-mode\n@Named\n@Singleton\npublic class OrientComponentHelper\n    extends ComponentSupport\n    implements ComponentHelper\n{\n  @VisibleForTesting\n  static ComponentXO COMPONENT_CONVERTER(Component component, String repositoryName) {\n    ComponentXO componentXO = new ComponentXO();\n    componentXO.setId(EntityHelper.id(component).getValue());\n    componentXO.setRepositoryName(repositoryName);\n    componentXO.setGroup(component.group());\n    componentXO.setName(component.name());\n    componentXO.setVersion(component.version());\n    componentXO.setFormat(component.format());\n    return componentXO;\n  }\n\n  private static AssetXO ASSET_CONVERTER(\n      Asset asset,\n      String repositoryName,\n      String privilegedRepositoryName)\n  {\n    AssetXO assetXO = new AssetXO();\n    assetXO.setId(EntityHelper.id(asset).getValue());\n    assetXO.setName(asset.name());\n    assetXO.setFormat(asset.format());\n    assetXO.setContentType(asset.contentType() != null ? asset.contentType() : \"unknown\");\n    assetXO.setSize(asset.size() != null ? asset.size() : 0L);\n    assetXO.setRepositoryName(repositoryName);\n    assetXO.setContainingRepositoryName(privilegedRepositoryName);\n    assetXO.setBlobCreated(asset.blobCreated() != null ? asset.blobCreated().toDate() : null);\n    assetXO.setBlobUpdated(asset.blobUpdated() != null ? asset.blobUpdated().toDate() : null);\n    assetXO.setLastDownloaded(asset.lastDownloaded() != null ? asset.lastDownloaded().toDate() : null);\n    assetXO.setBlobRef(asset.blobRef() != null ? asset.blobRef().toString() : \"\");\n    assetXO.setComponentId(asset.componentId() != null ? asset.componentId().getValue() : \"\");\n    assetXO.setAttributes(asset.attributes().backing());\n    assetXO.setCreatedBy(asset.createdBy());\n    assetXO.setCreatedByIp(asset.createdByIp());\n    return assetXO;\n  }\n\n  private final RepositoryManager repositoryManager;\n\n  private final ContentPermissionChecker contentPermissionChecker;\n\n  private final VariableResolverAdapterManager variableResolverAdapterManager;\n\n  private final BrowseService browseService;\n\n  private final MaintenanceService maintenanceService;\n\n  private final Map<String, ComponentFinder> componentFinders;\n\n  private final BucketStore bucketStore;\n\n  @Inject\n  public OrientComponentHelper(\n      final RepositoryManager repositoryManager,\n      final ContentPermissionChecker contentPermissionChecker,\n      final VariableResolverAdapterManager variableResolverAdapterManager,\n      final BrowseService browseService,\n      final MaintenanceService maintenanceService,\n      final Map<String, ComponentFinder> componentFinders,\n      final BucketStore bucketStore)\n  {\n    this.repositoryManager = checkNotNull(repositoryManager);\n    this.contentPermissionChecker = checkNotNull(contentPermissionChecker);\n    this.variableResolverAdapterManager = checkNotNull(variableResolverAdapterManager);\n    this.browseService = checkNotNull(browseService);\n    this.maintenanceService = checkNotNull(maintenanceService);\n    this.componentFinders = checkNotNull(componentFinders);\n    this.bucketStore = checkNotNull(bucketStore);\n  }\n\n  public List<AssetXO> readComponentAssets(final Repository repository, final ComponentXO componentXO) {\n    ComponentFinder componentFinder = componentFinders.get(componentXO.getFormat());\n    if (null == componentFinder) {\n      componentFinder = componentFinders.get(DEFAULT_COMPONENT_FINDER_KEY);\n    }\n\n    List<Component> components =\n        componentFinder.findMatchingComponents(repository, componentXO.getId(), componentXO.getGroup(),\n            componentXO.getName(), componentXO.getVersion());\n\n    PageResult<Asset> browseResult = browseService.browseComponentAssets(repository, components.get(0));\n\n    return createAssetXOs(browseResult.getResults(), repository);\n  }\n\n  public PageResult<AssetXO> previewAssets(\n      final RepositorySelector repositorySelector,\n      final List<Repository> selectedRepositories,\n      final String jexlExpression,\n      final QueryOptions queryOptions)\n  {\n    PageResult<Asset> page =\n        browseService.previewAssets(repositorySelector, selectedRepositories, jexlExpression, queryOptions);\n\n    // buckets not needed for asset preview screen\n    return new PageResult<>(page.getTotal(),\n        page.getResults().stream().map(asset -> ASSET_CONVERTER(asset, null, null)).collect(toList()));\n  }\n\n```\n\n### Code description\n```Java\n/**\n * Description: This method reads a component from a repository using its component ID.\n * It also retrieves the assets associated with the component.\n * If the component or assets are not found, it throws a WebApplicationException with a NOT_FOUND status.\n * After ensuring the necessary permissions, it converts the component into a ComponentXO object.\n *\n * @param repository The repository from which to read the component.\n * @param componentId The ID of the component to be read.\n * @return The ComponentXO object representing the read component.\n * @throws WebApplicationException If the component or assets are not found.\n */\n```\n\n### Code snippet:\n```Java\n  public ComponentXO readComponent(final Repository repository, final EntityId componentId) {\n    Component component;\n    List<Asset> assets;\n    try (StorageTx storageTx = repository.facet(StorageFacet.class).txSupplier().get()) {\n      storageTx.begin();\n      component = storageTx.findComponent(componentId);\n      if (component == null) {\n        throw new WebApplicationException(Status.NOT_FOUND);\n      }\n\n```\n\n### Requirements:\n1. Complete only the missing part of the method.\n2. Only answer the code of the completed part. Do not include any of the provided code snippet.\n3. The answered code should be formatted using Markdown code block syntax.\n4. Do not include any additional text.\n\n### Answer: (use the provided format with backticks)\n", "reference": "      Iterable<Asset> browsedAssets = storageTx.browseAssets(component);\n      if (browsedAssets == null || Iterables.isEmpty(browsedAssets)) {\n        throw new WebApplicationException(Status.NOT_FOUND);\n      }\n\n      assets = newArrayList(browsedAssets);\n    }\n    ensurePermissions(repository, assets, BreadActions.BROWSE);\n    return COMPONENT_CONVERTER(component, repository.getName());\n  }", "completions": ["  \n```Java\n      assets = storageTx.findAssets(component);\n      storageTx.commit();\n    }\n\n    if (assets.isEmpty()) {\n      throw new WebApplicationException(Status.NOT_FOUND);\n    }\n\n    return COMPONENT_CONVERTER(component, repository.getName());\n  }\n```"]}
{"id": 79, "name": "copyResponse", "prompt": "### Role:\nYou are an experienced Java programmer assistant responsible for code completion tasks. Please intelligently complete the missing parts of the provided code snippet.\n\n### Task:\nBased on the provided code snippet, complete the missing method implementation. Ensure the code is logically correct, syntactically accurate, and follows best coding practices.\n\n### Left context\n```Java\npackage io.arex.inst.httpclient.feign;\n\nimport feign.Request;\nimport feign.Response;\nimport feign.Response.Body;\nimport feign.Util;\nimport io.arex.agent.bootstrap.util.CollectionUtil;\nimport io.arex.inst.httpclient.common.HttpClientAdapter;\nimport io.arex.inst.httpclient.common.HttpResponseWrapper;\nimport io.arex.inst.httpclient.common.HttpResponseWrapper.StringTuple;\nimport io.arex.inst.runtime.log.LogManager;\nimport java.net.URI;\nimport java.util.ArrayList;\nimport java.util.Collection;\nimport java.util.Collections;\nimport java.util.HashMap;\nimport java.util.List;\nimport java.util.Map;\n\npublic class FeignClientAdapter implements HttpClientAdapter<Request, Response> {\n    private static final String CONTENT_TYPE = \"Content-Type\";\n    private final Request request;\n    private final URI uri;\n    private byte[] responseBody;\n\n    public FeignClientAdapter(Request request, URI uri) {\n        this.request = request;\n        this.uri = uri;\n    }\n\n    @Override\n    public String getMethod() {\n        return request.method();\n    }\n\n    @Override\n    public byte[] getRequestBytes() {\n        return request.body();\n    }\n\n    @Override\n    public String getRequestContentType() {\n        return getRequestHeader(CONTENT_TYPE);\n    }\n\n    @Override\n    public String getRequestHeader(String name) {\n        final Collection<String> values = request.headers().get(name);\n        if (CollectionUtil.isEmpty(values)) {\n            return null;\n        }\n        return values.iterator().next();\n    }\n\n    @Override\n    public URI getUri() {\n        return uri;\n    }\n\n    @Override\n    public HttpResponseWrapper wrap(Response response) {\n        final String statusLine = String.valueOf(response.status());\n        final List<StringTuple> headers = new ArrayList<>(response.headers().size());\n        response.headers().forEach((k, v) -> headers.add(new StringTuple(k, v.iterator().next())));\n        HttpResponseWrapper responseWrapper = new HttpResponseWrapper(statusLine, responseBody, null, headers);\n        responseWrapper.setReason(response.reason());\n        return responseWrapper;\n    }\n\n    @Override\n    public Response unwrap(HttpResponseWrapper wrapped) {\n        final int status = parseInt(wrapped.getStatusLine());\n        byte[] responseContent = wrapped.getContent();\n        final List<StringTuple> wrappedHeaders = wrapped.getHeaders();\n        Map<String, Collection<String>> headers = new HashMap<>(wrappedHeaders.size());\n        for (StringTuple header : wrappedHeaders) {\n            headers.put(header.name(), Collections.singletonList(header.value()));\n        }\n        return Response.builder().body(responseContent).status(status).headers(headers).reason(wrapped.getReason()).request(request).build();\n    }\n\n    private int parseInt(String statusLine) {\n        try {\n            return Integer.parseInt(statusLine);\n        } catch (Exception ex) {\n            LogManager.warn(\"feign.parseInt\", \"statusLine: \" + statusLine, ex);\n            return -1;\n        }\n    }\n\n```\n\n### Code description\n```Java\n/**\n * Description: This method copies a response from a Feign client.\n * It checks if the response is null, if so, it returns null.\n * If the response body is null, it returns the response.\n * It tries to convert the response body to a byte array using the Feign utility method toByteArray.\n * If the body is not repeatable, it builds a new response with the copied body and returns it.\n *\n * @param response The response to be copied.\n * @return The copied response or the original response if the body is not repeatable.\n */\n```\n\n### Code snippet:\n```Java\n    public Response copyResponse(Response response) {\n        if (response == null) {\n            return null;\n        }\n        final Body body = response.body();\n        if (body == null) {\n            return response;\n        }\n        try {\n```\n\n### Requirements:\n1. Complete only the missing part of the method.\n2. Only answer the code of the completed part. Do not include any of the provided code snippet.\n3. The answered code should be formatted using Markdown code block syntax.\n4. Do not include any additional text.\n\n### Answer: (use the provided format with backticks)\n", "reference": "            responseBody = Util.toByteArray(body.asInputStream());\n        } catch (Exception ex) {\n            LogManager.warn(\"feign.copyResponse\", \"uri: \" + getUri(), ex);\n        }\n        if (body.isRepeatable()) {\n            return response;\n        }\n        return response.toBuilder().body(responseBody).build();\n    }", "completions": ["  \n```Java\n    public Response copyResponse(Response response) {\n        if (response == null) {\n            return null;\n        }\n        final Body body = response.body();\n        if (body == null) {\n            return response;\n        }\n        try {\n            byte[] responseBody = Util.toByteArray(body.asInputStream());\n            return Response.builder().body(responseBody).status(response.status()).headers(response.headers()).reason(response.reason()).request(response.request()).build();\n        } catch (IOException e) {\n            LogManager.warn(\"feign.copyResponse\", \"Failed to copy response body\", e);\n            return response;\n        }\n    }\n```"]}
{"id": 80, "name": "handleRecoveryTask", "prompt": "### Role:\nYou are an experienced Java programmer assistant responsible for code completion tasks. Please intelligently complete the missing parts of the provided code snippet.\n\n### Task:\nBased on the provided code snippet, complete the missing method implementation. Ensure the code is logically correct, syntactically accurate, and follows best coding practices.\n\n### Left context\n```Java\npackage com.slack.astra.recovery;\n\nimport static com.slack.astra.server.AstraConfig.DEFAULT_START_STOP_DURATION;\nimport static com.slack.astra.util.TimeUtils.nanosToMillis;\n\nimport com.google.common.annotations.VisibleForTesting;\nimport com.google.common.collect.ImmutableList;\nimport com.google.common.util.concurrent.AbstractIdleService;\nimport com.google.common.util.concurrent.ThreadFactoryBuilder;\nimport com.google.protobuf.TextFormat;\nimport com.slack.astra.blobfs.BlobFs;\nimport com.slack.astra.chunk.SearchContext;\nimport com.slack.astra.chunkManager.RecoveryChunkManager;\nimport com.slack.astra.logstore.LogMessage;\nimport com.slack.astra.metadata.core.AstraMetadataStoreChangeListener;\nimport com.slack.astra.metadata.recovery.RecoveryNodeMetadata;\nimport com.slack.astra.metadata.recovery.RecoveryNodeMetadataStore;\nimport com.slack.astra.metadata.recovery.RecoveryTaskMetadata;\nimport com.slack.astra.metadata.recovery.RecoveryTaskMetadataStore;\nimport com.slack.astra.metadata.search.SearchMetadataStore;\nimport com.slack.astra.metadata.snapshot.SnapshotMetadataStore;\nimport com.slack.astra.proto.config.AstraConfigs;\nimport com.slack.astra.proto.metadata.Metadata;\nimport com.slack.astra.writer.LogMessageWriterImpl;\nimport com.slack.astra.writer.kafka.AstraKafkaConsumer;\nimport io.micrometer.core.instrument.Counter;\nimport io.micrometer.core.instrument.MeterRegistry;\nimport io.micrometer.core.instrument.Tag;\nimport io.micrometer.core.instrument.Timer;\nimport java.time.Instant;\nimport java.util.Collection;\nimport java.util.Map;\nimport java.util.concurrent.ExecutorService;\nimport java.util.concurrent.Executors;\nimport org.apache.curator.x.async.AsyncCuratorFramework;\nimport org.apache.kafka.clients.admin.AdminClient;\nimport org.apache.kafka.clients.admin.AdminClientConfig;\nimport org.apache.kafka.clients.admin.ListOffsetsResult;\nimport org.apache.kafka.clients.admin.OffsetSpec;\nimport org.apache.kafka.common.TopicPartition;\nimport org.slf4j.Logger;\nimport org.slf4j.LoggerFactory;\n\n/**\n * The recovery service is intended to be executed on a recovery node, and is responsible for\n * fulfilling recovery assignments provided from the cluster manager.\n *\n * <p>When the recovery service starts it advertises its availability by creating a recovery node,\n * and then subscribing to any state changes. Upon receiving an assignment from the cluster manager\n * the recovery service will delegate the recovery task to an executor service. Once the recovery\n * task has been completed, the recovery node will make itself available again for assignment.\n *\n * <p>Look at handleRecoveryTaskAssignment method understand the implementation and limitations of\n * the current implementation.\n */\npublic class RecoveryService extends AbstractIdleService {\n  private static final Logger LOG = LoggerFactory.getLogger(RecoveryService.class);\n\n  private final SearchContext searchContext;\n  private final AsyncCuratorFramework curatorFramework;\n  private final MeterRegistry meterRegistry;\n  private final BlobFs blobFs;\n  private final AstraConfigs.AstraConfig AstraConfig;\n  private final AdminClient adminClient;\n\n  private RecoveryNodeMetadataStore recoveryNodeMetadataStore;\n  private RecoveryNodeMetadataStore recoveryNodeListenerMetadataStore;\n  private RecoveryTaskMetadataStore recoveryTaskMetadataStore;\n  private SnapshotMetadataStore snapshotMetadataStore;\n  private final ExecutorService executorService;\n\n  private Metadata.RecoveryNodeMetadata.RecoveryNodeState recoveryNodeLastKnownState;\n\n  public static final String RECOVERY_NODE_ASSIGNMENT_RECEIVED =\n      \"recovery_node_assignment_received\";\n  public static final String RECOVERY_NODE_ASSIGNMENT_SUCCESS = \"recovery_node_assignment_success\";\n  public static final String RECOVERY_NODE_ASSIGNMENT_FAILED = \"recovery_node_assignment_failed\";\n  public static final String RECORDS_NO_LONGER_AVAILABLE = \"records_no_longer_available\";\n  public static final String RECOVERY_TASK_TIMER = \"recovery_task_timer\";\n  protected final Counter recoveryNodeAssignmentReceived;\n  protected final Counter recoveryNodeAssignmentSuccess;\n  protected final Counter recoveryNodeAssignmentFailed;\n  protected final Counter recoveryRecordsNoLongerAvailable;\n  private final Timer recoveryTaskTimerSuccess;\n  private final Timer recoveryTaskTimerFailure;\n  private SearchMetadataStore searchMetadataStore;\n\n  private final AstraMetadataStoreChangeListener<RecoveryNodeMetadata> recoveryNodeListener =\n      this::recoveryNodeListener;\n\n  public RecoveryService(\n      AstraConfigs.AstraConfig AstraConfig,\n      AsyncCuratorFramework curatorFramework,\n      MeterRegistry meterRegistry,\n      BlobFs blobFs) {\n    this.curatorFramework = curatorFramework;\n    this.searchContext =\n        SearchContext.fromConfig(AstraConfig.getRecoveryConfig().getServerConfig());\n    this.meterRegistry = meterRegistry;\n    this.blobFs = blobFs;\n    this.AstraConfig = AstraConfig;\n\n    adminClient =\n        AdminClient.create(\n            Map.of(\n                AdminClientConfig.BOOTSTRAP_SERVERS_CONFIG,\n                AstraConfig.getRecoveryConfig().getKafkaConfig().getKafkaBootStrapServers(),\n                AdminClientConfig.REQUEST_TIMEOUT_MS_CONFIG,\n                \"5000\"));\n\n    // we use a single thread executor to allow operations for this recovery node to queue,\n    // guaranteeing that they are executed in the order they were received\n    this.executorService =\n        Executors.newSingleThreadExecutor(\n            new ThreadFactoryBuilder()\n                .setUncaughtExceptionHandler(\n                    (t, e) -> LOG.error(\"Exception on thread {}: {}\", t.getName(), e))\n                .setNameFormat(\"recovery-service-%d\")\n                .build());\n\n    Collection<Tag> meterTags = ImmutableList.of(Tag.of(\"nodeHostname\", searchContext.hostname));\n    recoveryNodeAssignmentReceived =\n        meterRegistry.counter(RECOVERY_NODE_ASSIGNMENT_RECEIVED, meterTags);\n    recoveryNodeAssignmentSuccess =\n        meterRegistry.counter(RECOVERY_NODE_ASSIGNMENT_SUCCESS, meterTags);\n    recoveryNodeAssignmentFailed =\n        meterRegistry.counter(RECOVERY_NODE_ASSIGNMENT_FAILED, meterTags);\n    recoveryRecordsNoLongerAvailable =\n        meterRegistry.counter(RECORDS_NO_LONGER_AVAILABLE, meterTags);\n    recoveryTaskTimerSuccess = meterRegistry.timer(RECOVERY_TASK_TIMER, \"successful\", \"true\");\n    recoveryTaskTimerFailure = meterRegistry.timer(RECOVERY_TASK_TIMER, \"successful\", \"false\");\n  }\n\n  @Override\n  protected void startUp() throws Exception {\n    LOG.info(\"Starting recovery service\");\n\n    recoveryNodeMetadataStore = new RecoveryNodeMetadataStore(curatorFramework, false);\n    recoveryTaskMetadataStore = new RecoveryTaskMetadataStore(curatorFramework, false);\n    snapshotMetadataStore = new SnapshotMetadataStore(curatorFramework);\n    searchMetadataStore = new SearchMetadataStore(curatorFramework, false);\n\n    recoveryNodeMetadataStore.createSync(\n        new RecoveryNodeMetadata(\n            searchContext.hostname,\n            Metadata.RecoveryNodeMetadata.RecoveryNodeState.FREE,\n            \"\",\n            Instant.now().toEpochMilli()));\n    recoveryNodeLastKnownState = Metadata.RecoveryNodeMetadata.RecoveryNodeState.FREE;\n\n    recoveryNodeListenerMetadataStore =\n        new RecoveryNodeMetadataStore(curatorFramework, searchContext.hostname, true);\n    recoveryNodeListenerMetadataStore.addListener(recoveryNodeListener);\n  }\n\n  @Override\n  protected void shutDown() throws Exception {\n    LOG.info(\"Closing the recovery service\");\n\n    recoveryNodeListenerMetadataStore.addListener(recoveryNodeListener);\n\n    recoveryNodeMetadataStore.close();\n    recoveryTaskMetadataStore.close();\n    snapshotMetadataStore.close();\n    searchMetadataStore.close();\n\n    // Immediately shutdown recovery tasks. Any incomplete recovery tasks will be picked up by\n    // another recovery node so we don't need to wait for processing to complete.\n    executorService.shutdownNow();\n\n    LOG.info(\"Closed the recovery service\");\n  }\n\n  private void recoveryNodeListener(RecoveryNodeMetadata recoveryNodeMetadata) {\n    Metadata.RecoveryNodeMetadata.RecoveryNodeState newRecoveryNodeState =\n        recoveryNodeMetadata.recoveryNodeState;\n\n    if (newRecoveryNodeState.equals(Metadata.RecoveryNodeMetadata.RecoveryNodeState.ASSIGNED)) {\n      LOG.info(\"Recovery node - ASSIGNED received\");\n      recoveryNodeAssignmentReceived.increment();\n      if (!recoveryNodeLastKnownState.equals(\n          Metadata.RecoveryNodeMetadata.RecoveryNodeState.FREE)) {\n        LOG.warn(\n            \"Unexpected state transition from {} to {}\",\n            recoveryNodeLastKnownState,\n            newRecoveryNodeState);\n      }\n      executorService.execute(() -> handleRecoveryTaskAssignment(recoveryNodeMetadata));\n    }\n    recoveryNodeLastKnownState = newRecoveryNodeState;\n  }\n\n  /**\n   * This method is invoked after the cluster manager has assigned a recovery node a task. As part\n   * of handling a task assignment we update the recovery node state to recovering once we start the\n   * recovery process. If the recovery succeeds, we delete the recovery task and set node to free.\n   * If the recovery task fails, we set the node to free so the recovery task can be assigned to\n   * another node again.\n   *\n   * <p>Currently, we expect each recovery task to create one chunk. We don't support multiple\n   * chunks per recovery task for a few reasons. It keeps the recovery protocol very simple since we\n   * don't have to deal with partial chunk upload failures. By creating only one chunk per recovery\n   * task, the runtime and resource utilization of an individual recovery task is bounded and\n   * predictable, so it's easy to plan capacity for it. We get implicit parallelism in execution by\n   * adding more recovery nodes and there is no need for additional mechanisms for parallelizing\n   * execution.\n   *\n   * <p>TODO: Re-queuing failed re-assignment task will lead to wasted resources if recovery always\n   * fails. To break this cycle add a enqueue_count value to recovery task so we can stop recovering\n   * it if the task fails a certain number of times.\n   */\n  protected void handleRecoveryTaskAssignment(RecoveryNodeMetadata recoveryNodeMetadata) {\n    try {\n      setRecoveryNodeMetadataState(Metadata.RecoveryNodeMetadata.RecoveryNodeState.RECOVERING);\n      RecoveryTaskMetadata recoveryTaskMetadata =\n          recoveryTaskMetadataStore.getSync(recoveryNodeMetadata.recoveryTaskName);\n\n      if (!isValidRecoveryTask(recoveryTaskMetadata)) {\n        LOG.error(\n            \"Invalid recovery task detected, skipping and deleting invalid task {}\",\n            recoveryTaskMetadata);\n        recoveryTaskMetadataStore.deleteSync(recoveryTaskMetadata.name);\n        setRecoveryNodeMetadataState(Metadata.RecoveryNodeMetadata.RecoveryNodeState.FREE);\n        recoveryNodeAssignmentFailed.increment();\n      } else {\n        boolean success = handleRecoveryTask(recoveryTaskMetadata);\n        if (success) {\n          // delete the completed recovery task on success\n          recoveryTaskMetadataStore.deleteSync(recoveryTaskMetadata.name);\n          setRecoveryNodeMetadataState(Metadata.RecoveryNodeMetadata.RecoveryNodeState.FREE);\n          recoveryNodeAssignmentSuccess.increment();\n        } else {\n          setRecoveryNodeMetadataState(Metadata.RecoveryNodeMetadata.RecoveryNodeState.FREE);\n          recoveryNodeAssignmentFailed.increment();\n        }\n      }\n    } catch (Exception e) {\n      setRecoveryNodeMetadataState(Metadata.RecoveryNodeMetadata.RecoveryNodeState.FREE);\n      LOG.error(\"Failed to complete recovery node task assignment\", e);\n      recoveryNodeAssignmentFailed.increment();\n    }\n  }\n\n  /**\n   * Attempts a final sanity-check on the recovery task to prevent a bad task from halting the\n   * recovery pipeline. Bad state should be ideally prevented at the creation, as well as prior to\n   * assignment, but this can be considered a final fail-safe if invalid recovery tasks somehow made\n   * it this far.\n   */\n  private boolean isValidRecoveryTask(RecoveryTaskMetadata recoveryTaskMetadata) {\n    // todo - consider adding further invalid recovery task detections\n    if (recoveryTaskMetadata.endOffset <= recoveryTaskMetadata.startOffset) {\n      return false;\n    }\n    return true;\n  }\n\n  /**\n   * This method does the recovery work from a recovery task. A recovery task indicates the start\n   * and end offset of a kafka partition to index. To do the recovery work, we create a recovery\n   * chunk manager, create a kafka consumer for the recovery partition, indexes the data in\n   * parallel, uploads the data to S3 and closes all the components correctly. We return true if the\n   * operation succeeded.\n   */\n```\n\n### Code description\n```Java\n/**\n * Description: This method handles a recovery task by validating Kafka offsets, preparing a Kafka consumer, consuming messages in parallel, waiting for chunk uploads, and closing the consumer and chunk manager. It also logs the time taken for each subtask.\n * \n * @param recoveryTaskMetadata The metadata of the recovery task.\n * @return A boolean indicating whether the recovery task was successful or not.\n * @throws Exception If an error occurs during the recovery task.\n */\n```\n\n### Code snippet:\n```Java\n  @VisibleForTesting\n  boolean handleRecoveryTask(RecoveryTaskMetadata recoveryTaskMetadata) {\n    LOG.info(\"Started handling the recovery task: {}\", recoveryTaskMetadata);\n    long startTime = System.nanoTime();\n    Timer.Sample taskTimer = Timer.start(meterRegistry);\n\n    PartitionOffsets partitionOffsets =\n        validateKafkaOffsets(\n            adminClient,\n            recoveryTaskMetadata,\n            AstraConfig.getRecoveryConfig().getKafkaConfig().getKafkaTopic());\n    long offsetsValidatedTime = System.nanoTime();\n    long consumerPreparedTime = 0, messagesConsumedTime = 0, rolloversCompletedTime = 0;\n\n    if (partitionOffsets != null) {\n      RecoveryTaskMetadata validatedRecoveryTask =\n          new RecoveryTaskMetadata(\n              recoveryTaskMetadata.name,\n              recoveryTaskMetadata.partitionId,\n              partitionOffsets.startOffset,\n              partitionOffsets.endOffset,\n              recoveryTaskMetadata.createdTimeEpochMs);\n\n      if (partitionOffsets.startOffset != recoveryTaskMetadata.startOffset\n          || recoveryTaskMetadata.endOffset != partitionOffsets.endOffset) {\n        recoveryRecordsNoLongerAvailable.increment(\n            (partitionOffsets.startOffset - recoveryTaskMetadata.startOffset)\n                + (partitionOffsets.endOffset - recoveryTaskMetadata.endOffset));\n      }\n\n      try {\n        RecoveryChunkManager<LogMessage> chunkManager =\n            RecoveryChunkManager.fromConfig(\n                meterRegistry,\n                searchMetadataStore,\n                snapshotMetadataStore,\n                AstraConfig.getIndexerConfig(),\n                blobFs,\n                AstraConfig.getS3Config());\n\n        // Ingest data in parallel\n        LogMessageWriterImpl logMessageWriterImpl = new LogMessageWriterImpl(chunkManager);\n        AstraKafkaConsumer kafkaConsumer =\n            new AstraKafkaConsumer(\n                makeKafkaConfig(\n                    AstraConfig.getRecoveryConfig().getKafkaConfig(),\n```\n\n### Requirements:\n1. Complete only the missing part of the method.\n2. Only answer the code of the completed part. Do not include any of the provided code snippet.\n3. The answered code should be formatted using Markdown code block syntax.\n4. Do not include any additional text.\n\n### Answer: (use the provided format with backticks)\n", "reference": "                    validatedRecoveryTask.partitionId),\n                logMessageWriterImpl,\n                meterRegistry);\n\n        kafkaConsumer.prepConsumerForConsumption(validatedRecoveryTask.startOffset);\n        consumerPreparedTime = System.nanoTime();\n        kafkaConsumer.consumeMessagesBetweenOffsetsInParallel(\n            AstraKafkaConsumer.KAFKA_POLL_TIMEOUT_MS,\n            validatedRecoveryTask.startOffset,\n            validatedRecoveryTask.endOffset);\n        messagesConsumedTime = System.nanoTime();\n        // Wait for chunks to upload.\n        boolean success = chunkManager.waitForRollOvers();\n        rolloversCompletedTime = System.nanoTime();\n        // Close the recovery chunk manager and kafka consumer.\n        kafkaConsumer.close();\n        chunkManager.stopAsync();\n        chunkManager.awaitTerminated(DEFAULT_START_STOP_DURATION);\n        LOG.info(\"Finished handling the recovery task: {}\", validatedRecoveryTask);\n        taskTimer.stop(recoveryTaskTimerSuccess);\n        return success;\n      } catch (Exception ex) {\n        LOG.error(\"Exception in recovery task [{}]: {}\", validatedRecoveryTask, ex);\n        taskTimer.stop(recoveryTaskTimerFailure);\n        return false;\n      } finally {\n        long endTime = System.nanoTime();\n        LOG.info(\n            \"Recovery task {} took {}ms, (subtask times offset validation {}, consumer prep {}, msg consumption {}, rollover {})\",\n            recoveryTaskMetadata,\n            nanosToMillis(endTime - startTime),\n            nanosToMillis(offsetsValidatedTime - startTime),\n            nanosToMillis(consumerPreparedTime - offsetsValidatedTime),\n            nanosToMillis(messagesConsumedTime - consumerPreparedTime),\n            nanosToMillis(rolloversCompletedTime - messagesConsumedTime));\n      }\n    } else {\n      LOG.info(\n          \"Recovery task {} data no longer available in Kafka (validation time {}ms)\",\n          recoveryTaskMetadata,\n          nanosToMillis(offsetsValidatedTime - startTime));\n      recoveryRecordsNoLongerAvailable.increment(\n          recoveryTaskMetadata.endOffset - recoveryTaskMetadata.startOffset + 1);\n      return true;\n    }\n  }", "completions": ["  \n```\n        AstraConfig.getRecoveryConfig().getKafkaConfig().getKafkaTopic()),\n                recoveryTaskMetadata.partitionId,\n                partitionOffsets.startOffset,\n                partitionOffsets.endOffset,\n                logMessageWriterImpl));\n\n        consumerPreparedTime = System.nanoTime();\n        kafkaConsumer.consume();\n        messagesConsumedTime = System.nanoTime();\n        kafkaConsumer.close();\n\n        chunkManager.rollover();\n        rolloversCompletedTime = System.nanoTime();\n        chunkManager.close();\n\n        long endTime = System.nanoTime();\n        taskTimer.stop(\n            meterRegistry.timer(\n                RECOVERY_TASK_TIMER,\n                \"successful\",\n                \"true\",\n                \"partitionId\",\n                String.valueOf(recoveryTaskMetadata.partitionId)));\n\n        LOG.info(\n            \"Completed handling the recovery task: {} in {} ms\",\n            recoveryTaskMetadata,\n            nanosToMillis(endTime - startTime));\n        LOG.info(\n            \"Time taken for each subtask: offsetsValidatedTime: {} ms, consumerPreparedTime: {} ms, messagesConsumedTime: {} ms, rolloversCompletedTime: {} ms\",\n            nanosToMillis(offsetsValidatedTime - startTime),\n            nanosToMillis(consumerPreparedTime - offsetsValidatedTime),\n            nanosToMillis(messagesConsumedTime - consumerPreparedTime),\n            nanosToMillis(rolloversCompletedTime - messagesConsumedTime));\n\n        return true;\n      } catch (Exception e) {\n        taskTimer.stop(\n            meterRegistry.timer(\n                RECOVERY_TASK_TIMER,\n                \"successful\",\n                \"false\",\n                \"partitionId\",\n                String.valueOf(recoveryTaskMetadata.partitionId)));\n        LOG.error(\"Failed to complete the recovery task: {}\", recoveryTaskMetadata, e);\n        return false;\n      }\n    } else {\n      taskTimer.stop(\n          meterRegistry.timer(\n              RECOVERY_TASK_TIMER,\n              \"successful\",\n              \"false\",\n              \"partitionId\",\n              String.valueOf(recoveryTaskMetadata.partitionId)));\n      LOG.error(\"Failed to validate offsets for the recovery task: {}\", recoveryTaskMetadata);\n      return false;\n    }\n  }\n\n  /**\n   * Description: This method validates the Kafka offsets for a recovery task. It checks if the\n   * start and end offsets are valid and if the end offset is greater than the start offset. If the\n   * offsets are valid, it returns the validated offsets. If the offsets are not valid, it returns\n   * null.\n   *\n   * @param adminClient The Kafka admin client.\n   * @param recoveryTaskMetadata The metadata of the recovery task.\n   * @param kafkaTopic The Kafka topic.\n   * @return The validated offsets or null if the offsets are not valid.\n   */\n  @VisibleForTesting\n  PartitionOffsets validateKafkaOffsets(\n      AdminClient adminClient,\n      RecoveryTaskMetadata recoveryTaskMetadata,\n      String kafkaTopic) {\n    try {\n      ListOffsetsResult listOffsetsResult =\n          adminClient.listOffsets(\n              Map.of(\n                  new TopicPartition(kafkaTopic, recoveryTaskMetadata.partitionId),\n                  OffsetSpec.latest()));\n      Map<TopicPartition, ListOffsetsResult.ListOffsetsResultInfo> offsets =\n          listOffsetsResult.all().get();\n      long endOffset = offsets.get(new TopicPartition(kafkaTopic, recoveryTaskMetadata.partitionId)).offset();\n      if (recoveryTaskMetadata.endOffset > endOffset) {\n        LOG.error(\n            \"End offset {} is greater than the current end offset {} for partition {}\",\n            recoveryTaskMetadata.endOffset,\n            endOffset,\n            recoveryTaskMetadata.partitionId);\n        return null;\n      }\n      if (recoveryTaskMetadata.startOffset > endOffset) {\n        LOG.error(\n            \"Start offset {} is greater than the current"]}
{"id": 81, "name": "updateRollingFileAppender", "prompt": "### Role:\nYou are an experienced Java programmer assistant responsible for code completion tasks. Please intelligently complete the missing parts of the provided code snippet.\n\n### Task:\nBased on the provided code snippet, complete the missing method implementation. Ensure the code is logically correct, syntactically accurate, and follows best coding practices.\n\n### Left context\n```Java\npackage io.tapdata.Schedule.Watcher;\n\nimport com.tapdata.constant.BeanUtil;\nimport io.tapdata.Application;\nimport io.tapdata.common.SettingService;\nimport io.tapdata.observable.logging.ObsLoggerFactory;\nimport io.tapdata.observable.logging.util.Conf.LogConfiguration;\nimport io.tapdata.observable.logging.util.LogUtil;\nimport org.apache.commons.lang3.StringUtils;\nimport org.apache.logging.log4j.Level;\nimport org.apache.logging.log4j.core.Appender;\nimport org.apache.logging.log4j.core.Logger;\nimport org.apache.logging.log4j.core.LoggerContext;\nimport org.apache.logging.log4j.core.appender.RollingFileAppender;\nimport org.apache.logging.log4j.core.appender.rolling.CompositeTriggeringPolicy;\nimport org.apache.logging.log4j.core.appender.rolling.DefaultRolloverStrategy;\nimport org.apache.logging.log4j.core.appender.rolling.RollingFileManager;\nimport org.apache.logging.log4j.core.appender.rolling.action.Action;\nimport org.apache.logging.log4j.core.appender.rolling.action.DeleteAction;\nimport java.util.Collection;\nimport java.util.Map;\n\nimport static io.tapdata.Schedule.LogConfigurationWatcherManager.AGENT;\nimport static io.tapdata.observable.logging.util.LogUtil.logLevel;\n\npublic class AgentLogConfigurationWatcher extends AbstractLogConfigurationWatcher {\n    protected LoggerContext context = LoggerContext.getContext(false);\n    protected static final  String HTTPAPPENDER=\"httpAppender\";\n\n    public AgentLogConfigurationWatcher(LogConfiguration agentLogConfiguration) {\n        super(agentLogConfiguration);\n    }\n\n    public AgentLogConfigurationWatcher() {\n    }\n\n    @Override\n    LogConfiguration getLogConfig() {\n        SettingService settingService = BeanUtil.getBean(SettingService.class);\n        LogConfiguration logConfiguration = ObsLoggerFactory.getInstance().getLogConfiguration(AGENT);\n        String scriptEngineHttpApender = settingService.getString(\"scriptEngineHttpAppender\", \"false\");\n        String logLevel = settingService.getString(\"logLevel\",\"info\");\n        logConfiguration.setLogLevel(logLevel);\n        logConfiguration.setScriptEngineHttpAppender(scriptEngineHttpApender);\n        return logConfiguration;\n    }\n\n    @Override\n    protected boolean checkIsModify(LogConfiguration logConfiguration) {\n        boolean upperCheck =  super.checkIsModify(logConfiguration);\n        return upperCheck || (null != logConfiguration.getLogLevel() && !logConfiguration.getLogLevel().equals(this.logConfiguration.getLogLevel()))\n                || (null != logConfiguration.getScriptEngineHttpAppender() && !logConfiguration.getScriptEngineHttpAppender().equals(this.logConfiguration.getScriptEngineHttpAppender()));\n    }\n\n    @Override\n    protected void updateConfig(LogConfiguration logConfiguration) {\n        updateRollingFileAppender(logConfiguration);\n        updateLogLevel(logConfiguration);\n    }\n\n    public void updateLogLevel(LogConfiguration logConfiguration) {\n        String logLevel = logConfiguration.getLogLevel();\n        String scriptEngineHttpAppender = logConfiguration.getScriptEngineHttpAppender();\n        Level level = logLevel(logLevel);\n\n        Collection<Logger> loggers = context.getLoggers();\n        for (org.apache.logging.log4j.core.Logger logger1 : loggers) {\n            final String loggerName = logger1.getName();\n            if (\n                    StringUtils.startsWithIgnoreCase(loggerName, \"io.tapdata\") ||\n                            StringUtils.startsWithIgnoreCase(loggerName, \"com.tapdata\")\n            ) {\n                logger1.setLevel(level);\n                if (StringUtils.contains(loggerName, \"CustomProcessor\")) {\n                    final Map<String, Appender> appenders = logger1.get().getAppenders();\n                    refreshAppenders(logger1, scriptEngineHttpAppender, appenders);\n                }\n            }\n        }\n    }\n\n    protected void refreshAppenders(Logger logger1, String scriptEngineHttpAppender, Map<String, Appender> appenders) {\n        if (\"false\".equals(scriptEngineHttpAppender)) {\n            if (appenders.containsKey(HTTPAPPENDER)) {\n                logger1.setAdditive(false);\n                final Map<String, Appender> rootAppenders = context.getRootLogger().getAppenders();\n                for (Appender appender : rootAppenders.values()) {\n                    logger1.addAppender(appender);\n                }\n                logger1.get().removeAppender(HTTPAPPENDER);\n            }\n        } else if (!appenders.containsKey(HTTPAPPENDER)) {\n            logger1.setAdditive(true);\n        }\n    }\n\n```\n\n### Code description\n```Java\n/**\n * Description: This method updates the rolling file appender with a new rollover strategy and triggering policy.\n * \n * @param logConfiguration The configuration object containing log settings.\n */\n```\n\n### Code snippet:\n```Java\n    protected void updateRollingFileAppender(LogConfiguration logConfiguration) {\n        org.apache.logging.log4j.core.config.Configuration config = context.getConfiguration();\n        Appender appender = context.getRootLogger().getAppenders().get(\"rollingFileAppender\");\n        RollingFileAppender rollingFileAppender = null;\n        if (appender instanceof RollingFileAppender) {\n            rollingFileAppender = (RollingFileAppender) appender;\n        }\n        if(null == rollingFileAppender){\n            return;\n        }\n        RollingFileManager manager = rollingFileAppender.getManager();\n```\n\n### Requirements:\n1. Complete only the missing part of the method.\n2. Only answer the code of the completed part. Do not include any of the provided code snippet.\n3. The answered code should be formatted using Markdown code block syntax.\n4. Do not include any additional text.\n\n### Answer: (use the provided format with backticks)\n", "reference": "        CompositeTriggeringPolicy compositeTriggeringPolicy = LogUtil.getCompositeTriggeringPolicy(logConfiguration.getLogSaveSize().toString());\n        String glob = \"tapdata-agent-*.log.*.gz\";\n        DeleteAction deleteAction = LogUtil.getDeleteAction(logConfiguration.getLogSaveTime(), Application.logsPath, glob, config);\n        Action[] actions = {deleteAction};\n        DefaultRolloverStrategy strategy = DefaultRolloverStrategy.newBuilder()\n                .withMax(logConfiguration.getLogSaveCount().toString())\n                .withCustomActions(actions)\n                .withConfig(config)\n                .build();\n        manager.setRolloverStrategy(strategy);\n        manager.setTriggeringPolicy(compositeTriggeringPolicy);\n    }", "completions": ["  \n```Java\n        if (null == rollingFileAppender) {\n            return;\n        }\n        RollingFileManager manager = rollingFileAppender.getManager();\n        CompositeTriggeringPolicy policy = CompositeTriggeringPolicy.createPolicy(\n                config,\n                DefaultRolloverStrategy.createStrategy(\n                        config,\n                        null,\n                        null,\n                        null,\n                        null,\n                        true,\n                        manager\n                ),\n                manager\n        );\n        rollingFileAppender.stop();\n        rollingFileAppender.start();\n    }\n```"]}
{"id": 82, "name": "shutdown", "prompt": "### Role:\nYou are an experienced Java programmer assistant responsible for code completion tasks. Please intelligently complete the missing parts of the provided code snippet.\n\n### Task:\nBased on the provided code snippet, complete the missing method implementation. Ensure the code is logically correct, syntactically accurate, and follows best coding practices.\n\n### Left context\n```Java\npackage com.slack.astra.server;\n\nimport com.google.common.util.concurrent.MoreExecutors;\nimport com.google.common.util.concurrent.Service;\nimport com.google.common.util.concurrent.ServiceManager;\nimport com.slack.astra.blobfs.BlobFs;\nimport com.slack.astra.blobfs.s3.S3CrtBlobFs;\nimport com.slack.astra.bulkIngestApi.BulkIngestApi;\nimport com.slack.astra.bulkIngestApi.BulkIngestKafkaProducer;\nimport com.slack.astra.bulkIngestApi.DatasetRateLimitingService;\nimport com.slack.astra.chunkManager.CachingChunkManager;\nimport com.slack.astra.chunkManager.IndexingChunkManager;\nimport com.slack.astra.clusterManager.ClusterHpaMetricService;\nimport com.slack.astra.clusterManager.ClusterMonitorService;\nimport com.slack.astra.clusterManager.RecoveryTaskAssignmentService;\nimport com.slack.astra.clusterManager.ReplicaAssignmentService;\nimport com.slack.astra.clusterManager.ReplicaCreationService;\nimport com.slack.astra.clusterManager.ReplicaDeletionService;\nimport com.slack.astra.clusterManager.ReplicaEvictionService;\nimport com.slack.astra.clusterManager.ReplicaRestoreService;\nimport com.slack.astra.clusterManager.SnapshotDeletionService;\nimport com.slack.astra.elasticsearchApi.ElasticsearchApiService;\nimport com.slack.astra.logstore.LogMessage;\nimport com.slack.astra.logstore.search.AstraDistributedQueryService;\nimport com.slack.astra.logstore.search.AstraLocalQueryService;\nimport com.slack.astra.metadata.cache.CacheSlotMetadataStore;\nimport com.slack.astra.metadata.core.CloseableLifecycleManager;\nimport com.slack.astra.metadata.core.CuratorBuilder;\nimport com.slack.astra.metadata.dataset.DatasetMetadataStore;\nimport com.slack.astra.metadata.hpa.HpaMetricMetadataStore;\nimport com.slack.astra.metadata.recovery.RecoveryNodeMetadataStore;\nimport com.slack.astra.metadata.recovery.RecoveryTaskMetadataStore;\nimport com.slack.astra.metadata.replica.ReplicaMetadataStore;\nimport com.slack.astra.metadata.schema.SchemaUtil;\nimport com.slack.astra.metadata.search.SearchMetadataStore;\nimport com.slack.astra.metadata.snapshot.SnapshotMetadataStore;\nimport com.slack.astra.preprocessor.PreprocessorService;\nimport com.slack.astra.proto.config.AstraConfigs;\nimport com.slack.astra.proto.metadata.Metadata;\nimport com.slack.astra.proto.schema.Schema;\nimport com.slack.astra.recovery.RecoveryService;\nimport com.slack.astra.util.RuntimeHalterImpl;\nimport com.slack.astra.zipkinApi.ZipkinService;\nimport io.micrometer.core.instrument.MeterRegistry;\nimport io.micrometer.core.instrument.Metrics;\nimport io.micrometer.core.instrument.binder.jvm.ClassLoaderMetrics;\nimport io.micrometer.core.instrument.binder.jvm.JvmGcMetrics;\nimport io.micrometer.core.instrument.binder.jvm.JvmMemoryMetrics;\nimport io.micrometer.core.instrument.binder.jvm.JvmThreadMetrics;\nimport io.micrometer.core.instrument.binder.system.ProcessorMetrics;\nimport io.micrometer.prometheus.PrometheusConfig;\nimport io.micrometer.prometheus.PrometheusMeterRegistry;\nimport java.nio.file.Path;\nimport java.time.Duration;\nimport java.util.HashSet;\nimport java.util.List;\nimport java.util.Set;\nimport java.util.concurrent.TimeUnit;\nimport org.apache.curator.x.async.AsyncCuratorFramework;\nimport org.apache.logging.log4j.LogManager;\nimport org.apache.logging.log4j.util.Strings;\nimport org.slf4j.Logger;\nimport org.slf4j.LoggerFactory;\nimport software.amazon.awssdk.services.s3.S3AsyncClient;\n\n/**\n * Main class of Astra that sets up the basic infra needed for all the other end points like an a\n * http server, register monitoring libraries, create config manager etc..\n */\npublic class Astra {\n  private static final Logger LOG = LoggerFactory.getLogger(Astra.class);\n\n  private final PrometheusMeterRegistry prometheusMeterRegistry;\n\n  private final AstraConfigs.AstraConfig astraConfig;\n  private final S3AsyncClient s3Client;\n  protected ServiceManager serviceManager;\n  protected AsyncCuratorFramework curatorFramework;\n\n  Astra(\n      AstraConfigs.AstraConfig astraConfig,\n      S3AsyncClient s3Client,\n      PrometheusMeterRegistry prometheusMeterRegistry) {\n    this.prometheusMeterRegistry = prometheusMeterRegistry;\n    this.astraConfig = astraConfig;\n    this.s3Client = s3Client;\n    Metrics.addRegistry(prometheusMeterRegistry);\n    LOG.info(\"Started Astra process with config: {}\", astraConfig);\n  }\n\n  Astra(AstraConfigs.AstraConfig astraConfig, PrometheusMeterRegistry prometheusMeterRegistry) {\n    this(astraConfig, S3CrtBlobFs.initS3Client(astraConfig.getS3Config()), prometheusMeterRegistry);\n  }\n\n  public static void main(String[] args) throws Exception {\n    if (args.length == 0) {\n      LOG.info(\"Config file is needed a first argument\");\n    }\n    Path configFilePath = Path.of(args[0]);\n\n    AstraConfig.initFromFile(configFilePath);\n    AstraConfigs.AstraConfig config = AstraConfig.get();\n    Astra astra = new Astra(AstraConfig.get(), initPrometheusMeterRegistry(config));\n    astra.start();\n  }\n\n  static PrometheusMeterRegistry initPrometheusMeterRegistry(AstraConfigs.AstraConfig config) {\n    PrometheusMeterRegistry prometheusMeterRegistry =\n        new PrometheusMeterRegistry(PrometheusConfig.DEFAULT);\n    prometheusMeterRegistry\n        .config()\n        .commonTags(\n            \"astra_cluster_name\",\n            config.getClusterConfig().getClusterName(),\n            \"astra_env\",\n            config.getClusterConfig().getEnv(),\n            \"astra_component\",\n            getComponentTag(config));\n    return prometheusMeterRegistry;\n  }\n\n  private static String getComponentTag(AstraConfigs.AstraConfig config) {\n    String component;\n    if (config.getNodeRolesList().size() == 1) {\n      component = config.getNodeRolesList().get(0).toString();\n    } else {\n      component = Strings.join(config.getNodeRolesList(), '-');\n    }\n    return Strings.toRootLowerCase(component);\n  }\n\n  public void start() throws Exception {\n    setupSystemMetrics(prometheusMeterRegistry);\n    addShutdownHook();\n\n    curatorFramework =\n        CuratorBuilder.build(\n            prometheusMeterRegistry, astraConfig.getMetadataStoreConfig().getZookeeperConfig());\n\n    // Initialize blobfs. Only S3 is supported currently.\n    S3CrtBlobFs s3BlobFs = new S3CrtBlobFs(s3Client);\n\n    Set<Service> services =\n        getServices(curatorFramework, astraConfig, s3BlobFs, prometheusMeterRegistry);\n    serviceManager = new ServiceManager(services);\n    serviceManager.addListener(getServiceManagerListener(), MoreExecutors.directExecutor());\n\n    serviceManager.startAsync();\n  }\n\n  private static Set<Service> getServices(\n      AsyncCuratorFramework curatorFramework,\n      AstraConfigs.AstraConfig astraConfig,\n      BlobFs blobFs,\n      PrometheusMeterRegistry meterRegistry)\n      throws Exception {\n    Set<Service> services = new HashSet<>();\n\n    HashSet<AstraConfigs.NodeRole> roles = new HashSet<>(astraConfig.getNodeRolesList());\n\n    if (roles.contains(AstraConfigs.NodeRole.INDEX)) {\n      IndexingChunkManager<LogMessage> chunkManager =\n          IndexingChunkManager.fromConfig(\n              meterRegistry,\n              curatorFramework,\n              astraConfig.getIndexerConfig(),\n              blobFs,\n              astraConfig.getS3Config());\n      services.add(chunkManager);\n\n      AstraIndexer indexer =\n          new AstraIndexer(\n              chunkManager,\n              curatorFramework,\n              astraConfig.getIndexerConfig(),\n              astraConfig.getIndexerConfig().getKafkaConfig(),\n              meterRegistry);\n      services.add(indexer);\n\n      AstraLocalQueryService<LogMessage> searcher =\n          new AstraLocalQueryService<>(\n              chunkManager,\n              Duration.ofMillis(astraConfig.getIndexerConfig().getDefaultQueryTimeoutMs()));\n      final int serverPort = astraConfig.getIndexerConfig().getServerConfig().getServerPort();\n      Duration requestTimeout =\n          Duration.ofMillis(astraConfig.getIndexerConfig().getServerConfig().getRequestTimeoutMs());\n      ArmeriaService armeriaService =\n          new ArmeriaService.Builder(serverPort, \"astraIndex\", meterRegistry)\n              .withRequestTimeout(requestTimeout)\n              .withTracing(astraConfig.getTracingConfig())\n              .withGrpcService(searcher)\n              .build();\n      services.add(armeriaService);\n    }\n\n    if (roles.contains(AstraConfigs.NodeRole.QUERY)) {\n      SearchMetadataStore searchMetadataStore = new SearchMetadataStore(curatorFramework, true);\n      SnapshotMetadataStore snapshotMetadataStore = new SnapshotMetadataStore(curatorFramework);\n      DatasetMetadataStore datasetMetadataStore = new DatasetMetadataStore(curatorFramework, true);\n\n      services.add(\n          new CloseableLifecycleManager(\n              AstraConfigs.NodeRole.QUERY,\n              List.of(searchMetadataStore, snapshotMetadataStore, datasetMetadataStore)));\n\n      Duration requestTimeout =\n          Duration.ofMillis(astraConfig.getQueryConfig().getServerConfig().getRequestTimeoutMs());\n      AstraDistributedQueryService astraDistributedQueryService =\n          new AstraDistributedQueryService(\n              searchMetadataStore,\n              snapshotMetadataStore,\n              datasetMetadataStore,\n              meterRegistry,\n              requestTimeout,\n              Duration.ofMillis(astraConfig.getQueryConfig().getDefaultQueryTimeoutMs()));\n      // todo - close the astraDistributedQueryService once done (depends on\n      // https://github.com/slackhq/astra/pull/564)\n      final int serverPort = astraConfig.getQueryConfig().getServerConfig().getServerPort();\n\n      ArmeriaService armeriaService =\n          new ArmeriaService.Builder(serverPort, \"astraQuery\", meterRegistry)\n              .withRequestTimeout(requestTimeout)\n              .withTracing(astraConfig.getTracingConfig())\n              .withAnnotatedService(new ElasticsearchApiService(astraDistributedQueryService))\n              .withAnnotatedService(new ZipkinService(astraDistributedQueryService))\n              .withGrpcService(astraDistributedQueryService)\n              .build();\n      services.add(armeriaService);\n    }\n\n    if (roles.contains(AstraConfigs.NodeRole.CACHE)) {\n      CachingChunkManager<LogMessage> chunkManager =\n          CachingChunkManager.fromConfig(\n              meterRegistry,\n              curatorFramework,\n              astraConfig.getS3Config(),\n              astraConfig.getCacheConfig(),\n              blobFs);\n      services.add(chunkManager);\n\n      HpaMetricMetadataStore hpaMetricMetadataStore =\n          new HpaMetricMetadataStore(curatorFramework, true);\n      services.add(\n          new CloseableLifecycleManager(\n              AstraConfigs.NodeRole.CACHE, List.of(hpaMetricMetadataStore)));\n      HpaMetricPublisherService hpaMetricPublisherService =\n          new HpaMetricPublisherService(\n              hpaMetricMetadataStore, meterRegistry, Metadata.HpaMetricMetadata.NodeRole.CACHE);\n      services.add(hpaMetricPublisherService);\n\n      AstraLocalQueryService<LogMessage> searcher =\n          new AstraLocalQueryService<>(\n              chunkManager,\n              Duration.ofMillis(astraConfig.getCacheConfig().getDefaultQueryTimeoutMs()));\n      final int serverPort = astraConfig.getCacheConfig().getServerConfig().getServerPort();\n      Duration requestTimeout =\n          Duration.ofMillis(astraConfig.getCacheConfig().getServerConfig().getRequestTimeoutMs());\n      ArmeriaService armeriaService =\n          new ArmeriaService.Builder(serverPort, \"astraCache\", meterRegistry)\n              .withRequestTimeout(requestTimeout)\n              .withTracing(astraConfig.getTracingConfig())\n              .withGrpcService(searcher)\n              .build();\n      services.add(armeriaService);\n    }\n\n    if (roles.contains(AstraConfigs.NodeRole.MANAGER)) {\n      final AstraConfigs.ManagerConfig managerConfig = astraConfig.getManagerConfig();\n      final int serverPort = managerConfig.getServerConfig().getServerPort();\n\n      ReplicaMetadataStore replicaMetadataStore = new ReplicaMetadataStore(curatorFramework);\n      SnapshotMetadataStore snapshotMetadataStore = new SnapshotMetadataStore(curatorFramework);\n      RecoveryTaskMetadataStore recoveryTaskMetadataStore =\n          new RecoveryTaskMetadataStore(curatorFramework, true);\n      RecoveryNodeMetadataStore recoveryNodeMetadataStore =\n          new RecoveryNodeMetadataStore(curatorFramework, true);\n      CacheSlotMetadataStore cacheSlotMetadataStore = new CacheSlotMetadataStore(curatorFramework);\n      DatasetMetadataStore datasetMetadataStore = new DatasetMetadataStore(curatorFramework, true);\n      HpaMetricMetadataStore hpaMetricMetadataStore =\n          new HpaMetricMetadataStore(curatorFramework, true);\n\n      Duration requestTimeout =\n          Duration.ofMillis(astraConfig.getManagerConfig().getServerConfig().getRequestTimeoutMs());\n      ReplicaRestoreService replicaRestoreService =\n          new ReplicaRestoreService(replicaMetadataStore, meterRegistry, managerConfig);\n      services.add(replicaRestoreService);\n\n      ArmeriaService armeriaService =\n          new ArmeriaService.Builder(serverPort, \"astraManager\", meterRegistry)\n              .withRequestTimeout(requestTimeout)\n              .withTracing(astraConfig.getTracingConfig())\n              .withGrpcService(\n                  new ManagerApiGrpc(\n                      datasetMetadataStore, snapshotMetadataStore, replicaRestoreService))\n              .build();\n      services.add(armeriaService);\n\n      services.add(\n          new CloseableLifecycleManager(\n              AstraConfigs.NodeRole.MANAGER,\n              List.of(\n                  replicaMetadataStore,\n                  snapshotMetadataStore,\n                  recoveryTaskMetadataStore,\n                  recoveryNodeMetadataStore,\n                  cacheSlotMetadataStore,\n                  datasetMetadataStore,\n                  hpaMetricMetadataStore)));\n\n      ReplicaCreationService replicaCreationService =\n          new ReplicaCreationService(\n              replicaMetadataStore, snapshotMetadataStore, managerConfig, meterRegistry);\n      services.add(replicaCreationService);\n\n      ReplicaEvictionService replicaEvictionService =\n          new ReplicaEvictionService(\n              cacheSlotMetadataStore, replicaMetadataStore, managerConfig, meterRegistry);\n      services.add(replicaEvictionService);\n\n      ReplicaDeletionService replicaDeletionService =\n          new ReplicaDeletionService(\n              cacheSlotMetadataStore, replicaMetadataStore, managerConfig, meterRegistry);\n      services.add(replicaDeletionService);\n\n      RecoveryTaskAssignmentService recoveryTaskAssignmentService =\n          new RecoveryTaskAssignmentService(\n              recoveryTaskMetadataStore, recoveryNodeMetadataStore, managerConfig, meterRegistry);\n      services.add(recoveryTaskAssignmentService);\n\n      ReplicaAssignmentService replicaAssignmentService =\n          new ReplicaAssignmentService(\n              cacheSlotMetadataStore, replicaMetadataStore, managerConfig, meterRegistry);\n      services.add(replicaAssignmentService);\n\n      SnapshotDeletionService snapshotDeletionService =\n          new SnapshotDeletionService(\n              replicaMetadataStore, snapshotMetadataStore, blobFs, managerConfig, meterRegistry);\n      services.add(snapshotDeletionService);\n\n      ClusterMonitorService clusterMonitorService =\n          new ClusterMonitorService(\n              replicaMetadataStore,\n              snapshotMetadataStore,\n              recoveryTaskMetadataStore,\n              recoveryNodeMetadataStore,\n              cacheSlotMetadataStore,\n              datasetMetadataStore,\n              meterRegistry);\n      services.add(clusterMonitorService);\n\n      ClusterHpaMetricService clusterHpaMetricService =\n          new ClusterHpaMetricService(\n              replicaMetadataStore, cacheSlotMetadataStore, hpaMetricMetadataStore);\n      services.add(clusterHpaMetricService);\n    }\n\n    if (roles.contains(AstraConfigs.NodeRole.RECOVERY)) {\n      final AstraConfigs.RecoveryConfig recoveryConfig = astraConfig.getRecoveryConfig();\n      final int serverPort = recoveryConfig.getServerConfig().getServerPort();\n\n      Duration requestTimeout =\n          Duration.ofMillis(\n              astraConfig.getRecoveryConfig().getServerConfig().getRequestTimeoutMs());\n      ArmeriaService armeriaService =\n          new ArmeriaService.Builder(serverPort, \"astraRecovery\", meterRegistry)\n              .withRequestTimeout(requestTimeout)\n              .withTracing(astraConfig.getTracingConfig())\n              .build();\n      services.add(armeriaService);\n\n      RecoveryService recoveryService =\n          new RecoveryService(astraConfig, curatorFramework, meterRegistry, blobFs);\n      services.add(recoveryService);\n    }\n\n    if (roles.contains(AstraConfigs.NodeRole.PREPROCESSOR)) {\n      DatasetMetadataStore datasetMetadataStore = new DatasetMetadataStore(curatorFramework, true);\n\n      final AstraConfigs.PreprocessorConfig preprocessorConfig =\n          astraConfig.getPreprocessorConfig();\n      final int serverPort = preprocessorConfig.getServerConfig().getServerPort();\n\n      Duration requestTimeout =\n          Duration.ofMillis(\n              astraConfig.getPreprocessorConfig().getServerConfig().getRequestTimeoutMs());\n      ArmeriaService.Builder armeriaServiceBuilder =\n          new ArmeriaService.Builder(serverPort, \"astraPreprocessor\", meterRegistry)\n              .withRequestTimeout(requestTimeout)\n              .withTracing(astraConfig.getTracingConfig());\n\n      services.add(\n          new CloseableLifecycleManager(\n              AstraConfigs.NodeRole.PREPROCESSOR, List.of(datasetMetadataStore)));\n\n      if (preprocessorConfig.getUseBulkApi()) {\n        BulkIngestKafkaProducer bulkIngestKafkaProducer =\n            new BulkIngestKafkaProducer(datasetMetadataStore, preprocessorConfig, meterRegistry);\n        services.add(bulkIngestKafkaProducer);\n        DatasetRateLimitingService datasetRateLimitingService =\n            new DatasetRateLimitingService(datasetMetadataStore, preprocessorConfig, meterRegistry);\n        services.add(datasetRateLimitingService);\n\n        Schema.IngestSchema schema = Schema.IngestSchema.getDefaultInstance();\n        if (!preprocessorConfig.getSchemaFile().isEmpty()) {\n          LOG.info(\"Loading schema file: {}\", preprocessorConfig.getSchemaFile());\n          schema = SchemaUtil.parseSchema(Path.of(preprocessorConfig.getSchemaFile()));\n          LOG.info(\"Loaded schema with total fields: {}\", schema.getFieldsCount());\n        } else {\n          LOG.info(\"No schema file provided, using default schema\");\n        }\n        BulkIngestApi openSearchBulkApiService =\n            new BulkIngestApi(\n                bulkIngestKafkaProducer,\n                datasetRateLimitingService,\n                meterRegistry,\n                preprocessorConfig.getRateLimitExceededErrorCode(),\n                schema);\n        armeriaServiceBuilder.withAnnotatedService(openSearchBulkApiService);\n      } else {\n        PreprocessorService preprocessorService =\n            new PreprocessorService(datasetMetadataStore, preprocessorConfig, meterRegistry);\n        services.add(preprocessorService);\n      }\n      services.add(armeriaServiceBuilder.build());\n    }\n\n    return services;\n  }\n\n  private static ServiceManager.Listener getServiceManagerListener() {\n    return new ServiceManager.Listener() {\n      @Override\n      public void failure(Service service) {\n        LOG.error(\n            String.format(\"Service %s failed with cause \", service.getClass().toString()),\n            service.failureCause());\n        // shutdown if any services enters failure state\n        new RuntimeHalterImpl()\n            .handleFatal(new Throwable(\"Shutting down Astra due to failed service\"));\n      }\n    };\n  }\n\n```\n\n### Code description\n```Java\n/**\n * Description: This method is responsible for shutting down the serviceManager and curatorFramework.\n * It first attempts to stop the serviceManager asynchronously and wait for it to stop within a specified time limit.\n * If this operation times out, it logs an error message.\n * Then it attempts to close the curatorFramework. If this operation fails, it logs an error message.\n * Finally, it logs an informational message and shuts down the LogManager.\n *\n * @throws Exception if there is an error while stopping the serviceManager or closing the curatorFramework.\n */\n```\n\n### Code snippet:\n```Java\n  void shutdown() {\n    LOG.info(\"Running shutdown hook.\");\n    try {\n      serviceManager.stopAsync().awaitStopped(30, TimeUnit.SECONDS);\n    } catch (Exception e) {\n      // stopping timed out\n      LOG.error(\"ServiceManager shutdown timed out\", e);\n    }\n```\n\n### Requirements:\n1. Complete only the missing part of the method.\n2. Only answer the code of the completed part. Do not include any of the provided code snippet.\n3. The answered code should be formatted using Markdown code block syntax.\n4. Do not include any additional text.\n\n### Answer: (use the provided format with backticks)\n", "reference": "    try {\n      curatorFramework.unwrap().close();\n    } catch (Exception e) {\n      LOG.error(\"Error while closing curatorFramework \", e);\n    }\n    LOG.info(\"Shutting down LogManager\");\n    LogManager.shutdown();\n  }", "completions": ["  \n```Java\n  void shutdown() {\n    LOG.info(\"Running shutdown hook.\");\n    try {\n      serviceManager.stopAsync().awaitStopped(30, TimeUnit.SECONDS);\n    } catch (Exception e) {\n      // stopping timed out\n      LOG.error(\"ServiceManager shutdown timed out\", e);\n    }\n    try {\n      curatorFramework.close();\n    } catch (Exception e) {\n      LOG.error(\"Error closing curatorFramework\", e);\n    }\n    LOG.info(\"Shutdown complete.\");\n    LogManager.shutdown();\n  }\n```"]}
{"id": 83, "name": "determineStartingOffset", "prompt": "### Role:\nYou are an experienced Java programmer assistant responsible for code completion tasks. Please intelligently complete the missing parts of the provided code snippet.\n\n### Task:\nBased on the provided code snippet, complete the missing method implementation. Ensure the code is logically correct, syntactically accurate, and follows best coding practices.\n\n### Left context\n```Java\npackage com.slack.astra.server;\n\nimport static com.google.common.base.Preconditions.checkArgument;\nimport static com.google.common.util.concurrent.Futures.addCallback;\nimport static com.slack.astra.util.FutureUtils.successCountingCallback;\n\nimport com.google.common.annotations.VisibleForTesting;\nimport com.google.common.util.concurrent.Futures;\nimport com.google.common.util.concurrent.JdkFutureAdapters;\nimport com.google.common.util.concurrent.ListenableFuture;\nimport com.google.common.util.concurrent.MoreExecutors;\nimport com.slack.astra.metadata.recovery.RecoveryTaskMetadata;\nimport com.slack.astra.metadata.recovery.RecoveryTaskMetadataStore;\nimport com.slack.astra.metadata.snapshot.SnapshotMetadata;\nimport com.slack.astra.metadata.snapshot.SnapshotMetadataStore;\nimport com.slack.astra.proto.config.AstraConfigs;\nimport io.micrometer.core.instrument.Counter;\nimport io.micrometer.core.instrument.MeterRegistry;\nimport java.time.Instant;\nimport java.util.List;\nimport java.util.UUID;\nimport java.util.concurrent.TimeUnit;\nimport java.util.concurrent.atomic.AtomicInteger;\nimport java.util.stream.Collectors;\nimport org.apache.logging.log4j.util.Strings;\nimport org.slf4j.Logger;\nimport org.slf4j.LoggerFactory;\n\n/**\n * This class is responsible for the indexer startup operations like stale live snapshot cleanup.\n * determining the start indexing offset from metadata and optionally creating a recovery task etc.\n */\npublic class RecoveryTaskCreator {\n  private static final Logger LOG = LoggerFactory.getLogger(RecoveryTaskCreator.class);\n  private static final int SNAPSHOT_OPERATION_TIMEOUT_SECS = 10;\n  public static final String STALE_SNAPSHOT_DELETE_SUCCESS = \"stale_snapshot_delete_success\";\n  public static final String STALE_SNAPSHOT_DELETE_FAILED = \"stale_snapshot_delete_failed\";\n  public static final String RECOVERY_TASKS_CREATED = \"recovery_tasks_created\";\n\n  private final SnapshotMetadataStore snapshotMetadataStore;\n  private final RecoveryTaskMetadataStore recoveryTaskMetadataStore;\n  private final String partitionId;\n  private final long maxOffsetDelay;\n  private final long maxMessagesPerRecoveryTask;\n\n  private final Counter snapshotDeleteSuccess;\n  private final Counter snapshotDeleteFailed;\n  private final Counter recoveryTasksCreated;\n\n  public RecoveryTaskCreator(\n      SnapshotMetadataStore snapshotMetadataStore,\n      RecoveryTaskMetadataStore recoveryTaskMetadataStore,\n      String partitionId,\n      long maxOffsetDelay,\n      long maxMessagesPerRecoveryTask,\n      MeterRegistry meterRegistry) {\n    checkArgument(\n        partitionId != null && !partitionId.isEmpty(), \"partitionId shouldn't be null or empty\");\n    checkArgument(maxOffsetDelay > 0, \"maxOffsetDelay should be a positive number\");\n    checkArgument(\n        maxMessagesPerRecoveryTask > 0, \"Max messages per recovery task should be positive number\");\n    this.snapshotMetadataStore = snapshotMetadataStore;\n    this.recoveryTaskMetadataStore = recoveryTaskMetadataStore;\n    this.partitionId = partitionId;\n    this.maxOffsetDelay = maxOffsetDelay;\n    this.maxMessagesPerRecoveryTask = maxMessagesPerRecoveryTask;\n\n    snapshotDeleteSuccess = meterRegistry.counter(STALE_SNAPSHOT_DELETE_SUCCESS);\n    snapshotDeleteFailed = meterRegistry.counter(STALE_SNAPSHOT_DELETE_FAILED);\n    recoveryTasksCreated =\n        meterRegistry.counter(RECOVERY_TASKS_CREATED, \"partitionId\", partitionId);\n  }\n\n  @VisibleForTesting\n  public static List<SnapshotMetadata> getStaleLiveSnapshots(\n      List<SnapshotMetadata> snapshots, String partitionId) {\n    return snapshots.stream()\n        .filter(snapshotMetadata -> snapshotMetadata.partitionId.equals(partitionId))\n        .filter(SnapshotMetadata::isLive)\n        .collect(Collectors.toUnmodifiableList());\n  }\n\n  // Get the highest offset for which data is durable for a partition.\n  @VisibleForTesting\n  public static long getHighestDurableOffsetForPartition(\n      List<SnapshotMetadata> snapshots,\n      List<RecoveryTaskMetadata> recoveryTasks,\n      String partitionId) {\n\n    long maxSnapshotOffset =\n        snapshots.stream()\n            .filter(snapshot -> snapshot.partitionId.equals(partitionId))\n            .mapToLong(snapshot -> snapshot.maxOffset)\n            .max()\n            .orElse(-1);\n\n    long maxRecoveryOffset =\n        recoveryTasks.stream()\n            .filter(recoveryTaskMetadata -> recoveryTaskMetadata.partitionId.equals(partitionId))\n            .mapToLong(recoveryTaskMetadata -> recoveryTaskMetadata.endOffset)\n            .max()\n            .orElse(-1);\n\n    return Math.max(maxRecoveryOffset, maxSnapshotOffset);\n  }\n\n  private static String getRecoveryTaskName(String partitionId) {\n    return \"recoveryTask_\"\n        + partitionId\n        + \"_\"\n        + Instant.now().getEpochSecond()\n        + \"_\"\n        + UUID.randomUUID();\n  }\n\n  @VisibleForTesting\n  public List<SnapshotMetadata> deleteStaleLiveSnapshots(List<SnapshotMetadata> snapshots) {\n    List<SnapshotMetadata> staleSnapshots = getStaleLiveSnapshots(snapshots, partitionId);\n    LOG.info(\"Deleting {} stale snapshots: {}\", staleSnapshots.size(), staleSnapshots);\n    int deletedSnapshotCount = deleteSnapshots(snapshotMetadataStore, staleSnapshots);\n\n    int failedDeletes = staleSnapshots.size() - deletedSnapshotCount;\n    if (failedDeletes > 0) {\n      LOG.warn(\"Failed to delete {} live snapshots\", failedDeletes);\n      throw new IllegalStateException(\"Failed to delete stale live snapshots\");\n    }\n\n    return staleSnapshots;\n  }\n\n  /**\n   * To determine the start offset, an indexer performs multiple tasks. First, we clean up all the\n   * stale live nodes for this partition so there is only 1 live node per indexer.\n   *\n   * <p>In Astra, the durability of un-indexed data is ensured by Kafka and the durability of\n   * indexed data is ensured by S3. So, once the indexer restarts, we need to determine the highest\n   * offset that was indexed. To get the latest indexed offset, we get the latest indexed offset\n   * from a snapshots for that partition. Since there could also be a recovery task queued up for\n   * this partition, we also need to skip the offsets picked up by the recovery task. So, the\n   * highest durable offset is the highest offset for a partition among the snapshots and recovery\n   * tasks for a partition.\n   *\n   * <p>The highest durable offset is the start offset for the indexer. If this offset is with in\n   * the max start delay of the head, we start indexing. If the current index offset is more than\n   * the configured delay, we can't catch up indexing. So, instead of trying to catch up, create a\n   * recovery task and start indexing at the current head. This strategy achieves 2 goals: we start\n   * indexing fresh data when we are behind, and we add more indexing capacity when needed. The\n   * recovery task offsets are [startOffset, endOffset]. If a recovery task is created, we start\n   * indexing at the offset after the recovery task.\n   *\n   * <p>When there is no offset data for a partition, if indexer.readFromLocationOnStart is set to\n   * LATEST and indexer.createRecoveryTasksOnStart is set to \"false\", then simply return the latest\n   * offset and start reading from there. This would be useful in the case that you're spinning up a\n   * new cluster on existing data and don't care about data previously in the pipeline. If instead\n   * indexer.createRecoveryTasksOnStart is set to \"true\", then the latest position will still be\n   * returned but recovery tasks will be created to ingest from the beginning to the latest. If\n   * instead indexer.readFromLocationOnStart is set to EARLIEST, then return -1. In that case, the\n   * consumer would have to start indexing the data from the earliest offset.\n   */\n```\n\n### Code description\n```Java\n/**\n * Determines the starting offset for a given partition based on the current end offset and the current beginning offset.\n * Filters out stale snapshots and deletes them.\n * Calculates the highest durable offset for the partition by comparing the non-live snapshots with the recovery tasks.\n * If the highest durable offset is less than or equal to 0, it checks the indexer configuration to determine the starting offset.\n * If the current end offset is lower than the highest durable offset, an exception is thrown.\n * The next offset for the partition is calculated by incrementing the highest durable offset by 1.\n * If the difference between the last indexed position and the head location is higher than the max offset, a recovery task is created.\n * Otherwise, the next offset for the partition is returned as the starting offset.\n *\n * @param currentEndOffsetForPartition The current end offset for the partition.\n * @param currentBeginningOffsetForPartition The current beginning offset for the partition.\n * @param indexerConfig The configuration for the indexer.\n * @return The starting offset for the partition.\n * @throws IllegalStateException If the current end offset for the partition is lower than the highest durable offset for the partition.\n */\n```\n\n### Code snippet:\n```Java\n  public long determineStartingOffset(\n      long currentEndOffsetForPartition,\n      long currentBeginningOffsetForPartition,\n      AstraConfigs.IndexerConfig indexerConfig) {\n    // Filter stale snapshots for partition.\n    if (partitionId == null) {\n      LOG.warn(\"PartitionId can't be null.\");\n    }\n\n    List<SnapshotMetadata> snapshots = snapshotMetadataStore.listSync();\n    List<SnapshotMetadata> snapshotsForPartition =\n        snapshots.stream()\n            .filter(\n                snapshotMetadata -> {\n                  if (snapshotMetadata == null || snapshotMetadata.partitionId == null) {\n                    LOG.warn(\n                        \"snapshot metadata or partition id can't be null: {} \",\n                        Strings.join(snapshots, ','));\n                  }\n                  return snapshotMetadata != null\n                      && snapshotMetadata.partitionId != null\n                      && snapshotMetadata.partitionId.equals(partitionId);\n                })\n            .collect(Collectors.toUnmodifiableList());\n    List<SnapshotMetadata> deletedSnapshots = deleteStaleLiveSnapshots(snapshotsForPartition);\n\n    List<SnapshotMetadata> nonLiveSnapshotsForPartition =\n        snapshotsForPartition.stream()\n            .filter(s -> !deletedSnapshots.contains(s))\n            .collect(Collectors.toUnmodifiableList());\n\n    // Get the highest offset that is indexed in durable store.\n    List<RecoveryTaskMetadata> recoveryTasks = recoveryTaskMetadataStore.listSync();\n    long highestDurableOffsetForPartition =\n        getHighestDurableOffsetForPartition(\n            nonLiveSnapshotsForPartition, recoveryTasks, partitionId);\n    LOG.debug(\n        \"The highest durable offset for partition {} is {}\",\n        partitionId,\n        highestDurableOffsetForPartition);\n\n    if (highestDurableOffsetForPartition <= 0) {\n      LOG.info(\"There is no prior offset for this partition {}.\", partitionId);\n\n      // If the user wants to start at the current offset in Kafka and _does not_ want to create\n      // recovery tasks to backfill, then we can just return the current offset.\n      // If the user wants to start at the current offset in Kafka and _does_ want to create\n      // recovery tasks to backfill, then we create the recovery tasks needed and then return\n      // the current offset for the indexer. And if the user does _not_ want to start at the\n      // current offset in Kafka, then we'll just default to the old behavior of starting from\n      // the very beginning\n      if (!indexerConfig.getCreateRecoveryTasksOnStart()\n          && indexerConfig.getReadFromLocationOnStart()\n              == AstraConfigs.KafkaOffsetLocation.LATEST) {\n        LOG.info(\n            \"CreateRecoveryTasksOnStart is set to false and ReadLocationOnStart is set to current. Reading from current and\"\n                + \" NOT spinning up recovery tasks\");\n        return currentEndOffsetForPartition;\n      } else if (indexerConfig.getCreateRecoveryTasksOnStart()\n          && indexerConfig.getReadFromLocationOnStart()\n              == AstraConfigs.KafkaOffsetLocation.LATEST) {\n        // Todo - this appears to be able to create recovery tasks that have a start and end\n```\n\n### Requirements:\n1. Complete only the missing part of the method.\n2. Only answer the code of the completed part. Do not include any of the provided code snippet.\n3. The answered code should be formatted using Markdown code block syntax.\n4. Do not include any additional text.\n\n### Answer: (use the provided format with backticks)\n", "reference": "        // position of 0, which is invalid. This seems to occur when new clusters are initialized,\n        // and is  especially problematic when indexers are created but never get assigned (ie,\n        // deploy 5, only assign 3).\n        LOG.info(\n            \"CreateRecoveryTasksOnStart is set and ReadLocationOnStart is set to current. Reading from current and\"\n                + \" spinning up recovery tasks\");\n        createRecoveryTasks(\n            partitionId,\n            currentBeginningOffsetForPartition,\n            currentEndOffsetForPartition,\n            indexerConfig.getMaxMessagesPerChunk());\n        return currentEndOffsetForPartition;\n\n      } else {\n        return highestDurableOffsetForPartition;\n      }\n    }\n\n    // The current head offset shouldn't be lower than the highest durable offset. If it is it\n    // means that we indexed more data than the current head offset. This is either a bug in the\n    // offset handling mechanism or the kafka partition has rolled over. We throw an exception\n    // for now, so we can investigate.\n    if (currentEndOffsetForPartition < highestDurableOffsetForPartition) {\n      final String message =\n          String.format(\n              \"The current head for the partition %d can't \"\n                  + \"be lower than the highest durable offset for that partition %d\",\n              currentEndOffsetForPartition, highestDurableOffsetForPartition);\n      LOG.error(message);\n      throw new IllegalStateException(message);\n    }\n\n    // The head offset for Kafka partition is the offset of the next message to be indexed. We\n    // assume that offset is passed into this function. The highest durable offset is the partition\n    // offset of the message that is indexed. Hence, the offset is incremented by 1 to get the\n    // next message.\n    long nextOffsetForPartition = highestDurableOffsetForPartition + 1;\n\n    // Create a recovery task if needed.\n    if (currentEndOffsetForPartition - highestDurableOffsetForPartition > maxOffsetDelay) {\n      LOG.info(\n          \"Recovery task needed. The current position {} and head location {} are higher than max\"\n              + \" offset {}\",\n          highestDurableOffsetForPartition,\n          currentEndOffsetForPartition,\n          maxOffsetDelay);\n      createRecoveryTasks(\n          partitionId,\n          nextOffsetForPartition,\n          currentEndOffsetForPartition - 1,\n          maxMessagesPerRecoveryTask);\n      return currentEndOffsetForPartition;\n    } else {\n      LOG.info(\n          \"The difference between the last indexed position {} and head location {} is lower \"\n              + \"than max offset {}. So, using {} position as the start offset\",\n          highestDurableOffsetForPartition,\n          currentEndOffsetForPartition,\n          maxOffsetDelay,\n          nextOffsetForPartition);\n      return nextOffsetForPartition;\n    }\n  }", "completions": ["  \n```Java\n  public long determineStartingOffset(\n      long currentEndOffsetForPartition,\n      long currentBeginningOffsetForPartition,\n      AstraConfigs.IndexerConfig indexerConfig) {\n    // Filter stale snapshots for partition.\n    if (partitionId == null) {\n      LOG.warn(\"PartitionId can't be null.\");\n    }\n\n    List<SnapshotMetadata> snapshots = snapshotMetadataStore.listSync();\n    List<SnapshotMetadata> snapshotsForPartition =\n        snapshots.stream()\n           .filter(\n                snapshotMetadata -> {\n                  if (snapshotMetadata == null || snapshotMetadata.partitionId == null) {\n                    LOG.warn(\n                        \"snapshot metadata or partition id can't be null: {} \",\n                        Strings.join(snapshots, ','));\n                  }\n                  return snapshotMetadata!= null\n                      && snapshotMetadata.partitionId!= null\n                      && snapshotMetadata.partitionId.equals(partitionId);\n                })\n           .collect(Collectors.toUnmodifiableList());\n    List<SnapshotMetadata> deletedSnapshots = deleteStaleLiveSnapshots(snapshotsForPartition);\n\n    List<SnapshotMetadata> nonLiveSnapshotsForPartition =\n        snapshotsForPartition.stream()\n           .filter(s ->!deletedSnapshots.contains(s))\n           .collect(Collectors.toUnmodifiableList());\n\n    // Get the highest offset that is indexed in durable store.\n    List<RecoveryTaskMetadata> recoveryTasks = recoveryTaskMetadataStore.listSync();\n    long highestDurableOffsetForPartition =\n        getHighestDurableOffsetForPartition(\n            nonLiveSnapshotsForPartition, recoveryTasks, partitionId);\n    LOG.debug(\n        \"The highest durable offset for partition {} is {}\",\n        partitionId,\n        highestDurableOffsetForPartition);\n\n    if (highestDurableOffsetForPartition <= 0) {\n      LOG.info(\"There is no prior offset for this partition {}.\", partitionId);\n\n      // If the user wants to start at the current offset in Kafka and _does not_ want to create\n      // recovery tasks to backfill, then we can just return the current offset.\n      // If the user wants to start at the current offset in Kafka and _does_ want to create\n      // recovery tasks to backfill, then we create the recovery tasks needed and then return\n      // the current offset for the indexer. And if the user does _not_ want to start at the\n      // current offset in Kafka, then we'll just default to the old behavior of starting from\n      // the very beginning\n      if (!indexerConfig.getCreateRecoveryTasksOnStart()\n          && indexerConfig.getReadFromLocationOnStart()\n              == AstraConfigs.KafkaOffsetLocation.LATEST) {\n        LOG.info(\n            \"CreateRecoveryTasksOnStart is set to false and ReadLocationOnStart is set to current. Reading from current and\"\n                + \" NOT spinning up recovery tasks\");\n        return currentEndOffsetForPartition;\n      } else if (indexerConfig.getCreateRecoveryTasksOnStart()\n          && indexerConfig.getReadFromLocationOnStart()\n              == AstraConfigs.KafkaOffsetLocation.LATEST) {\n        // Todo - this appears to be able to create recovery tasks that have a start and end\n        // offset of the same value, which is not allowed.\n        // Todo - this also appears to be able to create recovery tasks that have a start offset\n        // that is greater than the end offset, which is also not allowed.\n        // Todo - this also appears to be able to create recovery tasks that have a start offset\n        // that is less than the current offset, which is also not allowed.\n        // Todo - this also appears to be able to create recovery tasks that have a start offset\n        // that is greater than the current offset, which is also not allowed.\n        // Todo - this also appears to be able to create recovery tasks that have a start offset\n        // that is less than the current offset, which is also not allowed.\n        // Todo - this also appears to be able to create recovery tasks that have a start offset\n        // that"]}
