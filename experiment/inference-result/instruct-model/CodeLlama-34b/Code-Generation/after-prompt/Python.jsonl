{"id": 0, "name": "extract_best_config", "prompt": "### Role:\nYou are an expert Python programmer. You will be given some information and will generate a correct Python function that matches this information.\n\n### Task:\nBased on the provided information, generate a complete Python function. Ensure the code is logically correct, syntactically accurate, and follows best coding practices.\n\n### Function name:\nextract_best_config\n\n### Doc string and function signature:\n```Python\ndef extract_best_config(trial_path: str, output_path: Optional[str] = None) -> Dict :\n    \"\"\"\n    Description: This function extracts the best configuration from a trial path.\n\n    Args:\n        trial_path (str): The path to the trial directory.\n        output_path (Optional[str]): The path to the output file. If not provided, the function will not write the output to a file.\n\n    Returns:\n        Dict: The best configuration in a dictionary format. If output_path is provided, the function will also write the configuration to the specified file in YAML format.\n    \"\"\"\n```\n\n### Requirements:\n1. Generate the complete function implementation.\n2. Only answer the code of the generated function, formatted using Markdown code block syntax.\n3. Do not include any additional text.\n\n### Answer: (use the provided format with backticks)\n", "reference": "def extract_best_config(trial_path: str, output_path: Optional[str] = None) -> Dict:\n    summary_path = os.path.join(trial_path, 'summary.csv')\n    if not os.path.exists(summary_path):\n        raise ValueError(f\"summary.csv does not exist in {trial_path}.\")\n    trial_summary_df = load_summary_file(summary_path, dict_columns=['best_module_params'])\n    config_yaml_path = os.path.join(trial_path, 'config.yaml')\n    with open(config_yaml_path, 'r') as f:\n        config_dict = yaml.safe_load(f)\n    yaml_dict = summary_df_to_yaml(trial_summary_df, config_dict)\n    if output_path is not None:\n        with open(output_path, 'w') as f:\n            yaml.dump(yaml_dict, f)\n    return yaml_dict", "completions": ["  \n```\ndef extract_best_config(trial_path: str, output_path: Optional[str] = None) -> Dict:\n    \"\"\"\n    Description: This function extracts the best configuration from a trial path.\n\n    Args:\n        trial_path (str): The path to the trial directory.\n        output_path (Optional[str]): The path to the output file. If not provided, the function will not write the output to a file.\n\n    Returns:\n        Dict: The best configuration in a dictionary format. If output_path is provided, the function will also write the configuration to the specified file in YAML format.\n    \"\"\"\n    # Implement your function here\n    pass\n```"]}
{"id": 1, "name": "s4d_kernel", "prompt": "### Role:\nYou are an expert Python programmer. You will be given some information and will generate a correct Python function that matches this information.\n\n### Task:\nBased on the provided information, generate a complete Python function. Ensure the code is logically correct, syntactically accurate, and follows best coding practices.\n\n### Function name:\ns4d_kernel\n\n### Doc string and function signature:\n```Python\ndef s4d_kernel(\n    A: torch.Tensor, B: torch.Tensor, C: torch.Tensor, dt: float, L: int\n) -> torch.Tensor :\n    \"\"\"\n    Description: This function performs a specific computation using the Vandermonde matrix and a convolution kernel.\n\n    Args:\n        A (torch.Tensor): The first input tensor.\n        B (torch.Tensor): The second input tensor.\n        C (torch.Tensor): The third input tensor.\n        dt (float): The time step.\n        L (int): The sequence length.\n\n    Returns:\n        torch.Tensor: The result of the computation.\n\n    Raises:\n        ValueError: If the last dimension of tensors A, B, and C do not match or have incompatible batch dimensions.\n        TypeError: If dt is not a float or L is not an integer.\n\n    Notes:\n        The function uses torch.sum for summing along a specified dimension of the input tensor.\n        The function uses torch.exp for computing the exponential of the input tensor element-wise.\n    \"\"\"\n```\n\n### Requirements:\n1. Generate the complete function implementation.\n2. Only answer the code of the generated function, formatted using Markdown code block syntax.\n3. Do not include any additional text.\n\n### Answer: (use the provided format with backticks)\n", "reference": "def s4d_kernel(\n    A: torch.Tensor, B: torch.Tensor, C: torch.Tensor, dt: float, L: int\n) -> torch.Tensor:\n\n    # Ensure A, B, and C have the same size in the last dimension and compatible batch dimensions\n    if (\n        A.size(-1) != B.size(-1)\n        or A.size(-1) != C.size(-1)\n        or A.shape[:-1] != B.shape[:-1]\n        or A.shape[:-1] != C.shape[:-1]\n    ):\n        raise ValueError(\n            \"The last dimension of tensors A, B, and C must match and have\"\n            \" compatible batch dimensions.\"\n        )\n\n    # Check that dt is a float and L is an integer\n    if not isinstance(dt, float):\n        raise TypeError(\"The time step dt must be a float.\")\n    if not isinstance(L, int):\n        raise TypeError(\"The sequence length L must be an integer.\")\n\n    # Create a range of values from 0 to L-1 and reshape for broadcasting\n    arange_L = torch.arange(L, dtype=A.dtype, device=A.device).view(L, 1)\n\n    # Expand A and B for broadcasting with the sequence length\n    A_expanded = A.unsqueeze(1)  # Shape: (batch_size, 1, dim)\n    B_expanded = B.unsqueeze(1)  # Shape: (batch_size, 1, dim)\n\n    # Perform the convolution kernel operation with proper broadcasting\n    vandermonde = torch.exp(\n        arange_L * dt * A_expanded\n    )  # Shape: (seqlen, batch_size, dim)\n    result = torch.sum(\n        vandermonde\n        * B_expanded\n        * (torch.exp(dt * A_expanded) - 1)\n        / A_expanded,\n        dim=0,\n    )\n    result = C.unsqueeze(1) * result  # Shape: (batch_size, seqlen, dim)\n\n    return result", "completions": ["  \n```\ndef s4d_kernel(A: torch.Tensor, B: torch.Tensor, C: torch.Tensor, dt: float, L: int) -> torch.Tensor:\n    \"\"\"\n    Description: This function performs a specific computation using the Vandermonde matrix and a convolution kernel.\n\n    Args:\n        A (torch.Tensor): The first input tensor.\n        B (torch.Tensor): The second input tensor.\n        C (torch.Tensor): The third input tensor.\n        dt (float): The time step.\n        L (int): The sequence length.\n\n    Returns:\n        torch.Tensor: The result of the computation.\n\n    Raises:\n        ValueError: If the last dimension of tensors A, B, and C do not match or have incompatible batch dimensions.\n        TypeError: If dt is not a float or L is not an integer.\n\n    Notes:\n        The function uses torch.sum for summing along a specified dimension of the input tensor.\n        The function uses torch.exp for computing the exponential of the input tensor element-wise.\n    \"\"\"\n    # Check input dimensions\n    if A.shape[-1]!= B.shape[-1] or A.shape[-1]!= C.shape[-1]:\n        raise ValueError(\"Incompatible input dimensions\")\n    if A.shape[0]!= B.shape[0] or A.shape[0]!= C.shape[0]:\n        raise ValueError(\"Incompatible batch dimensions\")\n\n    # Compute Vandermonde matrix\n    V = torch.zeros((L, L))\n    for i in range(L):\n        V[i, :] = torch.exp(dt * i * torch.arange(L))\n\n    # Compute convolution kernel\n    K = torch.matmul(V, V.T)\n\n    # Compute result\n    result = torch.sum(A * K * B * C, dim=-1)\n\n    return result\n```"]}
{"id": 2, "name": "_calculate_intersection_score", "prompt": "### Role:\nYou are an expert Python programmer. You will be given some information and will generate a correct Python function that matches this information.\n\n### Task:\nBased on the provided information, generate a complete Python function. Ensure the code is logically correct, syntactically accurate, and follows best coding practices.\n\n### Function name:\n_calculate_intersection_score\n\n### Doc string and function signature:\n```Python\ndef _calculate_intersection_score(\n    elem1: dict[str, Any], elem2: dict[str, Any]\n) -> float :\n    \"\"\"\n    Description: This function calculates the intersection score between two elements based on their segmentation masks.\n\n    Args:\n        elem1 (dict[str, Any]): The first element, which is a dictionary containing a 'segmentation' key with a numpy array as its value and an 'area' key with a numeric value.\n        elem2 (dict[str, Any]): The second element, which is a dictionary containing a 'segmentation' key with a numpy array as its value and an 'area' key with a numeric value.\n\n    Returns:\n        float: The intersection score between the two elements' segmentation masks. The score is calculated as the ratio of the sum of the logical AND of their segmentations to the sum of the segmentation of the smaller element.\n\n    Raises:\n        ValueError: If the masks of the two elements do not have the same dimensions.\n    \"\"\"\n```\n\n### Requirements:\n1. Generate the complete function implementation.\n2. Only answer the code of the generated function, formatted using Markdown code block syntax.\n3. Do not include any additional text.\n\n### Answer: (use the provided format with backticks)\n", "reference": "def _calculate_intersection_score(\n    elem1: dict[str, Any], elem2: dict[str, Any]\n) -> float:\n\n  # Check if the masks have the same dimensions.\n  if elem1['segmentation'].shape != elem2['segmentation'].shape:\n    raise ValueError('The masks must have the same dimensions.')\n\n  min_elem = elem1 if elem1['area'] < elem2['area'] else elem2\n  intersection = np.logical_and(elem1['segmentation'], elem2['segmentation'])\n  score = np.sum(intersection) / np.sum(min_elem['segmentation'])\n  return score", "completions": ["  \n```\ndef _calculate_intersection_score(elem1: dict[str, Any], elem2: dict[str, Any]) -> float:\n    \"\"\"\n    Description: This function calculates the intersection score between two elements based on their segmentation masks.\n\n    Args:\n        elem1 (dict[str, Any]): The first element, which is a dictionary containing a'segmentation' key with a numpy array as its value and an 'area' key with a numeric value.\n        elem2 (dict[str, Any]): The second element, which is a dictionary containing a'segmentation' key with a numpy array as its value and an 'area' key with a numeric value.\n\n    Returns:\n        float: The intersection score between the two elements' segmentation masks. The score is calculated as the ratio of the sum of the logical AND of their segmentations to the sum of the segmentation of the smaller element.\n\n    Raises:\n        ValueError: If the masks of the two elements do not have the same dimensions.\n    \"\"\"\n    # Check if the masks have the same dimensions\n    if elem1['segmentation'].shape!= elem2['segmentation'].shape:\n        raise ValueError('The masks of the two elements do not have the same dimensions.')\n\n    # Calculate the intersection score\n    intersection = np.logical_and(elem1['segmentation'], elem2['segmentation'])\n    intersection_score = intersection.sum() / min(elem1['area'], elem2['area'])\n\n    return intersection_score\n```"]}
{"id": 3, "name": "t_map", "prompt": "### Role:\nYou are an expert Python programmer. You will be given some information and will generate a correct Python function that matches this information.\n\n### Task:\nBased on the provided information, generate a complete Python function. Ensure the code is logically correct, syntactically accurate, and follows best coding practices.\n\n### Function name:\nt_map\n\n### Doc string and function signature:\n```Python\ndef t_map(self, t: Union[float, torch.Tensor]) -> torch.Tensor :\n    \"\"\"\n    Description: This function maps a given time value to a new value within a specified range.\n\n    Args:\n        t (Union[float, torch.Tensor]): The time value to be mapped. It can be either a float or a torch.Tensor.\n\n    Returns:\n        torch.Tensor: The mapped time value within the range [t_min, t_max].\n    \"\"\"\n```\n\n### Requirements:\n1. Generate the complete function implementation.\n2. Only answer the code of the generated function, formatted using Markdown code block syntax.\n3. Do not include any additional text.\n\n### Answer: (use the provided format with backticks)\n", "reference": "    def t_map(self, t: Union[float, torch.Tensor]) -> torch.Tensor:\n        if not isinstance(t, torch.Tensor):\n            t = torch.Tensor([t]).float()\n\n        t_max = self.t_max.to(t.device)\n        t_min = self.t_min.to(t.device)\n        t_tilde = t_min + (t_max - t_min) * t\n\n        return t_tilde", "completions": ["  \n```Python\ndef t_map(self, t: Union[float, torch.Tensor]) -> torch.Tensor :\n    \"\"\"\n    Description: This function maps a given time value to a new value within a specified range.\n\n    Args:\n        t (Union[float, torch.Tensor]): The time value to be mapped. It can be either a float or a torch.Tensor.\n\n    Returns:\n        torch.Tensor: The mapped time value within the range [t_min, t_max].\n    \"\"\"\n    t_min = 0.0\n    t_max = 1.0\n    if isinstance(t, float):\n        return torch.tensor(t).clamp(t_min, t_max)\n    else:\n        return t.clamp(t_min, t_max)\n```"]}
{"id": 4, "name": "sample_treatment_and_effect", "prompt": "### Role:\nYou are an expert Python programmer. You will be given some information and will generate a correct Python function that matches this information.\n\n### Task:\nBased on the provided information, generate a complete Python function. Ensure the code is logically correct, syntactically accurate, and follows best coding practices.\n\n### Function name:\nsample_treatment_and_effect\n\n### Doc string and function signature:\n```Python\ndef sample_treatment_and_effect(\n    graph: torch.Tensor, node_names: Sequence[str], ensure_effect: bool = True, num_effects: int = 1\n) -> tuple[str, list[str]] :\n    \"\"\"\n    Description: This function is for generating a treatment and effect nodes in a graph.\n\n    Args:\n        graph (torch.Tensor): The graph represented as a tensor.\n        node_names (Sequence[str]): The names of the nodes in the graph.\n        ensure_effect (bool): A flag to ensure that the effect node is a descendant of the treatment node. Defaults to True.\n        num_effects (int): The number of effect nodes to generate. Defaults to 1.\n\n    Returns:\n        tuple[str, list[str]]: A tuple containing the name of the treatment node and a list of effect node names.\n    \"\"\"\n```\n\n### Requirements:\n1. Generate the complete function implementation.\n2. Only answer the code of the generated function, formatted using Markdown code block syntax.\n3. Do not include any additional text.\n\n### Answer: (use the provided format with backticks)\n", "reference": "def sample_treatment_and_effect(\n    graph: torch.Tensor, node_names: Sequence[str], ensure_effect: bool = True, num_effects: int = 1\n) -> tuple[str, list[str]]:\n    if ensure_effect:\n        assert num_effects == 1, \"Can only ensure effect for one effect node.\"\n\n        nx_graph = nx.from_numpy_array(graph.numpy(), create_using=nx.DiGraph)\n        nx_graph = nx.relabel_nodes(nx_graph, dict(enumerate(node_names)))\n\n        topological_order = nx.topological_sort(nx_graph)\n        topological_order_with_descendants = [n for n in topological_order if nx.descendants(nx_graph, n)]\n\n        if topological_order_with_descendants:\n            treatment = np.random.choice(topological_order_with_descendants, size=1, replace=False).item()\n            # randomly sample a effect not that is a descendant of the treatment\n            descendants = nx.descendants(nx_graph, treatment)\n            effects = np.random.choice(list(descendants), size=num_effects, replace=False)\n            return treatment, effects.tolist()\n        else:\n            warnings.warn(\"No edges found. Defaulting to random sampling.\")\n\n    samples = np.random.choice(node_names, size=1 + num_effects, replace=False)\n    treatment = samples[0]\n    effects = samples[1:]\n\n    return treatment, effects.tolist()", "completions": ["  \n```\ndef sample_treatment_and_effect(\n    graph: torch.Tensor, node_names: Sequence[str], ensure_effect: bool = True, num_effects: int = 1\n) -> tuple[str, list[str]] :\n    \"\"\"\n    Description: This function is for generating a treatment and effect nodes in a graph.\n\n    Args:\n        graph (torch.Tensor): The graph represented as a tensor.\n        node_names (Sequence[str]): The names of the nodes in the graph.\n        ensure_effect (bool): A flag to ensure that the effect node is a descendant of the treatment node. Defaults to True.\n        num_effects (int): The number of effect nodes to generate. Defaults to 1.\n\n    Returns:\n        tuple[str, list[str]]: A tuple containing the name of the treatment node and a list of effect node names.\n    \"\"\"\n    # Generate a random treatment node\n    treatment_node = random.choice(node_names)\n\n    # Generate a list of effect nodes\n    effect_nodes = []\n    for i in range(num_effects):\n        # Generate a random effect node that is a descendant of the treatment node\n        effect_node = random.choice(graph[treatment_node])\n        while ensure_effect and effect_node not in graph[treatment_node]:\n            effect_node = random.choice(graph[treatment_node])\n        effect_nodes.append(effect_node)\n\n    return treatment_node, effect_nodes\n```"]}
{"id": 5, "name": "generate", "prompt": "### Role:\nYou are an expert Python programmer. You will be given some information and will generate a correct Python function that matches this information.\n\n### Task:\nBased on the provided information, generate a complete Python function. Ensure the code is logically correct, syntactically accurate, and follows best coding practices.\n\n### Function name:\ngenerate\n\n### Doc string and function signature:\n```Python\ndef generate(self, model, system_message, prompt, **kwargs) :\n    \"\"\"\n    Description: This function generates a response using the OpenAI API.\n\n    Args:\n        model (type): The model to use for generation.\n        system_message (type): The system message to use for the generation.\n        prompt (type): The prompt to use for the generation.\n        **kwargs (type): Additional generation parameters.\n\n    Returns:\n        type: The generated response.\n    \"\"\"\n```\n\n### Requirements:\n1. Generate the complete function implementation.\n2. Only answer the code of the generated function, formatted using Markdown code block syntax.\n3. Do not include any additional text.\n\n### Answer: (use the provided format with backticks)\n", "reference": "    def generate(self, model, system_message, prompt, **kwargs):\n\n        self.check_api_key()\n\n        temperature = kwargs.get(\"temperature\", 0.1)\n        top_p = kwargs.get(\"top_p\", 1)\n        frequency_penalty = kwargs.get(\"frequency_penalty\", 0)\n        presence_penalty = kwargs.get(\"presence_penalty\", 0)\n        max_new_tokens = kwargs.get(\"max_new_tokens\")\n        # check if there are any generation parameters that are not supported\n        unsupported_params = [param for param in kwargs.keys() if param not in LLM_GENERATION_PARAMETERS]\n        if len(unsupported_params) > 0:\n            # log warning\n            logging.warning(f\"Unused generation parameters sent as input: {unsupported_params}.\"\\\n                             f\"For OpenAI, only the following parameters are supported: {LLM_GENERATION_PARAMETERS}\")\n        params = {\n            \"model\": model.model_name,\n            \"temperature\": temperature,\n            \"max_tokens\": max_new_tokens,\n            \"top_p\": top_p,\n            \"frequency_penalty\": frequency_penalty,\n            \"presence_penalty\": presence_penalty,\n        }\n        if model.parsing_helper_tokens[\"start_token\"]:\n            prompt += model.parsing_helper_tokens[\"start_token\"]\n        messages = [\n            {\n                \"role\": \"system\",\n                \"content\": system_message\n            },\n            {\n                \"role\": \"user\",\n                \"content\": prompt\n            }\n        ]\n        params[\"messages\"] = messages\n\n        counter = 0\n        choice = None\n        # initiate response so exception logic doesnt error out when checking for error in response\n        response = {}\n        while counter <= 5:\n            try:\n                openai_headers = {\n                    \"Authorization\": f\"Bearer {self.api_key}\",\n                    \"Content-Type\": \"application/json\",\n                }\n                response = requests.post(\n                    OPENAI_URL, headers=openai_headers, json=params, timeout=50\n                )\n                response = response.json()\n                choice = response[\"choices\"][0][\"message\"][\"content\"].strip(\"'\")\n                break\n            except Exception as e:\n                if (\"error\" in response and\n                        \"code\" in response[\"error\"] and\n                        response[\"error\"][\"code\"] == 'invalid_api_key'):\n                    raise Exception(f\"The supplied OpenAI API key {self.api_key} is invalid\")\n                if counter == 5:\n                    raise Exception(f\"OpenAI API failed to generate a response: {e}\")\n                counter += 1\n                time.sleep(2 ** counter)\n                continue\n\n        if not choice:\n            raise Exception(\"OpenAI API failed to generate a response\")\n        \n        if model.parsing_helper_tokens[\"end_token\"]:\n            # remove the end token from the choice\n            choice = choice.split(model.parsing_helper_tokens[\"end_token\"])[0]\n            # check if starting token is in choice\n            if model.parsing_helper_tokens[\"start_token\"] in choice:\n                # remove the starting token from the choice\n                choice = choice.split(model.parsing_helper_tokens[\"start_token\"])[-1]\n        return choice", "completions": ["  \n```\ndef generate(self, model, system_message, prompt, **kwargs) :\n    \"\"\"\n    Description: This function generates a response using the OpenAI API.\n\n    Args:\n        model (type): The model to use for generation.\n        system_message (type): The system message to use for the generation.\n        prompt (type): The prompt to use for the generation.\n        **kwargs (type): Additional generation parameters.\n\n    Returns:\n        type: The generated response.\n    \"\"\"\n    # Implement the function here\n    pass\n```"]}
{"id": 6, "name": "query_tags_from_docker_hub", "prompt": "### Role:\nYou are an expert Python programmer. You will be given some information and will generate a correct Python function that matches this information.\n\n### Task:\nBased on the provided information, generate a complete Python function. Ensure the code is logically correct, syntactically accurate, and follows best coding practices.\n\n### Function name:\nquery_tags_from_docker_hub\n\n### Doc string and function signature:\n```Python\ndef query_tags_from_docker_hub(\n    filter_func: Callable[[str], bool],\n    namespace: str,\n    repository: str,\n    docker_hub_token: str,\n    num_tags: Optional[int] = None,\n) -> List[str] :\n    \"\"\"\n    Description: This function queries tags from Docker Hub based on a filter function, namespace, repository, Docker Hub token, and the number of tags to return.\n\n    Args:\n        filter_func (Callable[[str], bool]): A function that takes a string (tag name) and returns a boolean indicating whether the tag should be included.\n        namespace (str): The namespace of the Docker Hub repository.\n        repository (str): The name of the Docker Hub repository.\n        docker_hub_token (str): The Docker Hub API token for authentication.\n        num_tags (Optional[int]): The maximum number of tags to return. If None, all tags are returned.\n\n    Returns:\n        List[str]: A list of fully qualified Docker image tags (in the format 'namespace/repository:tag') that pass the filter function and are sorted in ascending order.\n    \"\"\"\n```\n\n### Requirements:\n1. Generate the complete function implementation.\n2. Only answer the code of the generated function, formatted using Markdown code block syntax.\n3. Do not include any additional text.\n\n### Answer: (use the provided format with backticks)\n", "reference": "def query_tags_from_docker_hub(\n    filter_func: Callable[[str], bool],\n    namespace: str,\n    repository: str,\n    docker_hub_token: str,\n    num_tags: Optional[int] = None,\n) -> List[str]:\n    filtered_tags = []\n    headers = {\n        \"Authorization\": f\"Bearer {docker_hub_token}\",\n    }\n\n    page_count = 1\n    while page_count:\n        logger.info(f\"Querying page {page_count}\")\n        url = f\"https://hub.docker.com/v2/namespaces/{namespace}/repositories/{repository}/tags?page={page_count}&page_size=100\"  # noqa E501\n\n        response = requests.get(url, headers=headers)\n        response_json = response.json()\n\n        # Stop querying if Docker Hub API returns an error\n        if response.status_code != 200:\n            logger.info(f\"Failed to query tags from Docker Hub: Error: {response_json}\")\n            return sorted([f\"{namespace}/{repository}:{t}\" for t in filtered_tags])\n\n        result = response_json[\"results\"]\n        tags = [tag[\"name\"] for tag in result]\n        filtered_tags_page = list(filter(filter_func, tags))  # Filter tags\n\n        # Add enough tags to not exceed num_tags if num_tags is specified\n        if num_tags:\n            if len(filtered_tags) + len(filtered_tags_page) > num_tags:\n                filtered_tags.extend(\n                    filtered_tags_page[: num_tags - len(filtered_tags)]\n                )\n                break\n        filtered_tags.extend(filtered_tags_page)\n\n        logger.info(f\"Tag count: {len(filtered_tags)}\")\n        if not response_json[\"next\"]:\n            break\n        page_count += 1\n    return sorted([f\"{namespace}/{repository}:{t}\" for t in filtered_tags])", "completions": ["  \n```\ndef query_tags_from_docker_hub(\n    filter_func: Callable[[str], bool],\n    namespace: str,\n    repository: str,\n    docker_hub_token: str,\n    num_tags: Optional[int] = None,\n) -> List[str] :\n    \"\"\"\n    Description: This function queries tags from Docker Hub based on a filter function, namespace, repository, Docker Hub token, and the number of tags to return.\n\n    Args:\n        filter_func (Callable[[str], bool]): A function that takes a string (tag name) and returns a boolean indicating whether the tag should be included.\n        namespace (str): The namespace of the Docker Hub repository.\n        repository (str): The name of the Docker Hub repository.\n        docker_hub_token (str): The Docker Hub API token for authentication.\n        num_tags (Optional[int]): The maximum number of tags to return. If None, all tags are returned.\n\n    Returns:\n        List[str]: A list of fully qualified Docker image tags (in the format 'namespace/repository:tag') that pass the filter function and are sorted in ascending order.\n    \"\"\"\n    import requests\n    import json\n\n    # Build the API endpoint URL\n    url = f\"https://hub.docker.com/v2/repositories/{namespace}/{repository}/tags\"\n\n    # Set up the API request headers\n    headers = {\n        \"Authorization\": f\"Bearer {docker_hub_token}\",\n        \"Accept\": \"application/json\",\n    }\n\n    # Make the API request\n    response = requests.get(url, headers=headers)\n\n    # Check the response status code\n    if response.status_code == 200:\n        # Parse the JSON response data\n        data = json.loads(response.content)\n\n        # Extract the list of tags from the response data\n        tags = [tag[\"name\"] for tag in data[\"results\"]]\n\n        # Filter the tags using the provided filter function\n        filtered_tags = list(filter(filter_func, tags))\n\n        # Sort the filtered tags in ascending order\n        sorted_tags = sorted(filtered_tags)\n\n        # Return the sorted tags\n        return sorted_tags\n    else:\n        # Handle the error response\n        raise Exception(f\"Error querying Docker Hub API: {response.status_code} {response.reason}\")\n```"]}
{"id": 7, "name": "select", "prompt": "### Role:\nYou are an expert Python programmer. You will be given some information and will generate a correct Python function that matches this information.\n\n### Task:\nBased on the provided information, generate a complete Python function. Ensure the code is logically correct, syntactically accurate, and follows best coding practices.\n\n### Function name:\nselect\n\n### Doc string and function signature:\n```Python\ndef select(\n        self,\n        pairs: List[Tuple[str, int]]\n    ) -> Dict[str, List[model_pb2.Model]] :\n    \"\"\"\n    Description: This function selects a specified number of models for each learner ID from a cache.\n\n    Args:\n        pairs (List[Tuple[str, int]]): A list of tuples, where each tuple contains a learner ID and the number of models to select.\n\n    Raises:\n        ValueError: If a learner ID is not found in the cache.\n\n    Returns:\n        Dict[str, List[model_pb2.Model]]: A dictionary where the keys are learner IDs and the values are lists of selected models.\n\n    Notes:\n        - If the number of models requested is greater than the number of models available, a warning is logged and all available models are returned.\n        - If the number of models requested is less than or equal to 0, all available models are returned.\n    \"\"\"\n```\n\n### Requirements:\n1. Generate the complete function implementation.\n2. Only answer the code of the generated function, formatted using Markdown code block syntax.\n3. Do not include any additional text.\n\n### Answer: (use the provided format with backticks)\n", "reference": "    def select(\n        self,\n        pairs: List[Tuple[str, int]]\n    ) -> Dict[str, List[model_pb2.Model]]:\n\n        result = {}\n\n        for pair in pairs:\n            learner_id, num_models = pair\n\n            if learner_id not in self.store_cache.keys():\n                raise ValueError(\n                    f\"No models found for learner id {learner_id}\")\n\n            history_length = len(self.store_cache[learner_id])\n\n            if num_models > history_length:\n                # TODO: check if continue is the right thing to do here. How about we return all the models we have?\n                logger.warn(\n                    f\"Number of models requested ({num_models}) is greater than \"\n                    f\"the number of models available ({history_length}) for \"\n                    f\"learner id {learner_id}\")\n                continue\n\n            if num_models <= 0:\n                num_models = history_length\n\n            result[learner_id] = self.store_cache[learner_id][-num_models:]\n\n        return result", "completions": ["  \n```\ndef select(\n        self,\n        pairs: List[Tuple[str, int]]\n    ) -> Dict[str, List[model_pb2.Model]] :\n    \"\"\"\n    Description: This function selects a specified number of models for each learner ID from a cache.\n\n    Args:\n        pairs (List[Tuple[str, int]]): A list of tuples, where each tuple contains a learner ID and the number of models to select.\n\n    Raises:\n        ValueError: If a learner ID is not found in the cache.\n\n    Returns:\n        Dict[str, List[model_pb2.Model]]: A dictionary where the keys are learner IDs and the values are lists of selected models.\n\n    Notes:\n        - If the number of models requested is greater than the number of models available, a warning is logged and all available models are returned.\n        - If the number of models requested is less than or equal to 0, all available models are returned.\n    \"\"\"\n    selected_models = {}\n    for learner_id, num_models in pairs:\n        if learner_id not in self.cache:\n            raise ValueError(f\"Learner ID {learner_id} not found in cache.\")\n        models = self.cache[learner_id]\n        if num_models > len(models):\n            warnings.warn(f\"Number of models requested ({num_models}) is greater than the number of models available ({len(models)}).\")\n            num_models = len(models)\n        selected_models[learner_id] = models[:num_models]\n    return selected_models\n```"]}
{"id": 8, "name": "get_boto3_session", "prompt": "### Role:\nYou are an expert Python programmer. You will be given some information and will generate a correct Python function that matches this information.\n\n### Task:\nBased on the provided information, generate a complete Python function. Ensure the code is logically correct, syntactically accurate, and follows best coding practices.\n\n### Function name:\nget_boto3_session\n\n### Doc string and function signature:\n```Python\ndef get_boto3_session(self) -> Session :\n    \"\"\"\n    Description: This function is used to get a boto3 session using AWS STS (Security Token Service).\n\n    Args:\n        self: The instance of the class that this method belongs to.\n\n    Returns:\n        Session: A boto3 session object with temporary AWS credentials.\n    \"\"\"\n```\n\n### Requirements:\n1. Generate the complete function implementation.\n2. Only answer the code of the generated function, formatted using Markdown code block syntax.\n3. Do not include any additional text.\n\n### Answer: (use the provided format with backticks)\n", "reference": "    def get_boto3_session(self) -> Session:\n        credentials = self.sts_client_manager.get_credentials(\n            self.configurations.aws_assume_role_arn,\n            self.configurations.aws_assume_role_session_name,\n        )\n        tmp_access_key = credentials[\"AccessKeyId\"]\n        tmp_secret_key = credentials[\"SecretAccessKey\"]\n        security_token = credentials[\"SessionToken\"]\n        return Session(\n            aws_access_key_id=tmp_access_key,\n            aws_secret_access_key=tmp_secret_key,\n            aws_session_token=security_token,\n        )", "completions": ["  \n```\ndef get_boto3_session(self) -> Session:\n    \"\"\"\n    This function is used to get a boto3 session using AWS STS (Security Token Service).\n\n    Args:\n        self: The instance of the class that this method belongs to.\n\n    Returns:\n        Session: A boto3 session object with temporary AWS credentials.\n    \"\"\"\n    # Import the necessary libraries\n    import boto3\n    from botocore.exceptions import ClientError\n\n    # Create an STS client\n    sts = boto3.client('sts')\n\n    # Call the assume_role method of the STS client to assume a role\n    try:\n        assumed_role_object = sts.assume_role(\n            RoleArn='string',\n            RoleSessionName='string',\n            DurationSeconds=123,\n            ExternalId='string',\n            SerialNumber='string',\n            TokenCode='string',\n            SourceIdentity='string'\n        )\n    except ClientError as e:\n        # If the assume_role call fails, log the error and raise an exception\n        print(e.response['Error']['Message'])\n        raise\n    else:\n        # If the assume_role call succeeds, extract the temporary credentials from the response\n        credentials = assumed_role_object['Credentials']\n        access_key = credentials['AccessKeyId']\n        secret_key = credentials['SecretAccessKey']\n        session_token = credentials['SessionToken']\n\n        # Create a boto3 session using the temporary credentials\n        session = boto3.Session(\n            aws_access_key_id=access_key,\n            aws_secret_access_key=secret_key,\n            aws_session_token=session_token\n        )\n\n        # Return the boto3 session\n        return session\n```"]}
{"id": 9, "name": "generate_clebsch_gordan_rsh", "prompt": "### Role:\nYou are an expert Python programmer. You will be given some information and will generate a correct Python function that matches this information.\n\n### Task:\nBased on the provided information, generate a complete Python function. Ensure the code is logically correct, syntactically accurate, and follows best coding practices.\n\n### Function name:\ngenerate_clebsch_gordan_rsh\n\n### Doc string and function signature:\n```Python\ndef generate_clebsch_gordan_rsh(lmax: int, parity_invariance: bool = True) -> torch.Tensor :\n    \"\"\"\n    Description: This function generates a Clebsch-Gordan coefficients tensor in real spherical harmonics (RSH) representation.\n\n    Args:\n        lmax (int): The maximum degree of the spherical harmonics.\n        parity_invariance (bool, optional): If True, the function applies parity invariance to the Clebsch-Gordan coefficients. Defaults to True.\n\n    Returns:\n        torch.Tensor: The Clebsch-Gordan coefficients tensor in real spherical harmonics representation.\n    \"\"\"\n```\n\n### Requirements:\n1. Generate the complete function implementation.\n2. Only answer the code of the generated function, formatted using Markdown code block syntax.\n3. Do not include any additional text.\n\n### Answer: (use the provided format with backticks)\n", "reference": "def generate_clebsch_gordan_rsh(lmax: int, parity_invariance: bool = True) -> torch.Tensor:\n    lidx, _ = sh_indices(lmax)\n    cg = generate_clebsch_gordan(lmax).to(dtype=torch.complex64)\n    complex_to_real = generate_sh_to_rsh(lmax)  # (real, complex)\n    cg_rsh = torch.einsum(\n        \"ijk,mi,nj,ok->mno\",\n        cg,\n        complex_to_real,\n        complex_to_real,\n        complex_to_real.conj(),\n    )\n\n    if parity_invariance:\n        parity = (-1.0) ** lidx\n        pmask = parity[:, None, None] * parity[None, :, None] == parity[None, None, :]\n        cg_rsh *= pmask\n    else:\n        lsum = lidx[:, None, None] + lidx[None, :, None] - lidx[None, None, :]\n        cg_rsh *= 1.0j**lsum\n\n    # cast to real\n    cg_rsh = cg_rsh.real.to(matgl.float_th)\n    return cg_rsh", "completions": ["  \n```\ndef generate_clebsch_gordan_rsh(lmax: int, parity_invariance: bool = True) -> torch.Tensor :\n    \"\"\"\n    Description: This function generates a Clebsch-Gordan coefficients tensor in real spherical harmonics (RSH) representation.\n\n    Args:\n        lmax (int): The maximum degree of the spherical harmonics.\n        parity_invariance (bool, optional): If True, the function applies parity invariance to the Clebsch-Gordan coefficients. Defaults to True.\n\n    Returns:\n        torch.Tensor: The Clebsch-Gordan coefficients tensor in real spherical harmonics representation.\n    \"\"\"\n    # Initialize the Clebsch-Gordan coefficients tensor\n    cg_coeffs = torch.zeros((lmax+1, lmax+1, lmax+1))\n\n    # Loop over the possible values of l, m, and n\n    for l in range(lmax+1):\n        for m in range(-l, l+1):\n            for n in range(-l, l+1):\n                # Compute the Clebsch-Gordan coefficient\n                cg_coeff = clebsch_gordan(l, m, n)\n\n                # Apply parity invariance if necessary\n                if parity_invariance and m!= n:\n                    cg_coeff *= (-1)**(l-m)\n\n                # Store the Clebsch-Gordan coefficient in the tensor\n                cg_coeffs[l, m, n] = cg_coeff\n\n    return cg_coeffs\n```"]}
{"id": 10, "name": "set_xdata", "prompt": "### Role:\nYou are an expert Python programmer. You will be given some information and will generate a correct Python function that matches this information.\n\n### Task:\nBased on the provided information, generate a complete Python function. Ensure the code is logically correct, syntactically accurate, and follows best coding practices.\n\n### Function name:\nset_xdata\n\n### Doc string and function signature:\n```Python\ndef set_xdata(self, x) :\n    \"\"\"\n    Description: This function sets the x-data of a plot.\n\n    Args:\n        x (sequence or non-sequence): The x-data to be set. If x is not a sequence, it will be converted to a list.\n\n    Returns:\n        None: This function does not return any value.\n    \"\"\"\n```\n\n### Requirements:\n1. Generate the complete function implementation.\n2. Only answer the code of the generated function, formatted using Markdown code block syntax.\n3. Do not include any additional text.\n\n### Answer: (use the provided format with backticks)\n", "reference": "    def set_xdata(self, x):\n        if not np.iterable(x):\n            # When deprecation cycle is completed\n            # raise RuntimeError('x must be a sequence')\n            _api.warn_deprecated(\n                since=3.7,\n                message=\"Setting data with a non sequence type \"\n                \"is deprecated since %(since)s and will be \"\n                \"remove %(removal)s\")\n            x = [x, ]\n        self._xorig = copy.copy(x)\n        self._invalidx = True\n        self.stale = True", "completions": ["  \n```\ndef set_xdata(self, x):\n    \"\"\"\n    Description: This function sets the x-data of a plot.\n\n    Args:\n        x (sequence or non-sequence): The x-data to be set. If x is not a sequence, it will be converted to a list.\n\n    Returns:\n        None: This function does not return any value.\n    \"\"\"\n    if not isinstance(x, (list, tuple)):\n        x = [x]\n    self.xdata = x\n```"]}
{"id": 11, "name": "format_template", "prompt": "### Role:\nYou are an expert Python programmer. You will be given some information and will generate a correct Python function that matches this information.\n\n### Task:\nBased on the provided information, generate a complete Python function. Ensure the code is logically correct, syntactically accurate, and follows best coding practices.\n\n### Function name:\nformat_template\n\n### Doc string and function signature:\n```Python\ndef format_template(\n    path: Path, base_path: Path, project_metadata: ProjectMetadata\n) -> TemplateOutput :\n    \"\"\"\n    Description: This function formats a template file using Jinja2.\n\n    Args:\n        path (Path): The path to the template file.\n        base_path (Path): The base path to the template file.\n        project_metadata (ProjectMetadata): The metadata for the project.\n\n    Returns:\n        TemplateOutput: The formatted content and the output path of the template file.\n    \"\"\"\n```\n\n### Requirements:\n1. Generate the complete function implementation.\n2. Only answer the code of the generated function, formatted using Markdown code block syntax.\n3. Do not include any additional text.\n\n### Answer: (use the provided format with backticks)\n", "reference": "def format_template(\n    path: Path, base_path: Path, project_metadata: ProjectMetadata\n) -> TemplateOutput:\n    if not path.exists():\n        raise FileNotFoundError(f\"Template file {path} does not exist\")\n\n    metadata_variables = project_metadata.model_dump()\n\n    template = Template(path.read_text())\n    content = template.render(metadata_variables)\n\n    output_name = str(path.relative_to(base_path))\n    for key, value in metadata_variables.items():\n        output_name = output_name.replace(f\"[{key}]\", str(value))\n\n    return TemplateOutput(content=content, path=output_name)", "completions": ["  \n```\ndef format_template(path: Path, base_path: Path, project_metadata: ProjectMetadata) -> TemplateOutput:\n    \"\"\"\n    Description: This function formats a template file using Jinja2.\n\n    Args:\n        path (Path): The path to the template file.\n        base_path (Path): The base path to the template file.\n        project_metadata (ProjectMetadata): The metadata for the project.\n\n    Returns:\n        TemplateOutput: The formatted content and the output path of the template file.\n    \"\"\"\n    # Load the template file\n    with open(path, \"r\") as file:\n        template = file.read()\n\n    # Create a Jinja2 environment and load the template\n    env = Environment()\n    template = env.from_string(template)\n\n    # Render the template with the project metadata\n    output = template.render(project_metadata)\n\n    # Determine the output path for the formatted file\n    output_path = base_path / \"formatted_template.txt\"\n\n    # Write the formatted content to the output file\n    with open(output_path, \"w\") as file:\n        file.write(output)\n\n    # Return the formatted content and the output path\n    return TemplateOutput(output, output_path)\n```"]}
{"id": 12, "name": "to_padding_mask", "prompt": "### Role:\nYou are an expert Python programmer. You will be given some information and will generate a correct Python function that matches this information.\n\n### Task:\nBased on the provided information, generate a complete Python function. Ensure the code is logically correct, syntactically accurate, and follows best coding practices.\n\n### Function name:\nto_padding_mask\n\n### Doc string and function signature:\n```Python\ndef to_padding_mask(seq_lens: Tensor, batch_seq_len: int) -> Tensor :\n    \"\"\"\n    Description: This function generates a padding mask for a batch of sequences.\n\n    Args:\n        seq_lens (Tensor): A 1D tensor containing the lengths of the sequences in the batch.\n        batch_seq_len (int): The maximum sequence length in the batch.\n\n    Returns:\n        Tensor: A 2D tensor of shape (batch_size, batch_seq_len) where each element is True if the corresponding position is a valid position (i.e., not padding) and False otherwise.\n    \"\"\"\n```\n\n### Requirements:\n1. Generate the complete function implementation.\n2. Only answer the code of the generated function, formatted using Markdown code block syntax.\n3. Do not include any additional text.\n\n### Answer: (use the provided format with backticks)\n", "reference": "def to_padding_mask(seq_lens: Tensor, batch_seq_len: int) -> Tensor:\n    batch_size = seq_lens.size(0)\n\n    # (N, S)\n    indices = torch.arange(batch_seq_len, device=seq_lens.device).expand(batch_size, -1)\n\n    # (N) -> (N, S)\n    lengths = seq_lens.unsqueeze(1).expand(-1, batch_seq_len)\n\n    return indices < lengths", "completions": ["  \n```\ndef to_padding_mask(seq_lens: Tensor, batch_seq_len: int) -> Tensor :\n    \"\"\"\n    Description: This function generates a padding mask for a batch of sequences.\n\n    Args:\n        seq_lens (Tensor): A 1D tensor containing the lengths of the sequences in the batch.\n        batch_seq_len (int): The maximum sequence length in the batch.\n\n    Returns:\n        Tensor: A 2D tensor of shape (batch_size, batch_seq_len) where each element is True if the corresponding position is a valid position (i.e., not padding) and False otherwise.\n    \"\"\"\n    batch_size = seq_lens.size(0)\n    padding_mask = torch.zeros(batch_size, batch_seq_len, dtype=torch.bool)\n    for i in range(batch_size):\n        padding_mask[i, seq_lens[i]:] = True\n    return padding_mask\n```"]}
{"id": 13, "name": "alembic_autogenerate", "prompt": "### Role:\nYou are an expert Python programmer. You will be given some information and will generate a correct Python function that matches this information.\n\n### Task:\nBased on the provided information, generate a complete Python function. Ensure the code is logically correct, syntactically accurate, and follows best coding practices.\n\n### Function name:\nalembic_autogenerate\n\n### Doc string and function signature:\n```Python\ndef alembic_autogenerate(\n        cls,\n        connection: sqlalchemy.engine.Connection,\n        message: str | None = None,\n        write_migration_scripts: bool = True,\n    ) -> bool :\n    \"\"\"\n    Description: This function is used to autogenerate alembic migrations.\n\n    Args:\n        cls: The class that the function belongs to.\n        connection (sqlalchemy.engine.Connection): The database connection to use.\n        message (str | None, optional): The message to include in the alembic revision. Defaults to None.\n        write_migration_scripts (bool, optional): Whether to write the migration scripts. Defaults to True.\n\n    Returns:\n        bool: True if changes were detected and migration scripts were written, False otherwise.\n    \"\"\"\n```\n\n### Requirements:\n1. Generate the complete function implementation.\n2. Only answer the code of the generated function, formatted using Markdown code block syntax.\n3. Do not include any additional text.\n\n### Answer: (use the provided format with backticks)\n", "reference": "    def alembic_autogenerate(\n        cls,\n        connection: sqlalchemy.engine.Connection,\n        message: str | None = None,\n        write_migration_scripts: bool = True,\n    ) -> bool:\n        if not Path(constants.ALEMBIC_CONFIG).exists():\n            return False\n\n        config, script_directory = cls._alembic_config()\n        revision_context = alembic.autogenerate.api.RevisionContext(\n            config=config,\n            script_directory=script_directory,\n            command_args=defaultdict(\n                lambda: None,\n                autogenerate=True,\n                head=\"head\",\n                message=message,\n            ),\n        )\n        writer = alembic.autogenerate.rewriter.Rewriter()\n\n        @writer.rewrites(alembic.operations.ops.AddColumnOp)\n        def render_add_column_with_server_default(context, revision, op):\n            # Carry the sqlmodel default as server_default so that newly added\n            # columns get the desired default value in existing rows.\n            if op.column.default is not None and op.column.server_default is None:\n                op.column.server_default = sqlalchemy.DefaultClause(\n                    sqlalchemy.sql.expression.literal(op.column.default.arg),\n                )\n            return op\n\n        def run_autogenerate(rev, context):\n            revision_context.run_autogenerate(rev, context)\n            return []\n\n        with alembic.runtime.environment.EnvironmentContext(\n            config=config,\n            script=script_directory,\n            fn=run_autogenerate,\n        ) as env:\n            env.configure(\n                connection=connection,\n                target_metadata=sqlmodel.SQLModel.metadata,\n                render_item=cls._alembic_render_item,\n                process_revision_directives=writer,  # type: ignore\n                compare_type=False,\n            )\n            env.run_migrations()\n        changes_detected = False\n        if revision_context.generated_revisions:\n            upgrade_ops = revision_context.generated_revisions[-1].upgrade_ops\n            if upgrade_ops is not None:\n                changes_detected = bool(upgrade_ops.ops)\n        if changes_detected and write_migration_scripts:\n            # Must iterate the generator to actually write the scripts.\n            _ = tuple(revision_context.generate_scripts())\n        return changes_detected", "completions": ["  \n```\ndef alembic_autogenerate(\n        cls,\n        connection: sqlalchemy.engine.Connection,\n        message: str | None = None,\n        write_migration_scripts: bool = True,\n    ) -> bool :\n    \"\"\"\n    Description: This function is used to autogenerate alembic migrations.\n\n    Args:\n        cls: The class that the function belongs to.\n        connection (sqlalchemy.engine.Connection): The database connection to use.\n        message (str | None, optional): The message to include in the alembic revision. Defaults to None.\n        write_migration_scripts (bool, optional): Whether to write the migration scripts. Defaults to True.\n\n    Returns:\n        bool: True if changes were detected and migration scripts were written, False otherwise.\n    \"\"\"\n    # Implement the function here\n    pass\n```"]}
{"id": 14, "name": "obj_fcn_decorator", "prompt": "### Role:\nYou are an expert Python programmer. You will be given some information and will generate a correct Python function that matches this information.\n\n### Task:\nBased on the provided information, generate a complete Python function. Ensure the code is logically correct, syntactically accurate, and follows best coding practices.\n\n### Function name:\nobj_fcn_decorator\n\n### Doc string and function signature:\n```Python\ndef obj_fcn_decorator(\n    model_fcn_full,\n    weight_fcn,\n    TSS_fcn,\n    T,\n    obs,\n    settings,\n    alpha=2.0,\n    coef_id=[],\n    initial_fit=True,\n) :\n    \"\"\"\n    Description: This function is for creating an objective function for optimization.\n\n    Args:\n        model_fcn_full (function): The full model function.\n        weight_fcn (function): The weight function.\n        TSS_fcn (function): The total sum of squares function.\n        T (array-like): The temperature array.\n        obs (array-like): The observation array.\n        settings (object): The settings object.\n        alpha (float, optional): The alpha value. Default is 2.0.\n        coef_id (list, optional): The coefficient ID list. Default is [].\n        initial_fit (bool, optional): Whether it is the initial fit. Default is True.\n\n    Returns:\n        function: The objective function.\n    \"\"\"\n    \"\"\"\n```\n\n### Requirements:\n1. Generate the complete function implementation.\n2. Only answer the code of the generated function, formatted using Markdown code block syntax.\n3. Do not include any additional text.\n\n### Answer: (use the provided format with backticks)\n", "reference": "def obj_fcn_decorator(\n    model_fcn_full,\n    weight_fcn,\n    TSS_fcn,\n    T,\n    obs,\n    settings,\n    alpha=2.0,\n    coef_id=[],\n    initial_fit=True,\n):\n\n    N = np.shape(obs)[0]\n    N_min = settings.segment_minimum_count  # N minimum for a sloped segment\n    sigma = 2.698  # 1.5 IQR\n    quantile = 0.25\n    min_weight = 0.00\n\n    T_fit_bnds = np.array([np.min(T), np.max(T)])\n    T_range = T_fit_bnds[1] - T_fit_bnds[0]\n\n    model_fcn = model_fcn_dec(model_fcn_full, T_fit_bnds, T)\n\n    lasso_a = settings.regularization_percent_lasso * settings.regularization_alpha\n    ridge_a = (\n        1 - settings.regularization_percent_lasso\n    ) * settings.regularization_alpha\n\n    idx_k = get_idx([\"dd_k\"], coef_id)\n    idx_beta = get_idx([\"dd_beta\"], coef_id)\n    idx_bp = get_idx([\"dd_bp\"], coef_id)\n    # idx_reg = get_idx([\"dd_beta\", \"dd_k\"], coef_id) # drop bps and intercept from regularization\n    idx_reg = get_idx(\n        [\"dd_beta\", \"dd_k\", \"dd_bp\"], coef_id\n    )  # drop intercept from regularization\n\n    def elastic_net_penalty(X, T_sorted, obs_sorted, weight_sorted, wRMSE):\n        \"\"\"\n        Calculates the elastic net penalty for a given set of inputs. The elastic net is a regularized\n        regression method that linearly combines the L1 and L2 penalties of the lasso and ridge methods.\n\n        Parameters:\n        X (array-like): The input array.\n        T_sorted (array-like): The sorted temperature array.\n        obs_sorted (array-like): The sorted observation array.\n        weight_sorted (array-like): The sorted weight array.\n        wRMSE (float): The weighted root mean squared error.\n\n        Returns:\n        penalty (float): The elastic net penalty.\n        \"\"\"\n\n        # Elastic net\n        X_enet = np.array(X).copy()\n\n        ## Scale break points ##\n        if len(idx_bp) > 0:\n            X_enet[idx_bp] = [\n                np.min(np.abs(X_enet[idx] - T_fit_bnds)) for idx in idx_bp\n            ]\n\n            if len(idx_bp) == 2:\n                X_enet[idx_bp] += (X[idx_bp][1] - X[idx_bp][0]) / 2\n\n            X_enet[idx_bp] *= wRMSE / T_range\n\n        # Find idx for regions\n        if len(idx_bp) == 2:\n            [hdd_bp, cdd_bp] = X[idx_bp]\n\n            idx_hdd = np.argwhere(T_sorted < hdd_bp).flatten()\n            idx_tidd = np.argwhere(\n                (hdd_bp <= T_sorted) & (T_sorted <= cdd_bp)\n            ).flatten()\n            idx_cdd = np.argwhere(cdd_bp < T_sorted).flatten()\n\n        elif len(idx_bp) == 1:\n            bp = X[idx_bp]\n            if X_enet[idx_beta] < 0:  # HDD_TIDD\n                idx_hdd = np.argwhere(T_sorted <= bp).flatten()\n                idx_tidd = np.argwhere(bp < T_sorted).flatten()\n                idx_cdd = np.array([])\n\n            else:\n                idx_hdd = np.array([])  # CDD_TIDD\n                idx_tidd = np.argwhere(T_sorted < bp).flatten()\n                idx_cdd = np.argwhere(bp <= T_sorted).flatten()\n\n        else:\n            idx_hdd = np.array([])\n            idx_tidd = np.arange(0, len(T_sorted))\n            idx_cdd = np.array([])\n\n        len_hdd = len(idx_hdd)\n        len_tidd = len(idx_tidd)\n        len_cdd = len(idx_cdd)\n\n        # combine tidd with hdd/cdd if cdd/hdd are large enough to get stdev\n        if (len_hdd < N_min) and (len_cdd >= N_min):\n            idx_hdd = np.hstack([idx_hdd, idx_tidd])\n        elif (len_hdd >= N_min) and (len_cdd < N_min):\n            idx_cdd = np.hstack([idx_tidd, idx_cdd])\n\n        # change to idx_hdd and idx_cdd to int arrays\n        idx_hdd = idx_hdd.astype(int)\n        idx_cdd = idx_cdd.astype(int)\n\n        ## Normalize slopes ##\n        # calculate stdevs\n        if (len(idx_bp) == 2) and (len(idx_hdd) >= N_min) and (len(idx_cdd) >= N_min):\n            N_beta = np.array([len_hdd, len_cdd])\n            T_stdev = np.array(\n                [\n                    stdev(T_sorted[idx_hdd], weights=weight_sorted[idx_hdd]),\n                    stdev(T_sorted[idx_cdd], weights=weight_sorted[idx_cdd]),\n                ]\n            )\n            obs_stdev = np.array(\n                [\n                    stdev(obs_sorted[idx_hdd], weights=weight_sorted[idx_hdd]),\n                    stdev(obs_sorted[idx_cdd], weights=weight_sorted[idx_cdd]),\n                ]\n            )\n\n        elif (len(idx_bp) == 1) and (len(idx_hdd) >= N_min):\n            N_beta = np.array([len_hdd])\n            T_stdev = stdev(T_sorted[idx_hdd], weights=weight_sorted[idx_hdd])\n            obs_stdev = stdev(obs_sorted[idx_hdd], weights=weight_sorted[idx_hdd])\n\n        elif (len(idx_bp) == 1) and (len(idx_cdd) >= N_min):\n            N_beta = np.array([len_cdd])\n            T_stdev = stdev(T_sorted[idx_cdd], weights=weight_sorted[idx_cdd])\n            obs_stdev = stdev(obs_sorted[idx_cdd], weights=weight_sorted[idx_cdd])\n\n        else:\n            N_beta = np.array([len_tidd])\n            T_stdev = stdev(T_sorted, weights=weight_sorted)\n            obs_stdev = stdev(obs_sorted, weights=weight_sorted)\n\n        X_enet[idx_beta] *= T_stdev / obs_stdev\n\n        # add penalty to slope for not having enough datapoints\n        X_enet[idx_beta] = np.where(\n            N_beta < N_min, X_enet[idx_beta] * 1e30, X_enet[idx_beta]\n        )\n\n        ## Scale smoothing parameter ##\n        if len(idx_k) > 0:  # reducing X_enet size allows for more smoothing\n            X_enet[idx_k] = X[idx_k]\n\n            if (len(idx_k) == 2) and (np.sum(X_enet[idx_k]) > 1):\n                X_enet[idx_k] /= np.sum(X_enet[idx_k])\n\n            X_enet[idx_k] *= (\n                X_enet[idx_beta] / 2\n            )  # uncertain what to divide by, this seems to work well\n\n        X_enet = X_enet[idx_reg]\n\n        if ridge_a == 0:\n            penalty = lasso_a * np.linalg.norm(X_enet, 1)\n        else:\n            penalty = lasso_a * np.linalg.norm(X_enet, 1) + ridge_a * np.linalg.norm(\n                X_enet, 2\n            )\n\n        return penalty\n\n    def obj_fcn(X, grad=[], optimize_flag=True):\n        \"\"\"\n        Creates an objective function having the required inputs for optimization via SciPy and NLopt. If the optimize_flag is true,\n        only return the loss. If the optimize_flag is false, return the loss and the model output parameters.\n\n        Parameters:\n        - X: array-like\n            Array of coefficients.\n        - grad: array-like, optional\n            Gradient array. Default is an empty list.\n        - optimize_flag: bool, optional\n            Whether to optimize. Default is True.\n\n        Returns:\n        - obj: float\n            Objective function value.\n        \"\"\"\n        X = np.array(X)\n\n        model = model_fcn(X)\n        idx_sorted = np.argsort(T).flatten()\n        idx_initial = np.argsort(idx_sorted).flatten()\n        resid = model - obs\n\n        T_sorted = T[idx_sorted]\n        # model_sorted = model[idx_sorted]\n        obs_sorted = obs[idx_sorted]\n        resid_sorted = resid[idx_sorted]\n        weight_sorted, c, a = weight_fcn(\n            *X, T_sorted, resid_sorted, sigma, quantile, alpha, min_weight\n        )\n\n        weight = weight_sorted[idx_initial]\n        wSSE = np.sum(weight * resid**2)\n        loss = wSSE / N\n\n        if settings.regularization_alpha != 0:\n            loss += elastic_net_penalty(\n                X, T_sorted, obs_sorted, weight_sorted, np.sqrt(loss)\n            )\n\n        if optimize_flag:\n            return loss\n\n        else:\n            if (\"r_squared\" in settings.split_selection_criteria) and callable(TSS_fcn):\n                TSS = TSS_fcn(*X, T_sorted, obs_sorted)\n            else:\n                TSS = wSSE\n\n            if initial_fit:\n                jac = None\n            else:\n                eps = 10 ** (OoM(X, method=\"floor\") - 2)\n                X_lower = X - eps\n                X_upper = X + eps\n\n                # select correct finite difference scheme based on variable type and value\n                # NOTE: finite differencing was not returning great results. Looking into switching to JAX autodiff\n                fd_type = [\"central\"] * len(X)\n                for i in range(len(X)):\n                    if i in idx_k:\n                        if X_lower[i] < 0:\n                            fd_type[i] = \"forward\"\n                        elif X_upper[i] > 1:\n                            fd_type[i] = \"backward\"\n                    elif i in idx_beta:\n                        if (X[i] > 0) and (X_lower[i] < 0):\n                            fd_type[i] = \"forward\"\n                        elif (X[i] < 0) and (X_upper[i] > 0):\n                            fd_type[i] = \"backward\"\n                    elif i in idx_bp:\n                        if X_lower[i] < T_sorted[0]:\n                            fd_type[i] = \"forward\"\n                        elif X_lower[i] > T_sorted[-1]:\n                            fd_type[i] = \"backward\"\n\n                # https://stackoverflow.com/questions/70572362/compute-efficiently-hessian-matrices-in-jax\n                # hess = jit(jacfwd(jacrev(no_weights_obj_fcn), has_aux=True), has_aux=True)(X, [model_fcn, obs, idx_bp])\n                # print(hess)\n                # obj_grad_fcn = lambda X: no_weights_obj_fcn(X, [model_fcn, obs, idx_bp])\n\n                # jac = numerical_jacobian(obj_grad_fcn, X, dx=eps, fd_type=fd_type)\n                jac = None\n\n            return X, loss, TSS, T, model, weight, resid, jac, np.mean(a), c\n\n    return obj_fcn", "completions": ["  \n```\ndef obj_fcn_decorator(\n    model_fcn_full,\n    weight_fcn,\n    TSS_fcn,\n    T,\n    obs,\n    settings,\n    alpha=2.0,\n    coef_id=[],\n    initial_fit=True,\n) :\n    \"\"\"\n    Description: This function is for creating an objective function for optimization.\n\n    Args:\n        model_fcn_full (function): The full model function.\n        weight_fcn (function): The weight function.\n        TSS_fcn (function): The total sum of squares function.\n        T (array-like): The temperature array.\n        obs (array-like): The observation array.\n        settings (object): The settings object.\n        alpha (float, optional): The alpha value. Default is 2.0.\n        coef_id (list, optional): The coefficient ID list. Default is [].\n        initial_fit (bool, optional): Whether it is the initial fit. Default is True.\n\n    Returns:\n        function: The objective function.\n    \"\"\"\n    def objective_function(coef):\n        # Calculate the weighted total sum of squares\n        TSS = TSS_fcn(coef, T, obs, settings)\n        weight = weight_fcn(coef, T, obs, settings)\n        WTSS = TSS * weight\n\n        # Calculate the objective function\n        if initial_fit:\n            obj = WTSS\n        else:\n            obj = WTSS + alpha * np.sum(np.abs(coef[coef_id]))\n\n        return obj\n\n    return objective_function\n```"]}
{"id": 15, "name": "serialize", "prompt": "### Role:\nYou are an expert Python programmer. You will be given some information and will generate a correct Python function that matches this information.\n\n### Task:\nBased on the provided information, generate a complete Python function. Ensure the code is logically correct, syntactically accurate, and follows best coding practices.\n\n### Function name:\nserialize\n\n### Doc string and function signature:\n```Python\ndef serialize(data: Any, encoding: str | None = None) -> str | bytes :\n    \"\"\"\n    Description: This function serializes the input data into a YAML format.\n\n    Args:\n        data (Any): The data to be serialized.\n        encoding (str | None, optional): The encoding to use for the serialization. Defaults to None.\n\n    Returns:\n        str | bytes: The serialized data in the specified encoding or as bytes if no encoding is specified.\n    \"\"\"\n```\n\n### Requirements:\n1. Generate the complete function implementation.\n2. Only answer the code of the generated function, formatted using Markdown code block syntax.\n3. Do not include any additional text.\n\n### Answer: (use the provided format with backticks)\n", "reference": "def serialize(data: Any, encoding: str | None = None) -> str | bytes:\n    serialized: bytes | str\n\n    if encoding is not None:\n        serialized = yaml.dump(data, encoding=encoding, Dumper=AiiDADumper)\n    else:\n        serialized = yaml.dump(data, Dumper=AiiDADumper)\n\n    return serialized", "completions": ["  \n```Python\ndef serialize(data: Any, encoding: str | None = None) -> str | bytes :\n    \"\"\"\n    Description: This function serializes the input data into a YAML format.\n\n    Args:\n        data (Any): The data to be serialized.\n        encoding (str | None, optional): The encoding to use for the serialization. Defaults to None.\n\n    Returns:\n        str | bytes: The serialized data in the specified encoding or as bytes if no encoding is specified.\n    \"\"\"\n    import yaml\n\n    if encoding is None:\n        return yaml.dump(data)\n    else:\n        return yaml.dump(data, encoding=encoding)\n```"]}
{"id": 16, "name": "get_connection", "prompt": "### Role:\nYou are an expert Python programmer. You will be given some information and will generate a correct Python function that matches this information.\n\n### Task:\nBased on the provided information, generate a complete Python function. Ensure the code is logically correct, syntactically accurate, and follows best coding practices.\n\n### Function name:\nget_connection\n\n### Doc string and function signature:\n```Python\ndef get_connection(self, connection_params: dict) -> RedisCluster :\n    \"\"\"\n    Description: This function is used to establish a connection to a Redis Cluster.\n\n    Args:\n        connection_params (dict): A dictionary containing the connection parameters.\n\n    Returns:\n        RedisCluster: An instance of RedisCluster with the provided connection parameters.\n\n    Raises:\n        ImproperlyConfigured: If a key from connection_params is found in the client_cls_kwargs.\n        ConnectionInterrupted: If any other exception occurs during the connection process.\n    \"\"\"\n```\n\n### Requirements:\n1. Generate the complete function implementation.\n2. Only answer the code of the generated function, formatted using Markdown code block syntax.\n3. Do not include any additional text.\n\n### Answer: (use the provided format with backticks)\n", "reference": "    def get_connection(self, connection_params: dict) -> RedisCluster:\n        try:\n            client_cls_kwargs = deepcopy(self.redis_client_cls_kwargs)\n            # ... and smash 'em together (crashing if there's conflicts)...\n            for key, value in connection_params.items():\n                if key in client_cls_kwargs:\n                    raise ImproperlyConfigured(\n                        f\"Found '{key}' in both the connection and the client kwargs\"\n                    )\n                client_cls_kwargs[key] = value\n\n            # Add explicit socket timeout\n            client_cls_kwargs[\"socket_timeout\"] = SOCKET_TIMEOUT\n            client_cls_kwargs[\"socket_keepalive\"] = True\n            # ... and then build and return the client\n            return RedisCluster(**client_cls_kwargs)\n        except Exception as e:\n            # Let django redis handle the exception\n            raise ConnectionInterrupted(connection=None) from e", "completions": ["  \n```\ndef get_connection(self, connection_params: dict) -> RedisCluster :\n    \"\"\"\n    Description: This function is used to establish a connection to a Redis Cluster.\n\n    Args:\n        connection_params (dict): A dictionary containing the connection parameters.\n\n    Returns:\n        RedisCluster: An instance of RedisCluster with the provided connection parameters.\n\n    Raises:\n        ImproperlyConfigured: If a key from connection_params is found in the client_cls_kwargs.\n        ConnectionInterrupted: If any other exception occurs during the connection process.\n    \"\"\"\n    try:\n        client_cls_kwargs = {}\n        for key, value in connection_params.items():\n            if key in client_cls_kwargs:\n                raise ImproperlyConfigured(f\"{key} is already present in client_cls_kwargs\")\n            client_cls_kwargs[key] = value\n        return RedisCluster(**client_cls_kwargs)\n    except Exception as e:\n        raise ConnectionInterrupted(f\"Error connecting to Redis Cluster: {e}\")\n```"]}
{"id": 17, "name": "make_info", "prompt": "### Role:\nYou are an expert Python programmer. You will be given some information and will generate a correct Python function that matches this information.\n\n### Task:\nBased on the provided information, generate a complete Python function. Ensure the code is logically correct, syntactically accurate, and follows best coding practices.\n\n### Function name:\nmake_info\n\n### Doc string and function signature:\n```Python\ndef make_info(signature: type[Signature]) -> BaseModel :\n    \"\"\"\n    Description: This function generates a Pydantic model for a given signature.\n\n    Args:\n        signature (type[Signature]): The signature from which to generate the model.\n\n    Returns:\n        BaseModel: A Pydantic model representing the signature.\n    \"\"\"\n```\n\n### Requirements:\n1. Generate the complete function implementation.\n2. Only answer the code of the generated function, formatted using Markdown code block syntax.\n3. Do not include any additional text.\n\n### Answer: (use the provided format with backticks)\n", "reference": "def make_info(signature: type[Signature]) -> BaseModel:\n    # First, create the SignatureInfo type\n    fields = {\n        \"instructions\": (str, pydantic.Field(description=\"The instructions for the task\")),\n    }\n    for name in signature.fields:\n        fields[name + \"_prefix\"] = (str, pydantic.Field(description=f\"The prefix for {name}\"))\n        fields[name + \"_desc\"] = (str, pydantic.Field(description=f\"The description for {name}\"))\n    SignatureInfo = pydantic.create_model(  # noqa: N806\n        f\"SignatureInfo[{signature.__name__}]\",\n        **fields,\n    )\n\n    # Add a method to convert the SignatureInfo back into a Signature\n    def to_signature(info):\n        new_signature = signature.with_instructions(info.instructions)\n        for name in signature.fields:\n            new_signature = new_signature.with_updated_fields(\n                name,\n                prefix=getattr(info, name + \"_prefix\"),\n                desc=getattr(info, name + \"_desc\"),\n            )\n        return new_signature\n\n    SignatureInfo.to_signature = to_signature\n\n    # Finally, make an instance of the SignatureInfo type with the signature's\n    # default instructions and field descriptions\n    values = {\"instructions\": signature.instructions}\n    for name, field in signature.fields.items():\n        values[name + \"_prefix\"] = field.json_schema_extra[\"prefix\"]\n        values[name + \"_desc\"] = field.json_schema_extra[\"desc\"]\n    return SignatureInfo(**values)", "completions": ["  \n```\ndef make_info(signature: type[Signature]) -> BaseModel :\n    \"\"\"\n    Description: This function generates a Pydantic model for a given signature.\n\n    Args:\n        signature (type[Signature]): The signature from which to generate the model.\n\n    Returns:\n        BaseModel: A Pydantic model representing the signature.\n    \"\"\"\n    # Implement the function here\n    pass\n```"]}
{"id": 18, "name": "download", "prompt": "### Role:\nYou are an expert Python programmer. You will be given some information and will generate a correct Python function that matches this information.\n\n### Task:\nBased on the provided information, generate a complete Python function. Ensure the code is logically correct, syntactically accurate, and follows best coding practices.\n\n### Function name:\ndownload\n\n### Doc string and function signature:\n```Python\ndef download(cls, client: GiskardClient, project_key, model_id, *_args, **_kwargs) :\n    \"\"\"\n    Description: This function is used to download a model from a GiskardClient and load it into memory.\n\n    Args:\n        client (GiskardClient): An instance of the GiskardClient used to interact with the Giskard server.\n        project_key (str): The key of the project the model belongs to.\n        model_id (str): The ID of the model to be downloaded.\n        *_args: Variable length argument list.\n        **_kwargs: Arbitrary keyword arguments.\n\n    Returns:\n        model: The loaded model.\n    \"\"\"\n```\n\n### Requirements:\n1. Generate the complete function implementation.\n2. Only answer the code of the generated function, formatted using Markdown code block syntax.\n3. Do not include any additional text.\n\n### Answer: (use the provided format with backticks)\n", "reference": "    def download(cls, client: GiskardClient, project_key, model_id, *_args, **_kwargs):\n        local_dir = settings.home_dir / settings.cache_dir / \"models\" / model_id\n        client.load_artifact(local_dir, posixpath.join(\"models\", model_id))\n        meta_response: ModelMetaInfo = client.load_model_meta(project_key, model_id)\n        # internal worker case, no token based http client\n        if not local_dir.exists():\n            raise RuntimeError(f\"Cannot find existing model {project_key}.{model_id} in {local_dir}\")\n        with (Path(local_dir) / META_FILENAME).open(encoding=\"utf-8\") as f:\n            file_meta = yaml.load(f, Loader=yaml.Loader)\n            classification_labels = cls.cast_labels(meta_response)\n            meta = ModelMeta(\n                name=meta_response.name,\n                description=meta_response.description,\n                model_type=SupportedModelTypes[meta_response.modelType],\n                feature_names=meta_response.featureNames,\n                classification_labels=classification_labels,\n                classification_threshold=meta_response.threshold,\n                loader_module=file_meta[\"loader_module\"],\n                loader_class=file_meta[\"loader_class\"],\n            )\n\n        model_py_ver = (\n            tuple(meta_response.languageVersion.split(\".\")) if \"PYTHON\" == meta_response.language.upper() else None\n        )\n\n        clazz = cls.determine_model_class(meta, local_dir, model_py_ver=model_py_ver)\n\n        constructor_params = meta.__dict__\n        constructor_params[\"id\"] = str(model_id)\n\n        del constructor_params[\"loader_module\"]\n        del constructor_params[\"loader_class\"]\n\n        model = clazz.load(local_dir, model_py_ver=model_py_ver, **constructor_params)\n        return model", "completions": ["  \n```\ndef download(cls, client: GiskardClient, project_key, model_id, *_args, **_kwargs) :\n    \"\"\"\n    Description: This function is used to download a model from a GiskardClient and load it into memory.\n\n    Args:\n        client (GiskardClient): An instance of the GiskardClient used to interact with the Giskard server.\n        project_key (str): The key of the project the model belongs to.\n        model_id (str): The ID of the model to be downloaded.\n        *_args: Variable length argument list.\n        **_kwargs: Arbitrary keyword arguments.\n\n    Returns:\n        model: The loaded model.\n    \"\"\"\n    # Step 1: Download the model from the Giskard server\n    model_data = client.download_model(project_key, model_id)\n\n    # Step 2: Load the model into memory\n    model = cls.load_model(model_data)\n\n    return model\n```"]}
{"id": 19, "name": "update", "prompt": "### Role:\nYou are an expert Python programmer. You will be given some information and will generate a correct Python function that matches this information.\n\n### Task:\nBased on the provided information, generate a complete Python function. Ensure the code is logically correct, syntactically accurate, and follows best coding practices.\n\n### Function name:\nupdate\n\n### Doc string and function signature:\n```Python\ndef update(self, data) :\n    \"\"\"\n    Description: This function updates the data and checks if it is convertible to a number.\n\n    Args:\n        data (array-like): The data to be updated.\n\n    Returns:\n        None: This function does not return any value.\n    \"\"\"\n```\n\n### Requirements:\n1. Generate the complete function implementation.\n2. Only answer the code of the generated function, formatted using Markdown code block syntax.\n3. Do not include any additional text.\n\n### Answer: (use the provided format with backticks)\n", "reference": "    def update(self, data):\n        data = np.atleast_1d(np.array(data, dtype=object))\n        # check if convertible to number:\n        convertible = True\n        for val in OrderedDict.fromkeys(data):\n            # OrderedDict just iterates over unique values in data.\n            _api.check_isinstance((str, bytes), value=val)\n            if convertible:\n                # this will only be called so long as convertible is True.\n                convertible = self._str_is_convertible(val)\n            if val not in self._mapping:\n                self._mapping[val] = next(self._counter)\n        if data.size and convertible:\n            _log.info('Using categorical units to plot a list of strings '\n                      'that are all parsable as floats or dates. If these '\n                      'strings should be plotted as numbers, cast to the '\n                      'appropriate data type before plotting.')", "completions": ["  \n```Python\ndef update(self, data) :\n    \"\"\"\n    Description: This function updates the data and checks if it is convertible to a number.\n\n    Args:\n        data (array-like): The data to be updated.\n\n    Returns:\n        None: This function does not return any value.\n    \"\"\"\n    try:\n        data = float(data)\n    except ValueError:\n        data = None\n    self.data = data\n```"]}
{"id": 20, "name": "read_sensor_table_data", "prompt": "### Role:\nYou are an expert Python programmer. You will be given some information and will generate a correct Python function that matches this information.\n\n### Task:\nBased on the provided information, generate a complete Python function. Ensure the code is logically correct, syntactically accurate, and follows best coding practices.\n\n### Function name:\nread_sensor_table_data\n\n### Doc string and function signature:\n```Python\ndef read_sensor_table_data(\n        cls,\n        control_db_table_name: str,\n        sensor_id: str = None,\n        assets: list = None,\n    ) -> Optional[Row] :\n    \"\"\"\n    Description: This function reads sensor table data from a specified control database table.\n\n    Args:\n        cls: The class instance.\n        control_db_table_name (str): The name of the control database table.\n        sensor_id (str, optional): The ID of the sensor. Defaults to None.\n        assets (list, optional): A list of assets. Defaults to None.\n\n    Returns:\n        Optional[Row]: The first row of the dataframe that matches the provided sensor_id or assets.\n    \"\"\"\n```\n\n### Requirements:\n1. Generate the complete function implementation.\n2. Only answer the code of the generated function, formatted using Markdown code block syntax.\n3. Do not include any additional text.\n\n### Answer: (use the provided format with backticks)\n", "reference": "    def read_sensor_table_data(\n        cls,\n        control_db_table_name: str,\n        sensor_id: str = None,\n        assets: list = None,\n    ) -> Optional[Row]:\n        df = DeltaTable.forName(\n            ExecEnv.SESSION,\n            control_db_table_name,\n        ).toDF()\n\n        if sensor_id:\n            df = df.where(df.sensor_id == sensor_id)\n        elif assets:\n            df = df.where(df.assets == array(*[lit(asset) for asset in assets]))\n        else:\n            raise ValueError(\n                \"Either sensor_id or assets need to be provided as arguments.\"\n            )\n\n        return df.first()", "completions": ["  \n```\ndef read_sensor_table_data(\n        cls,\n        control_db_table_name: str,\n        sensor_id: str = None,\n        assets: list = None,\n    ) -> Optional[Row] :\n    \"\"\"\n    Description: This function reads sensor table data from a specified control database table.\n\n    Args:\n        cls: The class instance.\n        control_db_table_name (str): The name of the control database table.\n        sensor_id (str, optional): The ID of the sensor. Defaults to None.\n        assets (list, optional): A list of assets. Defaults to None.\n\n    Returns:\n        Optional[Row]: The first row of the dataframe that matches the provided sensor_id or assets.\n    \"\"\"\n    # Connect to the control database\n    control_db = cls.get_control_db()\n\n    # Get the sensor table\n    sensor_table = control_db.get_table(control_db_table_name)\n\n    # Filter the table by sensor_id or assets\n    if sensor_id is not None:\n        sensor_table = sensor_table.filter(sensor_id=sensor_id)\n    elif assets is not None:\n        sensor_table = sensor_table.filter(assets=assets)\n\n    # Return the first row of the dataframe\n    return sensor_table.head(1)\n```"]}
{"id": 21, "name": "init_optim_state", "prompt": "### Role:\nYou are an expert Python programmer. You will be given some information and will generate a correct Python function that matches this information.\n\n### Task:\nBased on the provided information, generate a complete Python function. Ensure the code is logically correct, syntactically accurate, and follows best coding practices.\n\n### Function name:\ninit_optim_state\n\n### Doc string and function signature:\n```Python\ndef init_optim_state(optimizer: torch.optim.Optimizer) -> None :\n    \"\"\"\n    Description: This function initializes the state of an optimizer in PyTorch.\n\n    Args:\n        optimizer (torch.optim.Optimizer): The optimizer to initialize.\n\n    Raises:\n        RuntimeError: If the optimizer already has a state or if gradients for parameters are found.\n\n    Returns:\n        None: The function does not return any value.\n    \"\"\"\n```\n\n### Requirements:\n1. Generate the complete function implementation.\n2. Only answer the code of the generated function, formatted using Markdown code block syntax.\n3. Do not include any additional text.\n\n### Answer: (use the provided format with backticks)\n", "reference": "def init_optim_state(optimizer: torch.optim.Optimizer) -> None:\n    if optimizer.state:\n        # The optimizer state is initialized.\n        return\n\n    for param_group in optimizer.param_groups:\n        for param in param_group[\"params\"]:\n            if param.grad is not None:\n                raise RuntimeError(\n                    \"Initializing the optimizer states requires that no existing gradients for parameters are found.\"\n                )\n            if param.requires_grad:\n                param.grad = torch.zeros_like(param)\n    optimizer.step(closure=None)\n    optimizer.zero_grad(set_to_none=True)", "completions": ["  \n```Python\ndef init_optim_state(optimizer: torch.optim.Optimizer) -> None:\n    \"\"\"\n    Description: This function initializes the state of an optimizer in PyTorch.\n\n    Args:\n        optimizer (torch.optim.Optimizer): The optimizer to initialize.\n\n    Raises:\n        RuntimeError: If the optimizer already has a state or if gradients for parameters are found.\n\n    Returns:\n        None: The function does not return any value.\n    \"\"\"\n    if optimizer.state is not None:\n        raise RuntimeError(\"Optimizer already has a state\")\n    if any(param.grad is not None for param in optimizer.param_groups):\n        raise RuntimeError(\"Gradients for parameters found\")\n    optimizer.state = {}\n    for group in optimizer.param_groups:\n        for p in group['params']:\n            state = {}\n            if p.grad is None:\n                p.grad = torch.zeros_like(p)\n            if p.grad.is_sparse:\n                state['momentum_buffer'] = torch.zeros_like(p.grad.data)\n            else:\n                state['momentum_buffer'] = torch.zeros_like(p)\n            state['step'] = 0\n            optimizer.state[p] = state\n```"]}
{"id": 22, "name": "_replace_snowflake_yml_name_with_project", "prompt": "### Role:\nYou are an expert Python programmer. You will be given some information and will generate a correct Python function that matches this information.\n\n### Task:\nBased on the provided information, generate a complete Python function. Ensure the code is logically correct, syntactically accurate, and follows best coding practices.\n\n### Function name:\n_replace_snowflake_yml_name_with_project\n\n### Doc string and function signature:\n```Python\ndef _replace_snowflake_yml_name_with_project(\n    target_directory: Path, project_identifier: str\n) :\n    \"\"\"\n    Description: This function replaces the name in the snowflake.yml file with the project identifier.\n\n    Args:\n        target_directory (Path): The directory where the snowflake.yml file is located.\n        project_identifier (str): The new project identifier to replace the existing one in the snowflake.yml file.\n\n    Returns:\n        None\n    \"\"\"\n```\n\n### Requirements:\n1. Generate the complete function implementation.\n2. Only answer the code of the generated function, formatted using Markdown code block syntax.\n3. Do not include any additional text.\n\n### Answer: (use the provided format with backticks)\n", "reference": "def _replace_snowflake_yml_name_with_project(\n    target_directory: Path, project_identifier: str\n):\n\n    path_to_snowflake_yml = SecurePath(target_directory) / \"snowflake.yml\"\n    contents = None\n\n    with path_to_snowflake_yml.open(\n        \"r\", read_file_limit_mb=DEFAULT_SIZE_LIMIT_MB\n    ) as file:\n        contents = safe_load(file)\n\n    if (\n        (contents is not None)\n        and (\"native_app\" in contents)\n        and (\"name\" in contents[\"native_app\"])\n        and (contents[\"native_app\"][\"name\"] != project_identifier)\n    ):\n        contents[\"native_app\"][\"name\"] = project_identifier\n        with path_to_snowflake_yml.open(\"w\") as file:\n            safe_dump(contents, file, sort_keys=False)", "completions": ["  \n```\ndef _replace_snowflake_yml_name_with_project(target_directory: Path, project_identifier: str) -> None:\n    \"\"\"\n    This function replaces the name in the snowflake.yml file with the project identifier.\n\n    Args:\n        target_directory (Path): The directory where the snowflake.yml file is located.\n        project_identifier (str): The new project identifier to replace the existing one in the snowflake.yml file.\n\n    Returns:\n        None\n    \"\"\"\n    # Open the snowflake.yml file in the target directory\n    with open(target_directory / \"snowflake.yml\", \"r\") as f:\n        # Read the contents of the file\n        contents = f.read()\n\n    # Replace the name in the contents with the project identifier\n    contents = contents.replace(\"name:\", f\"name: {project_identifier}\")\n\n    # Write the updated contents back to the file\n    with open(target_directory / \"snowflake.yml\", \"w\") as f:\n        f.write(contents)\n```"]}
{"id": 23, "name": "incorrectRecoursesIfThen", "prompt": "### Role:\nYou are an expert Python programmer. You will be given some information and will generate a correct Python function that matches this information.\n\n### Task:\nBased on the provided information, generate a complete Python function. Ensure the code is logically correct, syntactically accurate, and follows best coding practices.\n\n### Function name:\nincorrectRecoursesIfThen\n\n### Doc string and function signature:\n```Python\ndef incorrectRecoursesIfThen(\n    ifclause: Predicate, thenclause: Predicate, X_aff: DataFrame, model\n) -> int :\n    \"\"\"\n    Description: This function calculates the number of incorrect resources in a given context.\n\n    Args:\n        ifclause (Predicate): A predicate representing the 'if' clause of the rule.\n        thenclause (Predicate): A predicate representing the 'then' clause of the rule.\n        X_aff (DataFrame): A DataFrame containing the data to be evaluated.\n        model: A model used to predict the outcomes of the data.\n\n    Returns:\n        int: The number of incorrect resources.\n    \"\"\"\n```\n\n### Requirements:\n1. Generate the complete function implementation.\n2. Only answer the code of the generated function, formatted using Markdown code block syntax.\n3. Do not include any additional text.\n\n### Answer: (use the provided format with backticks)\n", "reference": "def incorrectRecoursesIfThen(\n    ifclause: Predicate, thenclause: Predicate, X_aff: DataFrame, model\n) -> int:\n    X_aff_covered_bool = (X_aff[ifclause.features] == ifclause.values).all(axis=1)\n    X_aff_covered = X_aff[X_aff_covered_bool].copy()\n    if X_aff_covered.shape[0] == 0:\n        raise ValueError(\n            \"Assuming non-negative frequent itemset threshold, total absence of covered instances should be impossible!\"\n        )\n\n    X_aff_covered[thenclause.features] = thenclause.values\n\n    preds = model.predict(X_aff_covered)\n    return np.shape(preds)[0] - np.sum(preds)", "completions": ["  \n```\ndef incorrectRecoursesIfThen(ifclause: Predicate, thenclause: Predicate, X_aff: DataFrame, model) -> int:\n    \"\"\"\n    Description: This function calculates the number of incorrect resources in a given context.\n\n    Args:\n        ifclause (Predicate): A predicate representing the 'if' clause of the rule.\n        thenclause (Predicate): A predicate representing the 'then' clause of the rule.\n        X_aff (DataFrame): A DataFrame containing the data to be evaluated.\n        model: A model used to predict the outcomes of the data.\n\n    Returns:\n        int: The number of incorrect resources.\n    \"\"\"\n    # Calculate the number of incorrect resources\n    incorrect_resources = 0\n    for index, row in X_aff.iterrows():\n        if ifclause(row):\n            if not thenclause(row):\n                incorrect_resources += 1\n    return incorrect_resources\n```"]}
{"id": 24, "name": "split_continuous_linear_decay_stream", "prompt": "### Role:\nYou are an expert Python programmer. You will be given some information and will generate a correct Python function that matches this information.\n\n### Task:\nBased on the provided information, generate a complete Python function. Ensure the code is logically correct, syntactically accurate, and follows best coding practices.\n\n### Function name:\nsplit_continuous_linear_decay_stream\n\n### Doc string and function signature:\n```Python\ndef split_continuous_linear_decay_stream(\n    original_stream: Iterable[DatasetExperience[TCLDataset]],\n    experience_size: int,\n    iters_per_virtual_epoch: int,\n    beta: float,\n    shuffle: bool,\n) -> CLStream[DatasetExperience[TCLDataset]] :\n    \"\"\"\n    Description: This function generates a continuous linear decay stream from a given original stream.\n\n    Args:\n        original_stream (Iterable[DatasetExperience[TCLDataset]]): An iterable of dataset experiences.\n        experience_size (int): The size of each experience in the stream.\n        iters_per_virtual_epoch (int): The number of iterations per virtual epoch.\n        beta (float): A parameter for creating task probabilities.\n        shuffle (bool): A boolean indicating whether to shuffle the original stream.\n\n    Returns:\n        CLStream[DatasetExperience[TCLDataset]]: A continuous linear decay stream of dataset experiences.\n\n    Functions used:\n        torch.cat: Concatenates the given sequence of seq tensors in the given dimension.\n        torch.zeros: Returns a tensor filled with the scalar value 0, with the shape defined by the variable argument size.\n        torch.zeros_like: Returns a tensor filled with the scalar value 0, with the same size as input.\n        torch.FloatTensor: Constructs a tensor with data type float.\n        torch.Size: Defines the size of a tensor.\n    \"\"\"\n```\n\n### Requirements:\n1. Generate the complete function implementation.\n2. Only answer the code of the generated function, formatted using Markdown code block syntax.\n3. Do not include any additional text.\n\n### Answer: (use the provided format with backticks)\n", "reference": "def split_continuous_linear_decay_stream(\n    original_stream: Iterable[DatasetExperience[TCLDataset]],\n    experience_size: int,\n    iters_per_virtual_epoch: int,\n    beta: float,\n    shuffle: bool,\n) -> CLStream[DatasetExperience[TCLDataset]]:\n\n    def _get_linear_line(start, end, direction=\"up\"):\n        if direction == \"up\":\n            return torch.FloatTensor(\n                [(i - start) / (end - start) for i in range(start, end)]\n            )\n        return torch.FloatTensor(\n            [1 - ((i - start) / (end - start)) for i in range(start, end)]\n        )\n\n    def _create_task_probs(iters, tasks, task_id, beta=3):\n        if beta <= 1:\n            peak_start = int((task_id / tasks) * iters)\n            peak_end = int(((task_id + 1) / tasks) * iters)\n            start = peak_start\n            end = peak_end\n        else:\n            start = max(int(((beta * task_id - 1) * iters) / (beta * tasks)), 0)\n            peak_start = int(((beta * task_id + 1) * iters) / (beta * tasks))\n            peak_end = int(((beta * task_id + (beta - 1)) * iters) / (beta * tasks))\n            end = min(\n                int(((beta * task_id + (beta + 1)) * iters) / (beta * tasks)), iters\n            )\n\n        probs = torch.zeros(iters, dtype=torch.float)\n        if task_id == 0:\n            probs[start:peak_start].add_(1)\n        else:\n            probs[start:peak_start] = _get_linear_line(\n                start, peak_start, direction=\"up\"\n            )\n        probs[peak_start:peak_end].add_(1)\n        if task_id == tasks - 1:\n            probs[peak_end:end].add_(1)\n        else:\n            probs[peak_end:end] = _get_linear_line(peak_end, end, direction=\"down\")\n        return probs\n\n    # Total number of iterations\n    total_iters = len(original_stream) * iters_per_virtual_epoch\n\n    # Probabilities over all iterations (sub-experiences)\n    n_experiences = len(original_stream)\n    tasks_probs_over_iterations = [\n        _create_task_probs(total_iters, n_experiences, exp_id, beta=beta)\n        for exp_id in range(n_experiences)\n    ]\n\n    # Normalize probabilities\n    normalize_probs = torch.zeros_like(tasks_probs_over_iterations[0])\n    for probs in tasks_probs_over_iterations:\n        normalize_probs.add_(probs)\n    for probs in tasks_probs_over_iterations:\n        probs.div_(normalize_probs)\n    tasks_probs_over_iterations = torch.cat(tasks_probs_over_iterations).view(\n        -1, tasks_probs_over_iterations[0].shape[0]\n    )\n    tasks_probs_over_iterations_lst = []\n    for col in range(tasks_probs_over_iterations.shape[1]):\n        tasks_probs_over_iterations_lst.append(tasks_probs_over_iterations[:, col])\n    tasks_probs_over_iterations = tasks_probs_over_iterations_lst\n\n    # Random cylic samplers over the datasets of all experiences in the stream\n    samplers = [\n        iter(CyclicSampler(len(exp.dataset), shuffle=shuffle))\n        for exp in original_stream\n    ]\n\n    # The main iterator for the stream\n    def exps_iter():\n        for sub_exp_id in range(total_iters):\n            is_first_sub_exp = is_last_sub_exp = False\n            if sub_exp_id == 0:\n                is_first_sub_exp = True\n            if sub_exp_id == total_iters - 1:\n                is_last_sub_exp = True\n\n            n_samples = torch.Size([experience_size])\n            exp_per_sample_list = Categorical(\n                probs=tasks_probs_over_iterations[sub_exp_id]\n            ).sample(n_samples)\n\n            yield create_sub_exp_from_multi_exps(\n                original_stream,\n                samplers,\n                exp_per_sample_list,\n                total_iters,\n                is_first_sub_exp,\n                is_last_sub_exp,\n            )\n\n    stream_name: str = getattr(original_stream, \"name\", \"train\")\n    return CLStream(\n        name=stream_name,\n        exps_iter=exps_iter(),\n        set_stream_info=True,\n    )", "completions": ["  \n```\ndef split_continuous_linear_decay_stream(\n    original_stream: Iterable[DatasetExperience[TCLDataset]],\n    experience_size: int,\n    iters_per_virtual_epoch: int,\n    beta: float,\n    shuffle: bool,\n) -> CLStream[DatasetExperience[TCLDataset]] :\n    \"\"\"\n    Description: This function generates a continuous linear decay stream from a given original stream.\n\n    Args:\n        original_stream (Iterable[DatasetExperience[TCLDataset]]): An iterable of dataset experiences.\n        experience_size (int): The size of each experience in the stream.\n        iters_per_virtual_epoch (int): The number of iterations per virtual epoch.\n        beta (float): A parameter for creating task probabilities.\n        shuffle (bool): A boolean indicating whether to shuffle the original stream.\n\n    Returns:\n        CLStream[DatasetExperience[TCLDataset]]: A continuous linear decay stream of dataset experiences.\n\n    Functions used:\n        torch.cat: Concatenates the given sequence of seq tensors in the given dimension.\n        torch.zeros: Returns a tensor filled with the scalar value 0, with the shape defined by the variable argument size.\n        torch.zeros_like: Returns a tensor filled with the scalar value 0, with the same size as input.\n        torch.FloatTensor: Constructs a tensor with data type float.\n        torch.Size: Defines the size of a tensor.\n    \"\"\"\n    # Initialize the stream\n    stream = CLStream[DatasetExperience[TCLDataset]]()\n\n    # Iterate over the original stream\n    for experience in original_stream:\n        # Get the task probabilities for the current experience\n        task_probs = get_task_probs(experience, beta)\n\n        # Create a new experience with the task probabilities\n        new_experience = DatasetExperience[TCLDataset](\n            experience.dataset,\n            experience.target,\n            task_probs,\n            experience.size,\n        )\n\n        # Add the new experience to the stream\n        stream.append(new_experience)\n\n    # Shuffle the stream if necessary\n    if shuffle:\n        stream.shuffle()\n\n    # Return the stream\n    return stream\n```"]}
{"id": 25, "name": "one_hot_encode", "prompt": "### Role:\nYou are an expert Python programmer. You will be given some information and will generate a correct Python function that matches this information.\n\n### Task:\nBased on the provided information, generate a complete Python function. Ensure the code is logically correct, syntactically accurate, and follows best coding practices.\n\n### Function name:\none_hot_encode\n\n### Doc string and function signature:\n```Python\ndef one_hot_encode(data: pd.DataFrame, columns=None, drop_first: bool = False, encoder: OneHotEncoder = None) :\n    \"\"\"\n    Description: This function is used to one-hot encode categorical variables in a pandas DataFrame.\n\n    Args:\n        data (pd.DataFrame): The DataFrame to be encoded.\n        columns (list-like, optional): The columns to be encoded. If None, all columns of object, string, or category dtype will be encoded.\n        drop_first (bool, optional): Whether to get k-1 dummies out of k categorical levels by removing the first level. Defaults to False.\n        encoder (OneHotEncoder, optional): An existing OneHotEncoder instance to use for encoding. If None, a new OneHotEncoder will be created.\n\n    Returns:\n        tuple: A tuple containing the encoded DataFrame and the OneHotEncoder used. The encoded DataFrame has the same index as the input DataFrame, and the OneHotEncoder is None if the input DataFrame has no categorical columns.\n    \"\"\"\n```\n\n### Requirements:\n1. Generate the complete function implementation.\n2. Only answer the code of the generated function, formatted using Markdown code block syntax.\n3. Do not include any additional text.\n\n### Answer: (use the provided format with backticks)\n", "reference": "def one_hot_encode(data: pd.DataFrame, columns=None, drop_first: bool = False, encoder: OneHotEncoder = None):\n\n    # Determine columns being encoded\n    if columns is None:\n        dtypes_to_encode = [\"object\", \"string\", \"category\"]\n        data_to_encode = data.select_dtypes(include=dtypes_to_encode)\n    elif not is_list_like(columns):\n        raise TypeError(\"Input must be a list-like for parameter `columns`\")\n    else:\n        data_to_encode = data[columns]\n\n    # If all columns are already numerical, there may be nothing to encode.\n    # In this case, return original data.\n    if len(data_to_encode.columns) == 0:\n        return data, encoder  # Encoder may be None\n\n    # Columns to keep in the result - not encoded.\n    columns_to_keep = data.columns.difference(data_to_encode.columns)\n    df_columns_to_keep = data[columns_to_keep].reset_index(drop=True)\n\n    if encoder is None:  # Create new encoder\n        drop = None\n        if drop_first:\n            drop = \"first\"\n        encoder = OneHotEncoder(drop=drop, sparse=False)  # NB sparse renamed to sparse_output in sklearn 1.2+\n\n        encoded_data = encoder.fit_transform(data_to_encode)\n\n    else:  # Use existing encoder\n        encoded_data = encoder.transform(data_to_encode)\n\n    # Convert the encoded data to a DataFrame\n    columns_encoded = encoder.get_feature_names_out(data_to_encode.columns)\n\n    df_encoded = pd.DataFrame(encoded_data, columns=columns_encoded).reset_index(drop=True)  # drop index from original\n\n    # Concatenate the encoded DataFrame with the original non-categorical columns\n    df_result = pd.concat([df_columns_to_keep, df_encoded], axis=1)\n\n    return df_result, encoder", "completions": ["  \n```\ndef one_hot_encode(data: pd.DataFrame, columns=None, drop_first: bool = False, encoder: OneHotEncoder = None) :\n    \"\"\"\n    Description: This function is used to one-hot encode categorical variables in a pandas DataFrame.\n\n    Args:\n        data (pd.DataFrame): The DataFrame to be encoded.\n        columns (list-like, optional): The columns to be encoded. If None, all columns of object, string, or category dtype will be encoded.\n        drop_first (bool, optional): Whether to get k-1 dummies out of k categorical levels by removing the first level. Defaults to False.\n        encoder (OneHotEncoder, optional): An existing OneHotEncoder instance to use for encoding. If None, a new OneHotEncoder will be created.\n\n    Returns:\n        tuple: A tuple containing the encoded DataFrame and the OneHotEncoder used. The encoded DataFrame has the same index as the input DataFrame, and the OneHotEncoder is None if the input DataFrame has no categorical columns.\n    \"\"\"\n    if encoder is None:\n        encoder = OneHotEncoder(drop=drop_first)\n    encoded_data = encoder.fit_transform(data)\n    encoded_columns = encoder.get_feature_names(data.columns)\n    encoded_data = pd.DataFrame(encoded_data, columns=encoded_columns, index=data.index)\n    return encoded_data, encoder\n```"]}
{"id": 26, "name": "mean", "prompt": "### Role:\nYou are an expert Python programmer. You will be given some information and will generate a correct Python function that matches this information.\n\n### Task:\nBased on the provided information, generate a complete Python function. Ensure the code is logically correct, syntactically accurate, and follows best coding practices.\n\n### Function name:\nmean\n\n### Doc string and function signature:\n```Python\ndef mean(self, lower=0.0, upper=np.inf) :\n    \"\"\"\n    Description: This function calculates the mean of the exponential function integrated over a specified range.\n\n    Args:\n        self (object): The instance of the class containing the data to be processed.\n        lower (float, optional): The lower limit of the integration range. Defaults to 0.0.\n        upper (float, optional): The upper limit of the integration range. Defaults to positive infinity.\n\n    Returns:\n        pd.DataFrame: A DataFrame containing the mean values of the exponential function integrated over the specified range for each index.\n    \"\"\"\n```\n\n### Requirements:\n1. Generate the complete function implementation.\n2. Only answer the code of the generated function, formatted using Markdown code block syntax.\n3. Do not include any additional text.\n\n### Answer: (use the provided format with backticks)\n", "reference": "    def mean(self, lower=0.0, upper=np.inf):\n        loc = []\n        for idx in self.index:\n            qpd = self.qpd.loc[idx, :].values[0]\n            l, _ = quad(exp_func, args=(qpd), a=lower, b=upper)\n            loc.append(l)\n        loc_arr = np.array(loc)\n        return pd.DataFrame(loc_arr, index=self.index, columns=self.columns)", "completions": ["  \n```\ndef mean(self, lower=0.0, upper=np.inf) :\n    \"\"\"\n    Description: This function calculates the mean of the exponential function integrated over a specified range.\n\n    Args:\n        self (object): The instance of the class containing the data to be processed.\n        lower (float, optional): The lower limit of the integration range. Defaults to 0.0.\n        upper (float, optional): The upper limit of the integration range. Defaults to positive infinity.\n\n    Returns:\n        pd.DataFrame: A DataFrame containing the mean values of the exponential function integrated over the specified range for each index.\n    \"\"\"\n    # Calculate the mean of the exponential function integrated over the specified range\n    mean_value = (np.exp(-self.rate * lower) - np.exp(-self.rate * upper)) / (self.rate * (upper - lower))\n\n    # Return the mean value as a DataFrame\n    return pd.DataFrame({'mean': mean_value}, index=self.index)\n```"]}
{"id": 27, "name": "allocate", "prompt": "### Role:\nYou are an expert Python programmer. You will be given some information and will generate a correct Python function that matches this information.\n\n### Task:\nBased on the provided information, generate a complete Python function. Ensure the code is logically correct, syntactically accurate, and follows best coding practices.\n\n### Function name:\nallocate\n\n### Doc string and function signature:\n```Python\ndef allocate(self, num_blocks: int) -> torch.Tensor :\n    \"\"\"\n    Description: This function allocates a specified number of blocks from the KV-cache.\n\n    Args:\n        self: The instance of the class that this function is called on.\n        num_blocks (int): The number of blocks to allocate.\n\n    Returns:\n        torch.Tensor: A tensor containing the indices of the allocated blocks.\n\n    Raises:\n        ValueError: If there are not enough free blocks in the KV-cache to allocate the specified number of blocks.\n    \"\"\"\n```\n\n### Requirements:\n1. Generate the complete function implementation.\n2. Only answer the code of the generated function, formatted using Markdown code block syntax.\n3. Do not include any additional text.\n\n### Answer: (use the provided format with backticks)\n", "reference": "    def allocate(self, num_blocks: int) -> torch.Tensor:\n        if num_blocks > self._free_blocks:\n            raise ValueError(f'Not enough free blocks in the KV-cache to allocate {num_blocks} blocks')\n\n        allocated_blocks = torch.zeros(num_blocks, dtype=torch.int32)\n        for i in range(num_blocks):\n            allocated_blocks[i] = self._head\n            self._head = self._blocks[self._head].item()\n            self._blocks[allocated_blocks[i]] = -1  # Mark as used\n            self._free_blocks -= 1\n\n        return allocated_blocks", "completions": ["  \n```\ndef allocate(self, num_blocks: int) -> torch.Tensor :\n    \"\"\"\n    Description: This function allocates a specified number of blocks from the KV-cache.\n\n    Args:\n        self: The instance of the class that this function is called on.\n        num_blocks (int): The number of blocks to allocate.\n\n    Returns:\n        torch.Tensor: A tensor containing the indices of the allocated blocks.\n\n    Raises:\n        ValueError: If there are not enough free blocks in the KV-cache to allocate the specified number of blocks.\n    \"\"\"\n    if num_blocks > self.num_free_blocks:\n        raise ValueError(f\"There are not enough free blocks in the KV-cache to allocate {num_blocks} blocks.\")\n\n    allocated_blocks = []\n    for i in range(num_blocks):\n        block_index = self.free_blocks.pop()\n        allocated_blocks.append(block_index)\n\n    return torch.tensor(allocated_blocks)\n```"]}
{"id": 28, "name": "bind_all", "prompt": "### Role:\nYou are an expert Python programmer. You will be given some information and will generate a correct Python function that matches this information.\n\n### Task:\nBased on the provided information, generate a complete Python function. Ensure the code is logically correct, syntactically accurate, and follows best coding practices.\n\n### Function name:\nbind_all\n\n### Doc string and function signature:\n```Python\ndef bind_all(self, circuit: QuantumCircuit) -> np.ndarray :\n    \"\"\"\n    Description: This function generates a numpy array of a given shape, filled with the results of the 'bind' method applied to each index of the array.\n\n    Args:\n        circuit (QuantumCircuit): The quantum circuit to bind to each index of the array.\n\n    Returns:\n        np.ndarray: A numpy array filled with the results of the 'bind' method applied to each index of the array.\n    \"\"\"\n```\n\n### Requirements:\n1. Generate the complete function implementation.\n2. Only answer the code of the generated function, formatted using Markdown code block syntax.\n3. Do not include any additional text.\n\n### Answer: (use the provided format with backticks)\n", "reference": "    def bind_all(self, circuit: QuantumCircuit) -> np.ndarray:\n        arr = np.empty(self.shape, dtype=object)\n        for idx in np.ndindex(self.shape):\n            arr[idx] = self.bind(circuit, idx)\n        return arr", "completions": ["  \n```\ndef bind_all(self, circuit: QuantumCircuit) -> np.ndarray:\n    \"\"\"\n    Description: This function generates a numpy array of a given shape, filled with the results of the 'bind' method applied to each index of the array.\n\n    Args:\n        circuit (QuantumCircuit): The quantum circuit to bind to each index of the array.\n\n    Returns:\n        np.ndarray: A numpy array filled with the results of the 'bind' method applied to each index of the array.\n    \"\"\"\n    # Generate a numpy array of the given shape\n    array = np.empty(self.shape, dtype=object)\n\n    # Iterate over each index of the array\n    for index in np.ndindex(self.shape):\n        # Bind the quantum circuit to the current index\n        bound_circuit = circuit.bind(index)\n\n        # Add the bound circuit to the array\n        array[index] = bound_circuit\n\n    return array\n```"]}
{"id": 29, "name": "lengths", "prompt": "### Role:\nYou are an expert Python programmer. You will be given some information and will generate a correct Python function that matches this information.\n\n### Task:\nBased on the provided information, generate a complete Python function. Ensure the code is logically correct, syntactically accurate, and follows best coding practices.\n\n### Function name:\nlengths\n\n### Doc string and function signature:\n```Python\ndef lengths(\n    atom_i: torch.Tensor, atom_j: torch.Tensor, distance_eps: float = 1e-3\n) -> torch.Tensor :\n    \"\"\"\n    Description: This function calculates the bond length between two atoms in a molecule.\n\n    Args:\n        atom_i (torch.Tensor): The coordinates of the first atom.\n        atom_j (torch.Tensor): The coordinates of the second atom.\n        distance_eps (float, optional): A small value added to the distance to prevent division by zero. Defaults to 1e-3.\n\n    Returns:\n        torch.Tensor: The bond length between the two atoms.\n    \"\"\"\n```\n\n### Requirements:\n1. Generate the complete function implementation.\n2. Only answer the code of the generated function, formatted using Markdown code block syntax.\n3. Do not include any additional text.\n\n### Answer: (use the provided format with backticks)\n", "reference": "def lengths(\n    atom_i: torch.Tensor, atom_j: torch.Tensor, distance_eps: float = 1e-3\n) -> torch.Tensor:\n    # Bond length of i-j\n    dX = atom_j - atom_i\n    L = torch.sqrt((dX ** 2).sum(dim=-1) + distance_eps)\n    return L", "completions": ["  \n```\ndef lengths(atom_i: torch.Tensor, atom_j: torch.Tensor, distance_eps: float = 1e-3) -> torch.Tensor:\n    \"\"\"\n    Description: This function calculates the bond length between two atoms in a molecule.\n\n    Args:\n        atom_i (torch.Tensor): The coordinates of the first atom.\n        atom_j (torch.Tensor): The coordinates of the second atom.\n        distance_eps (float, optional): A small value added to the distance to prevent division by zero. Defaults to 1e-3.\n\n    Returns:\n        torch.Tensor: The bond length between the two atoms.\n    \"\"\"\n    distance = torch.sqrt(torch.sum((atom_i - atom_j) ** 2, dim=-1))\n    return distance + distance_eps\n```"]}
{"id": 30, "name": "load", "prompt": "### Role:\nYou are an expert Python programmer. You will be given some information and will generate a correct Python function that matches this information.\n\n### Task:\nBased on the provided information, generate a complete Python function. Ensure the code is logically correct, syntactically accurate, and follows best coding practices.\n\n### Function name:\nload\n\n### Doc string and function signature:\n```Python\ndef load(cls, config_filepath: str) -> \"AIConfigRuntime\" :\n    \"\"\"\n    Description: This function loads AI configuration from a file.\n\n    Args:\n        cls (type): The class of the object to be loaded.\n        config_filepath (str): The file path of the configuration file.\n\n    Returns:\n        AIConfigRuntime: The loaded AI configuration runtime object.\n    \"\"\"\n```\n\n### Requirements:\n1. Generate the complete function implementation.\n2. Only answer the code of the generated function, formatted using Markdown code block syntax.\n3. Do not include any additional text.\n\n### Answer: (use the provided format with backticks)\n", "reference": "    def load(cls, config_filepath: str) -> \"AIConfigRuntime\":\n\n        with open(config_filepath) as file:\n            if is_yaml_ext(config_filepath):\n                yaml_data = yaml.safe_load(file)\n                data = json.dumps(yaml_data)\n            else:\n                data = file.read()\n\n        config_runtime = cls.load_json(data)\n        # set the file path. This is used when saving the config\n        config_runtime.file_path = config_filepath\n        return config_runtime", "completions": ["  \n```\ndef load(cls, config_filepath: str) -> \"AIConfigRuntime\" :\n    \"\"\"\n    Description: This function loads AI configuration from a file.\n\n    Args:\n        cls (type): The class of the object to be loaded.\n        config_filepath (str): The file path of the configuration file.\n\n    Returns:\n        AIConfigRuntime: The loaded AI configuration runtime object.\n    \"\"\"\n    # Load the configuration file\n    with open(config_filepath, \"r\") as f:\n        config_data = f.read()\n\n    # Parse the configuration data\n    config = json.loads(config_data)\n\n    # Create an instance of the AIConfigRuntime class\n    ai_config = AIConfigRuntime(config)\n\n    return ai_config\n```"]}
{"id": 31, "name": "get_stress", "prompt": "### Role:\nYou are an expert Python programmer. You will be given some information and will generate a correct Python function that matches this information.\n\n### Task:\nBased on the provided information, generate a complete Python function. Ensure the code is logically correct, syntactically accurate, and follows best coding practices.\n\n### Function name:\nget_stress\n\n### Doc string and function signature:\n```Python\ndef get_stress(\n        self,\n        strain: float,\n    ) -> float :\n    \"\"\"\n    Description: This function calculates the stress value corresponding to a given strain value using linear interpolation.\n\n    Args:\n        self: The instance of the class that this method belongs to.\n        strain (float): The strain value for which the stress value is to be calculated.\n\n    Returns:\n        float: The stress value corresponding to the given strain value.\n    \"\"\"\n```\n\n### Requirements:\n1. Generate the complete function implementation.\n2. Only answer the code of the generated function, formatted using Markdown code block syntax.\n3. Do not include any additional text.\n\n### Answer: (use the provided format with backticks)\n", "reference": "    def get_stress(\n        self,\n        strain: float,\n    ) -> float:\n        # create interpolation function\n        stress_function = interp1d(\n            x=self.strains,\n            y=self.stresses,\n            kind=\"linear\",\n            fill_value=\"extrapolate\",\n        )\n\n        return stress_function(strain)", "completions": ["  \n```\ndef get_stress(self, strain: float) -> float:\n    \"\"\"\n    Description: This function calculates the stress value corresponding to a given strain value using linear interpolation.\n\n    Args:\n        self: The instance of the class that this method belongs to.\n        strain (float): The strain value for which the stress value is to be calculated.\n\n    Returns:\n        float: The stress value corresponding to the given strain value.\n    \"\"\"\n    # Calculate the stress value using linear interpolation\n    stress = self.stress_at_strain_0 + (self.stress_at_strain_1 - self.stress_at_strain_0) * (strain - self.strain_at_stress_0) / (self.strain_at_stress_1 - self.strain_at_stress_0)\n\n    return stress\n```"]}
{"id": 32, "name": "clip", "prompt": "### Role:\nYou are an expert Python programmer. You will be given some information and will generate a correct Python function that matches this information.\n\n### Task:\nBased on the provided information, generate a complete Python function. Ensure the code is logically correct, syntactically accurate, and follows best coding practices.\n\n### Function name:\nclip\n\n### Doc string and function signature:\n```Python\ndef clip(self, box_size: Tuple[int, int]) -> None :\n    \"\"\"\n    Description: This function is used to clip the box tensor to the given box size.\n\n    Args:\n        self (object): The instance of the class containing the tensor attribute.\n        box_size (Tuple[int, int]): The size of the box to which the tensor is to be clipped.\n\n    Returns:\n        None: The function modifies the tensor attribute of the instance in-place.\n    \"\"\"\n```\n\n### Requirements:\n1. Generate the complete function implementation.\n2. Only answer the code of the generated function, formatted using Markdown code block syntax.\n3. Do not include any additional text.\n\n### Answer: (use the provided format with backticks)\n", "reference": "    def clip(self, box_size: Tuple[int, int]) -> None:\n        assert torch.isfinite(self.tensor).all(), \"Box tensor contains infinite or NaN!\"\n        h, w = box_size\n        x1 = self.tensor[:, 0].clamp(min=0, max=w)\n        y1 = self.tensor[:, 1].clamp(min=0, max=h)\n        x2 = self.tensor[:, 2].clamp(min=0, max=w)\n        y2 = self.tensor[:, 3].clamp(min=0, max=h)\n        self.tensor = torch.stack((x1, y1, x2, y2), dim=-1)", "completions": ["  \n```\ndef clip(self, box_size: Tuple[int, int]) -> None:\n    \"\"\"\n    This function is used to clip the box tensor to the given box size.\n\n    Args:\n        self (object): The instance of the class containing the tensor attribute.\n        box_size (Tuple[int, int]): The size of the box to which the tensor is to be clipped.\n\n    Returns:\n        None: The function modifies the tensor attribute of the instance in-place.\n    \"\"\"\n    # Calculate the minimum and maximum values for the box\n    min_x, min_y = 0, 0\n    max_x, max_y = box_size\n\n    # Clip the tensor values to the box size\n    self.tensor = torch.clamp(self.tensor, min=min_x, max=max_x)\n    self.tensor = torch.clamp(self.tensor, min=min_y, max=max_y)\n```"]}
{"id": 33, "name": "angles", "prompt": "### Role:\nYou are an expert Python programmer. You will be given some information and will generate a correct Python function that matches this information.\n\n### Task:\nBased on the provided information, generate a complete Python function. Ensure the code is logically correct, syntactically accurate, and follows best coding practices.\n\n### Function name:\nangles\n\n### Doc string and function signature:\n```Python\ndef angles(\n    atom_i: torch.Tensor,\n    atom_j: torch.Tensor,\n    atom_k: torch.Tensor,\n    distance_eps: float = 1e-3,\n    degrees: bool = False,\n) -> torch.Tensor :\n    \"\"\"\n    Description: This function calculates the bond angle between three atoms in a molecule.\n\n    Args:\n        atom_i (torch.Tensor): The coordinates of the first atom.\n        atom_j (torch.Tensor): The coordinates of the second atom.\n        atom_k (torch.Tensor): The coordinates of the third atom.\n        distance_eps (float, optional): A small value to prevent division by zero when normalizing vectors. Defaults to 1e-3.\n        degrees (bool, optional): If True, the angle is returned in degrees. If False, the angle is returned in radians. Defaults to False.\n\n    Returns:\n        torch.Tensor: The bond angle between the atoms.\n    \"\"\"\n```\n\n### Requirements:\n1. Generate the complete function implementation.\n2. Only answer the code of the generated function, formatted using Markdown code block syntax.\n3. Do not include any additional text.\n\n### Answer: (use the provided format with backticks)\n", "reference": "def angles(\n    atom_i: torch.Tensor,\n    atom_j: torch.Tensor,\n    atom_k: torch.Tensor,\n    distance_eps: float = 1e-3,\n    degrees: bool = False,\n) -> torch.Tensor:\n    # Bond angle of i-j-k\n    U_ji = normed_vec(atom_i - atom_j, distance_eps=distance_eps)\n    U_jk = normed_vec(atom_k - atom_j, distance_eps=distance_eps)\n    inner_prod = torch.einsum(\"bix,bix->bi\", U_ji, U_jk)\n    inner_prod = torch.clamp(inner_prod, -1, 1)\n    A = torch.acos(inner_prod)\n    if degrees:\n        A = A * 180.0 / np.pi\n    return A", "completions": ["  \n```\ndef angles(\n    atom_i: torch.Tensor,\n    atom_j: torch.Tensor,\n    atom_k: torch.Tensor,\n    distance_eps: float = 1e-3,\n    degrees: bool = False,\n) -> torch.Tensor :\n    \"\"\"\n    Description: This function calculates the bond angle between three atoms in a molecule.\n\n    Args:\n        atom_i (torch.Tensor): The coordinates of the first atom.\n        atom_j (torch.Tensor): The coordinates of the second atom.\n        atom_k (torch.Tensor): The coordinates of the third atom.\n        distance_eps (float, optional): A small value to prevent division by zero when normalizing vectors. Defaults to 1e-3.\n        degrees (bool, optional): If True, the angle is returned in degrees. If False, the angle is returned in radians. Defaults to False.\n\n    Returns:\n        torch.Tensor: The bond angle between the atoms.\n    \"\"\"\n    # Calculate the vectors between the atoms\n    vector_ij = atom_j - atom_i\n    vector_ik = atom_k - atom_i\n\n    # Normalize the vectors\n    vector_ij = vector_ij / (torch.norm(vector_ij) + distance_eps)\n    vector_ik = vector_ik / (torch.norm(vector_ik) + distance_eps)\n\n    # Calculate the dot product of the vectors\n    dot_product = torch.dot(vector_ij, vector_ik)\n\n    # Calculate the angle between the vectors\n    angle = torch.acos(dot_product)\n\n    # Convert the angle to degrees if necessary\n    if degrees:\n        angle = angle * 180 / math.pi\n\n    return angle\n```"]}
{"id": 34, "name": "return_components", "prompt": "### Role:\nYou are an expert Python programmer. You will be given some information and will generate a correct Python function that matches this information.\n\n### Task:\nBased on the provided information, generate a complete Python function. Ensure the code is logically correct, syntactically accurate, and follows best coding practices.\n\n### Function name:\nreturn_components\n\n### Doc string and function signature:\n```Python\ndef return_components(self, to_origin_space=True, include_impacts=False) :\n    \"\"\"\n    Description: This function returns a concatenation of different components of a time series analysis.\n\n    Args:\n        to_origin_space (bool): A boolean flag indicating whether to convert the components to the original space.\n        include_impacts (bool): A boolean flag indicating whether to include impacts in the returned components.\n\n    Returns:\n        pandas.DataFrame: A DataFrame containing the processed components, trend, and impacts (if specified).\n    \"\"\"\n```\n\n### Requirements:\n1. Generate the complete function implementation.\n2. Only answer the code of the generated function, formatted using Markdown code block syntax.\n3. Do not include any additional text.\n\n### Answer: (use the provided format with backticks)\n", "reference": "    def return_components(self, to_origin_space=True, include_impacts=False):\n        plot_list = []\n        plot_list.append(self.process_components(to_origin_space=to_origin_space))\n        trend = self.predicted_trend.copy()\n        trend.columns = pd.MultiIndex.from_arrays(\n            [trend.columns, ['trend'] * len(trend.columns)]\n        )\n        plot_list.append(trend)\n        if self.impacts is not None and include_impacts:\n            impacts = self.impacts.copy()\n            impacts.columns = pd.MultiIndex.from_arrays(\n                [impacts.columns, ['impacts'] * len(impacts.columns)]\n            )\n            plot_list.append(impacts)\n        return pd.concat(plot_list, axis=1)", "completions": ["  \n```\ndef return_components(self, to_origin_space=True, include_impacts=False) :\n    \"\"\"\n    Description: This function returns a concatenation of different components of a time series analysis.\n\n    Args:\n        to_origin_space (bool): A boolean flag indicating whether to convert the components to the original space.\n        include_impacts (bool): A boolean flag indicating whether to include impacts in the returned components.\n\n    Returns:\n        pandas.DataFrame: A DataFrame containing the processed components, trend, and impacts (if specified).\n    \"\"\"\n    # Generate the components\n    components = self.components_\n\n    # Convert the components to the original space\n    if to_origin_space:\n        components = self.inverse_transform(components)\n\n    # Include impacts in the returned components\n    if include_impacts:\n        impacts = self.impacts_\n        components = pd.concat([components, impacts], axis=1)\n\n    return components\n```"]}
{"id": 35, "name": "to_html", "prompt": "### Role:\nYou are an expert Python programmer. You will be given some information and will generate a correct Python function that matches this information.\n\n### Task:\nBased on the provided information, generate a complete Python function. Ensure the code is logically correct, syntactically accurate, and follows best coding practices.\n\n### Function name:\nto_html\n\n### Doc string and function signature:\n```Python\ndef to_html(self, filename=None, embed=False) :\n    \"\"\"\n    Description: This function generates an HTML representation of a scan report.\n\n    Args:\n        filename (str, optional): The name of the file to write the HTML to. If not provided, the HTML is returned as a string.\n        embed (bool, optional): Whether to embed the HTML in a larger HTML document. Defaults to False.\n\n    Returns:\n        str: The HTML representation of the scan report. If a filename is provided, the function returns None.\n    \"\"\"\n```\n\n### Requirements:\n1. Generate the complete function implementation.\n2. Only answer the code of the generated function, formatted using Markdown code block syntax.\n3. Do not include any additional text.\n\n### Answer: (use the provided format with backticks)\n", "reference": "    def to_html(self, filename=None, embed=False):\n        from ..visualization.widget import ScanReportWidget\n\n        widget = ScanReportWidget(self)\n\n        with pd.option_context(\"display.max_colwidth\", None):\n            html = widget.render_html(embed=embed)\n\n        if filename is not None:\n            with open(filename, \"w\") as f:\n                f.write(html)\n            return\n\n        return html", "completions": ["  \n```Python\ndef to_html(self, filename=None, embed=False) :\n    \"\"\"\n    Description: This function generates an HTML representation of a scan report.\n\n    Args:\n        filename (str, optional): The name of the file to write the HTML to. If not provided, the HTML is returned as a string.\n        embed (bool, optional): Whether to embed the HTML in a larger HTML document. Defaults to False.\n\n    Returns:\n        str: The HTML representation of the scan report. If a filename is provided, the function returns None.\n    \"\"\"\n    # Generate the HTML code for the scan report\n    html = \"<html><head><title>Scan Report</title></head><body>\"\n    html += \"<h1>Scan Report</h1>\"\n    html += \"<table>\"\n    for section in self.sections:\n        html += \"<tr><td>\" + section.title + \"</td><td>\" + section.description + \"</td></tr>\"\n    html += \"</table>\"\n    html += \"</body></html>\"\n\n    # If a filename is provided, write the HTML to the file\n    if filename:\n        with open(filename, \"w\") as f:\n            f.write(html)\n        return None\n    else:\n        return html\n```"]}
{"id": 36, "name": "from_measurements", "prompt": "### Role:\nYou are an expert Python programmer. You will be given some information and will generate a correct Python function that matches this information.\n\n### Task:\nBased on the provided information, generate a complete Python function. Ensure the code is logically correct, syntactically accurate, and follows best coding practices.\n\n### Function name:\nfrom_measurements\n\n### Doc string and function signature:\n```Python\ndef from_measurements(\n        cls, measurements: List[float], batch_size: int, gpu_clocks: Optional[List[float]] = None\n    ) -> \"ProfilingResult\" :\n    \"\"\"\n    Description: This function calculates various performance metrics from a list of measurements and an optional list of GPU clocks.\n\n    Args:\n        cls: The class to which this function belongs.\n        measurements (List[float]): A list of latency measurements.\n        batch_size (int): The batch size used for the measurements.\n        gpu_clocks (Optional[List[float]]): A list of GPU clock measurements. Defaults to None.\n\n    Returns:\n        ProfilingResult: An instance of the ProfilingResult class with various performance metrics calculated from the measurements.\n    \"\"\"\n```\n\n### Requirements:\n1. Generate the complete function implementation.\n2. Only answer the code of the generated function, formatted using Markdown code block syntax.\n3. Do not include any additional text.\n\n### Answer: (use the provided format with backticks)\n", "reference": "    def from_measurements(\n        cls, measurements: List[float], batch_size: int, gpu_clocks: Optional[List[float]] = None\n    ) -> \"ProfilingResult\":\n        with warnings.catch_warnings():\n            warnings.simplefilter(\"ignore\", category=RuntimeWarning)\n            if gpu_clocks:\n                avg_gpu_clock = float(np.nanmean([gpu_clock for gpu_clock in gpu_clocks if gpu_clock is not None]))\n            else:\n                avg_gpu_clock = None\n\n        avg_latency = float(np.mean(measurements))\n        return cls(\n            avg_latency=avg_latency,\n            std_latency=float(np.std(measurements)),\n            p50_latency=float(np.percentile(measurements, 50)),\n            p90_latency=float(np.percentile(measurements, 90)),\n            p95_latency=float(np.percentile(measurements, 95)),\n            p99_latency=float(np.percentile(measurements, 99)),\n            throughput=(1000 * (batch_size or 1) / avg_latency),\n            batch_size=batch_size,\n            request_count=len(measurements),\n            avg_gpu_clock=avg_gpu_clock,\n        )", "completions": ["  \n```\ndef from_measurements(\n        cls, measurements: List[float], batch_size: int, gpu_clocks: Optional[List[float]] = None\n    ) -> \"ProfilingResult\" :\n    \"\"\"\n    Description: This function calculates various performance metrics from a list of measurements and an optional list of GPU clocks.\n\n    Args:\n        cls: The class to which this function belongs.\n        measurements (List[float]): A list of latency measurements.\n        batch_size (int): The batch size used for the measurements.\n        gpu_clocks (Optional[List[float]]): A list of GPU clock measurements. Defaults to None.\n\n    Returns:\n        ProfilingResult: An instance of the ProfilingResult class with various performance metrics calculated from the measurements.\n    \"\"\"\n    # Calculate the average latency\n    average_latency = sum(measurements) / len(measurements)\n\n    # Calculate the standard deviation of latency\n    std_latency = np.std(measurements)\n\n    # Calculate the throughput\n    throughput = batch_size / average_latency\n\n    # Calculate the GPU utilization\n    if gpu_clocks is not None:\n        gpu_utilization = sum(gpu_clocks) / len(gpu_clocks)\n    else:\n        gpu_utilization = None\n\n    # Return a ProfilingResult instance with the calculated metrics\n    return ProfilingResult(average_latency, std_latency, throughput, gpu_utilization)\n```"]}
{"id": 37, "name": "if_group_average_recourse_cost_conditional", "prompt": "### Role:\nYou are an expert Python programmer. You will be given some information and will generate a correct Python function that matches this information.\n\n### Task:\nBased on the provided information, generate a complete Python function. Ensure the code is logically correct, syntactically accurate, and follows best coding practices.\n\n### Function name:\nif_group_average_recourse_cost_conditional\n\n### Doc string and function signature:\n```Python\ndef if_group_average_recourse_cost_conditional(\n    ifclause: Predicate,\n    thens: List[Tuple[Predicate, float, float]],\n) -> float :\n    \"\"\"\n    Description: This function calculates the average recourse cost conditional on a given if clause and a list of tuples.\n\n    Args:\n        ifclause (Predicate): The if clause to be evaluated.\n        thens (List[Tuple[Predicate, float, float]]): A list of tuples, where each tuple contains a predicate, a correlation, and a cost.\n\n    Returns:\n        float: The average recourse cost conditional on the if clause. If the total probability is less than or equal to zero, it returns infinity.\n\n    Notes:\n        This function uses the numpy.dot and numpy.diff functions to calculate the average recourse cost. The numpy.dot function is used to calculate the dot product of the minimum cost probability mass function (mincost_pmf) and the costs. The numpy.diff function is used to calculate the discrete difference of the minimum cost cumulative distribution function (mincost_cdf).\n    \"\"\"\n```\n\n### Requirements:\n1. Generate the complete function implementation.\n2. Only answer the code of the generated function, formatted using Markdown code block syntax.\n3. Do not include any additional text.\n\n### Answer: (use the provided format with backticks)\n", "reference": "def if_group_average_recourse_cost_conditional(\n    ifclause: Predicate,\n    thens: List[Tuple[Predicate, float, float]],\n) -> float:\n    mincost_cdf = np.array([corr for then, corr, cost in thens])\n    costs = np.array([cost for then, corr, cost in thens])\n\n    mincost_pmf = np.diff(mincost_cdf, prepend=0)\n\n    total_prob = np.sum(mincost_pmf)\n    if total_prob > 0:\n        return np.dot(mincost_pmf, costs) / np.sum(mincost_pmf)\n    else:\n        return np.inf", "completions": ["  \n```\ndef if_group_average_recourse_cost_conditional(ifclause: Predicate, thens: List[Tuple[Predicate, float, float]]) -> float:\n    \"\"\"\n    Description: This function calculates the average recourse cost conditional on a given if clause and a list of tuples.\n\n    Args:\n        ifclause (Predicate): The if clause to be evaluated.\n        thens (List[Tuple[Predicate, float, float]]): A list of tuples, where each tuple contains a predicate, a correlation, and a cost.\n\n    Returns:\n        float: The average recourse cost conditional on the if clause. If the total probability is less than or equal to zero, it returns infinity.\n\n    Notes:\n        This function uses the numpy.dot and numpy.diff functions to calculate the average recourse cost. The numpy.dot function is used to calculate the dot product of the minimum cost probability mass function (mincost_pmf) and the costs. The numpy.diff function is used to calculate the discrete difference of the minimum cost cumulative distribution function (mincost_cdf).\n    \"\"\"\n    import numpy as np\n\n    # Calculate the minimum cost probability mass function (mincost_pmf)\n    mincost_pmf = np.zeros(len(thens))\n    for i, (predicate, correlation, cost) in enumerate(thens):\n        if ifclause.implies(predicate):\n            mincost_pmf[i] = correlation\n\n    # Calculate the minimum cost cumulative distribution function (mincost_cdf)\n    mincost_cdf = np.cumsum(mincost_pmf)\n\n    # Calculate the average recourse cost\n    total_probability = np.sum(mincost_pmf)\n    if total_probability <= 0:\n        return np.inf\n    else:\n        return np.dot(mincost_pmf, costs) / total_probability\n```"]}
{"id": 38, "name": "normalize", "prompt": "### Role:\nYou are an expert Python programmer. You will be given some information and will generate a correct Python function that matches this information.\n\n### Task:\nBased on the provided information, generate a complete Python function. Ensure the code is logically correct, syntactically accurate, and follows best coding practices.\n\n### Function name:\nnormalize\n\n### Doc string and function signature:\n```Python\ndef normalize(x, axis=-1, order=2) :\n    \"\"\"\n    Description: This function normalizes the input array along a specified axis using a specified order of norm.\n\n    Args:\n        x (numpy.ndarray or tensor): The input array or tensor to be normalized.\n        axis (int, optional): The axis along which to compute the norm. Defaults to -1.\n        order (int, optional): The order of the norm. Defaults to 2.\n\n    Returns:\n        numpy.ndarray or tensor: The normalized array or tensor.\n    \"\"\"\n```\n\n### Requirements:\n1. Generate the complete function implementation.\n2. Only answer the code of the generated function, formatted using Markdown code block syntax.\n3. Do not include any additional text.\n\n### Answer: (use the provided format with backticks)\n", "reference": "def normalize(x, axis=-1, order=2):\n    from keras.src import ops\n\n    if isinstance(x, np.ndarray):\n        # NumPy input\n        norm = np.atleast_1d(np.linalg.norm(x, order, axis))\n        norm[norm == 0] = 1\n\n        # axis cannot be `None`\n        axis = axis or -1\n        return x / np.expand_dims(norm, axis)\n\n    # Backend tensor input\n    return ops.nn.normalize(x, axis=axis, order=order)", "completions": ["  \n```\ndef normalize(x, axis=-1, order=2):\n    \"\"\"\n    This function normalizes the input array along a specified axis using a specified order of norm.\n\n    Args:\n        x (numpy.ndarray or tensor): The input array or tensor to be normalized.\n        axis (int, optional): The axis along which to compute the norm. Defaults to -1.\n        order (int, optional): The order of the norm. Defaults to 2.\n\n    Returns:\n        numpy.ndarray or tensor: The normalized array or tensor.\n    \"\"\"\n    if isinstance(x, np.ndarray):\n        norm = np.linalg.norm(x, ord=order, axis=axis, keepdims=True)\n        return x / norm\n    elif isinstance(x, tf.Tensor):\n        norm = tf.norm(x, ord=order, axis=axis, keepdims=True)\n        return x / norm\n    else:\n        raise ValueError(\"Invalid input type. Expected numpy.ndarray or tf.Tensor.\")\n```"]}
{"id": 39, "name": "calculate_psnr_pt", "prompt": "### Role:\nYou are an expert Python programmer. You will be given some information and will generate a correct Python function that matches this information.\n\n### Task:\nBased on the provided information, generate a complete Python function. Ensure the code is logically correct, syntactically accurate, and follows best coding practices.\n\n### Function name:\ncalculate_psnr_pt\n\n### Doc string and function signature:\n```Python\ndef calculate_psnr_pt(img, img2, crop_border, test_y_channel=False, **kwargs) :\n    \"\"\"\n    Description: This function calculates the Peak Signal-to-Noise Ratio (PSNR) between two images.\n\n    Args:\n        img (torch.Tensor): The first image tensor.\n        img2 (torch.Tensor): The second image tensor.\n        crop_border (int): The number of pixels to crop from each border of the image.\n        test_y_channel (bool): If True, the images are converted to YCbCr format and only the Y channel is used.\n        **kwargs: Additional keyword arguments.\n\n    Returns:\n        torch.Tensor: The PSNR value between the two images.\n    \"\"\"\n```\n\n### Requirements:\n1. Generate the complete function implementation.\n2. Only answer the code of the generated function, formatted using Markdown code block syntax.\n3. Do not include any additional text.\n\n### Answer: (use the provided format with backticks)\n", "reference": "def calculate_psnr_pt(img, img2, crop_border, test_y_channel=False, **kwargs):\n\n    assert img.shape == img2.shape, f\"Image shapes are different: {img.shape}, {img2.shape}.\"\n\n    if crop_border != 0:\n        img = img[:, :, crop_border:-crop_border, crop_border:-crop_border]\n        img2 = img2[:, :, crop_border:-crop_border, crop_border:-crop_border]\n\n    if test_y_channel:\n        img = rgb2ycbcr_pt(img, y_only=True)\n        img2 = rgb2ycbcr_pt(img2, y_only=True)\n\n    img = img.to(torch.float64)\n    img2 = img2.to(torch.float64)\n\n    mse = torch.mean((img - img2) ** 2, dim=[1, 2, 3])\n    return 10.0 * torch.log10(1.0 / (mse + 1e-8))", "completions": ["  \n```\ndef calculate_psnr_pt(img, img2, crop_border, test_y_channel=False, **kwargs) :\n    \"\"\"\n    Description: This function calculates the Peak Signal-to-Noise Ratio (PSNR) between two images.\n\n    Args:\n        img (torch.Tensor): The first image tensor.\n        img2 (torch.Tensor): The second image tensor.\n        crop_border (int): The number of pixels to crop from each border of the image.\n        test_y_channel (bool): If True, the images are converted to YCbCr format and only the Y channel is used.\n        **kwargs: Additional keyword arguments.\n\n    Returns:\n        torch.Tensor: The PSNR value between the two images.\n    \"\"\"\n    # Convert images to YCbCr format if test_y_channel is True\n    if test_y_channel:\n        img = img.convert('YCbCr')\n        img2 = img2.convert('YCbCr')\n\n    # Crop borders from images\n    img = img[:, crop_border:-crop_border, crop_border:-crop_border]\n    img2 = img2[:, crop_border:-crop_border, crop_border:-crop_border]\n\n    # Calculate PSNR\n    mse = torch.mean((img - img2) ** 2)\n    if mse == 0:\n        return torch.tensor(float('inf'))\n    else:\n        return torch.tensor(20 * torch.log10(1.0 / torch.sqrt(mse)))\n```"]}
{"id": 40, "name": "fit_c_hdd_tidd", "prompt": "### Role:\nYou are an expert Python programmer. You will be given some information and will generate a correct Python function that matches this information.\n\n### Task:\nBased on the provided information, generate a complete Python function. Ensure the code is logically correct, syntactically accurate, and follows best coding practices.\n\n### Function name:\nfit_c_hdd_tidd\n\n### Doc string and function signature:\n```Python\ndef fit_c_hdd_tidd(\n    T,\n    obs,\n    settings,\n    opt_options,\n    smooth,\n    x0: Optional[ModelCoefficients] = None,\n    bnds=None,\n    initial_fit=False,\n) :\n    \"\"\"\n    Description: This function fits a model using the provided data and settings.\n\n    Args:\n        T (array-like): The temperature data.\n        obs (array-like): The observed data.\n        settings (object): The settings for the model.\n        opt_options (object): The options for the optimization.\n        smooth (bool): Whether to use a smooth model or not.\n        x0 (Optional[ModelCoefficients]): The initial coefficients for the model. Defaults to None.\n        bnds (Optional[array-like]): The bounds for the optimization. Defaults to None.\n        initial_fit (bool): Whether this is the initial fit or not. Defaults to False.\n\n    Returns:\n        OptimizationResult: The result of the optimization.\n    \"\"\"\n```\n\n### Requirements:\n1. Generate the complete function implementation.\n2. Only answer the code of the generated function, formatted using Markdown code block syntax.\n3. Do not include any additional text.\n\n### Answer: (use the provided format with backticks)\n", "reference": "def fit_c_hdd_tidd(\n    T,\n    obs,\n    settings,\n    opt_options,\n    smooth,\n    x0: Optional[ModelCoefficients] = None,\n    bnds=None,\n    initial_fit=False,\n):\n\n    if initial_fit:\n        alpha = settings.alpha_selection\n    else:\n        alpha = settings.alpha_final\n\n    if x0 is None:\n        x0 = _c_hdd_tidd_x0(T, obs, alpha, settings, smooth)\n    else:\n        x0 = _c_hdd_tidd_x0_final(T, obs, x0, alpha, settings)\n\n    if x0.model_type in [ModelType.HDD_TIDD_SMOOTH, ModelType.HDD_TIDD]:\n        tdd_beta = x0.hdd_beta\n    elif x0.model_type in [ModelType.TIDD_CDD_SMOOTH, ModelType.TIDD_CDD]:\n        tdd_beta = x0.cdd_beta\n    else:\n        raise ValueError\n\n    # limit slope based on initial regression & configurable order of magnitude\n    max_slope = np.abs(tdd_beta) + 10 ** (\n        np.log10(np.abs(tdd_beta)) + np.log10(settings.maximum_slope_OoM_scaler)\n    )\n\n    # initial fit bounded by Tmin:Tmax, final fit has minimum T segment buffer\n    T_initial, T_segment = get_T_bnds(T, settings)\n    c_hdd_bnds = T_initial if initial_fit else T_segment\n\n    # set bounds and alter coefficient guess for single slope models w/o an intercept segment\n    if not smooth and not initial_fit:\n        T_min, T_max = T_initial\n        T_min_seg, T_max_seg = T_segment\n        rtol = 1e-5\n        if x0.model_type is ModelType.HDD_TIDD and (\n            x0.hdd_bp >= T_max_seg or isclose(x0.hdd_bp, T_max_seg, rel_tol=rtol)\n        ):\n            # model is heating only, and breakpoint is approximately within max temp buffer\n            x0.intercept -= x0.hdd_bp * T_max\n            x0.hdd_bp = T_max\n            c_hdd_bnds = [T_max, T_max]\n        if x0.model_type is ModelType.TIDD_CDD and (\n            x0.cdd_bp <= T_min_seg or isclose(x0.cdd_bp, T_min_seg, rel_tol=rtol)\n        ):\n            # model is cooling only, and breakpoint is approximately within min temp buffer\n            x0.intercept -= x0.cdd_bp * T_min\n            x0.cdd_bp = T_min\n            c_hdd_bnds = [T_min, T_min]\n\n    # not known whether heating or cooling model on initial fit\n    if initial_fit:\n        c_hdd_beta_bnds = [-max_slope, max_slope]\n    # stick with heating/cooling if using existing x0\n    elif tdd_beta < 0:\n        c_hdd_beta_bnds = [-max_slope, 0]\n    else:\n        c_hdd_beta_bnds = [0, max_slope]\n\n    intercept_bnds = np.quantile(obs, [0.01, 0.99])\n    if smooth:\n        c_hdd_k_bnds = [0, 1e3]\n        bnds_0 = [c_hdd_bnds, c_hdd_beta_bnds, c_hdd_k_bnds, intercept_bnds]\n    else:\n        bnds_0 = [c_hdd_bnds, c_hdd_beta_bnds, intercept_bnds]\n\n    bnds = _c_hdd_tidd_update_bnds(bnds, bnds_0, smooth)\n    if (\n        c_hdd_bnds[0] == c_hdd_bnds[1]\n    ):  # if breakpoint bounds are identical, don't expand\n        bnds[0, :] = c_hdd_bnds\n\n    if smooth:\n        coef_id = [\"c_hdd_bp\", \"c_hdd_beta\", \"c_hdd_k\", \"intercept\"]\n        model_fcn = _c_hdd_tidd_smooth\n        weight_fcn = _c_hdd_tidd_smooth_weight\n        TSS_fcn = None\n    else:\n        coef_id = [\"c_hdd_bp\", \"c_hdd_beta\", \"intercept\"]\n        model_fcn = _c_hdd_tidd\n        weight_fcn = _c_hdd_tidd_weight\n        TSS_fcn = _c_hdd_tidd_total_sum_of_squares\n    obj_fcn = obj_fcn_decorator(\n        model_fcn, weight_fcn, TSS_fcn, T, obs, settings, alpha, coef_id, initial_fit\n    )\n    res = Optimizer(\n        obj_fcn, x0.to_np_array(), bnds, coef_id, settings, opt_options\n    ).run()\n    return res", "completions": ["  \n```\ndef fit_c_hdd_tidd(T, obs, settings, opt_options, smooth, x0=None, bnds=None, initial_fit=False):\n    \"\"\"\n    Description: This function fits a model using the provided data and settings.\n\n    Args:\n        T (array-like): The temperature data.\n        obs (array-like): The observed data.\n        settings (object): The settings for the model.\n        opt_options (object): The options for the optimization.\n        smooth (bool): Whether to use a smooth model or not.\n        x0 (Optional[ModelCoefficients]): The initial coefficients for the model. Defaults to None.\n        bnds (Optional[array-like]): The bounds for the optimization. Defaults to None.\n        initial_fit (bool): Whether this is the initial fit or not. Defaults to False.\n\n    Returns:\n        OptimizationResult: The result of the optimization.\n    \"\"\"\n    # Import necessary libraries\n    from scipy.optimize import minimize\n    from.models import ModelCoefficients\n\n    # Define the objective function\n    def objective(x):\n        # Calculate the residual sum of squares\n        residuals = obs - settings.model(T, x)\n        rss = np.sum(residuals**2)\n\n        # Calculate the smoothness penalty\n        if smooth:\n            penalty = np.sum((x - x0)**2)\n        else:\n            penalty = 0\n\n        return rss + penalty\n\n    # Define the constraints\n    constraints = []\n    if bnds is not None:\n        constraints.append({'type': 'ineq', 'fun': lambda x: bnds[0] - x})\n        constraints.append({'type': 'ineq', 'fun': lambda x: x - bnds[1]})\n\n    # Define the initial guess\n    if x0 is None:\n        x0 = np.zeros(settings.n_coefficients)\n\n    # Run the optimization\n    result = minimize(objective, x0, method='SLSQP', constraints=constraints, options=opt_options)\n\n    return result\n```"]}
{"id": 41, "name": "process", "prompt": "### Role:\nYou are an expert Python programmer. You will be given some information and will generate a correct Python function that matches this information.\n\n### Task:\nBased on the provided information, generate a complete Python function. Ensure the code is logically correct, syntactically accurate, and follows best coding practices.\n\n### Function name:\nprocess\n\n### Doc string and function signature:\n```Python\ndef process(self, offset: int = 0) -> \"GeneratorStepOutput\" :\n    \"\"\"\n    Description: This function is used to process and evolve a set of instructions based on random mutations.\n\n    Args:\n        self: The instance of the class that this method belongs to.\n        offset (int): The offset value for the process. Defaults to 0.\n\n    Returns:\n        GeneratorStepOutput: A generator that yields a tuple containing a list of formatted output and a boolean indicating whether the process is finished.\n    \"\"\"\n```\n\n### Requirements:\n1. Generate the complete function implementation.\n2. Only answer the code of the generated function, formatted using Markdown code block syntax.\n3. Do not include any additional text.\n\n### Answer: (use the provided format with backticks)\n", "reference": "    def process(self, offset: int = 0) -> \"GeneratorStepOutput\":  # type: ignore\n        instructions = []\n        mutation_no = 0\n\n        iter_no = 0\n        while len(instructions) < self.num_instructions:\n            prompts = self._apply_random_mutation(iter_no=iter_no)\n\n            generated_prompts = flatten_responses(\n                self.llm.generate(prompts, **self.llm.generation_kwargs)  # type: ignore\n            )\n            for idx, generated_prompt in enumerate(generated_prompts):\n                generated_prompt = generated_prompt.split(\"Prompt#:\")[-1].strip()\n                if self.max_length >= len(generated_prompt) >= self.min_length:  # type: ignore\n                    instructions.append(generated_prompt)\n                    self._prompts[idx] = np.random.choice(self._seed_texts)  # type: ignore\n                else:\n                    self._prompts[idx] = generated_prompt  # type: ignore\n\n            self._logger.info(\n                f\"🔄 Ran iteration {iter_no} with {len(instructions)} instructions already evolved!\"\n            )\n            iter_no += 1\n\n            if len(instructions) > self.num_instructions:\n                instructions = instructions[: self.num_instructions]\n            if len(instructions) > mutation_no:\n                mutation_no = len(instructions) - mutation_no\n\n            if not self.generate_answers and len(instructions[-mutation_no:]) > 0:\n                yield (\n                    [\n                        self.format_output(mutated_instruction)\n                        for mutated_instruction in instructions[-mutation_no:]\n                    ],\n                    len(instructions) >= self.num_instructions,\n                )\n\n        self._logger.info(f\"🎉 Finished evolving {len(instructions)} instructions!\")\n\n        if self.generate_answers:\n            self._logger.info(\n                f\"🧠 Generating answers for the {len(instructions)} evolved instructions!\"\n            )\n\n            answers = self._generate_answers(instructions)\n\n            self._logger.info(\n                f\"🎉 Finished generating answers for the {len(instructions)} evolved instructions!\"\n            )\n\n            yield (\n                [\n                    self.format_output(instruction, answer)\n                    for instruction, answer in zip(instructions, answers)\n                ],\n                True,\n            )", "completions": ["  \n```Python\ndef process(self, offset: int = 0) -> \"GeneratorStepOutput\" :\n    \"\"\"\n    Description: This function is used to process and evolve a set of instructions based on random mutations.\n\n    Args:\n        self: The instance of the class that this method belongs to.\n        offset (int): The offset value for the process. Defaults to 0.\n\n    Returns:\n        GeneratorStepOutput: A generator that yields a tuple containing a list of formatted output and a boolean indicating whether the process is finished.\n    \"\"\"\n    # Initialize the output list\n    output = []\n\n    # Iterate over the instructions\n    for instruction in self.instructions:\n        # Check if the instruction is a mutation\n        if instruction.type == \"mutation\":\n            # Apply the mutation to the instruction\n            instruction.apply_mutation()\n\n        # Add the instruction to the output list\n        output.append(instruction)\n\n    # Check if the process is finished\n    if self.is_finished():\n        # Return the output list and a boolean indicating that the process is finished\n        return output, True\n    else:\n        # Return the output list and a boolean indicating that the process is not finished\n        return output, False\n```"]}
{"id": 42, "name": "reshape", "prompt": "### Role:\nYou are an expert Python programmer. You will be given some information and will generate a correct Python function that matches this information.\n\n### Task:\nBased on the provided information, generate a complete Python function. Ensure the code is logically correct, syntactically accurate, and follows best coding practices.\n\n### Function name:\nreshape\n\n### Doc string and function signature:\n```Python\ndef reshape(self, *shape: int | Iterable[int]) -> BindingsArray :\n    \"\"\"\n    Description: This function reshapes the BindingsArray object to the specified shape.\n\n    Args:\n        *shape (int | Iterable[int]): The new shape of the array. It can be a tuple of integers or a single integer.\n\n    Returns:\n        BindingsArray: A new BindingsArray object with the reshaped data.\n\n    Raises:\n        ValueError: If the reshaping operation changes the total number of elements in the array.\n\n    Notes:\n        This function uses the numpy.prod function to calculate the product of the dimensions of the array.\n        If any of the dimensions in the shape are negative, they are replaced with the appropriate dimension\n        that would maintain the total size of the array.\n    \"\"\"\n```\n\n### Requirements:\n1. Generate the complete function implementation.\n2. Only answer the code of the generated function, formatted using Markdown code block syntax.\n3. Do not include any additional text.\n\n### Answer: (use the provided format with backticks)\n", "reference": "    def reshape(self, *shape: int | Iterable[int]) -> BindingsArray:\n        shape = shape_tuple(shape)\n        if any(dim < 0 for dim in shape):\n            # to reliably catch the ValueError, we need to manually deal with negative values\n            positive_size = np.prod([dim for dim in shape if dim >= 0], dtype=int)\n            missing_dim = self.size // positive_size\n            shape = tuple(dim if dim >= 0 else missing_dim for dim in shape)\n\n        if np.prod(shape, dtype=int) != self.size:\n            raise ValueError(\"Reshaping cannot change the total number of elements.\")\n\n        data = {ps: val.reshape(shape + val.shape[-1:]) for ps, val in self._data.items()}\n        return BindingsArray(data, shape=shape)", "completions": ["  \n```\ndef reshape(self, *shape: int | Iterable[int]) -> BindingsArray :\n    \"\"\"\n    Description: This function reshapes the BindingsArray object to the specified shape.\n\n    Args:\n        *shape (int | Iterable[int]): The new shape of the array. It can be a tuple of integers or a single integer.\n\n    Returns:\n        BindingsArray: A new BindingsArray object with the reshaped data.\n\n    Raises:\n        ValueError: If the reshaping operation changes the total number of elements in the array.\n\n    Notes:\n        This function uses the numpy.prod function to calculate the product of the dimensions of the array.\n        If any of the dimensions in the shape are negative, they are replaced with the appropriate dimension\n        that would maintain the total size of the array.\n    \"\"\"\n    import numpy as np\n\n    if len(shape) == 1 and isinstance(shape[0], Iterable):\n        shape = tuple(shape[0])\n    else:\n        shape = tuple(shape)\n\n    if np.prod(shape)!= np.prod(self.shape):\n        raise ValueError(\"Reshaping operation changes the total number of elements in the array.\")\n\n    new_data = np.reshape(self.data, shape)\n    new_bindings = [Binding(self.bindings[i], new_data[i]) for i in range(len(new_data))]\n\n    return BindingsArray(new_data, new_bindings)\n```"]}
{"id": 43, "name": "keep_most_recent_files", "prompt": "### Role:\nYou are an expert Python programmer. You will be given some information and will generate a correct Python function that matches this information.\n\n### Task:\nBased on the provided information, generate a complete Python function. Ensure the code is logically correct, syntactically accurate, and follows best coding practices.\n\n### Function name:\nkeep_most_recent_files\n\n### Doc string and function signature:\n```Python\ndef keep_most_recent_files(pattern, maxsize=int(1e12), maxfiles=1000, debug=False) :\n    \"\"\"\n    Description: This function is used to keep the most recent files based on their modification time and size.\n\n    Args:\n        pattern (str): A string that represents the pattern to match the files.\n        maxsize (int, optional): The maximum size of files to keep. Defaults to int(1e12).\n        maxfiles (int, optional): The maximum number of files to keep. Defaults to 1000.\n        debug (bool, optional): A flag to enable debug mode. Defaults to False.\n\n    Returns:\n        None\n    \"\"\"\n```\n\n### Requirements:\n1. Generate the complete function implementation.\n2. Only answer the code of the generated function, formatted using Markdown code block syntax.\n3. Do not include any additional text.\n\n### Answer: (use the provided format with backticks)\n", "reference": "def keep_most_recent_files(pattern, maxsize=int(1e12), maxfiles=1000, debug=False):\n\n    # get the list of files in the directory\n    fnames = glob.glob(pattern)\n    # compute a list of (mtime, fname, size) triples\n    files = []\n    for fname in fnames:\n        try:\n            s = os.stat(fname)\n        except FileNotFoundError:\n            continue\n        files.append((s.st_mtime, fname, s.st_size))\n    # sort the list by mtime, most recent first\n    files.sort(reverse=True)\n    # compute an accumulated total of the file sizes in order using np.cumsum\n    sizes = np.cumsum([size for mtime, fname, size in files])\n    # compute a cutoff index based on maxsize\n    cutoff = np.searchsorted(sizes, maxsize)\n    # compute a cutoff index based on maxfiles\n    cutoff = min(cutoff, maxfiles)\n    # delete the files above the cutoff in reverse order\n    for mtime, fname, size in files[cutoff:][::-1]:\n        try:\n            os.unlink(fname)\n        except FileNotFoundError:\n            pass", "completions": ["  \n```\ndef keep_most_recent_files(pattern, maxsize=int(1e12), maxfiles=1000, debug=False) :\n    \"\"\"\n    Description: This function is used to keep the most recent files based on their modification time and size.\n\n    Args:\n        pattern (str): A string that represents the pattern to match the files.\n        maxsize (int, optional): The maximum size of files to keep. Defaults to int(1e12).\n        maxfiles (int, optional): The maximum number of files to keep. Defaults to 1000.\n        debug (bool, optional): A flag to enable debug mode. Defaults to False.\n\n    Returns:\n        None\n    \"\"\"\n    import os\n    import glob\n    import time\n\n    # Get the list of files that match the pattern\n    file_list = glob.glob(pattern)\n\n    # Sort the files by their modification time\n    file_list.sort(key=os.path.getmtime)\n\n    # Get the total size of the files\n    total_size = sum(os.path.getsize(f) for f in file_list)\n\n    # Check if the total size is greater than the maximum size\n    if total_size > maxsize:\n        # Get the number of files to delete\n        num_to_delete = len(file_list) - maxfiles\n\n        # Delete the oldest files\n        for i in range(num_to_delete):\n            os.remove(file_list[i])\n\n    # Check if debug mode is enabled\n    if debug:\n        # Print the list of files that were deleted\n        print(\"Deleted files:\")\n        for f in file_list[:num_to_delete]:\n            print(f)\n```"]}
{"id": 44, "name": "linear_to_mel_weight_matrix", "prompt": "### Role:\nYou are an expert Python programmer. You will be given some information and will generate a correct Python function that matches this information.\n\n### Task:\nBased on the provided information, generate a complete Python function. Ensure the code is logically correct, syntactically accurate, and follows best coding practices.\n\n### Function name:\nlinear_to_mel_weight_matrix\n\n### Doc string and function signature:\n```Python\ndef linear_to_mel_weight_matrix(\n    *,\n    num_filters: int,\n    num_spectrogram_bins: int,\n    sample_rate: float,\n    lower_edge_hertz: float,\n    upper_edge_hertz: float,\n    dtype: np.dtype = np.float64,\n) -> ArrayLike :\n    \"\"\"\n    Description: This function computes a weight matrix for converting linear frequency to mel frequency.\n\n    Args:\n        num_filters (int): The number of mel filters.\n        num_spectrogram_bins (int): The number of spectrogram bins.\n        sample_rate (float): The sample rate of the audio.\n        lower_edge_hertz (float): The lower edge of the mel spectrum in hertz.\n        upper_edge_hertz (float): The upper edge of the mel spectrum in hertz.\n        dtype (np.dtype): The data type of the output. Default is np.float64.\n\n    Returns:\n        ArrayLike: A weight matrix for converting linear frequency to mel frequency.\n    \"\"\"\n```\n\n### Requirements:\n1. Generate the complete function implementation.\n2. Only answer the code of the generated function, formatted using Markdown code block syntax.\n3. Do not include any additional text.\n\n### Answer: (use the provided format with backticks)\n", "reference": "def linear_to_mel_weight_matrix(\n    *,\n    num_filters: int,\n    num_spectrogram_bins: int,\n    sample_rate: float,\n    lower_edge_hertz: float,\n    upper_edge_hertz: float,\n    dtype: np.dtype = np.float64,\n) -> ArrayLike:\n    freq_scale = FrequencyScale.MEL_LN\n\n    # Compute mel spectrogram bins up to nyquist frequency. Drop the 0th bin.\n    linear_freqs = np.linspace(0, sample_rate // 2, num_spectrogram_bins)[1:].astype(dtype)\n    spectrogram_bins_mel = hertz_to_warped_scale(linear_freqs, freq_scale=freq_scale)[:, None]\n\n    # Compute lower and upper bound of the output mel spectrum.\n    lower_edge_mel = hertz_to_warped_scale(lower_edge_hertz, freq_scale=freq_scale)\n    upper_edge_mel = hertz_to_warped_scale(upper_edge_hertz, freq_scale=freq_scale)\n\n    # Compute num_filters triples of (lower_edge, center, upper_edge).\n    idx = np.arange(num_filters)[:, None] + np.arange(3)[None, :]\n    band_edges_mel = np.linspace(lower_edge_mel, upper_edge_mel, num_filters + 2)[idx].astype(dtype)\n    # Split the triples up and reshape them into [1, num_filters] tensors.\n    lower_edge_mel, center_mel, upper_edge_mel = tuple(\n        np.reshape(t, [1, num_filters]) for t in np.split(band_edges_mel, 3, axis=1)\n    )\n    upper_slopes = (upper_edge_mel - spectrogram_bins_mel) / (upper_edge_mel - center_mel)\n    lower_slopes = (spectrogram_bins_mel - lower_edge_mel) / (center_mel - lower_edge_mel)\n\n    # Add back the 0th bin dropped above.\n    return np.pad(np.maximum(np.minimum(upper_slopes, lower_slopes), 0.0), [[1, 0], [0, 0]])", "completions": ["  \n```\ndef linear_to_mel_weight_matrix(\n    *,\n    num_filters: int,\n    num_spectrogram_bins: int,\n    sample_rate: float,\n    lower_edge_hertz: float,\n    upper_edge_hertz: float,\n    dtype: np.dtype = np.float64,\n) -> ArrayLike :\n    \"\"\"\n    Description: This function computes a weight matrix for converting linear frequency to mel frequency.\n\n    Args:\n        num_filters (int): The number of mel filters.\n        num_spectrogram_bins (int): The number of spectrogram bins.\n        sample_rate (float): The sample rate of the audio.\n        lower_edge_hertz (float): The lower edge of the mel spectrum in hertz.\n        upper_edge_hertz (float): The upper edge of the mel spectrum in hertz.\n        dtype (np.dtype): The data type of the output. Default is np.float64.\n\n    Returns:\n        ArrayLike: A weight matrix for converting linear frequency to mel frequency.\n    \"\"\"\n    # Compute the center frequencies of the mel bins.\n    mel_freqs = np.linspace(lower_edge_hertz, upper_edge_hertz, num_filters + 2)\n    hz_per_sbin = 0.5 * sample_rate / num_spectrogram_bins\n    mel_freqs_mat = mel_freqs[:, np.newaxis] - hz_per_sbin * np.arange(num_spectrogram_bins)\n\n    # Compute the weight matrix.\n    weights_mat = np.maximum(0.0, np.minimum(mel_freqs_mat, 1.0))\n\n    # Normalize the weights matrix.\n    weights_mat = weights_mat / np.sum(weights_mat, axis=0, keepdims=True)\n\n    return weights_mat.astype(dtype)\n```"]}
{"id": 45, "name": "_iter_collection_raw_paths", "prompt": "### Role:\nYou are an expert Python programmer. You will be given some information and will generate a correct Python function that matches this information.\n\n### Task:\nBased on the provided information, generate a complete Python function. Ensure the code is logically correct, syntactically accurate, and follows best coding practices.\n\n### Function name:\n_iter_collection_raw_paths\n\n### Doc string and function signature:\n```Python\ndef _iter_collection_raw_paths(self, master_transform, paths,\n                                   all_transforms) :\n    \"\"\"\n    Description: This function is used to iterate over a collection of raw paths and apply transformations to them.\n\n    Args:\n        master_transform (matplotlib.transforms.Transform): The master transformation to be applied to all paths.\n        paths (list): A list of paths to be transformed.\n        all_transforms (list): A list of transformations to be applied to the paths.\n\n    Returns:\n        generator: A generator that yields tuples of (path, transform + master_transform) for each iteration.\n    \"\"\"\n```\n\n### Requirements:\n1. Generate the complete function implementation.\n2. Only answer the code of the generated function, formatted using Markdown code block syntax.\n3. Do not include any additional text.\n\n### Answer: (use the provided format with backticks)\n", "reference": "    def _iter_collection_raw_paths(self, master_transform, paths,\n                                   all_transforms):\n        Npaths = len(paths)\n        Ntransforms = len(all_transforms)\n        N = max(Npaths, Ntransforms)\n\n        if Npaths == 0:\n            return\n\n        transform = transforms.IdentityTransform()\n        for i in range(N):\n            path = paths[i % Npaths]\n            if Ntransforms:\n                transform = Affine2D(all_transforms[i % Ntransforms])\n            yield path, transform + master_transform", "completions": ["  \n```\ndef _iter_collection_raw_paths(self, master_transform, paths, all_transforms):\n    \"\"\"\n    Description: This function is used to iterate over a collection of raw paths and apply transformations to them.\n\n    Args:\n        master_transform (matplotlib.transforms.Transform): The master transformation to be applied to all paths.\n        paths (list): A list of paths to be transformed.\n        all_transforms (list): A list of transformations to be applied to the paths.\n\n    Returns:\n        generator: A generator that yields tuples of (path, transform + master_transform) for each iteration.\n    \"\"\"\n    for path in paths:\n        transform = master_transform + all_transforms[path]\n        yield path, transform\n```"]}
{"id": 46, "name": "compute_pair_vector_and_distance", "prompt": "### Role:\nYou are an expert Python programmer. You will be given some information and will generate a correct Python function that matches this information.\n\n### Task:\nBased on the provided information, generate a complete Python function. Ensure the code is logically correct, syntactically accurate, and follows best coding practices.\n\n### Function name:\ncompute_pair_vector_and_distance\n\n### Doc string and function signature:\n```Python\ndef compute_pair_vector_and_distance(g: dgl.DGLGraph) :\n    \"\"\"\n    Description: This function computes the pair vector and distance between nodes in a graph.\n\n    Args:\n        g (dgl.DGLGraph): The graph on which the computation is performed.\n\n    Returns:\n        tuple: A tuple containing two tensors. The first tensor is the pair vector, and the second tensor is the distance.\n    \"\"\"\n```\n\n### Requirements:\n1. Generate the complete function implementation.\n2. Only answer the code of the generated function, formatted using Markdown code block syntax.\n3. Do not include any additional text.\n\n### Answer: (use the provided format with backticks)\n", "reference": "def compute_pair_vector_and_distance(g: dgl.DGLGraph):\n    dst_pos = g.ndata[\"pos\"][g.edges()[1]] + g.edata[\"pbc_offshift\"]\n    src_pos = g.ndata[\"pos\"][g.edges()[0]]\n    bond_vec = dst_pos - src_pos\n    bond_dist = torch.norm(bond_vec, dim=1)\n\n    return bond_vec, bond_dist", "completions": ["  \n```\ndef compute_pair_vector_and_distance(g: dgl.DGLGraph) :\n    \"\"\"\n    Description: This function computes the pair vector and distance between nodes in a graph.\n\n    Args:\n        g (dgl.DGLGraph): The graph on which the computation is performed.\n\n    Returns:\n        tuple: A tuple containing two tensors. The first tensor is the pair vector, and the second tensor is the distance.\n    \"\"\"\n    # Compute the pair vector\n    pair_vector = g.ndata['feat']\n\n    # Compute the distance\n    distance = torch.norm(pair_vector, dim=1)\n\n    return pair_vector, distance\n```"]}
{"id": 47, "name": "dump_yaml", "prompt": "### Role:\nYou are an expert Python programmer. You will be given some information and will generate a correct Python function that matches this information.\n\n### Task:\nBased on the provided information, generate a complete Python function. Ensure the code is logically correct, syntactically accurate, and follows best coding practices.\n\n### Function name:\ndump_yaml\n\n### Doc string and function signature:\n```Python\ndef dump_yaml(data: Any, stream: TextIO) :\n    \"\"\"\n    Description: This function is used to dump data into a YAML format.\n\n    Args:\n        data (Any): The data to be dumped into YAML format.\n        stream (TextIO): The stream where the YAML data will be written.\n\n    Returns:\n        None: This function does not return any value.\n    \"\"\"\n```\n\n### Requirements:\n1. Generate the complete function implementation.\n2. Only answer the code of the generated function, formatted using Markdown code block syntax.\n3. Do not include any additional text.\n\n### Answer: (use the provided format with backticks)\n", "reference": "def dump_yaml(data: Any, stream: TextIO):\n    yaml.dump(\n        data,\n        stream,\n        Dumper=_YAMLDumper,\n        default_flow_style=False,\n        allow_unicode=True,\n        sort_keys=False,\n    )", "completions": ["  \n```\ndef dump_yaml(data: Any, stream: TextIO) -> None:\n    \"\"\"\n    Description: This function is used to dump data into a YAML format.\n\n    Args:\n        data (Any): The data to be dumped into YAML format.\n        stream (TextIO): The stream where the YAML data will be written.\n\n    Returns:\n        None: This function does not return any value.\n    \"\"\"\n    import yaml\n\n    yaml.dump(data, stream)\n```"]}
{"id": 48, "name": "get_environment", "prompt": "### Role:\nYou are an expert Python programmer. You will be given some information and will generate a correct Python function that matches this information.\n\n### Task:\nBased on the provided information, generate a complete Python function. Ensure the code is logically correct, syntactically accurate, and follows best coding practices.\n\n### Function name:\nget_environment\n\n### Doc string and function signature:\n```Python\ndef get_environment(self, variables, time, lon, lat, z, profiles) :\n    \"\"\"\n    Description: This function retrieves the environment variables for a given set of positions and time.\n\n    Args:\n        self: The instance of the class.\n        variables (list): A list of variable names to retrieve.\n        time (datetime): The time at which to retrieve the variables.\n        lon (np.ndarray): An array of longitudes.\n        lat (np.ndarray): An array of latitudes.\n        z (np.ndarray): An array of depths.\n        profiles (list): A list of variable names for which to retrieve vertical profiles.\n\n    Returns:\n        tuple: A tuple containing the environment variables, vertical profiles, and a mask indicating missing values.\n            The environment variables are a numpy recarray, the vertical profiles are a dictionary of numpy arrays,\n            and the mask is a numpy array of booleans indicating missing values.\n    \"\"\"\n```\n\n### Requirements:\n1. Generate the complete function implementation.\n2. Only answer the code of the generated function, formatted using Markdown code block syntax.\n3. Do not include any additional text.\n\n### Answer: (use the provided format with backticks)\n", "reference": "    def get_environment(self, variables, time, lon, lat, z, profiles):\n        assert self.__finalized__ is True, 'The environment has not been finalized.'\n\n        self.timer_start('main loop:readers')\n        # Initialise ndarray to hold environment variables\n        dtype = [(var, np.float32) for var in variables]\n        env = np.ma.array(np.zeros(len(lon)) * np.nan, dtype=dtype)\n\n        num_elements_active = len(lon)\n\n        # Discard any existing readers which are not relevant\n        for readername, reader in self.readers.copy().items():\n            self.discard_reader_if_not_relevant(reader, time)\n\n        if 'drift:truncate_ocean_model_below_m' in self._config:\n            truncate_depth = self.get_config(\n                'drift:truncate_ocean_model_below_m')\n            if truncate_depth is not None:\n                logger.debug('Truncating ocean models below %s m' %\n                             truncate_depth)\n                z = z.copy()\n                z[z < -truncate_depth] = -truncate_depth\n                if self.required_profiles_z_range is not None:\n                    self.required_profiles_z_range = np.array(\n                        self.required_profiles_z_range)\n                    self.required_profiles_z_range[\n                        self.required_profiles_z_range <\n                        -truncate_depth] = -truncate_depth\n\n        # Initialise more lazy readers if necessary\n        missing_variables = ['missingvar']\n        while (len(missing_variables) > 0 and len(self._lazy_readers()) > 0):\n            variable_groups, reader_groups, missing_variables = \\\n                self.get_reader_groups(variables)\n            if hasattr(self, 'desired_variables'):\n                missing_variables = list(\n                    set(missing_variables) - set(self.desired_variables))\n            if len(missing_variables) > 0:\n                logger.debug('Variables not covered by any reader: ' +\n                             str(missing_variables))\n                reader = 'NotNone'\n                while reader is not None:\n                    reader = self._initialise_next_lazy_reader()\n                    if reader is not None:\n                        if self.discard_reader_if_not_relevant(reader, time):\n                            reader = None\n                    if reader is not None:\n                        if (reader.covers_time(time) and len(\n                                reader.covers_positions(lon, lat)[0]) > 0):\n                            missing_variables = list(\n                                set(missing_variables) - set(reader.variables))\n                            if len(missing_variables) == 0:\n                                break  # We cover now all variables\n\n        # For each variable/reader group:\n        variable_groups, reader_groups, missing_variables = \\\n            self.get_reader_groups(variables)\n        for variable in variables:  # Fill with fallback value if no reader\n            co = self.get_config('environment:fallback:%s' % variable)\n            if co is not None:\n                env[variable] = np.ma.ones(env[variable].shape) * co\n\n        for variable_group, reader_group in zip(variable_groups,\n                                                reader_groups):\n            logger.debug('----------------------------------------')\n            logger.debug('Variable group %s' % (str(variable_group)))\n            logger.debug('----------------------------------------')\n            missing_indices = np.array(range(len(lon)))\n            # For each reader:\n            for reader_name in reader_group:\n                logger.debug('Calling reader ' + reader_name)\n                logger.debug('----------------------------------------')\n                self.timer_start('main loop:readers:' +\n                                 reader_name.replace(':', '<colon>'))\n                reader = self.readers[reader_name]\n                if not reader.covers_time(time):\n                    logger.debug('\\tOutside time coverage of reader.')\n                    if reader_name == reader_group[-1]:\n                        if self._initialise_next_lazy_reader() is not None:\n                            logger.debug(\n                                'Missing variables: calling get_environment recursively'\n                            )\n                            return self.get_environment(\n                                variables, time, lon, lat, z, profiles)\n                    continue\n                # Fetch given variables at given positions from current reader\n                try:\n                    logger.debug('Data needed for %i elements' %\n                                 len(missing_indices))\n                    # Check if vertical profiles are requested from reader\n                    if profiles is not None:\n                        profiles_from_reader = list(\n                            set(variable_group) & set(profiles))\n                        if profiles_from_reader == []:\n                            profiles_from_reader = None\n                    else:\n                        profiles_from_reader = None\n                    env_tmp, env_profiles_tmp = \\\n                        reader.get_variables_interpolated(\n                            variable_group, profiles_from_reader,\n                            self.required_profiles_z_range, time,\n                            lon[missing_indices], lat[missing_indices],\n                            z[missing_indices], self.proj_latlon)\n\n                except NotCoveredError as e:\n                    logger.info(e)\n                    self.timer_end('main loop:readers:' +\n                                   reader_name.replace(':', '<colon>'))\n                    if reader_name == reader_group[-1]:\n                        if self._initialise_next_lazy_reader() is not None:\n                            logger.debug(\n                                'Missing variables: calling get_environment recursively'\n                            )\n                            return self.get_environment(\n                                variables, time, lon, lat, z, profiles)\n                    continue\n\n                except Exception as e:  # Unknown error\n                    # TODO:\n                    # This could e.g. be due to corrupted files or\n                    # hangig thredds-servers. A reader could be discarded\n                    # after e.g. 3 such failed attempts\n                    logger.info('========================')\n                    logger.exception(e)\n                    logger.debug(traceback.format_exc())\n                    logger.info('========================')\n\n                    reader.number_of_fails = reader.number_of_fails + 1\n                    max_fails = self.get_config('readers:max_number_of_fails')\n                    if reader.number_of_fails > max_fails:\n                        logger.warning(\n                            f'Reader {reader.name} is discarded after failing '\n                            f'more times than allowed ({max_fails})')\n                        self.discard_reader(\n                            reader,\n                            reason=f'failed more than {max_fails} times')\n\n                    self.timer_end('main loop:readers:' +\n                                   reader_name.replace(':', '<colon>'))\n                    if reader_name == reader_group[-1]:\n                        if self._initialise_next_lazy_reader() is not None:\n                            logger.debug(\n                                'Missing variables: calling get_environment recursively'\n                            )\n                            return self.get_environment(\n                                variables, time, lon, lat, z, profiles)\n                    continue\n\n                # Copy retrieved variables to env array, and mask nan-values\n                for var in variable_group:\n                    if var not in self.required_variables:\n                        logger.debug('Not returning env-variable: ' + var)\n                        continue\n                    if var not in env.dtype.names:\n                        continue  # Skipping variables that are only used to derive needed variables\n                    env[var][missing_indices] = np.ma.masked_invalid(\n                        env_tmp[var][0:len(missing_indices)]).astype('float32')\n                    if profiles_from_reader is not None and var in profiles_from_reader:\n                        if 'env_profiles' not in locals():\n                            env_profiles = env_profiles_tmp\n                        # TODO: fix to be checked\n                        if var in env_profiles and var in env_profiles_tmp:\n                            # If one profile has fewer vertical layers than\n                            # the other, we use only the overlapping part\n                            if len(env_profiles['z']) != len(\n                                    env_profiles_tmp['z']):\n                                logger.debug('Warning: different number of '\n                                             ' vertical layers: %s and %s' %\n                                             (len(env_profiles['z']),\n                                              len(env_profiles_tmp['z'])))\n                            z_ind = np.arange(\n                                np.minimum(\n                                    len(env_profiles['z']) - 1,\n                                    len(env_profiles_tmp['z']) - 1))\n                            # len(missing_indices) since 2 points might have been added and not removed\n                            env_profiles_tmp[var] = np.ma.atleast_2d(\n                                env_profiles_tmp[var])\n                            env_profiles[var][np.ix_(z_ind, missing_indices)] = \\\n                                np.ma.masked_invalid(env_profiles_tmp[var][z_ind,0:len(missing_indices)]).astype('float32')\n                            # For profiles with different numbers of layers, we extrapolate\n                            if env_profiles[var].shape[0] > 1:\n                                missingbottom = np.isnan(\n                                    env_profiles[var][-1, :])\n                                env_profiles[var][\n                                    -1, missingbottom] = env_profiles[var][\n                                        -2, missingbottom]\n\n                # Detect elements with missing data, for present reader group\n                if not hasattr(env_tmp[variable_group[0]], 'mask'):\n                    env_tmp[variable_group[0]] = np.ma.masked_invalid(\n                        env_tmp[variable_group[0]])\n                try:\n                    del combined_mask\n                except:\n                    pass\n                for var in variable_group:\n                    tmp_var = np.ma.masked_invalid(env_tmp[var])\n                    # Changed 13 Oct 2016, but uncertain of effect\n                    # TODO: to be checked\n                    #tmp_var = env_tmp[var]\n                    if 'combined_mask' not in locals():\n                        combined_mask = np.ma.getmask(tmp_var)\n                    else:\n                        combined_mask = \\\n                            np.ma.mask_or(combined_mask,\n                                          np.ma.getmask(tmp_var),\n                                          shrink=False)\n                try:\n                    if len(missing_indices) != len(combined_mask):\n                        # TODO: mask mismatch due to 2 added points\n                        raise ValueError('Mismatch of masks')\n                    missing_indices = missing_indices[combined_mask]\n                except Exception as ex:  # Not sure what is happening here\n                    logger.info('Problems setting mask on missing_indices!')\n                    logger.exception(ex)\n                if (type(missing_indices)\n                        == np.int64) or (type(missing_indices) == np.int32):\n                    missing_indices = []\n                self.timer_end('main loop:readers:' +\n                               reader_name.replace(':', '<colon>'))\n                if len(missing_indices) == 0:\n                    logger.debug('Obtained data for all elements.')\n                    break\n                else:\n                    logger.debug('Data missing for %i elements.' %\n                                 (len(missing_indices)))\n                    if len(self._lazy_readers()) > 0:\n                        if self._initialise_next_lazy_reader() is not None:\n                            logger.warning(\n                                'Missing variables: calling get_environment recursively'\n                            )\n                            return self.get_environment(\n                                variables, time, lon, lat, z, profiles)\n\n        logger.debug('---------------------------------------')\n        logger.debug('Finished processing all variable groups')\n\n        self.timer_start('main loop:readers:postprocessing')\n        #for var in self.fallback_values:\n        #    if (var not in variables) and (profiles is None\n        #                                   or var not in profiles):\n        #        continue\n        for var in variables:\n            if self.get_config(f'environment:fallback:{var}') is None:\n                continue\n            mask = env[var].mask\n            fallback = self.get_config(f'environment:fallback:{var}')\n            if any(mask == True):\n                logger.debug(\n                    '    Using fallback value %s for %s for %s elements' %\n                    (fallback, var, np.sum(mask == True)))\n                env[var][mask] = fallback\n            # Profiles\n            if profiles is not None and var in profiles:\n                if 'env_profiles' not in locals():\n                    logger.debug('Creating empty dictionary for profiles not '\n                                 'profided by any reader: ' +\n                                 str(self.required_profiles))\n                    env_profiles = {}\n                    env_profiles['z'] = \\\n                        np.array(self.required_profiles_z_range)[::-1]\n                if var not in env_profiles:\n                    logger.debug(\n                        '      Using fallback value %s for %s for all profiles'\n                        % (fallback, var))\n                    env_profiles[var] = fallback*\\\n                        np.ma.ones((len(env_profiles['z']), num_elements_active))\n                else:\n                    mask = env_profiles[var].mask\n                    num_masked_values_per_element = np.sum(mask == True)\n                    num_missing_profiles = np.sum(num_masked_values_per_element\n                                                  == len(env_profiles['z']))\n                    env_profiles[var][mask] = fallback\n                    logger.debug(\n                        '      Using fallback value %s for %s for %s profiles'\n                        % (\n                            fallback,\n                            var,\n                            num_missing_profiles,\n                        ))\n                    num_missing_individual = np.sum(\n                        num_masked_values_per_element >\n                        0) - num_missing_profiles\n                    if num_missing_individual > 0:\n                        logger.debug(\n                            '        ...plus %s individual points in other profiles'\n                            % num_missing_individual)\n\n        #######################################################\n        # Some extra checks of units and realistic magnitude\n        #######################################################\n        if 'sea_water_temperature' in variables:\n            t_kelvin = np.where(env['sea_water_temperature'] > 100)[0]\n            if len(t_kelvin) > 0:\n                logger.warning(\n                    'Converting temperatures from Kelvin to Celcius')\n                env['sea_water_temperature'][\n                    t_kelvin] = env['sea_water_temperature'][t_kelvin] - 273.15\n                if 'env_profiles' in locals(\n                ) and 'sea_water_temperature' in env_profiles.keys():\n                    env_profiles['sea_water_temperature'][:,t_kelvin] = \\\n                      env_profiles['sea_water_temperature'][:,t_kelvin] - 273.15\n\n        ############################################################\n        # Parameterisation of unavailable variables\n        # TODO: use instead \"environment mapping\" mechanism for this\n        #############################################################\n        if 'drift:use_tabularised_stokes_drift' in self._config and self.get_config(\n                'drift:use_tabularised_stokes_drift') is True:\n            if 'x_wind' in variables:\n                if 'sea_surface_wave_stokes_drift_x_velocity' not in variables or (\n                        env['sea_surface_wave_stokes_drift_x_velocity'].max()\n                        == 0 and\n                        env['sea_surface_wave_stokes_drift_y_velocity'].max()\n                        == 0):\n                    logger.debug('Calculating parameterised stokes drift')\n                    env['sea_surface_wave_stokes_drift_x_velocity'], \\\n                    env['sea_surface_wave_stokes_drift_y_velocity'] = \\\n                        pm.wave_stokes_drift_parameterised((env['x_wind'], env['y_wind']),\n                            self.get_config('drift:tabularised_stokes_drift_fetch'))\n\n                if (env['sea_surface_wave_significant_height'].max() == 0):\n                    logger.debug(\n                        'Calculating parameterised significant wave height')\n                    env['sea_surface_wave_significant_height'] = \\\n                        pm.wave_significant_height_parameterised((env['x_wind'], env['y_wind']),\n                        self.get_config('drift:tabularised_stokes_drift_fetch'))\n\n        #############################\n        # Add uncertainty/diffusion\n        #############################\n        # Current\n        if 'x_sea_water_velocity' in variables and \\\n                'y_sea_water_velocity' in variables:\n            std = self.get_config('drift:current_uncertainty')\n            if std > 0:\n                logger.debug('Adding uncertainty for current: %s m/s' % std)\n                env['x_sea_water_velocity'] += np.random.normal(\n                    0, std, num_elements_active)\n                env['y_sea_water_velocity'] += np.random.normal(\n                    0, std, num_elements_active)\n            std = self.get_config('drift:current_uncertainty_uniform')\n            if std > 0:\n                logger.debug('Adding uncertainty for current: %s m/s' % std)\n                env['x_sea_water_velocity'] += np.random.uniform(\n                    -std, std, num_elements_active)\n                env['y_sea_water_velocity'] += np.random.uniform(\n                    -std, std, num_elements_active)\n        # Wind\n        if 'x_wind' in variables and 'y_wind' in variables:\n            std = self.get_config('drift:wind_uncertainty')\n            if std > 0:\n                logger.debug('Adding uncertainty for wind: %s m/s' % std)\n                env['x_wind'] += np.random.normal(0, std,\n                                                  num_elements_active)\n                env['y_wind'] += np.random.normal(0, std,\n                                                  num_elements_active)\n\n        #####################\n        # Diagnostic output\n        #####################\n        if len(env) > 0:\n            logger.debug('------------ SUMMARY -------------')\n            for var in variables:\n                logger.debug('    %s: %g (min) %g (max)' %\n                             (var, env[var].min(), env[var].max()))\n            logger.debug('---------------------------------')\n\n        # Prepare array indiciating which elements contain any invalid values\n        missing = np.ma.masked_invalid(env[variables[0]]).mask\n        for var in variables[1:]:\n            missing = np.ma.mask_or(missing,\n                                    np.ma.masked_invalid(env[var]).mask,\n                                    shrink=False)\n\n        # Convert dictionary to recarray and return\n        if 'env_profiles' not in locals():\n            env_profiles = None\n\n        # Convert masked arrays to regular arrays for increased performance\n        env = np.array(env)\n        if env_profiles is not None:\n            for var in env_profiles:\n                env_profiles[var] = np.array(env_profiles[var])\n\n        self.timer_end('main loop:readers:postprocessing')\n        self.timer_end('main loop:readers')\n\n        return env.view(np.recarray), env_profiles, missing", "completions": ["  \n```\ndef get_environment(self, variables, time, lon, lat, z, profiles) :\n    \"\"\"\n    Description: This function retrieves the environment variables for a given set of positions and time.\n\n    Args:\n        self: The instance of the class.\n        variables (list): A list of variable names to retrieve.\n        time (datetime): The time at which to retrieve the variables.\n        lon (np.ndarray): An array of longitudes.\n        lat (np.ndarray): An array of latitudes.\n        z (np.ndarray): An array of depths.\n        profiles (list): A list of variable names for which to retrieve vertical profiles.\n\n    Returns:\n        tuple: A tuple containing the environment variables, vertical profiles, and a mask indicating missing values.\n            The environment variables are a numpy recarray, the vertical profiles are a dictionary of numpy arrays,\n            and the mask is a numpy array of booleans indicating missing values.\n    \"\"\"\n    # Implement the function here\n    pass\n```"]}
{"id": 49, "name": "visualize_audio", "prompt": "### Role:\nYou are an expert Python programmer. You will be given some information and will generate a correct Python function that matches this information.\n\n### Task:\nBased on the provided information, generate a complete Python function. Ensure the code is logically correct, syntactically accurate, and follows best coding practices.\n\n### Function name:\nvisualize_audio\n\n### Doc string and function signature:\n```Python\ndef visualize_audio(self, clip_id) :\n    \"\"\"\n    Description: This function visualizes an audio clip using matplotlib.pyplot.subplots.\n\n    Args:\n        self: An instance of the class containing the function.\n        clip_id (str): The ID of the audio clip to visualize. If None, a random clip is chosen.\n\n    Returns:\n        None: This function does not return any value. It displays the audio waveform and Mel spectrogram in a matplotlib figure.\n    \"\"\"\n```\n\n### Requirements:\n1. Generate the complete function implementation.\n2. Only answer the code of the generated function, formatted using Markdown code block syntax.\n3. Do not include any additional text.\n\n### Answer: (use the provided format with backticks)\n", "reference": "def visualize_audio(self, clip_id):\n    if clip_id is None:  # Use the local variable\n        clip_id = np.random.choice(\n            list(self._index[\"clips\"].keys())\n        )  # Modify the local variable\n    clip = self.clip(clip_id)  # Use the local variable\n\n    stop_event = threading.Event()\n    current_time_lock = threading.Lock()\n\n    audio, sr = clip.audio\n    duration = len(audio) / sr\n\n    audio = audio / np.max(np.abs(audio))\n\n    # Convert to int16 for playback\n    audio_playback = np.int16(audio * 32767)\n\n    audio_segment = AudioSegment(\n        audio_playback.tobytes(), frame_rate=sr, sample_width=2, channels=1\n    )\n\n    # Truncate the audio to a maximum duration (e.g., 1 minute)\n    max_duration_secs = 60\n    print(\"Truncating audio to the first 1 minute if less than 1 minute.\")\n    audio = audio[: int(max_duration_secs * sr)]\n    duration = min(duration, max_duration_secs)\n\n    # Compute the Mel spectrogram\n    S = librosa.feature.melspectrogram(y=audio, sr=sr)\n    log_S = librosa.power_to_db(S, ref=np.max)\n\n    # Update the figure and axes to show both plots\n    fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(9, 4))\n    # Plotting the waveform\n    ax1.plot(np.linspace(0, duration, len(audio)), audio)\n    ax1.set_title(f\"Audio waveform for clip: {clip_id}\", fontsize=8)\n    ax1.set_xlabel(\"Time (s)\", fontsize=8)\n    ax1.set_ylabel(\"Amplitude\", fontsize=8)\n    ax1.set_xlim(0, duration)\n    (line1,) = ax1.plot([0, 0], [min(audio), max(audio)], color=\"#C9C9C9\")\n\n    # Adjusting the font size for axis labels\n    for label in ax1.get_xticklabels() + ax1.get_yticklabels():\n        label.set_fontsize(8)\n\n    # Plotting the Mel spectrogram\n    im = librosa.display.specshow(log_S, sr=sr, x_axis=\"time\", y_axis=\"mel\", ax=ax2)\n    ax2.set_title(\"Mel spectrogram\", fontsize=8)\n    ax2.set_xlim(0, duration)\n    (line2,) = ax2.plot([0, 0], ax2.get_ylim(), color=\"#126782\", linestyle=\"--\")\n\n    # Adjusting the font size for axis labels\n    ax2.set_xlabel(\"Time (s)\", fontsize=8)\n    ax2.set_ylabel(\"Hz\", fontsize=8)\n    for label in ax2.get_xticklabels() + ax2.get_yticklabels():\n        label.set_fontsize(8)\n\n    fig.subplots_adjust(right=0.8)  # Adjust the right side of the layout\n\n    # Add the colorbar\n    bbox = ax2.get_position()\n    cbar_ax = fig.add_axes([bbox.x1 + 0.01, bbox.y0, 0.015, bbox.height])\n    cbar = fig.colorbar(im, cax=cbar_ax)\n    cbar.set_label(\"dB\", rotation=270, labelpad=15, fontsize=8)\n    cbar.ax.tick_params(labelsize=8)  # Set the size of the tick labels\n\n    # Ensure the tight_layout does not overlap the axes with colorbar\n    plt.tight_layout(rect=[0, 0, 0.8, 1])\n\n    playing = [False]\n    current_time = [0.0]\n    play_thread = [None]\n\n    # Create UI elements\n    slider = FloatSlider(\n        value=0.0,\n        min=0.0,\n        max=duration,\n        step=0.1,\n        description=\"Seek:\",\n        tooltip=\"Drag the slider to a specific point in the audio to play from that time.\",\n    )\n    play_pause_button = Button(description=\"► Play\")\n    reset_button = Button(description=\"Reset\")\n\n    # Setting up event handlers\n    play_pause_button.on_click(\n        lambda b: on_play_pause_clicked(\n            playing,\n            current_time,\n            play_thread,\n            stop_event,\n            play_pause_button,\n            lambda start_time: play_segment(audio_segment, start_time, stop_event, sr),\n            lambda: update_line(\n                playing, current_time, duration, current_time_lock, line1, line2, fig\n            ),\n        )\n    )\n    reset_button.on_click(\n        lambda b: on_reset_clicked(\n            playing,\n            current_time,\n            play_thread,\n            stop_event,\n            line1,\n            line2,\n            slider,\n            play_pause_button,\n            fig,\n            current_time_lock,\n        )\n    )\n    slider.observe(\n        lambda change: on_slider_changed(\n            change,\n            playing,\n            current_time,\n            play_thread,\n            stop_event,\n            line1,\n            line2,\n            fig,\n            current_time_lock,\n        ),\n        names=\"value\",\n    )\n\n    # Display the UI elements\n    slider_label = Label(\"Drag the slider to navigate through the audio:\")\n    display(VBox([HBox([play_pause_button, reset_button]), slider_label, slider]))", "completions": ["  \n```\ndef visualize_audio(self, clip_id) :\n    \"\"\"\n    Description: This function visualizes an audio clip using matplotlib.pyplot.subplots.\n\n    Args:\n        self: An instance of the class containing the function.\n        clip_id (str): The ID of the audio clip to visualize. If None, a random clip is chosen.\n\n    Returns:\n        None: This function does not return any value. It displays the audio waveform and Mel spectrogram in a matplotlib figure.\n    \"\"\"\n    # Load the audio clip\n    clip = self.load_audio_clip(clip_id)\n\n    # Create a figure with two subplots\n    fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(10, 6))\n\n    # Plot the audio waveform on the first subplot\n    ax1.plot(clip.samples, clip.channels)\n    ax1.set_xlabel('Time (s)')\n    ax1.set_ylabel('Amplitude')\n    ax1.set_title('Audio Waveform')\n\n    # Plot the Mel spectrogram on the second subplot\n    mel_spectrogram = self.compute_mel_spectrogram(clip)\n    ax2.imshow(mel_spectrogram, cmap='inferno')\n    ax2.set_xlabel('Time (s)')\n    ax2.set_ylabel('Frequency (Hz)')\n    ax2.set_title('Mel Spectrogram')\n\n    # Show the figure\n    plt.show()\n```"]}
{"id": 50, "name": "merge_adapter_weights", "prompt": "### Role:\nYou are an expert Python programmer. You will be given some information and will generate a correct Python function that matches this information.\n\n### Task:\nBased on the provided information, generate a complete Python function. Ensure the code is logically correct, syntactically accurate, and follows best coding practices.\n\n### Function name:\nmerge_adapter_weights\n\n### Doc string and function signature:\n```Python\ndef merge_adapter_weights(\n    model_weights: Dict[str, torch.Tensor], \n    adapter_weights: Dict[str, torch.Tensor], \n    adapter_config: LoraConfig\n) -> Tuple[Dict[str, torch.Tensor], Set[str]] :\n    \"\"\"\n    Description: This function merges adapter weights into a model's weights.\n\n    Args:\n        model_weights (Dict[str, torch.Tensor]): A dictionary mapping weight names to their corresponding tensors.\n        adapter_weights (Dict[str, torch.Tensor]): A dictionary mapping adapter weight names to their corresponding tensors.\n        adapter_config (LoraConfig): Configuration for the adapter.\n\n    Returns:\n        Tuple[Dict[str, torch.Tensor], Set[str]]: A tuple containing the merged weights and a set of processed adapter weight names.\n    \"\"\"\n```\n\n### Requirements:\n1. Generate the complete function implementation.\n2. Only answer the code of the generated function, formatted using Markdown code block syntax.\n3. Do not include any additional text.\n\n### Answer: (use the provided format with backticks)\n", "reference": "def merge_adapter_weights(\n    model_weights: Dict[str, torch.Tensor], \n    adapter_weights: Dict[str, torch.Tensor], \n    adapter_config: LoraConfig\n) -> Tuple[Dict[str, torch.Tensor], Set[str]]:\n    module_mapping = defaultdict(dict)\n    processed_adapter_weight_names = set()\n\n    # map the original tensor names to their adapter counterparts\n    for weight_name in model_weights:\n        end_idx = weight_name.rfind(\".weight\")\n        key = weight_name[:end_idx]\n        for adapter_weight_name in adapter_weights:\n            if key in adapter_weight_name:\n                # example value: 'base_model.model.model.layers.10.self_attn.v_proj.lora_B.weight'\n                # matrix_type gets the second to last element in the module name, i.e. 'lora_B'\n                matrix_type = adapter_weight_name.split(\".\")[-2]\n                module_mapping[weight_name][matrix_type] = adapter_weight_name\n                processed_adapter_weight_names.add(adapter_weight_name)\n\n    # merge adapter weights into model weights\n    merged_weights = {}\n    for weight_name, adapter_weight_names in tqdm(\n        module_mapping.items(), desc=\"Merging adapter weights\", total=len(module_mapping)):\n\n        # TODO: support adapter types beyond LoRA\n        # TODO: put this on GPU if it is available. This should greatly speedup compute_delta_weight\n        lora_A = adapter_weights[adapter_weight_names[\"lora_A\"]]\n        lora_B = adapter_weights[adapter_weight_names[\"lora_B\"]]\n        delta_weight = compute_delta_weight(\n            lora_A,\n            lora_B,\n            adapter_config.fan_in_fan_out,\n            adapter_config.lora_alpha,\n            adapter_config.r,\n            uses_rslora=uses_rslora(adapter_config),\n        )\n\n        # transpose delta weight if necessary\n        # TODO(geoffrey): I believe this is required when using Conv1D layers (gpt2).\n        # We can likely take this out once we've switched to using Linear layers.\n        if (delta_weight.shape != model_weights[weight_name].shape and \n            delta_weight.T.shape == model_weights[weight_name].shape):\n            delta_weight = delta_weight.T\n        merged_weights[weight_name] = model_weights[weight_name] + delta_weight\n    return merged_weights, processed_adapter_weight_names", "completions": ["  \n```python\ndef merge_adapter_weights(\n    model_weights: Dict[str, torch.Tensor], \n    adapter_weights: Dict[str, torch.Tensor], \n    adapter_config: LoraConfig\n) -> Tuple[Dict[str, torch.Tensor], Set[str]] :\n    \"\"\"\n    Description: This function merges adapter weights into a model's weights.\n\n    Args:\n        model_weights (Dict[str, torch.Tensor]): A dictionary mapping weight names to their corresponding tensors.\n        adapter_weights (Dict[str, torch.Tensor]): A dictionary mapping adapter weight names to their corresponding tensors.\n        adapter_config (LoraConfig): Configuration for the adapter.\n\n    Returns:\n        Tuple[Dict[str, torch.Tensor], Set[str]]: A tuple containing the merged weights and a set of processed adapter weight names.\n    \"\"\"\n    # Initialize the merged weights dictionary\n    merged_weights = {}\n\n    # Iterate over the model weights\n    for weight_name, weight_tensor in model_weights.items():\n        # Check if the weight name is in the adapter weights\n        if weight_name in adapter_weights:\n            # Get the adapter weight tensor\n            adapter_weight_tensor = adapter_weights[weight_name]\n\n            # Check if the adapter weight tensor has the same shape as the model weight tensor\n            if adapter_weight_tensor.shape == weight_tensor.shape:\n                # Add the adapter weight tensor to the merged weights dictionary\n                merged_weights[weight_name] = adapter_weight_tensor\n\n    # Return the merged weights and the processed adapter weight names\n    return merged_weights, set(adapter_weights.keys())\n```"]}
{"id": 51, "name": "download_from_http_fileserver", "prompt": "### Role:\nYou are an expert Python programmer. You will be given some information and will generate a correct Python function that matches this information.\n\n### Task:\nBased on the provided information, generate a complete Python function. Ensure the code is logically correct, syntactically accurate, and follows best coding practices.\n\n### Function name:\ndownload_from_http_fileserver\n\n### Doc string and function signature:\n```Python\ndef download_from_http_fileserver(\n    url: str,\n    save_dir: str,\n    ignore_cert: bool = False,\n) :\n    \"\"\"\n    Description: This function is used to download files from an HTTP file server.\n\n    Args:\n        url (str): The URL of the file to be downloaded.\n        save_dir (str): The directory where the downloaded file will be saved.\n        ignore_cert (bool, optional): If True, it will ignore SSL certificate verification warnings. Defaults to False.\n\n    Returns:\n        None\n    \"\"\"\n```\n\n### Requirements:\n1. Generate the complete function implementation.\n2. Only answer the code of the generated function, formatted using Markdown code block syntax.\n3. Do not include any additional text.\n\n### Answer: (use the provided format with backticks)\n", "reference": "def download_from_http_fileserver(\n    url: str,\n    save_dir: str,\n    ignore_cert: bool = False,\n):\n    with requests.Session() as session:\n        # Temporarily suppress noisy SSL certificate verification warnings if ignore_cert is set to True\n        with warnings.catch_warnings():\n            if ignore_cert:\n                warnings.simplefilter('ignore', category=InsecureRequestWarning)\n\n            _recursive_download(session,\n                                url,\n                                '',\n                                save_dir,\n                                ignore_cert=ignore_cert)", "completions": ["  \n```\ndef download_from_http_fileserver(url: str, save_dir: str, ignore_cert: bool = False):\n    \"\"\"\n    This function is used to download files from an HTTP file server.\n\n    Args:\n        url (str): The URL of the file to be downloaded.\n        save_dir (str): The directory where the downloaded file will be saved.\n        ignore_cert (bool, optional): If True, it will ignore SSL certificate verification warnings. Defaults to False.\n\n    Returns:\n        None\n    \"\"\"\n    import requests\n    from requests.adapters import HTTPAdapter\n    from urllib3.util.retry import Retry\n\n    session = requests.Session()\n    retry = Retry(connect=3, backoff_factor=0.5)\n    adapter = HTTPAdapter(max_retries=retry)\n    session.mount('http://', adapter)\n    session.mount('https://', adapter)\n\n    response = session.get(url, verify=not ignore_cert)\n    response.raise_for_status()\n\n    with open(save_dir, 'wb') as f:\n        f.write(response.content)\n```"]}
{"id": 52, "name": "compute_quantile_df", "prompt": "### Role:\nYou are an expert Python programmer. You will be given some information and will generate a correct Python function that matches this information.\n\n### Task:\nBased on the provided information, generate a complete Python function. Ensure the code is logically correct, syntactically accurate, and follows best coding practices.\n\n### Function name:\ncompute_quantile_df\n\n### Doc string and function signature:\n```Python\ndef compute_quantile_df(x: List[int], a: float, b: float) :\n    \"\"\"\n    Description: This function computes the quantile of a given list of integers and returns a DataFrame with the quantile information.\n\n    Args:\n        x (List[int]): A list of integers.\n        a (float): The lower quantile value, should be in the range [0, 1] and less than or equal to b.\n        b (float): The upper quantile value, should be in the range [0, 1] and greater than or equal to a.\n\n    Returns:\n        pd.DataFrame: A DataFrame with columns 'length', 'count', and 'data_type'. The 'length' column contains the values from the input list, 'count' column contains the count of each value, and 'data_type' column contains the type of data (first a% quantile, last b% quantile, or a%-b% quantile).\n    \"\"\"\n```\n\n### Requirements:\n1. Generate the complete function implementation.\n2. Only answer the code of the generated function, formatted using Markdown code block syntax.\n3. Do not include any additional text.\n\n### Answer: (use the provided format with backticks)\n", "reference": "def compute_quantile_df(x: List[int], a: float, b: float):\n    if not x:\n        raise ValueError(\"Input list x is empty\")\n\n    if not 0 <= a <= b <= 1:\n        raise ValueError(\n            \"Values of a and b must be in [0, 1] \"\n            \"and a should be less than or equal to b\"\n        )\n\n    x_axis_description = \"length\"\n    df = pd.DataFrame(x, columns=[x_axis_description])\n    df[\"count\"] = 1\n    df_quantile = (\n        df.groupby([x_axis_description])\n        .sum()\n        .reset_index()\n        .sort_values(by=x_axis_description)[[x_axis_description, \"count\"]]\n    )\n    sorted_data = sorted(x)\n    first_quantile = sorted_data[int(len(sorted_data) * a)]\n    last_quantile = sorted_data[-int(len(sorted_data) * (1 - b))]\n\n    df_first = df_quantile.loc[df_quantile[x_axis_description] <= first_quantile].copy()\n    df_first[\"data_type\"] = f\"first {int(a * 100)}% quantile\"\n    df_last = df_quantile.loc[df_quantile[x_axis_description] >= last_quantile].copy()\n    df_last[\"data_type\"] = f\"last {100 - int(b * 100)}% quantile\"\n    df_quantile[\"data_type\"] = f\"{int(a * 100)}%-{int(b * 100)}% quantile\"\n    middle_quantile_min = max(0, len(df_first) - 1)\n    middle_quantile_max = (\n        min(len(df_quantile), (len(df_quantile) - len(df_last) - 1)) + 1\n    )\n    df_quantile = pd.concat(\n        [\n            df_first,\n            df_quantile.loc[middle_quantile_min:middle_quantile_max],\n            df_last,\n        ]\n    )\n    return df_quantile", "completions": ["  \n```\ndef compute_quantile_df(x: List[int], a: float, b: float) :\n    \"\"\"\n    Description: This function computes the quantile of a given list of integers and returns a DataFrame with the quantile information.\n\n    Args:\n        x (List[int]): A list of integers.\n        a (float): The lower quantile value, should be in the range [0, 1] and less than or equal to b.\n        b (float): The upper quantile value, should be in the range [0, 1] and greater than or equal to a.\n\n    Returns:\n        pd.DataFrame: A DataFrame with columns 'length', 'count', and 'data_type'. The 'length' column contains the values from the input list, 'count' column contains the count of each value, and 'data_type' column contains the type of data (first a% quantile, last b% quantile, or a%-b% quantile).\n    \"\"\"\n    import pandas as pd\n    import numpy as np\n\n    # Calculate the quantile values\n    quantiles = np.quantile(x, [a, b])\n\n    # Create a DataFrame with the quantile information\n    df = pd.DataFrame({'length': x, 'count': np.ones(len(x)), 'data_type': np.zeros(len(x))})\n\n    # Set the data type for the first a% quantile\n    df.loc[df['length'] <= quantiles[0], 'data_type'] = 1\n\n    # Set the data type for the last b% quantile\n    df.loc[df['length'] >= quantiles[1], 'data_type'] = 2\n\n    # Set the data type for the a%-b% quantile\n    df.loc[(df['length'] > quantiles[0]) & (df['length'] < quantiles[1]), 'data_type'] = 3\n\n    return df\n```"]}
{"id": 53, "name": "sparsify_clebsch_gordon", "prompt": "### Role:\nYou are an expert Python programmer. You will be given some information and will generate a correct Python function that matches this information.\n\n### Task:\nBased on the provided information, generate a complete Python function. Ensure the code is logically correct, syntactically accurate, and follows best coding practices.\n\n### Function name:\nsparsify_clebsch_gordon\n\n### Doc string and function signature:\n```Python\ndef sparsify_clebsch_gordon(\n    cg: torch.Tensor,\n) -> tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor] :\n    \"\"\"\n    Description: This function is used to sparsify a Clebsch-Gordon tensor.\n\n    Args:\n        cg (torch.Tensor): The Clebsch-Gordon tensor to be sparsified.\n\n    Returns:\n        tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]: A tuple containing the sparsified Clebsch-Gordon tensor, \n        the indices of the input tensor for the first dimension, the indices of the input tensor for the second dimension, \n        and the indices of the output tensor.\n    \"\"\"\n```\n\n### Requirements:\n1. Generate the complete function implementation.\n2. Only answer the code of the generated function, formatted using Markdown code block syntax.\n3. Do not include any additional text.\n\n### Answer: (use the provided format with backticks)\n", "reference": "def sparsify_clebsch_gordon(\n    cg: torch.Tensor,\n) -> tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]:\n    idx = torch.nonzero(cg)\n    idx_in_1, idx_in_2, idx_out = torch.split(idx, 1, dim=1)\n    idx_in_1, idx_in_2, idx_out = (\n        idx_in_1[:, 0],\n        idx_in_2[:, 0],\n        idx_out[:, 0],\n    )\n    cg_sparse = cg[idx_in_1, idx_in_2, idx_out]\n    return cg_sparse, idx_in_1, idx_in_2, idx_out", "completions": ["  \n```\ndef sparsify_clebsch_gordon(cg: torch.Tensor) -> tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]:\n    \"\"\"\n    Description: This function is used to sparsify a Clebsch-Gordon tensor.\n\n    Args:\n        cg (torch.Tensor): The Clebsch-Gordon tensor to be sparsified.\n\n    Returns:\n        tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]: A tuple containing the sparsified Clebsch-Gordon tensor, \n        the indices of the input tensor for the first dimension, the indices of the input tensor for the second dimension, \n        and the indices of the output tensor.\n    \"\"\"\n    # Calculate the number of non-zero elements in the Clebsch-Gordon tensor\n    nnz = torch.nonzero(cg).size(0)\n\n    # Create a sparse tensor with the same shape as the input tensor\n    sparse_cg = torch.sparse_coo_tensor(cg.shape)\n\n    # Create a list to store the indices of the non-zero elements\n    indices = []\n\n    # Iterate over the elements of the input tensor\n    for i in range(cg.shape[0]):\n        for j in range(cg.shape[1]):\n            if cg[i, j]!= 0:\n                # Add the index of the non-zero element to the list\n                indices.append((i, j))\n\n                # Set the corresponding element in the sparse tensor to the non-zero value\n                sparse_cg[i, j] = cg[i, j]\n\n    # Return the sparsified Clebsch-Gordon tensor, the indices of the input tensor for the first dimension, \n    # the indices of the input tensor for the second dimension, and the indices of the output tensor\n    return sparse_cg, torch.tensor(indices[:, 0]), torch.tensor(indices[:, 1]), torch.tensor(indices[:, 2])\n```"]}
{"id": 54, "name": "get_bounding_boxes", "prompt": "### Role:\nYou are an expert Python programmer. You will be given some information and will generate a correct Python function that matches this information.\n\n### Task:\nBased on the provided information, generate a complete Python function. Ensure the code is logically correct, syntactically accurate, and follows best coding practices.\n\n### Function name:\nget_bounding_boxes\n\n### Doc string and function signature:\n```Python\ndef get_bounding_boxes(self) -> Boxes :\n    \"\"\"\n    Description: This function calculates the bounding boxes for a set of polygons.\n\n    Args:\n        self (object): The instance of the class containing the polygons.\n\n    Returns:\n        Boxes: A Boxes object containing the calculated bounding boxes.\n    \"\"\"\n```\n\n### Requirements:\n1. Generate the complete function implementation.\n2. Only answer the code of the generated function, formatted using Markdown code block syntax.\n3. Do not include any additional text.\n\n### Answer: (use the provided format with backticks)\n", "reference": "    def get_bounding_boxes(self) -> Boxes:\n        boxes = torch.zeros(len(self.polygons), 4, dtype=torch.float32)\n        for idx, polygons_per_instance in enumerate(self.polygons):\n            minxy = torch.as_tensor([float(\"inf\"), float(\"inf\")], dtype=torch.float32)\n            maxxy = torch.zeros(2, dtype=torch.float32)\n            for polygon in polygons_per_instance:\n                coords = torch.from_numpy(polygon).view(-1, 2).to(dtype=torch.float32)\n                minxy = torch.min(minxy, torch.min(coords, dim=0).values)\n                maxxy = torch.max(maxxy, torch.max(coords, dim=0).values)\n            boxes[idx, :2] = minxy\n            boxes[idx, 2:] = maxxy\n        return Boxes(boxes)", "completions": ["  \n```\ndef get_bounding_boxes(self) -> Boxes:\n    \"\"\"\n    Description: This function calculates the bounding boxes for a set of polygons.\n\n    Args:\n        self (object): The instance of the class containing the polygons.\n\n    Returns:\n        Boxes: A Boxes object containing the calculated bounding boxes.\n    \"\"\"\n    # Initialize an empty list to store the bounding boxes\n    bounding_boxes = []\n\n    # Iterate over the polygons in the class instance\n    for polygon in self.polygons:\n        # Get the minimum and maximum x and y coordinates of the polygon\n        min_x, min_y, max_x, max_y = polygon.bounds\n\n        # Create a new bounding box with the minimum and maximum coordinates\n        bounding_box = Boxes(min_x, min_y, max_x, max_y)\n\n        # Add the bounding box to the list\n        bounding_boxes.append(bounding_box)\n\n    # Return the list of bounding boxes\n    return bounding_boxes\n```"]}
{"id": 55, "name": "_compute_recons_metrics", "prompt": "### Role:\nYou are an expert Python programmer. You will be given some information and will generate a correct Python function that matches this information.\n\n### Task:\nBased on the provided information, generate a complete Python function. Ensure the code is logically correct, syntactically accurate, and follows best coding practices.\n\n### Function name:\n_compute_recons_metrics\n\n### Doc string and function signature:\n```Python\ndef _compute_recons_metrics(cls, quant_module: ModuleInfo, act_func: Union[None, str], inp_data: torch.Tensor,\n                                out_data: torch.Tensor, param_to_adaround_tensor_quantizer: Dict,\n                                use_cuda: bool, device: int = 0) -> Tuple[float, float] :\n    \"\"\"\n    Description: This function computes the reconstruction metrics for a quantized module.\n\n    Args:\n        quant_module (ModuleInfo): The quantized module to compute metrics for.\n        act_func (Union[None, str]): The activation function to apply to the output data.\n        inp_data (torch.Tensor): The input data to the module.\n        out_data (torch.Tensor): The expected output data.\n        param_to_adaround_tensor_quantizer (Dict): A dictionary mapping module parameters to their corresponding quantizers.\n        use_cuda (bool): Whether to use CUDA for computations.\n        device (int): The CUDA device to use.\n\n    Returns:\n        Tuple[float, float]: The reconstruction error for hard and soft rounding.\n    \"\"\"\n```\n\n### Requirements:\n1. Generate the complete function implementation.\n2. Only answer the code of the generated function, formatted using Markdown code block syntax.\n3. Do not include any additional text.\n\n### Answer: (use the provided format with backticks)\n", "reference": "    def _compute_recons_metrics(cls, quant_module: ModuleInfo, act_func: Union[None, str], inp_data: torch.Tensor,\n                                out_data: torch.Tensor, param_to_adaround_tensor_quantizer: Dict,\n                                use_cuda: bool, device: int = 0) -> Tuple[float, float]:\n        adaround_quantizer = param_to_adaround_tensor_quantizer[quant_module.params['weight'].name]\n        torch_device = 'cpu'\n        if use_cuda:\n            torch_device = 'cuda:' + str(device)\n        weights = torch.from_numpy(numpy_helper.to_array(quant_module.params['weight'].tensor)).to(torch_device)\n        inp_data = inp_data.to(torch_device)\n        # Enable hard rounding and get quantized wrapper module's output\n        adaround_quantizer.use_soft_rounding = False\n        out_data_hard = cls._compute_output_with_adarounded_weights(weights, quant_module, inp_data, adaround_quantizer)\n\n        # Enable soft rounding and get quantized wrapper module's output\n        adaround_quantizer.use_soft_rounding = True\n        out_data_soft = cls._compute_output_with_adarounded_weights(weights, quant_module, inp_data, adaround_quantizer)\n\n        # If followed by an activation function\n        if act_func is not None:\n            out_data = ACTIVATION_MAP[act_func](out_data)\n            out_data_soft = ACTIVATION_MAP[act_func](out_data_soft)\n            out_data_hard = ACTIVATION_MAP[act_func](out_data_hard)\n\n        recons_err_soft = functional.mse_loss(out_data_soft, out_data)\n        recons_err_hard = functional.mse_loss(out_data_hard, out_data)\n\n        return float(recons_err_hard), float(recons_err_soft)", "completions": ["  \n```\ndef _compute_recons_metrics(cls, quant_module: ModuleInfo, act_func: Union[None, str], inp_data: torch.Tensor,\n                                out_data: torch.Tensor, param_to_adaround_tensor_quantizer: Dict,\n                                use_cuda: bool, device: int = 0) -> Tuple[float, float] :\n    \"\"\"\n    Description: This function computes the reconstruction metrics for a quantized module.\n\n    Args:\n        quant_module (ModuleInfo): The quantized module to compute metrics for.\n        act_func (Union[None, str]): The activation function to apply to the output data.\n        inp_data (torch.Tensor): The input data to the module.\n        out_data (torch.Tensor): The expected output data.\n        param_to_adaround_tensor_quantizer (Dict): A dictionary mapping module parameters to their corresponding quantizers.\n        use_cuda (bool): Whether to use CUDA for computations.\n        device (int): The CUDA device to use.\n\n    Returns:\n        Tuple[float, float]: The reconstruction error for hard and soft rounding.\n    \"\"\"\n    # Initialize the reconstruction error for hard and soft rounding\n    hard_recon_error = 0.0\n    soft_recon_error = 0.0\n\n    # Get the quantized module's parameters and their corresponding quantizers\n    params = quant_module.parameters()\n    param_quantizers = param_to_adaround_tensor_quantizer.values()\n\n    # Compute the reconstruction error for hard rounding\n    with torch.no_grad():\n        hard_recon_error = torch.sum(torch.abs(out_data - quant_module(inp_data)))\n\n    # Compute the reconstruction error for soft rounding\n    with torch.no_grad():\n        soft_recon_error = torch.sum(torch.abs(out_data - quant_module(inp_data, param_quantizers)))\n\n    return hard_recon_error, soft_recon_error\n```"]}
{"id": 56, "name": "selective_state_update_ref", "prompt": "### Role:\nYou are an expert Python programmer. You will be given some information and will generate a correct Python function that matches this information.\n\n### Task:\nBased on the provided information, generate a complete Python function. Ensure the code is logically correct, syntactically accurate, and follows best coding practices.\n\n### Function name:\nselective_state_update_ref\n\n### Doc string and function signature:\n```Python\ndef selective_state_update_ref(state, x, dt, A, B, C, D=None, z=None, dt_bias=None, dt_softplus=False) :\n    \"\"\"\n    Description: This function performs a selective state update operation. It updates the state based on the input, time difference, and other parameters.\n\n    Args:\n        state (torch.Tensor): The current state tensor of shape (batch, dim, dstate).\n        x (torch.Tensor): The input tensor of shape (batch, dim).\n        dt (torch.Tensor): The time difference tensor of shape (batch, dim).\n        A (torch.Tensor): The A tensor of shape (dim, dstate).\n        B (torch.Tensor): The B tensor of shape (batch, dstate).\n        C (torch.Tensor): The C tensor of shape (batch, dstate).\n        D (torch.Tensor, optional): The D tensor of shape (dim,). Defaults to None.\n        z (torch.Tensor, optional): The z tensor of shape (batch, dim). Defaults to None.\n        dt_bias (torch.Tensor, optional): The dt_bias tensor of shape (dim,). Defaults to None.\n        dt_softplus (bool, optional): Whether to apply softplus to dt. Defaults to False.\n\n    Returns:\n        torch.Tensor: The output tensor of shape (batch, dim).\n    \"\"\"\n```\n\n### Requirements:\n1. Generate the complete function implementation.\n2. Only answer the code of the generated function, formatted using Markdown code block syntax.\n3. Do not include any additional text.\n\n### Answer: (use the provided format with backticks)\n", "reference": "def selective_state_update_ref(state, x, dt, A, B, C, D=None, z=None, dt_bias=None, dt_softplus=False):\n    batch, dim, dstate = state.shape\n    assert x.shape == (batch, dim)\n    assert dt.shape == x.shape\n    assert A.shape == (dim, dstate)\n    assert B.shape == (batch, dstate)\n    assert C.shape == B.shape\n    if D is not None:\n        assert D.shape == (dim,)\n    if z is not None:\n        assert z.shape == x.shape\n    if dt_bias is not None:\n        assert dt_bias.shape == (dim,)\n        dt = dt + dt_bias\n    dt = F.softplus(dt) if dt_softplus else dt\n    dA = torch.exp(rearrange(dt, \"b d -> b d 1\") * A)  # (batch, dim, dstate)\n    dB = rearrange(dt, \"b d -> b d 1\") * rearrange(B, \"b n -> b 1 n\")  # (batch, dim, dstate)\n    state.copy_(state * dA + dB * rearrange(x, \"b d -> b d 1\"))  # (batch, dim, dstate\n    out = torch.einsum(\"bdn,bn->bd\", state.to(C.dtype), C)\n    if D is not None:\n        out += (x * D).to(out.dtype)\n    return (out if z is None else out * F.silu(z)).to(x.dtype)", "completions": ["  \n```\ndef selective_state_update_ref(state, x, dt, A, B, C, D=None, z=None, dt_bias=None, dt_softplus=False) :\n    \"\"\"\n    Description: This function performs a selective state update operation. It updates the state based on the input, time difference, and other parameters.\n\n    Args:\n        state (torch.Tensor): The current state tensor of shape (batch, dim, dstate).\n        x (torch.Tensor): The input tensor of shape (batch, dim).\n        dt (torch.Tensor): The time difference tensor of shape (batch, dim).\n        A (torch.Tensor): The A tensor of shape (dim, dstate).\n        B (torch.Tensor): The B tensor of shape (batch, dstate).\n        C (torch.Tensor): The C tensor of shape (batch, dstate).\n        D (torch.Tensor, optional): The D tensor of shape (dim,). Defaults to None.\n        z (torch.Tensor, optional): The z tensor of shape (batch, dim). Defaults to None.\n        dt_bias (torch.Tensor, optional): The dt_bias tensor of shape (dim,). Defaults to None.\n        dt_softplus (bool, optional): Whether to apply softplus to dt. Defaults to False.\n\n    Returns:\n        torch.Tensor: The output tensor of shape (batch, dim).\n    \"\"\"\n    # Apply softplus to dt if dt_softplus is True\n    if dt_softplus:\n        dt = torch.nn.functional.softplus(dt)\n\n    # Compute the update term\n    update = torch.matmul(A, state) + torch.matmul(B, x) + torch.matmul(C, dt)\n\n    # Apply the update term to the state\n    state = state + update\n\n    # Apply the bias term to the state\n    if dt_bias is not None:\n        state = state + dt_bias\n\n    # Apply the D tensor to the state\n    if D is not None:\n        state = torch.matmul(D, state)\n\n    # Apply the z tensor to the state\n    if z is not None:\n        state = state + z\n\n    return state\n```"]}
{"id": 57, "name": "step", "prompt": "### Role:\nYou are an expert Python programmer. You will be given some information and will generate a correct Python function that matches this information.\n\n### Task:\nBased on the provided information, generate a complete Python function. Ensure the code is logically correct, syntactically accurate, and follows best coding practices.\n\n### Function name:\nstep\n\n### Doc string and function signature:\n```Python\ndef step(self, closure: Any = None) -> None :\n    \"\"\"\n    Description: This function performs a step of optimization for a given parameter group.\n\n    Args:\n        self: The optimizer instance.\n        closure (Any, optional): A closure that reevaluates the model and returns the loss. Defaults to None.\n\n    Returns:\n        None\n    \"\"\"\n```\n\n### Requirements:\n1. Generate the complete function implementation.\n2. Only answer the code of the generated function, formatted using Markdown code block syntax.\n3. Do not include any additional text.\n\n### Answer: (use the provided format with backticks)\n", "reference": "    def step(self, closure: Any = None) -> None:\n\n        if closure is not None:\n            with torch.enable_grad():\n                # to fix linter, we do not keep the returned loss for use atm.\n                closure()\n\n        for group in self.param_groups:\n            beta1, beta2 = group[\"betas\"]\n            lr = group[\"lr\"]\n            weight_decay = group[\"weight_decay\"]\n            eps = group[\"eps\"]\n            use_kahan_summation = group[\"use_kahan_summation\"]\n\n            momentum_dtype = group[\"momentum_dtype\"]\n            variance_dtype = group[\"variance_dtype\"]\n            compensation_buffer_dtype = group[\"compensation_buffer_dtype\"]\n\n            for p in group[\"params\"]:\n                if p.grad is None:\n                    continue\n\n                if p.grad.is_sparse:\n                    raise RuntimeError(\n                        \"AnyPrecisionAdamW does not support sparse gradients\"\n                    )\n\n                state = self.state[p]\n\n                # State initialization\n                if len(state) == 0:\n                    state[\"step\"] = torch.tensor(0.0)\n\n                    # momentum - EMA of gradient values\n                    state[\"exp_avg\"] = torch.zeros_like(\n                        p,\n                        dtype=momentum_dtype,\n                    )\n\n                    # variance uncentered - EMA of squared gradient values\n                    state[\"exp_avg_sq\"] = torch.zeros_like(\n                        p,\n                        dtype=variance_dtype,\n                    )\n\n                    # optional Kahan summation - accumulated error tracker\n                    if use_kahan_summation:\n                        state[\"compensation\"] = torch.zeros_like(\n                            p,\n                            dtype=compensation_buffer_dtype,\n                        )\n\n                # main processing -------------------------\n\n                # update the steps for each param group update\n                state[\"step\"] += 1\n                step = state[\"step\"]\n\n                exp_avg = state[\"exp_avg\"]\n                exp_avg_sq = state[\"exp_avg_sq\"]\n\n                grad = p.grad\n\n                # weight decay, AdamW style\n                if weight_decay:\n                    p.data.mul_(1 - lr * weight_decay)\n\n                # update momentum\n                exp_avg.mul_(beta1).add_(grad, alpha=1 - beta1)\n\n                # update uncentered variance\n                exp_avg_sq.mul_(beta2).addcmul_(grad, grad, value=1 - beta2)\n\n                # adjust using bias1\n                bias_correction1 = 1 - beta1**step\n\n                step_size = lr / bias_correction1\n\n                # adjust using bias2\n                denom_correction = (1 - beta2**step) ** 0.5  # avoids math import\n\n                centered_variance = (exp_avg_sq.sqrt() / denom_correction).add_(eps)\n\n                # lr update to compensation\n                if use_kahan_summation:\n                    compensation = state[\"compensation\"]\n\n                    compensation.addcdiv_(exp_avg, centered_variance, value=-step_size)\n\n                    # update weights with compensation (Kahan summation)\n                    # save error back to compensation for next iteration\n                    temp_buffer = p.detach().clone()\n                    p.data.add_(compensation)\n                    compensation.add_(temp_buffer.sub_(p.data))\n\n                else:\n                    # usual AdamW updates\n                    p.data.addcdiv_(exp_avg, centered_variance, value=-step_size)", "completions": ["  \n```Python\ndef step(self, closure: Any = None) -> None:\n    \"\"\"\n    Description: This function performs a step of optimization for a given parameter group.\n\n    Args:\n        self: The optimizer instance.\n        closure (Any, optional): A closure that reevaluates the model and returns the loss. Defaults to None.\n\n    Returns:\n        None\n    \"\"\"\n    # Perform a step of optimization for a given parameter group\n    for group in self.param_groups:\n        for p in group['params']:\n            if p.grad is None:\n                continue\n            grad = p.grad.data\n            if grad.is_sparse:\n                raise RuntimeError('Adam does not support sparse gradients, please consider SparseAdam instead')\n            state = self.state[p]\n\n            # State initialization\n            if len(state) == 0:\n                state['step'] = 0\n                # Exponential moving average of gradient values\n                state['exp_avg'] = torch.zeros_like(p.data)\n                # Exponential moving average of squared gradient values\n                state['exp_avg_sq'] = torch.zeros_like(p.data)\n\n            exp_avg, exp_avg_sq = state['exp_avg'], state['exp_avg_sq']\n            beta1, beta2 = group['betas']\n\n            state['step'] += 1\n\n            # Decay the first and second moment running average coefficient\n            exp_avg.mul_(beta1).add_(1 - beta1, grad)\n            exp_avg_sq.mul_(beta2).addcmul_(1 - beta2, grad, grad)\n\n            denom = exp_avg_sq.sqrt().add_(group['eps'])\n\n            bias_correction1 = 1 - beta1 ** state['step']\n            bias_correction2 = 1 - beta2 ** state['step']\n            step_size = group['lr'] * math.sqrt(bias_correction2) / bias_correction1\n\n            p.data.addcdiv_(-step_size, exp_avg, denom)\n```"]}
{"id": 58, "name": "consequent_differs", "prompt": "### Role:\nYou are an expert Python programmer. You will be given some information and will generate a correct Python function that matches this information.\n\n### Task:\nBased on the provided information, generate a complete Python function. Ensure the code is logically correct, syntactically accurate, and follows best coding practices.\n\n### Function name:\nconsequent_differs\n\n### Doc string and function signature:\n```Python\ndef consequent_differs(\n    antecedents: Iterable[str] = [], eps: float = -1e8, event_dim: int = 0\n) -> Callable[[T], torch.Tensor] :\n    \"\"\"\n    Description: This function generates a consequent difference based on the antecedents and epsilon value.\n\n    Args:\n        antecedents (Iterable[str]): A list of antecedent strings.\n        eps (float): A small value used for conditioning.\n        event_dim (int): The dimension of the event.\n\n    Returns:\n        Callable[[T], torch.Tensor]: A function that takes a tensor as input and returns a tensor.\n    \"\"\"\n```\n\n### Requirements:\n1. Generate the complete function implementation.\n2. Only answer the code of the generated function, formatted using Markdown code block syntax.\n3. Do not include any additional text.\n\n### Answer: (use the provided format with backticks)\n", "reference": "def consequent_differs(\n    antecedents: Iterable[str] = [], eps: float = -1e8, event_dim: int = 0\n) -> Callable[[T], torch.Tensor]:\n\n    def _consequent_differs(consequent: T) -> torch.Tensor:\n        indices = IndexSet(\n            **{\n                name: ind\n                for name, ind in get_factual_indices().items()\n                if name in antecedents\n            }\n        )\n        not_eq: torch.Tensor = consequent != gather(\n            consequent, indices, event_dim=event_dim\n        )\n        for _ in range(event_dim):\n            not_eq = torch.all(not_eq, dim=-1, keepdim=False)\n        return cond(eps, 0.0, not_eq, event_dim=event_dim)\n\n    return _consequent_differs", "completions": ["  \n```\ndef consequent_differs(\n    antecedents: Iterable[str] = [], eps: float = -1e8, event_dim: int = 0\n) -> Callable[[T], torch.Tensor] :\n    \"\"\"\n    Description: This function generates a consequent difference based on the antecedents and epsilon value.\n\n    Args:\n        antecedents (Iterable[str]): A list of antecedent strings.\n        eps (float): A small value used for conditioning.\n        event_dim (int): The dimension of the event.\n\n    Returns:\n        Callable[[T], torch.Tensor]: A function that takes a tensor as input and returns a tensor.\n    \"\"\"\n    def inner_function(x: T) -> torch.Tensor:\n        # Implementation of the function\n        return torch.tensor(x)\n\n    return inner_function\n```"]}
{"id": 59, "name": "read_metrics", "prompt": "### Role:\nYou are an expert Python programmer. You will be given some information and will generate a correct Python function that matches this information.\n\n### Task:\nBased on the provided information, generate a complete Python function. Ensure the code is logically correct, syntactically accurate, and follows best coding practices.\n\n### Function name:\nread_metrics\n\n### Doc string and function signature:\n```Python\ndef read_metrics(directory: str) -> pd.Series :\n    \"\"\"\n    Description: This function reads metrics from a directory of CSV files and returns them as a pandas Series.\n\n    Args:\n        directory (str): The directory path where the CSV files are located.\n\n    Returns:\n        pd.Series: A pandas Series containing the metrics from the CSV files. The index of the Series is a MultiIndex composed of 'initial_time', 'lead_time', 'channel', and 'metric'. The values of the Series are the 'value' from the CSV files.\n    \"\"\"\n```\n\n### Requirements:\n1. Generate the complete function implementation.\n2. Only answer the code of the generated function, formatted using Markdown code block syntax.\n3. Do not include any additional text.\n\n### Answer: (use the provided format with backticks)\n", "reference": "def read_metrics(directory: str) -> pd.Series:\n\n    dfs = []\n    # Loop through all csv files in the directory\n    csv_files = [\n        filename for filename in os.listdir(directory) if filename.endswith(\".csv\")\n    ]\n\n    if len(csv_files) == 0:\n        return pd.Series()\n\n    for filename in csv_files:\n        filepath = os.path.join(directory, filename)\n        # Read the csv file into a pandas DataFrame\n        df = pd.read_csv(\n            filepath,\n            header=None,\n            names=[\n                \"initial_time_iso\",\n                \"lead_time_hours\",\n                \"channel\",\n                \"metric\",\n                \"value\",\n            ],\n        )\n        out = pd.DataFrame()\n        out[\"initial_time\"] = pd.to_datetime(df[\"initial_time_iso\"])\n        out[\"lead_time\"] = pd.to_timedelta(df[\"lead_time_hours\"], unit=\"h\")\n        out[\"channel\"] = df[\"channel\"]\n        out[\"metric\"] = df[\"metric\"]\n        out[\"value\"] = df[\"value\"]\n        dfs.append(out)\n\n    df = pd.concat(dfs, axis=0)\n    df.set_index([\"initial_time\", \"lead_time\", \"channel\", \"metric\"], inplace=True)\n    return df[\"value\"]", "completions": ["  \n```\ndef read_metrics(directory: str) -> pd.Series:\n    \"\"\"\n    This function reads metrics from a directory of CSV files and returns them as a pandas Series.\n\n    Args:\n        directory (str): The directory path where the CSV files are located.\n\n    Returns:\n        pd.Series: A pandas Series containing the metrics from the CSV files. The index of the Series is a MultiIndex composed of 'initial_time', 'lead_time', 'channel', and'metric'. The values of the Series are the 'value' from the CSV files.\n    \"\"\"\n    # Create an empty list to store the metrics\n    metrics = []\n\n    # Iterate over the CSV files in the directory\n    for file in os.listdir(directory):\n        # Check if the file is a CSV file\n        if file.endswith('.csv'):\n            # Read the CSV file into a pandas DataFrame\n            df = pd.read_csv(os.path.join(directory, file))\n\n            # Extract the metrics from the DataFrame\n            metrics.extend(df['value'].tolist())\n\n    # Create a MultiIndex for the Series\n    index = pd.MultiIndex.from_tuples([('initial_time', df['initial_time'].iloc[0]),\n                                       ('lead_time', df['lead_time'].iloc[0]),\n                                       ('channel', df['channel'].iloc[0]),\n                                       ('metric', df['metric'].iloc[0])])\n\n    # Create the Series\n    series = pd.Series(metrics, index=index)\n\n    return series\n```"]}
{"id": 60, "name": "load_probe", "prompt": "### Role:\nYou are an expert Python programmer. You will be given some information and will generate a correct Python function that matches this information.\n\n### Task:\nBased on the provided information, generate a complete Python function. Ensure the code is logically correct, syntactically accurate, and follows best coding practices.\n\n### Function name:\nload_probe\n\n### Doc string and function signature:\n```Python\ndef load_probe(probe_path) :\n    \"\"\"\n    Description: This function loads a probe file and returns a dictionary containing the probe's channel map, x-coordinates, y-coordinates, k-coordinates, and number of channels.\n\n    Args:\n        probe_path (str): The path to the probe file.\n\n    Returns:\n        dict: A dictionary containing the probe's channel map, x-coordinates, y-coordinates, k-coordinates, and number of channels.\n    \"\"\"\n```\n\n### Requirements:\n1. Generate the complete function implementation.\n2. Only answer the code of the generated function, formatted using Markdown code block syntax.\n3. Do not include any additional text.\n\n### Answer: (use the provided format with backticks)\n", "reference": "def load_probe(probe_path):\n    probe = {}\n    probe_path = Path(probe_path).resolve()\n    required_keys = ['chanMap', 'yc', 'xc', 'n_chan']\n\n    if probe_path.suffix == '.prb':\n        # Support for PRB files.\n        # !DOES NOT WORK FOR PHASE3A PROBES WITH DISCONNECTED CHANNELS!\n        # Also does not remove reference channel in PHASE3B probes\n        contents = probe_path.read_text()\n        metadata = {}\n        exec(contents, {}, metadata)\n        probe['chanMap'] = []\n        probe['xc'] = []\n        probe['yc'] = []\n        probe['kcoords'] = []\n        probe['n_chan'] = 0 \n        for cg in sorted(metadata['channel_groups']):\n            d = metadata['channel_groups'][cg]\n            ch = d['channels']\n            pos = d.get('geometry', {})\n            probe['chanMap'].append(ch)\n            probe['n_chan'] += len(ch)\n            probe['xc'].append([pos[c][0] for c in ch])\n            probe['yc'].append([pos[c][1] for c in ch])\n            probe['kcoords'].append([cg for c in ch])\n        probe['chanMap'] = np.concatenate(probe['chanMap']).ravel().astype(np.int32)\n        probe['xc'] = np.concatenate(probe['xc']).astype('float32')\n        probe['yc'] = np.concatenate(probe['yc']).astype('float32')\n        probe['kcoords'] = np.concatenate(probe['kcoords']).astype('float32')\n\n    elif probe_path.suffix == '.mat':\n        mat = loadmat(probe_path)\n        connected = mat['connected'].ravel().astype('bool')\n        probe['xc'] = mat['xcoords'].ravel().astype(np.float32)[connected]\n        nc = len(probe['xc'])\n        probe['yc'] = mat['ycoords'].ravel().astype(np.float32)[connected]\n        probe['kcoords'] = mat.get('kcoords', np.zeros(nc)).ravel().astype(np.float32)\n        probe['chanMap'] = (mat['chanMap'] - 1).ravel().astype(np.int32)[connected]  # NOTE: 0-indexing in Python\n        probe['n_chan'] = (mat['chanMap'] - 1).ravel().astype(np.int32).shape[0]  # NOTE: should match the # of columns in the raw data\n\n    elif probe_path.suffix == '.json':\n        with open(probe_path, 'r') as f:\n            probe = json.load(f)\n        for k in list(probe.keys()):\n            # Convert lists back to arrays\n            v = probe[k]\n            if isinstance(v, list):\n                dtype = np.int32 if k == 'chanMap' else np.float32\n                probe[k] = np.array(v, dtype=dtype)\n\n    for n in required_keys:\n        assert n in probe.keys()\n\n    return probe", "completions": ["  \n```\ndef load_probe(probe_path):\n    \"\"\"\n    Description: This function loads a probe file and returns a dictionary containing the probe's channel map, x-coordinates, y-coordinates, k-coordinates, and number of channels.\n\n    Args:\n        probe_path (str): The path to the probe file.\n\n    Returns:\n        dict: A dictionary containing the probe's channel map, x-coordinates, y-coordinates, k-coordinates, and number of channels.\n    \"\"\"\n    # Load the probe file\n    with open(probe_path, 'r') as f:\n        probe_data = f.read()\n\n    # Parse the probe data\n    channel_map = {}\n    x_coords = []\n    y_coords = []\n    k_coords = []\n    num_channels = 0\n\n    for line in probe_data.splitlines():\n        if line.startswith('#'):\n            continue\n        values = line.split()\n        if len(values)!= 4:\n            continue\n        channel_map[values[0]] = values[1]\n        x_coords.append(float(values[2]))\n        y_coords.append(float(values[3]))\n        k_coords.append(float(values[4]))\n        num_channels += 1\n\n    # Return the probe data as a dictionary\n    return {\n        'channel_map': channel_map,\n        'x_coords': x_coords,\n        'y_coords': y_coords,\n        'k_coords': k_coords,\n        'num_channels': num_channels\n    }\n```"]}
{"id": 61, "name": "set_radio_props", "prompt": "### Role:\nYou are an expert Python programmer. You will be given some information and will generate a correct Python function that matches this information.\n\n### Task:\nBased on the provided information, generate a complete Python function. Ensure the code is logically correct, syntactically accurate, and follows best coding practices.\n\n### Function name:\nset_radio_props\n\n### Doc string and function signature:\n```Python\ndef set_radio_props(self, props) :\n    \"\"\"\n    Description: This function sets the radio button properties.\n\n    Args:\n        props (dict): A dictionary containing the properties to be set.\n\n    Returns:\n        None: This function does not return any value.\n    \"\"\"\n```\n\n### Requirements:\n1. Generate the complete function implementation.\n2. Only answer the code of the generated function, formatted using Markdown code block syntax.\n3. Do not include any additional text.\n\n### Answer: (use the provided format with backticks)\n", "reference": "    def set_radio_props(self, props):\n        _api.check_isinstance(dict, props=props)\n        if 's' in props:  # Keep API consistent with constructor.\n            props['sizes'] = np.broadcast_to(props.pop('s'), len(self.labels))\n        self._buttons.update(props)\n        self._active_colors = self._buttons.get_facecolor()\n        if len(self._active_colors) == 1:\n            self._active_colors = np.repeat(self._active_colors,\n                                            len(self.labels), axis=0)\n        self._buttons.set_facecolor(\n            [activecolor if text.get_text() == self.value_selected else \"none\"\n             for text, activecolor in zip(self.labels, self._active_colors)])", "completions": ["  \n```Python\ndef set_radio_props(self, props) :\n    \"\"\"\n    Description: This function sets the radio button properties.\n\n    Args:\n        props (dict): A dictionary containing the properties to be set.\n\n    Returns:\n        None: This function does not return any value.\n    \"\"\"\n    for key, value in props.items():\n        if key == \"value\":\n            self.value = value\n        elif key == \"checked\":\n            self.checked = value\n        elif key == \"disabled\":\n            self.disabled = value\n        elif key == \"name\":\n            self.name = value\n        elif key == \"label\":\n            self.label = value\n        else:\n            raise ValueError(f\"Invalid property: {key}\")\n```"]}
{"id": 62, "name": "transform", "prompt": "### Role:\nYou are an expert Python programmer. You will be given some information and will generate a correct Python function that matches this information.\n\n### Task:\nBased on the provided information, generate a complete Python function. Ensure the code is logically correct, syntactically accurate, and follows best coding practices.\n\n### Function name:\ntransform\n\n### Doc string and function signature:\n```Python\ndef transform(self, effects) :\n    \"\"\"\n    Description: This function applies transformations to a font based on the given effects.\n\n    Args:\n        effects (dict): A dictionary containing the effects to be applied.\n\n    Returns:\n        Type1Font: A new Type1Font object with the transformed font properties.\n\n    Raises:\n        RuntimeError: If the text to be replaced with a new value spans the eexec boundary.\n\n    Notes:\n        This function uses the numpy.dot and numpy.eye functions to perform matrix transformations.\n        The effects dictionary can contain 'slant' and 'extend' keys, which are used to modify the font properties.\n        The font name, italic angle, and font matrix are updated based on the effects.\n        The function also handles the replacement of old font properties with the new ones in the PDF data.\n    \"\"\"\n```\n\n### Requirements:\n1. Generate the complete function implementation.\n2. Only answer the code of the generated function, formatted using Markdown code block syntax.\n3. Do not include any additional text.\n\n### Answer: (use the provided format with backticks)\n", "reference": "    def transform(self, effects):\n        fontname = self.prop['FontName']\n        italicangle = self.prop['ItalicAngle']\n\n        array = [\n            float(x) for x in (self.prop['FontMatrix']\n                               .lstrip('[').rstrip(']').split())\n        ]\n        oldmatrix = np.eye(3, 3)\n        oldmatrix[0:3, 0] = array[::2]\n        oldmatrix[0:3, 1] = array[1::2]\n        modifier = np.eye(3, 3)\n\n        if 'slant' in effects:\n            slant = effects['slant']\n            fontname += '_Slant_%d' % int(1000 * slant)\n            italicangle = round(\n                float(italicangle) - np.arctan(slant) / np.pi * 180,\n                5\n            )\n            modifier[1, 0] = slant\n\n        if 'extend' in effects:\n            extend = effects['extend']\n            fontname += '_Extend_%d' % int(1000 * extend)\n            modifier[0, 0] = extend\n\n        newmatrix = np.dot(modifier, oldmatrix)\n        array[::2] = newmatrix[0:3, 0]\n        array[1::2] = newmatrix[0:3, 1]\n        fontmatrix = (\n            '[%s]' % ' '.join(_format_approx(x, 6) for x in array)\n        )\n        replacements = (\n            [(x, '/FontName/%s def' % fontname)\n             for x in self._pos['FontName']]\n            + [(x, '/ItalicAngle %a def' % italicangle)\n               for x in self._pos['ItalicAngle']]\n            + [(x, '/FontMatrix %s readonly def' % fontmatrix)\n               for x in self._pos['FontMatrix']]\n            + [(x, '') for x in self._pos.get('UniqueID', [])]\n        )\n\n        data = bytearray(self.parts[0])\n        data.extend(self.decrypted)\n        len0 = len(self.parts[0])\n        for (pos0, pos1), value in sorted(replacements, reverse=True):\n            data[pos0:pos1] = value.encode('ascii', 'replace')\n            if pos0 < len(self.parts[0]):\n                if pos1 >= len(self.parts[0]):\n                    raise RuntimeError(\n                        f\"text to be replaced with {value} spans \"\n                        \"the eexec boundary\"\n                    )\n                len0 += len(value) - pos1 + pos0\n\n        data = bytes(data)\n        return Type1Font((\n            data[:len0],\n            self._encrypt(data[len0:], 'eexec'),\n            self.parts[2]\n        ))", "completions": ["  \n```\ndef transform(self, effects):\n    \"\"\"\n    Description: This function applies transformations to a font based on the given effects.\n\n    Args:\n        effects (dict): A dictionary containing the effects to be applied.\n\n    Returns:\n        Type1Font: A new Type1Font object with the transformed font properties.\n\n    Raises:\n        RuntimeError: If the text to be replaced with a new value spans the eexec boundary.\n\n    Notes:\n        This function uses the numpy.dot and numpy.eye functions to perform matrix transformations.\n        The effects dictionary can contain'slant' and 'extend' keys, which are used to modify the font properties.\n        The font name, italic angle, and font matrix are updated based on the effects.\n        The function also handles the replacement of old font properties with the new ones in the PDF data.\n    \"\"\"\n    # Initialize the new font properties\n    new_font_name = self.font_name\n    new_italic_angle = self.italic_angle\n    new_font_matrix = self.font_matrix\n\n    # Apply the effects to the font properties\n    if'slant' in effects:\n        new_italic_angle += effects['slant']\n    if 'extend' in effects:\n        new_font_matrix = np.dot(new_font_matrix, np.eye(3) * effects['extend'])\n\n    # Update the font name and italic angle\n    new_font_name = f\"{self.font_name}-{new_italic_angle:.2f}\"\n    new_italic_angle = new_italic_angle % 360\n\n    # Create a new Type1Font object with the transformed font properties\n    new_font = Type1Font(new_font_name, new_italic_angle, new_font_matrix)\n\n    # Replace the old font properties with the new ones in the PDF data\n    pdf_data = self.pdf_data\n    for key, value in pdf_data.items():\n        if key == 'FontName':\n            pdf_data[key] = new_font_name\n        elif key == 'ItalicAngle':\n            pdf_data[key] = new_italic_angle\n        elif key == 'FontMatrix':\n            pdf_data[key] = new_font_matrix\n\n    return new_font\n```"]}
{"id": 63, "name": "load_auto_model_for_seq2seq", "prompt": "### Role:\nYou are an expert Python programmer. You will be given some information and will generate a correct Python function that matches this information.\n\n### Task:\nBased on the provided information, generate a complete Python function. Ensure the code is logically correct, syntactically accurate, and follows best coding practices.\n\n### Function name:\nload_auto_model_for_seq2seq\n\n### Doc string and function signature:\n```Python\ndef load_auto_model_for_seq2seq(\n    model_name: str,\n    model_revision: Optional[str] = None,\n    tokenizer_name: Optional[str] = None,\n    tokenizer_revision: Optional[str] = None\n) -> Tuple[AutoTokenizer, AutoModelForSeq2SeqLM] :\n    \"\"\"\n    Description: This function loads a pre-trained tokenizer and model for sequence-to-sequence tasks.\n\n    Args:\n        model_name (str): The name of the model to load.\n        model_revision (Optional[str]): The revision of the model to load. Defaults to None.\n        tokenizer_name (Optional[str]): The name of the tokenizer to load. If not provided, it defaults to the model_name.\n        tokenizer_revision (Optional[str]): The revision of the tokenizer to load. Defaults to None.\n\n    Returns:\n        Tuple[AutoTokenizer, AutoModelForSeq2SeqLM]: A tuple containing the loaded tokenizer and model.\n    \"\"\"\n```\n\n### Requirements:\n1. Generate the complete function implementation.\n2. Only answer the code of the generated function, formatted using Markdown code block syntax.\n3. Do not include any additional text.\n\n### Answer: (use the provided format with backticks)\n", "reference": "def load_auto_model_for_seq2seq(\n    model_name: str,\n    model_revision: Optional[str] = None,\n    tokenizer_name: Optional[str] = None,\n    tokenizer_revision: Optional[str] = None\n) -> Tuple[AutoTokenizer, AutoModelForSeq2SeqLM]:\n    if tokenizer_name is None:\n        tokenizer_name = model_name\n    tokenizer = AutoTokenizer.from_pretrained(tokenizer_name,\n                                              revision=tokenizer_revision)\n    # There are \"Some weights are not used warning\" for some models, but we\n    # ignore it because that is intended.\n    with _handle_logging_level():\n        model = AutoModelForSeq2SeqLM.from_pretrained(model_name,\n                                                      revision=model_revision)\n    return tokenizer, model  # type: ignore", "completions": ["  \n```\ndef load_auto_model_for_seq2seq(\n    model_name: str,\n    model_revision: Optional[str] = None,\n    tokenizer_name: Optional[str] = None,\n    tokenizer_revision: Optional[str] = None\n) -> Tuple[AutoTokenizer, AutoModelForSeq2SeqLM] :\n    \"\"\"\n    Description: This function loads a pre-trained tokenizer and model for sequence-to-sequence tasks.\n\n    Args:\n        model_name (str): The name of the model to load.\n        model_revision (Optional[str]): The revision of the model to load. Defaults to None.\n        tokenizer_name (Optional[str]): The name of the tokenizer to load. If not provided, it defaults to the model_name.\n        tokenizer_revision (Optional[str]): The revision of the tokenizer to load. Defaults to None.\n\n    Returns:\n        Tuple[AutoTokenizer, AutoModelForSeq2SeqLM]: A tuple containing the loaded tokenizer and model.\n    \"\"\"\n    # Load the tokenizer\n    if tokenizer_name is None:\n        tokenizer_name = model_name\n    tokenizer = AutoTokenizer.from_pretrained(tokenizer_name, revision=tokenizer_revision)\n\n    # Load the model\n    model = AutoModelForSeq2SeqLM.from_pretrained(model_name, revision=model_revision)\n\n    return tokenizer, model\n```"]}
{"id": 64, "name": "set_xy", "prompt": "### Role:\nYou are an expert Python programmer. You will be given some information and will generate a correct Python function that matches this information.\n\n### Task:\nBased on the provided information, generate a complete Python function. Ensure the code is logically correct, syntactically accurate, and follows best coding practices.\n\n### Function name:\nset_xy\n\n### Doc string and function signature:\n```Python\ndef set_xy(self, xy) :\n    \"\"\"\n    Description: This function sets the xy coordinates of a path.\n\n    Args:\n        xy (array-like): The xy coordinates of the path.\n\n    Returns:\n        None\n\n    Raises:\n        ValueError: If the input xy coordinates are not in the correct format.\n\n    Notes:\n        This function uses the numpy.concatenate function to concatenate the xy coordinates.\n        If the path is closed and the first and last vertex are not the same, it appends the first vertex to the end.\n        If the path is not closed and the last vertex matches the first, it removes the last vertex.\n        The resulting path is stored in the _path attribute and the stale attribute is set to True.\n    \"\"\"\n```\n\n### Requirements:\n1. Generate the complete function implementation.\n2. Only answer the code of the generated function, formatted using Markdown code block syntax.\n3. Do not include any additional text.\n\n### Answer: (use the provided format with backticks)\n", "reference": "    def set_xy(self, xy):\n        xy = np.asarray(xy)\n        nverts, _ = xy.shape\n        if self._closed:\n            # if the first and last vertex are the \"same\", then we assume that\n            # the user explicitly passed the CLOSEPOLY vertex. Otherwise, we\n            # have to append one since the last vertex will be \"ignored\" by\n            # Path\n            if nverts == 1 or nverts > 1 and (xy[0] != xy[-1]).any():\n                xy = np.concatenate([xy, [xy[0]]])\n        else:\n            # if we aren't closed, and the last vertex matches the first, then\n            # we assume we have an unnecessary CLOSEPOLY vertex and remove it\n            if nverts > 2 and (xy[0] == xy[-1]).all():\n                xy = xy[:-1]\n        self._path = Path(xy, closed=self._closed)\n        self.stale = True", "completions": ["  \n```\ndef set_xy(self, xy):\n    \"\"\"\n    Description: This function sets the xy coordinates of a path.\n\n    Args:\n        xy (array-like): The xy coordinates of the path.\n\n    Returns:\n        None\n\n    Raises:\n        ValueError: If the input xy coordinates are not in the correct format.\n\n    Notes:\n        This function uses the numpy.concatenate function to concatenate the xy coordinates.\n        If the path is closed and the first and last vertex are not the same, it appends the first vertex to the end.\n        If the path is not closed and the last vertex matches the first, it removes the last vertex.\n        The resulting path is stored in the _path attribute and the stale attribute is set to True.\n    \"\"\"\n    try:\n        xy = np.asarray(xy)\n        if xy.ndim!= 2 or xy.shape[1]!= 2:\n            raise ValueError(\"xy coordinates must be a 2D array with shape (n, 2)\")\n        if self.closed and not np.allclose(xy[0], xy[-1]):\n            xy = np.concatenate((xy, [xy[0]]))\n        elif not self.closed and np.allclose(xy[0], xy[-1]):\n            xy = xy[:-1]\n        self._path = xy\n        self.stale = True\n    except ValueError as e:\n        raise ValueError(\"Invalid xy coordinates\") from e\n```"]}
{"id": 65, "name": "broadcast_data", "prompt": "### Role:\nYou are an expert Python programmer. You will be given some information and will generate a correct Python function that matches this information.\n\n### Task:\nBased on the provided information, generate a complete Python function. Ensure the code is logically correct, syntactically accurate, and follows best coding practices.\n\n### Function name:\nbroadcast_data\n\n### Doc string and function signature:\n```Python\ndef broadcast_data(keys, data, datatype) :\n    \"\"\"\n    Description: This function broadcasts data from rank 0 to all other ranks in a distributed system.\n\n    Args:\n        keys (list): A list of keys used to index the data dictionary.\n        data (dict): A dictionary containing data to be broadcasted.\n        datatype (torch.dtype): The data type of the data to be broadcasted.\n\n    Returns:\n        dict: A dictionary containing the broadcasted data.\n    \"\"\"\n```\n\n### Requirements:\n1. Generate the complete function implementation.\n2. Only answer the code of the generated function, formatted using Markdown code block syntax.\n3. Do not include any additional text.\n\n### Answer: (use the provided format with backticks)\n", "reference": "def broadcast_data(keys, data, datatype):\n    # Build (key, size) and (key, number of elements) dictionaries along\n    # with the total number of elements on all ranks.\n    key_size, key_numel, total_numel = _build_key_size_numel_dictionaries(keys,\n                                                                          data)\n\n    # Pack on rank zero.\n    if get_tensor_model_parallel_rank() == 0:\n        # Check that all keys have the same data type.\n        _check_data_types(keys, data, datatype)\n        # Flatten the data associated with the keys\n        flatten_data = torch.cat(\n            [data[key].contiguous().view(-1) for key in keys], dim=0).cuda()\n    else:\n        flatten_data = torch.empty(total_numel,\n                                   device=torch.cuda.current_device(),\n                                   dtype=datatype)\n\n    # Broadcast\n    torch.distributed.broadcast(flatten_data, get_tensor_model_parallel_src_rank(),\n                                group=get_tensor_model_parallel_group())\n\n    # Unpack\n    output = {}\n    offset = 0\n    for key in keys:\n        size = key_size[key]\n        numel = key_numel[key]\n        output[key] = flatten_data.narrow(0, offset, numel).view(size)\n        offset += numel\n\n    return output", "completions": ["  \n```\ndef broadcast_data(keys, data, datatype):\n    \"\"\"\n    Description: This function broadcasts data from rank 0 to all other ranks in a distributed system.\n\n    Args:\n        keys (list): A list of keys used to index the data dictionary.\n        data (dict): A dictionary containing data to be broadcasted.\n        datatype (torch.dtype): The data type of the data to be broadcasted.\n\n    Returns:\n        dict: A dictionary containing the broadcasted data.\n    \"\"\"\n    # Check if the data is a dictionary\n    if not isinstance(data, dict):\n        raise ValueError(\"Data must be a dictionary\")\n\n    # Check if the keys are a list\n    if not isinstance(keys, list):\n        raise ValueError(\"Keys must be a list\")\n\n    # Check if the datatype is a valid torch data type\n    if not isinstance(datatype, torch.dtype):\n        raise ValueError(\"Datatype must be a valid torch data type\")\n\n    # Broadcast the data from rank 0 to all other ranks\n    for key in keys:\n        if key in data:\n            data[key] = torch.tensor(data[key], dtype=datatype)\n            torch.distributed.broadcast(data[key], src=0)\n\n    return data\n```"]}
{"id": 66, "name": "sample_intervention", "prompt": "### Role:\nYou are an expert Python programmer. You will be given some information and will generate a correct Python function that matches this information.\n\n### Task:\nBased on the provided information, generate a complete Python function. Ensure the code is logically correct, syntactically accurate, and follows best coding practices.\n\n### Function name:\nsample_intervention\n\n### Doc string and function signature:\n```Python\ndef sample_intervention(\n    sem: SEM, tensordict_data: TensorDict, num_intervention_samples: int, treatment: str | None = None\n) -> InterventionData :\n    \"\"\"\n    Description: This function samples an intervention from a given SEM (Structural Equation Model) and a TensorDict data.\n\n    Args:\n        sem (SEM): The Structural Equation Model from which the intervention is sampled.\n        tensordict_data (TensorDict): The data from which the intervention is sampled.\n        num_intervention_samples (int): The number of samples to generate for the intervention.\n        treatment (str | None, optional): The treatment to be considered for the intervention. Defaults to None.\n\n    Returns:\n        InterventionData: The generated intervention data, including the samples, the intervention dictionary, and an empty TensorDict.\n    \"\"\"\n```\n\n### Requirements:\n1. Generate the complete function implementation.\n2. Only answer the code of the generated function, formatted using Markdown code block syntax.\n3. Do not include any additional text.\n\n### Answer: (use the provided format with backticks)\n", "reference": "def sample_intervention(\n    sem: SEM, tensordict_data: TensorDict, num_intervention_samples: int, treatment: str | None = None\n) -> InterventionData:\n    intervention_a = sample_intervention_dict(tensordict_data, treatment=treatment)\n\n    intervention_a_samples = sem.do(intervention_a).sample(\n        torch.Size(\n            [\n                num_intervention_samples,\n            ]\n        )\n    )\n\n    return InterventionData(\n        intervention_a_samples,\n        intervention_a,\n        TensorDict({}, batch_size=torch.Size()),\n    )", "completions": ["  \n```\ndef sample_intervention(sem: SEM, tensordict_data: TensorDict, num_intervention_samples: int, treatment: str | None = None) -> InterventionData:\n    \"\"\"\n    Description: This function samples an intervention from a given SEM (Structural Equation Model) and a TensorDict data.\n\n    Args:\n        sem (SEM): The Structural Equation Model from which the intervention is sampled.\n        tensordict_data (TensorDict): The data from which the intervention is sampled.\n        num_intervention_samples (int): The number of samples to generate for the intervention.\n        treatment (str | None, optional): The treatment to be considered for the intervention. Defaults to None.\n\n    Returns:\n        InterventionData: The generated intervention data, including the samples, the intervention dictionary, and an empty TensorDict.\n    \"\"\"\n    # Generate the intervention dictionary\n    intervention_dict = {}\n    for node in sem.nodes:\n        if node.is_observed:\n            intervention_dict[node.name] = tensordict_data[node.name]\n        else:\n            intervention_dict[node.name] = np.random.normal(size=num_intervention_samples)\n\n    # Generate the samples\n    samples = np.zeros((num_intervention_samples, len(sem.nodes)))\n    for i in range(num_intervention_samples):\n        for node in sem.nodes:\n            if node.is_observed:\n                samples[i, node.name] = tensordict_data[node.name]\n            else:\n                samples[i, node.name] = intervention_dict[node.name][i]\n\n    # Generate the empty TensorDict\n    empty_tensordict = TensorDict()\n    for node in sem.nodes:\n        empty_tensordict[node.name] = np.zeros(num_intervention_samples)\n\n    # Return the intervention data\n    return InterventionData(samples=samples, intervention_dict=intervention_dict, empty_tensordict=empty_tensordict)\n```"]}
{"id": 67, "name": "_set_alpha_for_array", "prompt": "### Role:\nYou are an expert Python programmer. You will be given some information and will generate a correct Python function that matches this information.\n\n### Task:\nBased on the provided information, generate a complete Python function. Ensure the code is logically correct, syntactically accurate, and follows best coding practices.\n\n### Function name:\n_set_alpha_for_array\n\n### Doc string and function signature:\n```Python\ndef _set_alpha_for_array(self, alpha) :\n    \"\"\"\n    Description: This function sets the alpha value for an array.\n\n    Args:\n        alpha (numeric or None or str): The alpha value to be set. If it is a string, a TypeError is raised. If it is not iterable, the function sets the alpha value and returns. If it is iterable, it is converted to an array. If the minimum or maximum value of the array is not between 0 and 1, inclusive, a ValueError is raised. The alpha value is then set to the array, the pchanged method is called, and the stale attribute is set to True.\n\n    Returns:\n        None\n    \"\"\"\n```\n\n### Requirements:\n1. Generate the complete function implementation.\n2. Only answer the code of the generated function, formatted using Markdown code block syntax.\n3. Do not include any additional text.\n\n### Answer: (use the provided format with backticks)\n", "reference": "    def _set_alpha_for_array(self, alpha):\n        if isinstance(alpha, str):\n            raise TypeError(\"alpha must be numeric or None, not a string\")\n        if not np.iterable(alpha):\n            Artist.set_alpha(self, alpha)\n            return\n        alpha = np.asarray(alpha)\n        if not (0 <= alpha.min() and alpha.max() <= 1):\n            raise ValueError('alpha must be between 0 and 1, inclusive, '\n                             f'but min is {alpha.min()}, max is {alpha.max()}')\n        self._alpha = alpha\n        self.pchanged()\n        self.stale = True", "completions": ["  \n```\ndef _set_alpha_for_array(self, alpha):\n    \"\"\"\n    Description: This function sets the alpha value for an array.\n\n    Args:\n        alpha (numeric or None or str): The alpha value to be set. If it is a string, a TypeError is raised. If it is not iterable, the function sets the alpha value and returns. If it is iterable, it is converted to an array. If the minimum or maximum value of the array is not between 0 and 1, inclusive, a ValueError is raised. The alpha value is then set to the array, the pchanged method is called, and the stale attribute is set to True.\n\n    Returns:\n        None\n    \"\"\"\n    if isinstance(alpha, str):\n        raise TypeError(\"Alpha value must be a numeric or None\")\n    if not hasattr(self, \"alpha\"):\n        self.alpha = alpha\n        self.pchanged()\n        self.stale = True\n    else:\n        if isinstance(alpha, Iterable):\n            alpha = np.array(alpha)\n        if np.amin(alpha) < 0 or np.amax(alpha) > 1:\n            raise ValueError(\"Alpha value must be between 0 and 1\")\n        self.alpha = alpha\n        self.pchanged()\n        self.stale = True\n```"]}
{"id": 68, "name": "run", "prompt": "### Role:\nYou are an expert Python programmer. You will be given some information and will generate a correct Python function that matches this information.\n\n### Task:\nBased on the provided information, generate a complete Python function. Ensure the code is logically correct, syntactically accurate, and follows best coding practices.\n\n### Function name:\nrun\n\n### Doc string and function signature:\n```Python\ndef run(self) -> None :\n    \"\"\"\n    Description: This function is for running a simulation. It pushes particles until a termination condition is satisfied.\n\n    Args:\n        self: The instance of the class that this method belongs to.\n\n    Returns:\n        None: This function does not return any value.\n    \"\"\"\n```\n\n### Requirements:\n1. Generate the complete function implementation.\n2. Only answer the code of the generated function, formatted using Markdown code block syntax.\n3. Do not include any additional text.\n\n### Answer: (use the provided format with backticks)\n", "reference": "    def run(self) -> None:\n\n        self._enforce_particle_creation()\n\n        # Keep track of how many push steps have occurred for trajectory tracing\n        self.iteration_number = 0\n\n        self.time: Union[NDArray[np.float64], float] = (\n            np.zeros((self.nparticles, 1)) if not self.is_synchronized_time_step else 0\n        )\n        # Create flags for tracking when particles during the simulation\n        # on_grid -> zero if the particle is off grid, 1\n        # shape [nparticles, ngrids]\n        self.on_grid: NDArray[np.bool_] = np.zeros(\n            [self.nparticles, self.num_grids]\n        ).astype(np.bool_)\n\n        # Entered grid -> non-zero if particle EVER entered a grid\n        self.entered_grid: NDArray[np.bool_] = np.zeros([self.nparticles]).astype(\n            np.bool_\n        )\n\n        # Initialize a \"progress bar\" (really more of a meter)\n        # Setting sys.stdout lets this play nicely with regular print()\n        pbar = tqdm(\n            initial=0,\n            total=self.termination_condition.total,\n            disable=not self.verbose,\n            desc=self.termination_condition.progress_description,\n            unit=self.termination_condition.units_string,\n            bar_format=\"{l_bar}{bar}{n:.1e}/{total:.1e} {unit}\",\n            file=sys.stdout,\n        )\n\n        # Push the particles until the termination condition is satisfied\n        is_finished = False\n        while not (is_finished or self.nparticles_tracked == 0):\n            is_finished = self.termination_condition.is_finished\n            progress = min(\n                self.termination_condition.progress, self.termination_condition.total\n            )\n\n            pbar.n = progress\n            pbar.last_print_n = progress\n            pbar.update(0)\n\n            self._push()\n\n            if self.save_routine is not None:\n                self.save_routine.post_push_hook()\n\n        if self.save_routine is not None:\n            self.save_routine.post_push_hook(force_save=True)\n\n        pbar.close()\n\n        # Log a summary of the run\n\n        self._log(\"Run completed\")\n\n        # Simulation has not run, because creating new particles changes the simulation\n        self._has_run = True", "completions": ["  \n```Python\ndef run(self) -> None:\n    \"\"\"\n    Description: This function is for running a simulation. It pushes particles until a termination condition is satisfied.\n\n    Args:\n        self: The instance of the class that this method belongs to.\n\n    Returns:\n        None: This function does not return any value.\n    \"\"\"\n    while not self.termination_condition_satisfied():\n        self.push_particles()\n```"]}
{"id": 69, "name": "assert_connectivity", "prompt": "### Role:\nYou are an expert Python programmer. You will be given some information and will generate a correct Python function that matches this information.\n\n### Task:\nBased on the provided information, generate a complete Python function. Ensure the code is logically correct, syntactically accurate, and follows best coding practices.\n\n### Function name:\nassert_connectivity\n\n### Doc string and function signature:\n```Python\ndef assert_connectivity(connectivity: nx.Graph, circuit: Circuit) :\n    \"\"\"\n    Description: This function asserts the connectivity of a circuit with respect to a given graph.\n\n    Args:\n        connectivity (nx.Graph): The graph representing the connectivity of the circuit.\n        circuit (Circuit): The circuit to be checked for connectivity.\n\n    Raises:\n        ConnectivityError: If the circuit does not respect the connectivity of the graph or if a gate acts on more than two qubits.\n\n    Returns:\n        None\n    \"\"\"\n```\n\n### Requirements:\n1. Generate the complete function implementation.\n2. Only answer the code of the generated function, formatted using Markdown code block syntax.\n3. Do not include any additional text.\n\n### Answer: (use the provided format with backticks)\n", "reference": "def assert_connectivity(connectivity: nx.Graph, circuit: Circuit):\n    if list(connectivity.nodes) != list(range(connectivity.number_of_nodes())):\n        node_mapping = {node: i for i, node in enumerate(connectivity.nodes)}\n        new_connectivity = nx.Graph()\n        new_connectivity.add_edges_from(\n            [(node_mapping[u], node_mapping[v]) for u, v in connectivity.edges]\n        )\n        connectivity = new_connectivity\n    for gate in circuit.queue:\n        if len(gate.qubits) > 2 and not isinstance(gate, gates.M):\n            raise_error(ConnectivityError, f\"{gate.name} acts on more than two qubits.\")\n        if len(gate.qubits) == 2:\n            if (gate.qubits[0], gate.qubits[1]) not in connectivity.edges:\n                raise_error(\n                    ConnectivityError,\n                    f\"Circuit does not respect connectivity. {gate.name} acts on {gate.qubits}.\",\n                )", "completions": ["  \n```\ndef assert_connectivity(connectivity: nx.Graph, circuit: Circuit) :\n    \"\"\"\n    Description: This function asserts the connectivity of a circuit with respect to a given graph.\n\n    Args:\n        connectivity (nx.Graph): The graph representing the connectivity of the circuit.\n        circuit (Circuit): The circuit to be checked for connectivity.\n\n    Raises:\n        ConnectivityError: If the circuit does not respect the connectivity of the graph or if a gate acts on more than two qubits.\n\n    Returns:\n        None\n    \"\"\"\n    # Check if the circuit is connected\n    if not nx.is_connected(circuit.to_networkx_graph()):\n        raise ConnectivityError(\"The circuit is not connected.\")\n\n    # Check if the circuit respects the connectivity of the graph\n    for node in circuit.qubits:\n        if not connectivity.has_node(node):\n            raise ConnectivityError(f\"The qubit {node} is not present in the connectivity graph.\")\n        for neighbor in connectivity.neighbors(node):\n            if not circuit.has_edge(node, neighbor):\n                raise ConnectivityError(f\"The qubits {node} and {neighbor} are not connected in the circuit.\")\n\n    # Check if a gate acts on more than two qubits\n    for gate in circuit.gates:\n        if len(gate.qubits) > 2:\n            raise ConnectivityError(f\"The gate {gate.name} acts on more than two qubits.\")\n```"]}
{"id": 70, "name": "get_aws_credentials", "prompt": "### Role:\nYou are an expert Python programmer. You will be given some information and will generate a correct Python function that matches this information.\n\n### Task:\nBased on the provided information, generate a complete Python function. Ensure the code is logically correct, syntactically accurate, and follows best coding practices.\n\n### Function name:\nget_aws_credentials\n\n### Doc string and function signature:\n```Python\ndef get_aws_credentials(\n    s3_aws_credentials_config: S3AWSCredentials,\n) -> Optional[Dict[str, str]] :\n    \"\"\"\n    Description: This function is used to get AWS credentials from a specified URL.\n\n    Args:\n        s3_aws_credentials_config (S3AWSCredentials): An instance of S3AWSCredentials containing the necessary configuration for AWS credentials.\n\n    Returns:\n        Optional[Dict[str, str]]: A dictionary containing the AWS credentials if the request is successful, otherwise None.\n    \"\"\"\n```\n\n### Requirements:\n1. Generate the complete function implementation.\n2. Only answer the code of the generated function, formatted using Markdown code block syntax.\n3. Do not include any additional text.\n\n### Answer: (use the provided format with backticks)\n", "reference": "def get_aws_credentials(\n    s3_aws_credentials_config: S3AWSCredentials,\n) -> Optional[Dict[str, str]]:\n    token = (\n        os.getenv(s3_aws_credentials_config.auth_token_env_variable)\n        if s3_aws_credentials_config.auth_token_env_variable\n        else None\n    )\n    headers = {\"Authorization\": f\"Bearer {token}\"} if token else None\n    resp = requests.post(\n        s3_aws_credentials_config.create_aws_credentials_url, headers=headers\n    )\n    if not resp.ok:\n        logger.error(f\"Request to create AWS credentials had failed with {resp.reason}\")\n        return None\n\n    env = resp.json()\n    return env", "completions": ["  \n```\ndef get_aws_credentials(s3_aws_credentials_config: S3AWSCredentials) -> Optional[Dict[str, str]]:\n    \"\"\"\n    This function is used to get AWS credentials from a specified URL.\n\n    Args:\n        s3_aws_credentials_config (S3AWSCredentials): An instance of S3AWSCredentials containing the necessary configuration for AWS credentials.\n\n    Returns:\n        Optional[Dict[str, str]]: A dictionary containing the AWS credentials if the request is successful, otherwise None.\n    \"\"\"\n    try:\n        response = requests.get(s3_aws_credentials_config.url)\n        response.raise_for_status()\n        credentials = response.json()\n        return {\n            \"aws_access_key_id\": credentials[\"AccessKeyId\"],\n            \"aws_secret_access_key\": credentials[\"SecretAccessKey\"],\n            \"aws_session_token\": credentials[\"SessionToken\"],\n        }\n    except requests.exceptions.RequestException as e:\n        print(f\"Error getting AWS credentials: {e}\")\n        return None\n```"]}
{"id": 71, "name": "setup_cache", "prompt": "### Role:\nYou are an expert Python programmer. You will be given some information and will generate a correct Python function that matches this information.\n\n### Task:\nBased on the provided information, generate a complete Python function. Ensure the code is logically correct, syntactically accurate, and follows best coding practices.\n\n### Function name:\nsetup_cache\n\n### Doc string and function signature:\n```Python\ndef setup_cache(\n    max_batch_size, max_seq_len, dim, heads, layers, block_size, rope_base\n) :\n    \"\"\"\n    Description: This function sets up the cache for a transformer model.\n\n    Args:\n        max_batch_size (int): The maximum batch size for the transformer model.\n        max_seq_len (int): The maximum sequence length for the transformer model.\n        dim (int): The dimension of the transformer model.\n        heads (int): The number of heads in the transformer model.\n        layers (list): The list of layers in the transformer model.\n        block_size (int): The block size for the transformer model.\n        rope_base (float): The base for the RoPE (Rotary Positional Embedding) in the transformer model.\n\n    Returns:\n        tuple: A tuple containing the causal mask and the precomputed frequency cis for the transformer model.\n    \"\"\"\n```\n\n### Requirements:\n1. Generate the complete function implementation.\n2. Only answer the code of the generated function, formatted using Markdown code block syntax.\n3. Do not include any additional text.\n\n### Answer: (use the provided format with backticks)\n", "reference": "def setup_cache(\n    max_batch_size, max_seq_len, dim, heads, layers, block_size, rope_base\n):\n    if max_seq_len >= max_seq_len and max_batch_size >= max_batch_size:\n        return\n\n    head_dim = dim // heads\n    max_seq_len = find_multiple(max_seq_len, 8)\n\n    for b in layers:\n        b.attention.kv_cache = KVCache(\n            max_batch_size, max_seq_len, heads, head_dim\n        )\n\n    freq_cis = precompute_freq_cis(block_size, dim // heads, rope_base)\n    causal_mask = torch.tril(\n        torch.ones(max_seq_len, max_seq_len, dtype=torch.bool)\n    )\n\n    return causal_mask, freq_cis", "completions": ["  \n```\ndef setup_cache(max_batch_size, max_seq_len, dim, heads, layers, block_size, rope_base):\n    \"\"\"\n    Description: This function sets up the cache for a transformer model.\n\n    Args:\n        max_batch_size (int): The maximum batch size for the transformer model.\n        max_seq_len (int): The maximum sequence length for the transformer model.\n        dim (int): The dimension of the transformer model.\n        heads (int): The number of heads in the transformer model.\n        layers (list): The list of layers in the transformer model.\n        block_size (int): The block size for the transformer model.\n        rope_base (float): The base for the RoPE (Rotary Positional Embedding) in the transformer model.\n\n    Returns:\n        tuple: A tuple containing the causal mask and the precomputed frequency cis for the transformer model.\n    \"\"\"\n    causal_mask = torch.tril(torch.ones((max_seq_len, max_seq_len), device=device))\n    causal_mask = causal_mask.unsqueeze(0).unsqueeze(1)\n    causal_mask = causal_mask.repeat(max_batch_size, 1, 1, 1)\n\n    freq_cis = torch.zeros((max_seq_len, dim), device=device)\n    for i in range(max_seq_len):\n        freq_cis[i] = torch.cos(i * rope_base)\n    freq_cis = freq_cis.unsqueeze(0).unsqueeze(1)\n    freq_cis = freq_cis.repeat(max_batch_size, 1, 1, 1)\n\n    return causal_mask, freq_cis\n```"]}
{"id": 72, "name": "push_pop_nop", "prompt": "### Role:\nYou are an expert Python programmer. You will be given some information and will generate a correct Python function that matches this information.\n\n### Task:\nBased on the provided information, generate a complete Python function. Ensure the code is logically correct, syntactically accurate, and follows best coding practices.\n\n### Function name:\npush_pop_nop\n\n### Doc string and function signature:\n```Python\ndef push_pop_nop(ss: StackState,\n                 sharpen_pointer,\n                 should_push,\n                 should_pop,\n                 should_null_op,\n                 value) -> StackState :\n    \"\"\"\n    Description: This function performs operations on a stack-based data structure.\n\n    Args:\n        ss (StackState): The current state of the stack.\n        sharpen_pointer (type): A factor to sharpen the pointer.\n        should_push (type): A boolean tensor indicating which elements should be pushed onto the stack.\n        should_pop (type): A boolean tensor indicating which elements should be popped from the stack.\n        should_null_op (type): A boolean tensor indicating which operations should be null operations.\n        value (type): The value to be pushed onto the stack.\n\n    Returns:\n        StackState: The new state of the stack after the operations.\n        popped_val (type): The popped value from the stack.\n    \"\"\"\n```\n\n### Requirements:\n1. Generate the complete function implementation.\n2. Only answer the code of the generated function, formatted using Markdown code block syntax.\n3. Do not include any additional text.\n\n### Answer: (use the provided format with backticks)\n", "reference": "def push_pop_nop(ss: StackState,\n                 sharpen_pointer,\n                 should_push,\n                 should_pop,\n                 should_null_op,\n                 value) -> StackState:\n    # Push value onto stack\n    push_ss = push(ss, value)\n\n    # Pop value off of stack\n    pop_ss, popped_val = pop(ss)\n    popped_val = einsum('bv, b -> bv', popped_val, should_pop) # interpolate if pop not intended\n\n    new_stack = (\n        einsum('bnv, b -> bnv', ss.stack, should_null_op) +\n        einsum('bnv, b -> bnv', push_ss.stack, should_push) +\n        einsum('bnv, b -> bnv', pop_ss.stack, should_pop)\n    )\n    new_pointer = (\n        einsum('bn, b -> bn', ss.pointer, should_null_op) +\n        einsum('bn, b -> bn', push_ss.pointer, should_push) +\n        einsum('bn, b -> bn', pop_ss.pointer, should_pop)\n    )\n\n    ##########\n    # Sharpen (softmax) pointer\n    new_pointer = torch.softmax(new_pointer * sharpen_pointer, dim=1)\n    psum = new_pointer.sum(dim=1).unsqueeze(1)\n    new_pointer = new_pointer / torch.maximum(psum, torch.zeros_like(psum) + 1e-8)\n\n    new_ss = StackState(stack=new_stack,\n                        pointer=new_pointer,\n                        zero_vec=ss.zero_vec)\n    return new_ss, popped_val", "completions": ["  \n```\ndef push_pop_nop(ss: StackState,\n                 sharpen_pointer,\n                 should_push,\n                 should_pop,\n                 should_null_op,\n                 value) -> StackState :\n    \"\"\"\n    Description: This function performs operations on a stack-based data structure.\n\n    Args:\n        ss (StackState): The current state of the stack.\n        sharpen_pointer (type): A factor to sharpen the pointer.\n        should_push (type): A boolean tensor indicating which elements should be pushed onto the stack.\n        should_pop (type): A boolean tensor indicating which elements should be popped from the stack.\n        should_null_op (type): A boolean tensor indicating which operations should be null operations.\n        value (type): The value to be pushed onto the stack.\n\n    Returns:\n        StackState: The new state of the stack after the operations.\n        popped_val (type): The popped value from the stack.\n    \"\"\"\n    # Initialize the stack state\n    stack = ss.stack\n    pointer = ss.pointer\n\n    # Sharpen the pointer\n    pointer = sharpen_pointer(pointer)\n\n    # Push the value onto the stack\n    if should_push:\n        stack.append(value)\n\n    # Pop the value from the stack\n    if should_pop:\n        popped_val = stack.pop()\n\n    # Perform a null operation\n    if should_null_op:\n        pass\n\n    # Update the stack state\n    ss.stack = stack\n    ss.pointer = pointer\n\n    return ss, popped_val\n```"]}
{"id": 73, "name": "get_bounding_boxes", "prompt": "### Role:\nYou are an expert Python programmer. You will be given some information and will generate a correct Python function that matches this information.\n\n### Task:\nBased on the provided information, generate a complete Python function. Ensure the code is logically correct, syntactically accurate, and follows best coding practices.\n\n### Function name:\nget_bounding_boxes\n\n### Doc string and function signature:\n```Python\ndef get_bounding_boxes(self) -> Boxes :\n    \"\"\"\n    Description: This function calculates the bounding boxes of non-zero elements in a tensor.\n\n    Args:\n        self (object): An instance of the class containing the tensor attribute.\n\n    Returns:\n        Boxes: A Boxes object containing the bounding boxes of non-zero elements in the tensor.\n    \"\"\"\n```\n\n### Requirements:\n1. Generate the complete function implementation.\n2. Only answer the code of the generated function, formatted using Markdown code block syntax.\n3. Do not include any additional text.\n\n### Answer: (use the provided format with backticks)\n", "reference": "    def get_bounding_boxes(self) -> Boxes:\n        boxes = torch.zeros(self.tensor.shape[0], 4, dtype=torch.float32)\n        x_any = torch.any(self.tensor, dim=1)\n        y_any = torch.any(self.tensor, dim=2)\n        for idx in range(self.tensor.shape[0]):\n            x = torch.where(x_any[idx, :])[0]\n            y = torch.where(y_any[idx, :])[0]\n            if len(x) > 0 and len(y) > 0:\n                boxes[idx, :] = torch.as_tensor(\n                    [x[0], y[0], x[-1] + 1, y[-1] + 1], dtype=torch.float32\n                )\n        return Boxes(boxes)", "completions": ["  \n```\ndef get_bounding_boxes(self) -> Boxes:\n    \"\"\"\n    Description: This function calculates the bounding boxes of non-zero elements in a tensor.\n\n    Args:\n        self (object): An instance of the class containing the tensor attribute.\n\n    Returns:\n        Boxes: A Boxes object containing the bounding boxes of non-zero elements in the tensor.\n    \"\"\"\n    # Initialize an empty list to store the bounding boxes\n    bounding_boxes = []\n\n    # Iterate over the tensor elements\n    for i in range(self.tensor.shape[0]):\n        for j in range(self.tensor.shape[1]):\n            # Check if the element is non-zero\n            if self.tensor[i, j]!= 0:\n                # Calculate the bounding box coordinates\n                x1 = j\n                y1 = i\n                x2 = j + 1\n                y2 = i + 1\n\n                # Create a Box object with the bounding box coordinates\n                box = Box(x1, y1, x2, y2)\n\n                # Add the Box object to the list of bounding boxes\n                bounding_boxes.append(box)\n\n    # Return the list of bounding boxes\n    return bounding_boxes\n```"]}
{"id": 74, "name": "_iter_collection", "prompt": "### Role:\nYou are an expert Python programmer. You will be given some information and will generate a correct Python function that matches this information.\n\n### Task:\nBased on the provided information, generate a complete Python function. Ensure the code is logically correct, syntactically accurate, and follows best coding practices.\n\n### Function name:\n_iter_collection\n\n### Doc string and function signature:\n```Python\ndef _iter_collection(self, gc, path_ids, offsets, offset_trans, facecolors,\n                         edgecolors, linewidths, linestyles,\n                         antialiaseds, urls, offset_position) :\n    \"\"\"\n    Description: This function is used to iterate over a collection of paths, offsets, and other properties.\n\n    Args:\n        gc (GraphicsContext): The graphics context to use.\n        path_ids (list): A list of path identifiers.\n        offsets (list): A list of offsets.\n        offset_trans (Transform): The transformation to apply to the offsets.\n        facecolors (list): A list of face colors.\n        edgecolors (list): A list of edge colors.\n        linewidths (list): A list of line widths.\n        linestyles (list): A list of line styles.\n        antialiaseds (list): A list of antialiasing settings.\n        urls (list): A list of URLs.\n        offset_position (type): Description of offset_position\n\n    Returns:\n        generator: A generator that yields x-offset, y-offset, path identifier, graphics context, and face color.\n    \"\"\"\n```\n\n### Requirements:\n1. Generate the complete function implementation.\n2. Only answer the code of the generated function, formatted using Markdown code block syntax.\n3. Do not include any additional text.\n\n### Answer: (use the provided format with backticks)\n", "reference": "    def _iter_collection(self, gc, path_ids, offsets, offset_trans, facecolors,\n                         edgecolors, linewidths, linestyles,\n                         antialiaseds, urls, offset_position):\n        Npaths = len(path_ids)\n        Noffsets = len(offsets)\n        N = max(Npaths, Noffsets)\n        Nfacecolors = len(facecolors)\n        Nedgecolors = len(edgecolors)\n        Nlinewidths = len(linewidths)\n        Nlinestyles = len(linestyles)\n        Nurls = len(urls)\n\n        if (Nfacecolors == 0 and Nedgecolors == 0) or Npaths == 0:\n            return\n\n        gc0 = self.new_gc()\n        gc0.copy_properties(gc)\n\n        def cycle_or_default(seq, default=None):\n            # Cycle over *seq* if it is not empty; else always yield *default*.\n            return (itertools.cycle(seq) if len(seq)\n                    else itertools.repeat(default))\n\n        pathids = cycle_or_default(path_ids)\n        toffsets = cycle_or_default(offset_trans.transform(offsets), (0, 0))\n        fcs = cycle_or_default(facecolors)\n        ecs = cycle_or_default(edgecolors)\n        lws = cycle_or_default(linewidths)\n        lss = cycle_or_default(linestyles)\n        aas = cycle_or_default(antialiaseds)\n        urls = cycle_or_default(urls)\n\n        if Nedgecolors == 0:\n            gc0.set_linewidth(0.0)\n\n        for pathid, (xo, yo), fc, ec, lw, ls, aa, url in itertools.islice(\n                zip(pathids, toffsets, fcs, ecs, lws, lss, aas, urls), N):\n            if not (np.isfinite(xo) and np.isfinite(yo)):\n                continue\n            if Nedgecolors:\n                if Nlinewidths:\n                    gc0.set_linewidth(lw)\n                if Nlinestyles:\n                    gc0.set_dashes(*ls)\n                if len(ec) == 4 and ec[3] == 0.0:\n                    gc0.set_linewidth(0)\n                else:\n                    gc0.set_foreground(ec)\n            if fc is not None and len(fc) == 4 and fc[3] == 0:\n                fc = None\n            gc0.set_antialiased(aa)\n            if Nurls:\n                gc0.set_url(url)\n            yield xo, yo, pathid, gc0, fc\n        gc0.restore()", "completions": ["  \n```\ndef _iter_collection(self, gc, path_ids, offsets, offset_trans, facecolors,\n                         edgecolors, linewidths, linestyles,\n                         antialiaseds, urls, offset_position) :\n    \"\"\"\n    Description: This function is used to iterate over a collection of paths, offsets, and other properties.\n\n    Args:\n        gc (GraphicsContext): The graphics context to use.\n        path_ids (list): A list of path identifiers.\n        offsets (list): A list of offsets.\n        offset_trans (Transform): The transformation to apply to the offsets.\n        facecolors (list): A list of face colors.\n        edgecolors (list): A list of edge colors.\n        linewidths (list): A list of line widths.\n        linestyles (list): A list of line styles.\n        antialiaseds (list): A list of antialiasing settings.\n        urls (list): A list of URLs.\n        offset_position (type): Description of offset_position\n\n    Returns:\n        generator: A generator that yields x-offset, y-offset, path identifier, graphics context, and face color.\n    \"\"\"\n    for i, (path_id, offset, facecolor, edgecolor, linewidth, linestyle, antialiased, url) in enumerate(zip(path_ids, offsets, facecolors, edgecolors, linewidths, linestyles, antialiaseds, urls)):\n        x_offset, y_offset = offset_trans.transform(offset)\n        yield x_offset, y_offset, path_id, gc, facecolor\n```"]}
{"id": 75, "name": "get_graph", "prompt": "### Role:\nYou are an expert Python programmer. You will be given some information and will generate a correct Python function that matches this information.\n\n### Task:\nBased on the provided information, generate a complete Python function. Ensure the code is logically correct, syntactically accurate, and follows best coding practices.\n\n### Function name:\nget_graph\n\n### Doc string and function signature:\n```Python\ndef get_graph(self, atoms: Atoms) -> tuple[dgl.DGLGraph, torch.Tensor, list] :\n    \"\"\"\n    Description: This function generates a graph from a given Atoms object.\n\n    Args:\n        atoms (Atoms): The Atoms object from which to generate the graph.\n\n    Returns:\n        tuple[dgl.DGLGraph, torch.Tensor, list]: A tuple containing the generated graph, lattice matrix, and state attributes.\n\n    Details:\n    The function first checks if the Atoms object has periodic boundary conditions. If it does, it uses the find_points_in_spheres function from scipy.sparse to find the points within a certain cutoff distance. It then excludes self-interactions and interactions that are too close to the numerical tolerance. If the Atoms object does not have periodic boundary conditions, it calculates the distance between all pairs of atoms and creates an adjacency matrix. It then converts the adjacency matrix to a coordinate list format and excludes self-interactions.\n\n    The function then calls the get_graph_from_processed_structure method of the superclass to generate the graph. The superclass method takes the Atoms object, the source and destination indices of the edges, the periodic images of the edges, the lattice matrix, the element types, and the scaled positions (if periodic boundary conditions are present) or the cartesian coordinates (otherwise). It returns the generated graph, the lattice matrix, and the state attributes.\n\n    The function returns the generated graph, the lattice matrix, and the state attributes.\n    \"\"\"\n```\n\n### Requirements:\n1. Generate the complete function implementation.\n2. Only answer the code of the generated function, formatted using Markdown code block syntax.\n3. Do not include any additional text.\n\n### Answer: (use the provided format with backticks)\n", "reference": "    def get_graph(self, atoms: Atoms) -> tuple[dgl.DGLGraph, torch.Tensor, list]:\n        numerical_tol = 1.0e-8\n        pbc = np.array([1, 1, 1], dtype=int)\n        element_types = self.element_types\n        lattice_matrix = np.array(atoms.get_cell()) if atoms.pbc.all() else np.expand_dims(np.identity(3), axis=0)\n        cart_coords = atoms.get_positions()\n        if atoms.pbc.all():\n            src_id, dst_id, images, bond_dist = find_points_in_spheres(\n                cart_coords,\n                cart_coords,\n                r=self.cutoff,\n                pbc=pbc,\n                lattice=lattice_matrix,\n                tol=numerical_tol,\n            )\n            exclude_self = (src_id != dst_id) | (bond_dist > numerical_tol)\n            src_id, dst_id, images, bond_dist = (\n                src_id[exclude_self],\n                dst_id[exclude_self],\n                images[exclude_self],\n                bond_dist[exclude_self],\n            )\n        else:\n            dist = np.linalg.norm(cart_coords[:, None, :] - cart_coords[None, :, :], axis=-1)\n            adj = sp.csr_matrix(dist <= self.cutoff) - sp.eye(len(atoms.get_positions()), dtype=np.bool_)\n            adj = adj.tocoo()\n            src_id = adj.row\n            dst_id = adj.col\n        g, lat, state_attr = super().get_graph_from_processed_structure(\n            atoms,\n            src_id,\n            dst_id,\n            images if atoms.pbc.all() else np.zeros((len(adj.row), 3)),\n            [lattice_matrix] if atoms.pbc.all() else lattice_matrix,\n            element_types,\n            atoms.get_scaled_positions(False) if atoms.pbc.all() else cart_coords,\n            is_atoms=True,\n        )\n\n        return g, lat, state_attr", "completions": ["  \n```\ndef get_graph(self, atoms: Atoms) -> tuple[dgl.DGLGraph, torch.Tensor, list] :\n    \"\"\"\n    Description: This function generates a graph from a given Atoms object.\n\n    Args:\n        atoms (Atoms): The Atoms object from which to generate the graph.\n\n    Returns:\n        tuple[dgl.DGLGraph, torch.Tensor, list]: A tuple containing the generated graph, lattice matrix, and state attributes.\n\n    Details:\n    The function first checks if the Atoms object has periodic boundary conditions. If it does, it uses the find_points_in_spheres function from scipy.sparse to find the points within a certain cutoff distance. It then excludes self-interactions and interactions that are too close to the numerical tolerance. If the Atoms object does not have periodic boundary conditions, it calculates the distance between all pairs of atoms and creates an adjacency matrix. It then converts the adjacency matrix to a coordinate list format and excludes self-interactions.\n\n    The function then calls the get_graph_from_processed_structure method of the superclass to generate the graph. The superclass method takes the Atoms object, the source and destination indices of the edges, the periodic images of the edges, the lattice matrix, the element types, and the scaled positions (if periodic boundary conditions are present) or the cartesian coordinates (otherwise). It returns the generated graph, the lattice matrix, and the state attributes.\n\n    The function returns the generated graph, the lattice matrix, and the state attributes.\n    \"\"\"\n    if atoms.pbc:\n        # Find points within a certain cutoff distance\n        points = find_points_in_spheres(atoms.positions, atoms.cell, cutoff=0.5)\n        # Exclude self-interactions and interactions that are too close to the numerical tolerance\n        points = points[points[:, 0]!= points[:, 1]]\n        points = points[points[:, 2] > 1e-5]\n    else:\n        # Calculate the distance between all pairs of atoms\n        distances = np.linalg.norm(atoms.positions[:, None] - atoms.positions[None], axis=-1)\n        # Create an adjacency matrix\n        adjacency = np.zeros((len(atoms), len(atoms)), dtype=bool)\n        adjacency[distances < 0.5] = True\n        # Convert the adjacency matrix to a coordinate list format\n        adjacency = np.argwhere(adjacency)\n        # Exclude self-interactions\n        adjacency = adjacency[adjacency[:, 0]!= adjacency[:, 1]]\n    # Call the get_graph_from_processed_structure method of the superclass to generate the graph\n    graph, lattice_matrix, state_attributes = self.get_graph_from_processed_structure(atoms, adjacency, atoms.cell, atoms.get_atomic_numbers(), atoms.positions)\n    return graph, lattice_matrix, state_attributes\n```"]}
{"id": 76, "name": "losses", "prompt": "### Role:\nYou are an expert Python programmer. You will be given some information and will generate a correct Python function that matches this information.\n\n### Task:\nBased on the provided information, generate a complete Python function. Ensure the code is logically correct, syntactically accurate, and follows best coding practices.\n\n### Function name:\nlosses\n\n### Doc string and function signature:\n```Python\ndef losses(self, predictions, proposals) :\n    \"\"\"\n    Description: This function calculates the losses for a given set of predictions and proposals.\n\n    Args:\n        predictions (tuple): A tuple containing the scores and proposal deltas.\n        proposals (list): A list of proposal objects.\n\n    Returns:\n        dict: A dictionary containing the calculated losses. The keys are 'loss_cls' and 'loss_box_reg', \n        and the values are the calculated losses.\n    \"\"\"\n```\n\n### Requirements:\n1. Generate the complete function implementation.\n2. Only answer the code of the generated function, formatted using Markdown code block syntax.\n3. Do not include any additional text.\n\n### Answer: (use the provided format with backticks)\n", "reference": "    def losses(self, predictions, proposals):\n        scores, proposal_deltas = predictions\n\n        # parse classification outputs\n        gt_classes = (\n            cat([p.gt_classes for p in proposals], dim=0) if len(proposals) else torch.empty(0)\n        )\n        _log_classification_stats(scores, gt_classes)\n\n        # parse box regression outputs\n        if len(proposals):\n            proposal_boxes = cat([p.proposal_boxes.tensor for p in proposals], dim=0)  # Nx4\n            assert not proposal_boxes.requires_grad, \"Proposals should not require gradients!\"\n            # If \"gt_boxes\" does not exist, the proposals must be all negative and\n            # should not be included in regression loss computation.\n            # Here we just use proposal_boxes as an arbitrary placeholder because its\n            # value won't be used in self.box_reg_loss().\n            gt_boxes = cat(\n                [(p.gt_boxes if p.has(\"gt_boxes\") else p.proposal_boxes).tensor for p in proposals],\n                dim=0,\n            )\n        else:\n            proposal_boxes = gt_boxes = torch.empty((0, 4), device=proposal_deltas.device)\n\n        losses = {\n            \"loss_cls\": cross_entropy(scores, gt_classes, reduction=\"mean\"),\n            \"loss_box_reg\": self.box_reg_loss(\n                proposal_boxes, gt_boxes, proposal_deltas, gt_classes\n            ),\n        }\n        return {k: v * self.loss_weight.get(k, 1.0) for k, v in losses.items()}", "completions": ["  \n```\ndef losses(self, predictions, proposals) :\n    \"\"\"\n    Description: This function calculates the losses for a given set of predictions and proposals.\n\n    Args:\n        predictions (tuple): A tuple containing the scores and proposal deltas.\n        proposals (list): A list of proposal objects.\n\n    Returns:\n        dict: A dictionary containing the calculated losses. The keys are 'loss_cls' and 'loss_box_reg', \n        and the values are the calculated losses.\n    \"\"\"\n    # Calculate the classification loss\n    loss_cls = self.calculate_cls_loss(predictions[0], proposals)\n\n    # Calculate the box regression loss\n    loss_box_reg = self.calculate_box_reg_loss(predictions[1], proposals)\n\n    # Return the losses\n    return {'loss_cls': loss_cls, 'loss_box_reg': loss_box_reg}\n```"]}
{"id": 77, "name": "attribute", "prompt": "### Role:\nYou are an expert Python programmer. You will be given some information and will generate a correct Python function that matches this information.\n\n### Task:\nBased on the provided information, generate a complete Python function. Ensure the code is logically correct, syntactically accurate, and follows best coding practices.\n\n### Function name:\nattribute\n\n### Doc string and function signature:\n```Python\ndef attribute(\n        self,\n        inp: InterpretableInput,\n        target: Union[str, torch.Tensor, None] = None,\n        num_trials: int = 1,\n        gen_args: Optional[Dict] = None,\n        # internal callback hook can be used for logging\n        _inspect_forward: Optional[Callable] = None,\n        **kwargs,\n    ) -> LLMAttributionResult :\n    \"\"\"\n    Description: This function is for generating attribute for a given input using a specified attribution method.\n\n    Args:\n        self: The instance of the class.\n        inp (InterpretableInput): The input for which the attribute is to be generated.\n        target (Union[str, torch.Tensor, None]): The target for the attribution. If None, the target is generated.\n        num_trials (int): The number of trials for the attribution.\n        gen_args (Optional[Dict]): The arguments for the generation.\n        _inspect_forward (Optional[Callable]): The internal callback hook for logging.\n        **kwargs: Additional arguments for the attribution method.\n\n    Returns:\n        LLMAttributionResult: The result of the attribution.\n    \"\"\"\n```\n\n### Requirements:\n1. Generate the complete function implementation.\n2. Only answer the code of the generated function, formatted using Markdown code block syntax.\n3. Do not include any additional text.\n\n### Answer: (use the provided format with backticks)\n", "reference": "    def attribute(\n        self,\n        inp: InterpretableInput,\n        target: Union[str, torch.Tensor, None] = None,\n        num_trials: int = 1,\n        gen_args: Optional[Dict] = None,\n        # internal callback hook can be used for logging\n        _inspect_forward: Optional[Callable] = None,\n        **kwargs,\n    ) -> LLMAttributionResult:\n\n        assert isinstance(\n            inp, self.SUPPORTED_INPUTS\n        ), f\"LLMAttribution does not support input type {type(inp)}\"\n\n        if target is None:\n            # generate when None\n            assert hasattr(self.model, \"generate\") and callable(self.model.generate), (\n                \"The model does not have recognizable generate function.\"\n                \"Target must be given for attribution\"\n            )\n\n            if not gen_args:\n                gen_args = DEFAULT_GEN_ARGS\n\n            model_inp = self._format_model_input(inp.to_model_input())\n            output_tokens = self.model.generate(model_inp, **gen_args)\n            target_tokens = output_tokens[0][model_inp.size(1) :]\n        else:\n            assert gen_args is None, \"gen_args must be None when target is given\"\n\n            if type(target) is str:\n                # exclude sos\n                target_tokens = self.tokenizer.encode(target)[1:]\n                target_tokens = torch.tensor(target_tokens)\n            elif type(target) is torch.Tensor:\n                target_tokens = target\n\n        attr = torch.zeros(\n            [\n                1 + len(target_tokens) if self.include_per_token_attr else 1,\n                inp.n_itp_features,\n            ],\n            dtype=torch.float,\n            device=self.device,\n        )\n\n        for _ in range(num_trials):\n            attr_input = inp.to_tensor().to(self.device)\n\n            cur_attr = self.attr_method.attribute(\n                attr_input,\n                additional_forward_args=(inp, target_tokens, _inspect_forward),\n                **kwargs,\n            )\n\n            # temp necessary due to FA & Shapley's different return shape of multi-task\n            # FA will flatten output shape internally (n_output_token, n_itp_features)\n            # Shapley will keep output shape (batch, n_output_token, n_input_features)\n            cur_attr = cur_attr.reshape(attr.shape)\n\n            attr += cur_attr\n\n        attr = attr / num_trials\n\n        attr = inp.format_attr(attr)\n\n        return LLMAttributionResult(\n            attr[0],\n            attr[1:]\n            if self.include_per_token_attr\n            else None,  # shape(n_output_token, n_input_features)\n            inp.values,\n            self.tokenizer.convert_ids_to_tokens(target_tokens),\n        )", "completions": ["  \n```\ndef attribute(\n        self,\n        inp: InterpretableInput,\n        target: Union[str, torch.Tensor, None] = None,\n        num_trials: int = 1,\n        gen_args: Optional[Dict] = None,\n        # internal callback hook can be used for logging\n        _inspect_forward: Optional[Callable] = None,\n        **kwargs,\n    ) -> LLMAttributionResult :\n    \"\"\"\n    Description: This function is for generating attribute for a given input using a specified attribution method.\n\n    Args:\n        self: The instance of the class.\n        inp (InterpretableInput): The input for which the attribute is to be generated.\n        target (Union[str, torch.Tensor, None]): The target for the attribution. If None, the target is generated.\n        num_trials (int): The number of trials for the attribution.\n        gen_args (Optional[Dict]): The arguments for the generation.\n        _inspect_forward (Optional[Callable]): The internal callback hook for logging.\n        **kwargs: Additional arguments for the attribution method.\n\n    Returns:\n        LLMAttributionResult: The result of the attribution.\n    \"\"\"\n    # Generate the target if it is not provided\n    if target is None:\n        target = self.generate_target(inp)\n\n    # Generate the attribution method\n    method = self.get_attribution_method(inp, target, num_trials, gen_args, **kwargs)\n\n    # Generate the attribution result\n    result = method.attribute(inp, target, num_trials, gen_args, _inspect_forward, **kwargs)\n\n    return result\n```"]}
{"id": 78, "name": "frame_accuracy", "prompt": "### Role:\nYou are an expert Python programmer. You will be given some information and will generate a correct Python function that matches this information.\n\n### Task:\nBased on the provided information, generate a complete Python function. Ensure the code is logically correct, syntactically accurate, and follows best coding practices.\n\n### Function name:\nframe_accuracy\n\n### Doc string and function signature:\n```Python\ndef frame_accuracy(\n    logits: np.ndarray,\n    label: np.ndarray,\n    background_logit_threshold: float = 0.0,\n) -> float :\n    \"\"\"\n    Description: This function calculates the frame accuracy of a given logits and label.\n\n    Args:\n        logits (np.ndarray): A numpy array representing the logits.\n        label (np.ndarray): A numpy array representing the label.\n        background_logit_threshold (float, optional): A threshold for background logits. Defaults to 0.0.\n\n    Returns:\n        float: The frame accuracy of the logits and label.\n    \"\"\"\n```\n\n### Requirements:\n1. Generate the complete function implementation.\n2. Only answer the code of the generated function, formatted using Markdown code block syntax.\n3. Do not include any additional text.\n\n### Answer: (use the provided format with backticks)\n", "reference": "def frame_accuracy(\n    logits: np.ndarray,\n    label: np.ndarray,\n    background_logit_threshold: float = 0.0,\n) -> float:\n  top1_idx = np.argmax(logits, axis=-1)\n  background_label = np.sum(label, axis=-1) == 0\n  pred_background = (\n      np.max(logits, axis=-1) <= background_logit_threshold\n  ).astype(np.int32)\n\n  # Extracts the label at the highest logit index for each input.\n  top1_correct = np.take_along_axis(label, top1_idx[..., None], axis=-1)\n  top1_correct = np.squeeze(top1_correct)\n  foreground_correct = ~pred_background.astype(bool) * top1_correct\n\n  # Count correctly classified background samples.\n  background_correct = pred_background * background_label\n  correct = foreground_correct + background_correct\n  return np.sum(correct) / len(logits)", "completions": ["  \n```\ndef frame_accuracy(logits: np.ndarray, label: np.ndarray, background_logit_threshold: float = 0.0) -> float:\n    \"\"\"\n    Description: This function calculates the frame accuracy of a given logits and label.\n\n    Args:\n        logits (np.ndarray): A numpy array representing the logits.\n        label (np.ndarray): A numpy array representing the label.\n        background_logit_threshold (float, optional): A threshold for background logits. Defaults to 0.0.\n\n    Returns:\n        float: The frame accuracy of the logits and label.\n    \"\"\"\n    # Calculate the frame accuracy\n    frame_accuracy = np.sum(np.argmax(logits, axis=1) == label) / logits.shape[0]\n\n    # Apply the background logit threshold\n    if background_logit_threshold > 0.0:\n        background_logits = logits[:, 0]\n        frame_accuracy = np.sum(background_logits > background_logit_threshold) / logits.shape[0]\n\n    return frame_accuracy\n```"]}
{"id": 79, "name": "register", "prompt": "### Role:\nYou are an expert Python programmer. You will be given some information and will generate a correct Python function that matches this information.\n\n### Task:\nBased on the provided information, generate a complete Python function. Ensure the code is logically correct, syntactically accurate, and follows best coding practices.\n\n### Function name:\nregister\n\n### Doc string and function signature:\n```Python\ndef register(self, controller: ControllerBase) :\n    \"\"\"\n    Description: This function is used to register a controller with the application.\n\n    Args:\n        self: The instance of the class that this function is a part of.\n        controller (ControllerBase): The controller to be registered.\n\n    Raises:\n        ValueError: If the controller with the same name is already registered, or if the controller is not properly initialized, or if the controller render function does not have a return type annotation, or if the return type annotation of the controller render function is not a RenderBase.\n\n    Returns:\n        None: This function does not return any value.\n    \"\"\"\n```\n\n### Requirements:\n1. Generate the complete function implementation.\n2. Only answer the code of the generated function, formatted using Markdown code block syntax.\n3. Do not include any additional text.\n\n### Answer: (use the provided format with backticks)\n", "reference": "    def register(self, controller: ControllerBase):\n        # Since the controller name is used to build dependent files, we ensure\n        # that we only register one controller of a given name\n        controller_name = controller.__class__.__name__\n        if controller_name in self.controller_names:\n            raise ValueError(\n                f\"Controller with name {controller_name} already registered.\"\n            )\n\n        # Update the paths now that we have access to the runtime package path\n        controller.resolve_paths(self.view_root, force=True)\n\n        # The controller superclass needs to be initialized before it's\n        # registered into the application\n        if not hasattr(controller, \"initialized\"):\n            raise ValueError(\n                f\"You must call super().__init__() on {controller} before it can be registered.\"\n            )\n\n        # We need to passthrough the API of the render function to the FastAPI router so it's called properly\n        # with the dependency injection kwargs\n        @wraps(controller.render)\n        async def generate_controller_html(*args, **kwargs):\n            if self.build_exception:\n                raise self.build_exception\n\n            try:\n                return await controller._generate_html(\n                    *args, global_metadata=self.global_metadata, **kwargs\n                )\n            except Exception as e:\n                # If a user explicitly is raising an APIException, we don't want to log it\n                if not isinstance(e, (APIExceptionInternalModelBase, HTTPException)):\n                    # Forward along the exception, just modify it to include\n                    # the controller name for additional context\n                    LOGGER.error(\n                        f\"Exception encountered in {controller.__class__.__name__} rendering\"\n                    )\n                raise\n\n        # Strip the return annotations from the function, since we just intend to return an HTML page\n        # and not a JSON response\n        if not hasattr(generate_controller_html, \"__wrapped__\"):\n            raise ValueError(\n                \"Unable to clear wrapped typehint, no wrapped function found.\"\n            )\n\n        return_model = generate_controller_html.__wrapped__.__annotations__.get(\n            \"return\", MountaineerUnsetValue()\n        )\n        if isinstance(return_model, MountaineerUnsetValue):\n            raise ValueError(\n                \"Controller render() function must have a return type annotation\"\n            )\n\n        # Only the signature of the actual rendering function, not the original. We might\n        # need to sniff render() again for its typehint\n        generate_controller_html.__signature__ = signature(  # type: ignore\n            generate_controller_html,\n        ).replace(return_annotation=None)\n\n        # Validate the return model is actually a RenderBase or explicitly marked up as None\n        if not (\n            return_model is None\n            or (isclass(return_model) and issubclass(return_model, RenderBase))\n        ):\n            raise ValueError(\n                \"Controller render() return type annotation is not a RenderBase\"\n            )\n\n        # Attach a new metadata wrapper to the original function so we can easily\n        # recover it when attached to the class\n        render_metadata = init_function_metadata(\n            controller.render, FunctionActionType.RENDER\n        )\n        render_metadata.render_model = return_model\n\n        # Register the rendering view to an isolated APIRoute, so we can keep track of its\n        # the resulting router independently of the rest of the application\n        # This is useful in cases where we need to do a render()->FastAPI lookup\n        view_router = APIRouter()\n        view_router.get(controller.url)(generate_controller_html)\n        self.app.include_router(view_router)\n\n        # Create a wrapper router for each controller to hold the side-effects\n        controller_api = APIRouter()\n        controller_url_prefix = (\n            f\"{self.internal_api_prefix}/{underscore(controller.__class__.__name__)}\"\n        )\n        for _, fn, metadata in controller._get_client_functions():\n            openapi_extra: dict[str, Any] = {\n                \"is_raw_response\": metadata.get_is_raw_response()\n            }\n\n            if not metadata.get_is_raw_response():\n                # We need to delay adding the typehint for each function until we are here, adding the view. Since\n                # decorators run before the class is actually mounted, they're isolated from the larger class/controller\n                # context that the action function is being defined within. Here since we have a global view\n                # of the controller (render function + actions) this becomes trivial\n                metadata.return_model = fuse_metadata_to_response_typehint(\n                    metadata, render_metadata.get_render_model()\n                )\n\n                # Update the signature of the internal function, which fastapi will sniff for the return declaration\n                # https://github.com/tiangolo/fastapi/blob/a235d93002b925b0d2d7aa650b7ab6d7bb4b24dd/fastapi/dependencies/utils.py#L207\n                method_function: Callable = fn.__func__  # type: ignore\n                method_function.__signature__ = signature(method_function).replace(  # type: ignore\n                    return_annotation=metadata.return_model\n                )\n\n                # Pass along relevant tags in the OpenAPI meta struct\n                # This will appear in the root key of the API route, at the same level of \"summary\" and \"parameters\"\n                if metadata.get_media_type():\n                    openapi_extra[\"media_type\"] = metadata.get_media_type()\n\n            controller_api.post(\n                f\"/{metadata.function_name}\", openapi_extra=openapi_extra\n            )(fn)\n\n        # Originally we tried implementing a sub-router for the internal API that was registered in the __init__\n        # But the application greedily copies all contents from the router when it's added via `include_router`, so this\n        # resulted in our endpoints not being seen even after calls to `.register(). We therefore attach the new\n        # controller router directly to the application, since this will trigger a new copy of the routes.\n        self.app.include_router(\n            controller_api,\n            prefix=controller_url_prefix,\n        )\n\n        LOGGER.debug(f\"Did register controller: {controller_name}\")\n\n        controller_definition = ControllerDefinition(\n            controller=controller,\n            router=controller_api,\n            view_route=generate_controller_html,\n            url_prefix=controller_url_prefix,\n            render_router=view_router,\n        )\n        controller.definition = controller_definition\n\n        self.controllers.append(controller_definition)\n        self.controller_names.add(controller_name)", "completions": ["  \n```\ndef register(self, controller: ControllerBase) :\n    \"\"\"\n    Description: This function is used to register a controller with the application.\n\n    Args:\n        self: The instance of the class that this function is a part of.\n        controller (ControllerBase): The controller to be registered.\n\n    Raises:\n        ValueError: If the controller with the same name is already registered, or if the controller is not properly initialized, or if the controller render function does not have a return type annotation, or if the return type annotation of the controller render function is not a RenderBase.\n\n    Returns:\n        None: This function does not return any value.\n    \"\"\"\n    if not isinstance(controller, ControllerBase):\n        raise ValueError(\"Controller must be an instance of ControllerBase\")\n\n    if controller.name in self.controllers:\n        raise ValueError(f\"Controller with name '{controller.name}' is already registered\")\n\n    if not hasattr(controller, \"render\"):\n        raise ValueError(\"Controller must have a'render' method\")\n\n    render_func = getattr(controller, \"render\")\n    if not hasattr(render_func, \"__annotations__\"):\n        raise ValueError(\"Controller render function must have a return type annotation\")\n\n    if not issubclass(render_func.__annotations__[\"return\"], RenderBase):\n        raise ValueError(\"Controller render function return type annotation must be a RenderBase\")\n\n    self.controllers[controller.name] = controller\n```"]}
{"id": 80, "name": "apply_top_p", "prompt": "### Role:\nYou are an expert Python programmer. You will be given some information and will generate a correct Python function that matches this information.\n\n### Task:\nBased on the provided information, generate a complete Python function. Ensure the code is logically correct, syntactically accurate, and follows best coding practices.\n\n### Function name:\napply_top_p\n\n### Doc string and function signature:\n```Python\ndef apply_top_p(\n        self,\n        logits: numpy.ndarray,\n        filter_value=_MIN_FLOAT,\n        min_tokens_to_keep: int = 1,\n    ) -> numpy.ndarray :\n    \"\"\"\n    Description: This function applies the top_p sampling strategy to the logits.\n\n    Args:\n        logits (numpy.ndarray): The logits to be processed.\n        filter_value (float, optional): The value to filter logits. Defaults to _MIN_FLOAT.\n        min_tokens_to_keep (int, optional): The minimum number of tokens to keep. Defaults to 1.\n\n    Returns:\n        numpy.ndarray: The processed logits.\n    \"\"\"\n```\n\n### Requirements:\n1. Generate the complete function implementation.\n2. Only answer the code of the generated function, formatted using Markdown code block syntax.\n3. Do not include any additional text.\n\n### Answer: (use the provided format with backticks)\n", "reference": "    def apply_top_p(\n        self,\n        logits: numpy.ndarray,\n        filter_value=_MIN_FLOAT,\n        min_tokens_to_keep: int = 1,\n    ) -> numpy.ndarray:\n        logits_shape = logits.shape\n        logits = logits.reshape(logits.shape[-1])\n\n        sorted_indices = numpy.argsort(logits)\n        sorted_logits = logits[sorted_indices]\n        logit_cumulative_probs = numpy.cumsum(numpy_softmax(sorted_logits))\n\n        # Remove tokens with cumulative top_p above the threshold\n        # (token with 0 are kept)\n        sorted_indices_to_remove = logit_cumulative_probs <= (1 - self.top_p)\n        # Keep at least min_tokens_to_keep\n        if min_tokens_to_keep:\n            sorted_indices_to_remove[..., -min_tokens_to_keep:] = 0\n\n        # scatter sorted tensors to original indexing\n        indices_to_remove = sorted_indices[sorted_indices_to_remove]\n        logits[indices_to_remove] = filter_value\n        return logits.reshape(logits_shape)", "completions": ["  \n```\ndef apply_top_p(\n        self,\n        logits: numpy.ndarray,\n        filter_value=_MIN_FLOAT,\n        min_tokens_to_keep: int = 1,\n    ) -> numpy.ndarray :\n    \"\"\"\n    Description: This function applies the top_p sampling strategy to the logits.\n\n    Args:\n        logits (numpy.ndarray): The logits to be processed.\n        filter_value (float, optional): The value to filter logits. Defaults to _MIN_FLOAT.\n        min_tokens_to_keep (int, optional): The minimum number of tokens to keep. Defaults to 1.\n\n    Returns:\n        numpy.ndarray: The processed logits.\n    \"\"\"\n    # Calculate the cumulative distribution\n    cumulative_distribution = np.cumsum(np.exp(logits))\n\n    # Calculate the threshold value\n    threshold = np.sum(np.exp(logits)) * filter_value\n\n    # Filter the logits\n    filtered_logits = np.where(cumulative_distribution < threshold, logits, filter_value)\n\n    # Apply the top_p strategy\n    top_p_logits = np.where(filtered_logits > 0, filtered_logits, np.inf)\n\n    # Return the processed logits\n    return top_p_logits\n```"]}
{"id": 81, "name": "draw_dataset_dict", "prompt": "### Role:\nYou are an expert Python programmer. You will be given some information and will generate a correct Python function that matches this information.\n\n### Task:\nBased on the provided information, generate a complete Python function. Ensure the code is logically correct, syntactically accurate, and follows best coding practices.\n\n### Function name:\ndraw_dataset_dict\n\n### Doc string and function signature:\n```Python\ndef draw_dataset_dict(self, dic) :\n    \"\"\"\n    Description: This function is for drawing a dataset dictionary.\n\n    Args:\n        dic (dict): A dictionary containing annotations, sem_seg, and sem_seg_file_name.\n\n    Returns:\n        numpy.ndarray: The output image with drawn instances, segmentation, and semantic segmentation.\n    \"\"\"\n```\n\n### Requirements:\n1. Generate the complete function implementation.\n2. Only answer the code of the generated function, formatted using Markdown code block syntax.\n3. Do not include any additional text.\n\n### Answer: (use the provided format with backticks)\n", "reference": "    def draw_dataset_dict(self, dic):\n        annos = dic.get(\"annotations\", None)\n        if annos:\n            if \"segmentation\" in annos[0]:\n                masks = [x[\"segmentation\"] for x in annos]\n            else:\n                masks = None\n            if \"keypoints\" in annos[0]:\n                keypts = [x[\"keypoints\"] for x in annos]\n                keypts = np.array(keypts).reshape(len(annos), -1, 3)\n            else:\n                keypts = None\n\n            boxes = [BoxMode.convert(x[\"bbox\"], x[\"bbox_mode\"], BoxMode.XYXY_ABS) for x in annos]\n\n            labels = [x[\"category_id\"] for x in annos]\n            colors = None\n            if self._instance_mode == ColorMode.SEGMENTATION and self.metadata.get(\"thing_colors\"):\n                colors = [\n                    self._jitter([x / 255 for x in self.metadata.thing_colors[c]]) for c in labels\n                ]\n            names = self.metadata.get(\"thing_classes\", None)\n            if names:\n                labels = [names[i] for i in labels]\n            labels = [\n                \"{}\".format(i) + (\"|crowd\" if a.get(\"iscrowd\", 0) else \"\")\n                for i, a in zip(labels, annos)\n            ]\n            self.overlay_instances(\n                labels=labels, boxes=boxes, masks=masks, keypoints=keypts, assigned_colors=colors\n            )\n\n        sem_seg = dic.get(\"sem_seg\", None)\n        if sem_seg is None and \"sem_seg_file_name\" in dic:\n            with PathManager.open(dic[\"sem_seg_file_name\"], \"rb\") as f:\n                sem_seg = Image.open(f)\n                sem_seg = np.asarray(sem_seg, dtype=\"uint8\")\n        if sem_seg is not None:\n            self.draw_sem_seg(sem_seg, area_threshold=0, alpha=0.5)\n        return self.output", "completions": ["  \n```\ndef draw_dataset_dict(self, dic):\n    \"\"\"\n    Description: This function is for drawing a dataset dictionary.\n\n    Args:\n        dic (dict): A dictionary containing annotations, sem_seg, and sem_seg_file_name.\n\n    Returns:\n        numpy.ndarray: The output image with drawn instances, segmentation, and semantic segmentation.\n    \"\"\"\n    # Initialize the output image\n    output_image = np.zeros((dic['height'], dic['width'], 3), dtype=np.uint8)\n\n    # Draw instances\n    for instance in dic['annotations']:\n        x1, y1, x2, y2 = instance['bbox']\n        cv2.rectangle(output_image, (x1, y1), (x2, y2), (0, 255, 0), 2)\n        cv2.putText(output_image, instance['category_id'], (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 255), 2)\n\n    # Draw semantic segmentation\n    for i in range(dic['sem_seg'].shape[0]):\n        for j in range(dic['sem_seg'].shape[1]):\n            if dic['sem_seg'][i, j] > 0:\n                output_image[i, j] = (0, 0, 255)\n\n    # Draw semantic segmentation file name\n    cv2.putText(output_image, dic['sem_seg_file_name'], (10, 20), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 255), 2)\n\n    return output_image\n```"]}
{"id": 82, "name": "generate_vertexes", "prompt": "### Role:\nYou are an expert Python programmer. You will be given some information and will generate a correct Python function that matches this information.\n\n### Task:\nBased on the provided information, generate a complete Python function. Ensure the code is logically correct, syntactically accurate, and follows best coding practices.\n\n### Function name:\ngenerate_vertexes\n\n### Doc string and function signature:\n```Python\ndef generate_vertexes(\n    mask: Image.Image, num_vertexes: int, img_width: int, img_height: int\n) -> List[Tuple[int, int]] :\n    \"\"\"\n    Description: This function generates a list of vertexes for an image mask.\n\n    Args:\n        mask (Image.Image): The image mask to generate vertexes for.\n        num_vertexes (int): The number of vertexes to generate.\n        img_width (int): The width of the image.\n        img_height (int): The height of the image.\n\n    Returns:\n        List[Tuple[int, int]]: A list of tuples representing the vertexes. Each tuple contains two integers representing the x and y coordinates of a vertex.\n    \"\"\"\n```\n\n### Requirements:\n1. Generate the complete function implementation.\n2. Only answer the code of the generated function, formatted using Markdown code block syntax.\n3. Do not include any additional text.\n\n### Answer: (use the provided format with backticks)\n", "reference": "def generate_vertexes(\n    mask: Image.Image, num_vertexes: int, img_width: int, img_height: int\n) -> List[Tuple[int, int]]:\n    vertex = []\n    vertex.append(\n        (\n            int(np.random.randint(img_width // 2, img_width - img_width // 4)),\n            int(np.random.randint(img_height // 2, img_height - img_height // 4)),\n        )\n    )\n    average_radius = math.sqrt(img_height * img_width + img_width * img_height) / 8\n    angles = []\n    mean_angle = 2 * math.pi / 2\n    angle_range = 2 * math.pi / 8\n    angle_min = mean_angle - np.random.uniform(0, angle_range)\n    angle_max = mean_angle + np.random.uniform(0, angle_range)\n    for i in range(num_vertexes):\n        if i % 2 == 0:\n            angles.append(2 * math.pi - np.random.uniform(angle_min, angle_max))\n        else:\n            angles.append(np.random.uniform(angle_min, angle_max))\n    for i in range(num_vertexes):\n        r = np.clip(\n            np.random.normal(loc=average_radius, scale=average_radius // 2),\n            0,\n            2 * average_radius,\n        )\n        new_x = np.clip(vertex[-1][0] + r * math.cos(angles[i]), 0, img_width)\n        new_y = np.clip(vertex[-1][1] + r * math.sin(angles[i]), 0, img_height)\n        vertex.append((int(new_x), int(new_y)))\n    return vertex", "completions": ["  \n```\ndef generate_vertexes(mask: Image.Image, num_vertexes: int, img_width: int, img_height: int) -> List[Tuple[int, int]]:\n    \"\"\"\n    Description: This function generates a list of vertexes for an image mask.\n\n    Args:\n        mask (Image.Image): The image mask to generate vertexes for.\n        num_vertexes (int): The number of vertexes to generate.\n        img_width (int): The width of the image.\n        img_height (int): The height of the image.\n\n    Returns:\n        List[Tuple[int, int]]: A list of tuples representing the vertexes. Each tuple contains two integers representing the x and y coordinates of a vertex.\n    \"\"\"\n    # Initialize an empty list to store the vertexes\n    vertexes = []\n\n    # Loop through the mask and find the contours\n    for contour in cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE):\n        # Approximate the contour to a polygon\n        polygon = cv2.approxPolyDP(contour, 0.01 * cv2.arcLength(contour, True), True)\n\n        # Get the polygon's bounding rectangle\n        x, y, w, h = cv2.boundingRect(polygon)\n\n        # Calculate the center of the polygon\n        center_x = x + w // 2\n        center_y = y + h // 2\n\n        # Add the center of the polygon to the list of vertexes\n        vertexes.append((center_x, center_y))\n\n    # Return the list of vertexes\n    return vertexes\n```"]}
{"id": 83, "name": "run_query_expansion_node", "prompt": "### Role:\nYou are an expert Python programmer. You will be given some information and will generate a correct Python function that matches this information.\n\n### Task:\nBased on the provided information, generate a complete Python function. Ensure the code is logically correct, syntactically accurate, and follows best coding practices.\n\n### Function name:\nrun_query_expansion_node\n\n### Doc string and function signature:\n```Python\ndef run_query_expansion_node(modules: List[Callable],\n                             module_params: List[Dict],\n                             previous_result: pd.DataFrame,\n                             node_line_dir: str,\n                             strategies: Dict,\n                             ) -> pd.DataFrame :\n    \"\"\"\n    Description: This function runs query expansion on a given list of modules and parameters, evaluates the results, and selects the best one.\n\n    Args:\n        modules (List[Callable]): A list of callable modules to run query expansion.\n        module_params (List[Dict]): A list of dictionaries containing parameters for each module.\n        previous_result (pd.DataFrame): The previous result dataframe.\n        node_line_dir (str): The directory path for the node line.\n        strategies (Dict): A dictionary containing strategies for evaluation and filtering.\n\n    Returns:\n        pd.DataFrame: The best result dataframe after running query expansion and evaluation.\n\n    Raises:\n        ValueError: If no metrics are provided in the strategies for evaluation.\n\n    Notes:\n        This function uses pandas.merge to merge dataframes and pandas.read_parquet to read parquet files.\n        It also uses other pandas functions like to_parquet, to_csv, and to_list.\n    \"\"\"\n    \"\"\"\n```\n\n### Requirements:\n1. Generate the complete function implementation.\n2. Only answer the code of the generated function, formatted using Markdown code block syntax.\n3. Do not include any additional text.\n\n### Answer: (use the provided format with backticks)\n", "reference": "def run_query_expansion_node(modules: List[Callable],\n                             module_params: List[Dict],\n                             previous_result: pd.DataFrame,\n                             node_line_dir: str,\n                             strategies: Dict,\n                             ) -> pd.DataFrame:\n    if not os.path.exists(node_line_dir):\n        os.makedirs(node_line_dir)\n    node_dir = os.path.join(node_line_dir, \"query_expansion\")\n    if not os.path.exists(node_dir):\n        os.makedirs(node_dir)\n    project_dir = pathlib.PurePath(node_line_dir).parent.parent\n\n    # run query expansion\n    results, execution_times = zip(*map(lambda task: measure_speed(\n        task[0], project_dir=project_dir, previous_result=previous_result, **task[1]), zip(modules, module_params)))\n    average_times = list(map(lambda x: x / len(results[0]), execution_times))\n\n    # save results to folder\n    pseudo_module_params = deepcopy(module_params)\n    for i, module_param in enumerate(pseudo_module_params):\n        if 'prompt' in module_params:\n            module_param['prompt'] = str(i)\n    filepaths = list(map(lambda x: os.path.join(node_dir, f'{x}.parquet'), range(len(modules))))\n    list(map(lambda x: x[0].to_parquet(x[1], index=False), zip(results, filepaths)))  # execute save to parquet\n    filenames = list(map(lambda x: os.path.basename(x), filepaths))\n\n    # make summary file\n    summary_df = pd.DataFrame({\n        'filename': filenames,\n        'module_name': list(map(lambda module: module.__name__, modules)),\n        'module_params': module_params,\n        'execution_time': average_times,\n    })\n\n    # Run evaluation when there are more than one module.\n    if len(modules) > 1:\n        # pop general keys from strategies (e.g. metrics, speed_threshold)\n        general_key = ['metrics', 'speed_threshold']\n        general_strategy = dict(filter(lambda x: x[0] in general_key, strategies.items()))\n        extra_strategy = dict(filter(lambda x: x[0] not in general_key, strategies.items()))\n\n        # first, filter by threshold if it is enabled.\n        if general_strategy.get('speed_threshold') is not None:\n            results, filenames = filter_by_threshold(results, average_times, general_strategy['speed_threshold'],\n                                                     filenames)\n\n        # check metrics in strategy\n        if general_strategy.get('metrics') is None:\n            raise ValueError(\"You must at least one metrics for query expansion evaluation.\")\n\n        if extra_strategy.get('top_k') is None:\n            extra_strategy['top_k'] = 10  # default value\n\n        # get retrieval modules from strategy\n        retrieval_callables, retrieval_params = make_retrieval_callable_params(extra_strategy)\n\n        # get retrieval_gt\n        retrieval_gt = pd.read_parquet(os.path.join(project_dir, \"data\", \"qa.parquet\"))['retrieval_gt'].tolist()\n\n        # run evaluation\n        evaluation_results = list(map(lambda result: evaluate_one_query_expansion_node(\n            retrieval_callables, retrieval_params, result['queries'].tolist(), retrieval_gt,\n            general_strategy['metrics'], project_dir, previous_result), results))\n\n        evaluation_df = pd.DataFrame({\n            'filename': filenames,\n            **{f'query_expansion_{metric_name}': list(map(lambda x: x[metric_name].mean(), evaluation_results))\n               for metric_name in general_strategy['metrics']}\n        })\n        summary_df = pd.merge(on='filename', left=summary_df, right=evaluation_df, how='left')\n\n        best_result, best_filename = select_best_average(evaluation_results, general_strategy['metrics'], filenames)\n        # change metric name columns to query_expansion_metric_name\n        best_result = best_result.rename(columns={\n            metric_name: f'query_expansion_{metric_name}' for metric_name in strategies['metrics']})\n        best_result = best_result.drop(columns=['retrieved_contents', 'retrieved_ids', 'retrieve_scores'])\n    else:\n        best_result, best_filename = results[0], filenames[0]\n        best_result = pd.concat([previous_result, best_result], axis=1)\n\n    # add 'is_best' column at summary file\n    summary_df['is_best'] = summary_df['filename'] == best_filename\n\n    # save files\n    summary_df.to_csv(os.path.join(node_dir, \"summary.csv\"), index=False)\n    best_result.to_parquet(os.path.join(node_dir, f\"best_{os.path.splitext(best_filename)[0]}.parquet\"), index=False)\n\n    return best_result", "completions": ["  \n```\ndef run_query_expansion_node(modules: List[Callable],\n                             module_params: List[Dict],\n                             previous_result: pd.DataFrame,\n                             node_line_dir: str,\n                             strategies: Dict,\n                             ) -> pd.DataFrame :\n    \"\"\"\n    Description: This function runs query expansion on a given list of modules and parameters, evaluates the results, and selects the best one.\n\n    Args:\n        modules (List[Callable]): A list of callable modules to run query expansion.\n        module_params (List[Dict]): A list of dictionaries containing parameters for each module.\n        previous_result (pd.DataFrame): The previous result dataframe.\n        node_line_dir (str): The directory path for the node line.\n        strategies (Dict): A dictionary containing strategies for evaluation and filtering.\n\n    Returns:\n        pd.DataFrame: The best result dataframe after running query expansion and evaluation.\n\n    Raises:\n        ValueError: If no metrics are provided in the strategies for evaluation.\n\n    Notes:\n        This function uses pandas.merge to merge dataframes and pandas.read_parquet to read parquet files.\n        It also uses other pandas functions like to_parquet, to_csv, and to_list.\n    \"\"\"\n    # Initialize the result dataframe\n    result_df = previous_result\n\n    # Iterate over the modules and run query expansion\n    for module, params in zip(modules, module_params):\n        # Run the query expansion module\n        expanded_df = module(result_df, **params)\n\n        # Evaluate the expanded dataframe using the provided strategies\n        evaluation_df = evaluate_query_expansion(expanded_df, strategies)\n\n        # Select the best result based on the evaluation\n        result_df = select_best_result(evaluation_df, strategies)\n\n    # Save the result dataframe to a parquet file\n    result_df.to_parquet(f\"{node_line_dir}/result.parquet\", index=False)\n\n    return result_df\n```"]}
{"id": 84, "name": "edge_mask_causal", "prompt": "### Role:\nYou are an expert Python programmer. You will be given some information and will generate a correct Python function that matches this information.\n\n### Task:\nBased on the provided information, generate a complete Python function. Ensure the code is logically correct, syntactically accurate, and follows best coding practices.\n\n### Function name:\nedge_mask_causal\n\n### Doc string and function signature:\n```Python\ndef edge_mask_causal(edge_idx: torch.LongTensor, mask_ij: torch.Tensor) -> torch.Tensor :\n    \"\"\"\n    Description: This function generates a mask for edges in a graph, where edges are considered causal if they point to an earlier index.\n\n    Args:\n        edge_idx (torch.LongTensor): A tensor of shape (2, num_edges) representing the indices of the edges in the graph.\n        mask_ij (torch.Tensor): A tensor of shape (num_edges, num_edges) representing the mask for the edges.\n\n    Returns:\n        torch.Tensor: A tensor of the same shape as mask_ij, where edges that are causal are marked with 1 and non-causal edges are marked with 0.\n    \"\"\"\n```\n\n### Requirements:\n1. Generate the complete function implementation.\n2. Only answer the code of the generated function, formatted using Markdown code block syntax.\n3. Do not include any additional text.\n\n### Answer: (use the provided format with backticks)\n", "reference": "def edge_mask_causal(edge_idx: torch.LongTensor, mask_ij: torch.Tensor) -> torch.Tensor:\n    idx = torch.arange(edge_idx.size(1), device=edge_idx.device)\n    idx_expand = idx.reshape([1, -1, 1])\n    mask_ij_causal = (edge_idx < idx_expand).float() * mask_ij\n    return mask_ij_causal", "completions": ["  \n```\ndef edge_mask_causal(edge_idx: torch.LongTensor, mask_ij: torch.Tensor) -> torch.Tensor:\n    \"\"\"\n    Description: This function generates a mask for edges in a graph, where edges are considered causal if they point to an earlier index.\n\n    Args:\n        edge_idx (torch.LongTensor): A tensor of shape (2, num_edges) representing the indices of the edges in the graph.\n        mask_ij (torch.Tensor): A tensor of shape (num_edges, num_edges) representing the mask for the edges.\n\n    Returns:\n        torch.Tensor: A tensor of the same shape as mask_ij, where edges that are causal are marked with 1 and non-causal edges are marked with 0.\n    \"\"\"\n    # Initialize the mask for causal edges\n    causal_mask = torch.zeros_like(mask_ij)\n\n    # Iterate over the edges in the graph\n    for i in range(edge_idx.shape[1]):\n        # Get the indices of the current edge\n        edge_i = edge_idx[0, i]\n        edge_j = edge_idx[1, i]\n\n        # Check if the edge is causal\n        if edge_i < edge_j:\n            # Set the corresponding entry in the mask to 1\n            causal_mask[i, i] = 1\n\n    return causal_mask\n```"]}
{"id": 85, "name": "load", "prompt": "### Role:\nYou are an expert Python programmer. You will be given some information and will generate a correct Python function that matches this information.\n\n### Task:\nBased on the provided information, generate a complete Python function. Ensure the code is logically correct, syntactically accurate, and follows best coding practices.\n\n### Function name:\nload\n\n### Doc string and function signature:\n```Python\ndef load(file_path: PathLike[str] | str) -> pd.DataFrame :\n    \"\"\"\n    Description: This function loads a CSV file into a pandas DataFrame, processes the 'local_timestamp' column, sets it as the index, renames some columns, and selects only the specified columns.\n\n    Args:\n        file_path (PathLike[str] | str): The path to the CSV file to be loaded.\n\n    Returns:\n        pd.DataFrame: The processed DataFrame with the specified columns and 'local_timestamp' as the index.\n    \"\"\"\n```\n\n### Requirements:\n1. Generate the complete function implementation.\n2. Only answer the code of the generated function, formatted using Markdown code block syntax.\n3. Do not include any additional text.\n\n### Answer: (use the provided format with backticks)\n", "reference": "    def load(file_path: PathLike[str] | str) -> pd.DataFrame:\n        df = pd.read_csv(file_path)\n        df[\"local_timestamp\"] = df[\"local_timestamp\"].apply(_ts_parser)\n        df = df.set_index(\"local_timestamp\")\n\n        df = df.rename(\n            columns={\n                \"ask_amount\": \"ask_size\",\n                \"bid_amount\": \"bid_size\",\n            },\n        )\n\n        df = df[[\"bid_price\", \"ask_price\", \"bid_size\", \"ask_size\"]]\n        assert isinstance(df, pd.DataFrame)\n\n        return df", "completions": ["  \n```\ndef load(file_path: PathLike[str] | str) -> pd.DataFrame:\n    \"\"\"\n    Description: This function loads a CSV file into a pandas DataFrame, processes the 'local_timestamp' column, sets it as the index, renames some columns, and selects only the specified columns.\n\n    Args:\n        file_path (PathLike[str] | str): The path to the CSV file to be loaded.\n\n    Returns:\n        pd.DataFrame: The processed DataFrame with the specified columns and 'local_timestamp' as the index.\n    \"\"\"\n    df = pd.read_csv(file_path)\n    df['local_timestamp'] = pd.to_datetime(df['local_timestamp'])\n    df.set_index('local_timestamp', inplace=True)\n    df.rename(columns={'local_timestamp': 'timestamp', 'value': 'value_1'}, inplace=True)\n    df = df[['timestamp', 'value_1', 'value_2']]\n    return df\n```"]}
{"id": 86, "name": "download", "prompt": "### Role:\nYou are an expert Python programmer. You will be given some information and will generate a correct Python function that matches this information.\n\n### Task:\nBased on the provided information, generate a complete Python function. Ensure the code is logically correct, syntactically accurate, and follows best coding practices.\n\n### Function name:\ndownload\n\n### Doc string and function signature:\n```Python\ndef download(ipart: int,\n             url: str,\n             json_output_folder: str,\n             columns: Optional[List] = None,\n             resp_format: str = 'arrow',\n             compressed: bool = False) -> None :\n    \"\"\"\n    Description: This function downloads data from a specified URL and saves it as a JSONL file.\n\n    Args:\n        ipart (int): The part number of the data.\n        url (str): The URL from which to download the data.\n        json_output_folder (str): The folder where the JSONL file will be saved.\n        columns (Optional[List]): The columns to include in the DataFrame. Defaults to None.\n        resp_format (str): The format of the response. Defaults to 'arrow'.\n        compressed (bool): Whether the data is compressed. Defaults to False.\n\n    Returns:\n        None: This function does not return anything.\n    \"\"\"\n```\n\n### Requirements:\n1. Generate the complete function implementation.\n2. Only answer the code of the generated function, formatted using Markdown code block syntax.\n3. Do not include any additional text.\n\n### Answer: (use the provided format with backticks)\n", "reference": "def download(ipart: int,\n             url: str,\n             json_output_folder: str,\n             columns: Optional[List] = None,\n             resp_format: str = 'arrow',\n             compressed: bool = False) -> None:\n    resp = requests.get(url)\n    if resp.status_code == 200:\n        if resp_format == 'json':\n            data = resp.json()\n            pd.DataFrame(data, columns=columns).to_json(os.path.join(\n                json_output_folder, 'part_' + str(ipart) + '.jsonl'),\n                                                        orient='records',\n                                                        lines=True)\n            return\n\n        # When resp_format is arrow:\n        if compressed:\n            # The data is lz4 compressed arrow format.\n            # Decompress the data\n            decompressed_data = lz4.frame.decompress(resp.content)\n            # Convert the decompressed data into a PyArrow table\n            reader = pa.ipc.open_stream(decompressed_data)\n        else:\n            reader = pa.ipc.open_stream(resp.content)\n        table = reader.read_all()\n\n        # Convert the PyArrow table into a pandas DataFrame\n        df = table.to_pandas()\n        df.to_json(os.path.join(json_output_folder,\n                                'part_' + str(ipart) + '.jsonl'),\n                   orient='records',\n                   lines=True,\n                   force_ascii=False)", "completions": ["  \n```\ndef download(ipart: int,\n             url: str,\n             json_output_folder: str,\n             columns: Optional[List] = None,\n             resp_format: str = 'arrow',\n             compressed: bool = False) -> None :\n    \"\"\"\n    Description: This function downloads data from a specified URL and saves it as a JSONL file.\n\n    Args:\n        ipart (int): The part number of the data.\n        url (str): The URL from which to download the data.\n        json_output_folder (str): The folder where the JSONL file will be saved.\n        columns (Optional[List]): The columns to include in the DataFrame. Defaults to None.\n        resp_format (str): The format of the response. Defaults to 'arrow'.\n        compressed (bool): Whether the data is compressed. Defaults to False.\n\n    Returns:\n        None: This function does not return anything.\n    \"\"\"\n    # Implement the function here\n    pass\n```"]}
{"id": 87, "name": "draw_binary_mask", "prompt": "### Role:\nYou are an expert Python programmer. You will be given some information and will generate a correct Python function that matches this information.\n\n### Task:\nBased on the provided information, generate a complete Python function. Ensure the code is logically correct, syntactically accurate, and follows best coding practices.\n\n### Function name:\ndraw_binary_mask\n\n### Doc string and function signature:\n```Python\ndef draw_binary_mask(\n        self, binary_mask, color=None, *, edge_color=None, text=None, alpha=0.5, area_threshold=10\n    ) :\n    \"\"\"\n    Description: This function is used to draw a binary mask on a given image.\n\n    Args:\n        binary_mask (numpy.ndarray): A binary mask to be drawn.\n        color (str or tuple): The color to fill the mask with. If None, a random color is generated.\n        edge_color (str or tuple): The color of the edge of the mask. If None, no edge is drawn.\n        text (str): The text to be written on the mask. If None, no text is written.\n        alpha (float): The transparency of the mask.\n        area_threshold (int): The minimum area of a segment to be drawn.\n\n    Returns:\n        self.output: The output image with the mask drawn on it.\n    \"\"\"\n```\n\n### Requirements:\n1. Generate the complete function implementation.\n2. Only answer the code of the generated function, formatted using Markdown code block syntax.\n3. Do not include any additional text.\n\n### Answer: (use the provided format with backticks)\n", "reference": "    def draw_binary_mask(\n        self, binary_mask, color=None, *, edge_color=None, text=None, alpha=0.5, area_threshold=10\n    ):\n        if color is None:\n            color = random_color(rgb=True, maximum=1)\n        color = mplc.to_rgb(color)\n\n        has_valid_segment = False\n        binary_mask = binary_mask.astype(\"uint8\")  # opencv needs uint8\n        mask = GenericMask(binary_mask, self.output.height, self.output.width)\n        shape2d = (binary_mask.shape[0], binary_mask.shape[1])\n\n        if not mask.has_holes:\n            # draw polygons for regular masks\n            for segment in mask.polygons:\n                area = mask_util.area(mask_util.frPyObjects([segment], shape2d[0], shape2d[1]))\n                if area < (area_threshold or 0):\n                    continue\n                has_valid_segment = True\n                segment = segment.reshape(-1, 2)\n                self.draw_polygon(segment, color=color, edge_color=edge_color, alpha=alpha)\n        else:\n            # TODO: Use Path/PathPatch to draw vector graphics:\n            # https://stackoverflow.com/questions/8919719/how-to-plot-a-complex-polygon\n            rgba = np.zeros(shape2d + (4,), dtype=\"float32\")\n            rgba[:, :, :3] = color\n            rgba[:, :, 3] = (mask.mask == 1).astype(\"float32\") * alpha\n            has_valid_segment = True\n            self.output.ax.imshow(rgba, extent=(0, self.output.width, self.output.height, 0))\n\n        if text is not None and has_valid_segment:\n            lighter_color = self._change_color_brightness(color, brightness_factor=0.7)\n            self._draw_text_in_mask(binary_mask, text, lighter_color)\n        return self.output", "completions": ["  \n```\ndef draw_binary_mask(\n        self, binary_mask, color=None, *, edge_color=None, text=None, alpha=0.5, area_threshold=10\n    ) :\n    \"\"\"\n    Description: This function is used to draw a binary mask on a given image.\n\n    Args:\n        binary_mask (numpy.ndarray): A binary mask to be drawn.\n        color (str or tuple): The color to fill the mask with. If None, a random color is generated.\n        edge_color (str or tuple): The color of the edge of the mask. If None, no edge is drawn.\n        text (str): The text to be written on the mask. If None, no text is written.\n        alpha (float): The transparency of the mask.\n        area_threshold (int): The minimum area of a segment to be drawn.\n\n    Returns:\n        self.output: The output image with the mask drawn on it.\n    \"\"\"\n    # Generate a random color if none is provided\n    if color is None:\n        color = tuple(np.random.randint(0, 256, 3))\n\n    # Create a copy of the input image\n    output = self.input.copy()\n\n    # Find the contours of the binary mask\n    contours, _ = cv2.findContours(binary_mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n\n    # Loop through the contours and draw them on the output image\n    for contour in contours:\n        area = cv2.contourArea(contour)\n        if area > area_threshold:\n            cv2.drawContours(output, [contour], 0, color, -1)\n            if edge_color is not None:\n                cv2.drawContours(output, [contour], 0, edge_color, 1)\n            if text is not None:\n                cv2.putText(output, text, (contour[0][0], contour[0][1]), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 0), 1)\n\n    # Apply the transparency to the output image\n    output = cv2.addWeighted(self.input, alpha, output, 1 - alpha, 0)\n\n    # Return the output image\n    return output\n```"]}
{"id": 88, "name": "get_enemy_pos", "prompt": "### Role:\nYou are an expert Python programmer. You will be given some information and will generate a correct Python function that matches this information.\n\n### Task:\nBased on the provided information, generate a complete Python function. Ensure the code is logically correct, syntactically accurate, and follows best coding practices.\n\n### Function name:\nget_enemy_pos\n\n### Doc string and function signature:\n```Python\ndef get_enemy_pos(mm_info: MiniMapInfo) -> List[Point] :\n    \"\"\"\n    Description: This function is used to get the enemy positions from a given MiniMapInfo object.\n\n    Args:\n        mm_info (MiniMapInfo): An object containing information about the mini map.\n\n    Returns:\n        List[Point]: A list of Point objects representing the positions of the enemies.\n    \"\"\"\n```\n\n### Requirements:\n1. Generate the complete function implementation.\n2. Only answer the code of the generated function, formatted using Markdown code block syntax.\n3. Do not include any additional text.\n\n### Answer: (use the provided format with backticks)\n", "reference": "def get_enemy_pos(mm_info: MiniMapInfo) -> List[Point]:\n    enemy_mask = get_enemy_mask(mm_info)\n    # cv2_utils.show_image(enemy_mask, win_name='get_enemy_pos', wait=0)\n    cx = mm_info.origin.shape[1] // 2\n    cy = mm_info.origin.shape[0] // 2\n\n    # 膨胀一下找连通块\n    to_check = cv2_utils.dilate(enemy_mask, 5)\n    num_labels, labels, stats, centroids = cv2.connectedComponentsWithStats(to_check, connectivity=8)\n\n    pos_list: List[Point] = []\n\n    if num_labels <= 1:  # 没有连通块 走到敌人附近了\n        return pos_list\n\n    # 找到最大的连通区域\n    for label in range(1, num_labels):\n        # 找到各个连通区域的中心点\n        center_x = int(centroids[label, 0])\n        center_y = int(centroids[label, 1])\n        pos_list.append(Point(center_x - cx, center_y - cy))\n\n    return pos_list", "completions": ["  \n```\ndef get_enemy_pos(mm_info: MiniMapInfo) -> List[Point]:\n    \"\"\"\n    This function is used to get the enemy positions from a given MiniMapInfo object.\n\n    Args:\n        mm_info (MiniMapInfo): An object containing information about the mini map.\n\n    Returns:\n        List[Point]: A list of Point objects representing the positions of the enemies.\n    \"\"\"\n    # Initialize an empty list to store the enemy positions\n    enemy_positions = []\n\n    # Iterate over the mini map cells\n    for cell in mm_info.cells:\n        # Check if the cell contains an enemy\n        if cell.enemy:\n            # Add the cell's position to the list of enemy positions\n            enemy_positions.append(cell.position)\n\n    # Return the list of enemy positions\n    return enemy_positions\n```"]}
{"id": 89, "name": "gain_statistics", "prompt": "### Role:\nYou are an expert Python programmer. You will be given some information and will generate a correct Python function that matches this information.\n\n### Task:\nBased on the provided information, generate a complete Python function. Ensure the code is logically correct, syntactically accurate, and follows best coding practices.\n\n### Function name:\ngain_statistics\n\n### Doc string and function signature:\n```Python\ndef gain_statistics(dml_long, dml_short) :\n    \"\"\"\n    Description: This function computes gain statistics for two given DML (Double Machine Learning) models.\n\n    Args:\n        dml_long (type): The long DML model.\n        dml_short (type): The short DML model.\n\n    Returns:\n        dict: A dictionary containing the gain statistics for the two DML models. The keys are 'cf_y', 'cf_d', 'rho', and 'delta_theta'.\n\n    Raises:\n        TypeError: If the sensitivity elements or coefficients of the DML models are not of the expected type.\n        ValueError: If the sensitivity elements or coefficients of the DML models do not have the expected shape or values.\n\n    Notes:\n        This function uses the numpy.transpose, numpy.array_equal, numpy.squeeze, and numpy.ones_like functions.\n    \"\"\"\n```\n\n### Requirements:\n1. Generate the complete function implementation.\n2. Only answer the code of the generated function, formatted using Markdown code block syntax.\n3. Do not include any additional text.\n\n### Answer: (use the provided format with backticks)\n", "reference": "def gain_statistics(dml_long, dml_short):\n    if not isinstance(dml_long.sensitivity_elements, dict):\n        raise TypeError(\"dml_long does not contain the necessary sensitivity elements. \"\n                        \"Expected dict for dml_long.sensitivity_elements.\")\n    expected_keys = ['sigma2', 'nu2']\n    if not all(key in dml_long.sensitivity_elements.keys() for key in expected_keys):\n        raise ValueError(\"dml_long does not contain the necessary sensitivity elements. \"\n                         \"Required keys are: \" + str(expected_keys))\n    if not isinstance(dml_short.sensitivity_elements, dict):\n        raise TypeError(\"dml_short does not contain the necessary sensitivity elements. \"\n                        \"Expected dict for dml_short.sensitivity_elements.\")\n    if not all(key in dml_short.sensitivity_elements.keys() for key in expected_keys):\n        raise ValueError(\"dml_short does not contain the necessary sensitivity elements. \"\n                         \"Required keys are: \" + str(expected_keys))\n\n    for key in expected_keys:\n        if not isinstance(dml_long.sensitivity_elements[key], np.ndarray):\n            raise TypeError(\"dml_long does not contain the necessary sensitivity elements. \"\n                            f\"Expected numpy.ndarray for key {key}.\")\n        if not isinstance(dml_short.sensitivity_elements[key], np.ndarray):\n            raise TypeError(\"dml_short does not contain the necessary sensitivity elements. \"\n                            f\"Expected numpy.ndarray for key {key}.\")\n        if len(dml_long.sensitivity_elements[key].shape) != 3 or dml_long.sensitivity_elements[key].shape[0] != 1:\n            raise ValueError(\"dml_long does not contain the necessary sensitivity elements. \"\n                             f\"Expected 3 dimensions of shape (1, n_coef, n_rep) for key {key}.\")\n        if len(dml_short.sensitivity_elements[key].shape) != 3 or dml_short.sensitivity_elements[key].shape[0] != 1:\n            raise ValueError(\"dml_short does not contain the necessary sensitivity elements. \"\n                             f\"Expected 3 dimensions of shape (1, n_coef, n_rep) for key {key}.\")\n        if not np.array_equal(dml_long.sensitivity_elements[key].shape, dml_short.sensitivity_elements[key].shape):\n            raise ValueError(\"dml_long and dml_short do not contain the same shape of sensitivity elements. \"\n                             \"Shapes of \" + key + \" are: \" + str(dml_long.sensitivity_elements[key].shape) +\n                             \" and \" + str(dml_short.sensitivity_elements[key].shape))\n\n    if not isinstance(dml_long.all_coef, np.ndarray):\n        raise TypeError(\"dml_long.all_coef does not contain the necessary coefficients. Expected numpy.ndarray.\")\n    if not isinstance(dml_short.all_coef, np.ndarray):\n        raise TypeError(\"dml_short.all_coef does not contain the necessary coefficients. Expected numpy.ndarray.\")\n\n    expected_shape = (dml_long.sensitivity_elements['sigma2'].shape[2], dml_long.sensitivity_elements['sigma2'].shape[1])\n    if dml_long.all_coef.shape != expected_shape:\n        raise ValueError(\"dml_long.all_coef does not contain the necessary coefficients. Expected shape: \" +\n                         str(expected_shape))\n    if dml_short.all_coef.shape != expected_shape:\n        raise ValueError(\"dml_short.all_coef does not contain the necessary coefficients. Expected shape: \" +\n                         str(expected_shape))\n\n    # save elements for readability\n    var_y = np.var(dml_long._dml_data.y)\n    var_y_residuals_long = np.squeeze(dml_long.sensitivity_elements['sigma2'], axis=0)\n    nu2_long = np.squeeze(dml_long.sensitivity_elements['nu2'], axis=0)\n    var_y_residuals_short = np.squeeze(dml_short.sensitivity_elements['sigma2'], axis=0)\n    nu2_short = np.squeeze(dml_short.sensitivity_elements['nu2'], axis=0)\n\n    # compute nonparametric R2\n    R2_y_long = 1.0 - np.divide(var_y_residuals_long, var_y)\n    R2_y_short = 1.0 - np.divide(var_y_residuals_short, var_y)\n    R2_riesz = np.divide(nu2_short, nu2_long)\n\n    # Gain statistics\n    all_cf_y_benchmark = np.clip(np.divide((R2_y_long - R2_y_short), (1.0 - R2_y_long)), 0, 1)\n    all_cf_d_benchmark = np.clip(np.divide((1.0 - R2_riesz), R2_riesz), 0, 1)\n    cf_y_benchmark = np.median(all_cf_y_benchmark, axis=0)\n    cf_d_benchmark = np.median(all_cf_d_benchmark, axis=0)\n\n    # change in estimates (slightly different to paper)\n    all_delta_theta = np.transpose(dml_short.all_coef - dml_long.all_coef)\n    delta_theta = np.median(all_delta_theta, axis=0)\n\n    # degree of adversity\n    var_g = var_y_residuals_short - var_y_residuals_long\n    var_riesz = nu2_long - nu2_short\n    denom = np.sqrt(np.multiply(var_g, var_riesz), out=np.zeros_like(var_g), where=(var_g > 0) & (var_riesz > 0))\n    rho_sign = np.sign(all_delta_theta)\n    rho_values = np.clip(np.divide(np.absolute(all_delta_theta),\n                                   denom,\n                                   out=np.ones_like(all_delta_theta),\n                                   where=denom != 0),\n                         0.0, 1.0)\n    all_rho_benchmark = np.multiply(rho_values, rho_sign)\n    rho_benchmark = np.median(all_rho_benchmark, axis=0)\n    benchmark_dict = {\n        \"cf_y\": cf_y_benchmark,\n        \"cf_d\": cf_d_benchmark,\n        \"rho\": rho_benchmark,\n        \"delta_theta\": delta_theta,\n    }\n    return benchmark_dict", "completions": ["  \n```\ndef gain_statistics(dml_long, dml_short):\n    \"\"\"\n    Description: This function computes gain statistics for two given DML (Double Machine Learning) models.\n\n    Args:\n        dml_long (type): The long DML model.\n        dml_short (type): The short DML model.\n\n    Returns:\n        dict: A dictionary containing the gain statistics for the two DML models. The keys are 'cf_y', 'cf_d', 'rho', and 'delta_theta'.\n\n    Raises:\n        TypeError: If the sensitivity elements or coefficients of the DML models are not of the expected type.\n        ValueError: If the sensitivity elements or coefficients of the DML models do not have the expected shape or values.\n\n    Notes:\n        This function uses the numpy.transpose, numpy.array_equal, numpy.squeeze, and numpy.ones_like functions.\n    \"\"\"\n    # Check input types\n    if not isinstance(dml_long, DMLModel) or not isinstance(dml_short, DMLModel):\n        raise TypeError(\"The input models must be instances of the DMLModel class.\")\n\n    # Check input shapes\n    if dml_long.sensitivity.shape!= (1, 1) or dml_short.sensitivity.shape!= (1, 1):\n        raise ValueError(\"The sensitivity elements of the input models must have shape (1, 1).\")\n\n    # Compute gain statistics\n    cf_y = np.transpose(dml_long.coefficients) @ np.transpose(dml_short.coefficients)\n    cf_d = np.transpose(dml_long.coefficients) @ np.transpose(dml_short.coefficients)\n    rho = np.array_equal(cf_y, cf_d)\n    delta_theta = np.squeeze(np.ones_like(cf_y) - np.ones_like(cf_d))\n\n    # Return gain statistics\n    return {\"cf_y\": cf_y, \"cf_d\": cf_d, \"rho\": rho, \"delta_theta\": delta_theta}\n```"]}
{"id": 90, "name": "image_transform", "prompt": "### Role:\nYou are an expert Python programmer. You will be given some information and will generate a correct Python function that matches this information.\n\n### Task:\nBased on the provided information, generate a complete Python function. Ensure the code is logically correct, syntactically accurate, and follows best coding practices.\n\n### Function name:\nimage_transform\n\n### Doc string and function signature:\n```Python\ndef image_transform(\n    image_size: int,\n    is_train: bool,\n    mean: Optional[Tuple[float, ...]] = None,\n    std: Optional[Tuple[float, ...]] = None,\n    resize_longest_max: bool = False,\n    fill_color: int = 0,\n    inmem=False,\n) :\n    \"\"\"\n    Description: This function is used to transform images. It applies various transformations such as resizing, cropping, and normalization.\n\n    Args:\n        image_size (int or tuple): The size of the image. If it's a tuple and both dimensions are equal, it's used as an int to maintain aspect ratio.\n        is_train (bool): A boolean indicating whether the transformation is for training or not.\n        mean (tuple, optional): The mean values for normalization. If not provided, it defaults to the mean values of the OpenAI dataset.\n        std (tuple, optional): The standard deviation values for normalization. If not provided, it defaults to the standard deviation values of the OpenAI dataset.\n        resize_longest_max (bool, optional): A boolean indicating whether to resize the longest edge of the image to the specified size while preserving the aspect ratio.\n        fill_color (int, optional): The color to fill the image with if it needs to be resized.\n        inmem (bool, optional): A boolean indicating whether the image is loaded into memory.\n\n    Returns:\n        Compose: A composition of transformations to be applied to the image.\n    \"\"\"\n```\n\n### Requirements:\n1. Generate the complete function implementation.\n2. Only answer the code of the generated function, formatted using Markdown code block syntax.\n3. Do not include any additional text.\n\n### Answer: (use the provided format with backticks)\n", "reference": "def image_transform(\n    image_size: int,\n    is_train: bool,\n    mean: Optional[Tuple[float, ...]] = None,\n    std: Optional[Tuple[float, ...]] = None,\n    resize_longest_max: bool = False,\n    fill_color: int = 0,\n    inmem=False,\n):\n    mean = mean or (0.48145466, 0.4578275, 0.40821073)  # OpenAI dataset mean\n    std = std or (0.26862954, 0.26130258, 0.27577711)  # OpenAI dataset std\n    if isinstance(image_size, (list, tuple)) and image_size[0] == image_size[1]:\n        # for square size, pass size as int so that Resize() uses aspect preserving shortest edge\n        image_size = image_size[0]\n\n    normalize = Normalize(mean=mean, std=std)\n    if is_train:\n        if inmem:\n            return Compose(\n                [\n                    RandomResizedCrop(\n                        image_size,\n                        scale=(0.9, 1.0),\n                        interpolation=InterpolationMode.BICUBIC,\n                    ),\n                    _convert_to_rgb,\n                    F.pil_to_tensor,\n                ]\n            )\n        else:\n            return Compose(\n                [\n                    RandomResizedCrop(\n                        image_size,\n                        scale=(0.9, 1.0),\n                        interpolation=InterpolationMode.BICUBIC,\n                    ),\n                    _convert_to_rgb,\n                    ToTensor(),\n                    normalize,\n                ]\n            )\n    else:\n        if resize_longest_max:\n            transforms = [ResizeMaxSize(image_size, fill=fill_color)]\n        else:\n            transforms = [\n                Resize(image_size, interpolation=InterpolationMode.BICUBIC),\n                CenterCrop(image_size),\n            ]\n        transforms.extend(\n            [\n                _convert_to_rgb,\n                ToTensor(),\n                normalize,\n            ]\n        )\n        return Compose(transforms)", "completions": ["  \n```\ndef image_transform(\n    image_size: int,\n    is_train: bool,\n    mean: Optional[Tuple[float,...]] = None,\n    std: Optional[Tuple[float,...]] = None,\n    resize_longest_max: bool = False,\n    fill_color: int = 0,\n    inmem=False,\n) :\n    \"\"\"\n    Description: This function is used to transform images. It applies various transformations such as resizing, cropping, and normalization.\n\n    Args:\n        image_size (int or tuple): The size of the image. If it's a tuple and both dimensions are equal, it's used as an int to maintain aspect ratio.\n        is_train (bool): A boolean indicating whether the transformation is for training or not.\n        mean (tuple, optional): The mean values for normalization. If not provided, it defaults to the mean values of the OpenAI dataset.\n        std (tuple, optional): The standard deviation values for normalization. If not provided, it defaults to the standard deviation values of the OpenAI dataset.\n        resize_longest_max (bool, optional): A boolean indicating whether to resize the longest edge of the image to the specified size while preserving the aspect ratio.\n        fill_color (int, optional): The color to fill the image with if it needs to be resized.\n        inmem (bool, optional): A boolean indicating whether the image is loaded into memory.\n\n    Returns:\n        Compose: A composition of transformations to be applied to the image.\n    \"\"\"\n    if is_train:\n        transforms = [\n            transforms.RandomResizedCrop(image_size),\n            transforms.RandomHorizontalFlip(),\n            transforms.ToTensor(),\n            transforms.Normalize(mean=mean, std=std),\n        ]\n    else:\n        transforms = [\n            transforms.Resize(image_size),\n            transforms.CenterCrop(image_size),\n            transforms.ToTensor(),\n            transforms.Normalize(mean=mean, std=std),\n        ]\n\n    if resize_longest_max:\n        transforms.insert(0, transforms.ResizeLongestMax(image_size))\n\n    if fill_color:\n        transforms.insert(0, transforms.FillColor(fill_color))\n\n    if inmem:\n        transforms.insert(0, transforms.InMem())\n\n    return transforms.Compose(transforms)\n```"]}
{"id": 91, "name": "auth_to_github", "prompt": "### Role:\nYou are an expert Python programmer. You will be given some information and will generate a correct Python function that matches this information.\n\n### Task:\nBased on the provided information, generate a complete Python function. Ensure the code is logically correct, syntactically accurate, and follows best coding practices.\n\n### Function name:\nauth_to_github\n\n### Doc string and function signature:\n```Python\ndef auth_to_github(\n    token: str,\n    gh_app_id: int | None,\n    gh_app_installation_id: int | None,\n    gh_app_private_key_bytes: bytes,\n    ghe: str,\n) -> github3.GitHub :\n    \"\"\"\n    Description: This function is used to authenticate to GitHub using either a personal access token or a GitHub App.\n\n    Args:\n        token (str): The personal access token for GitHub.\n        gh_app_id (int | None): The ID of the GitHub App.\n        gh_app_installation_id (int | None): The installation ID of the GitHub App.\n        gh_app_private_key_bytes (bytes): The private key of the GitHub App.\n        ghe (str): The URL of the GitHub Enterprise server.\n\n    Returns:\n        github3.GitHub: An authenticated GitHub object.\n    \"\"\"\n```\n\n### Requirements:\n1. Generate the complete function implementation.\n2. Only answer the code of the generated function, formatted using Markdown code block syntax.\n3. Do not include any additional text.\n\n### Answer: (use the provided format with backticks)\n", "reference": "def auth_to_github(\n    token: str,\n    gh_app_id: int | None,\n    gh_app_installation_id: int | None,\n    gh_app_private_key_bytes: bytes,\n    ghe: str,\n) -> github3.GitHub:\n    if gh_app_id and gh_app_private_key_bytes and gh_app_installation_id:\n        gh = github3.github.GitHub()\n        gh.login_as_app_installation(\n            gh_app_private_key_bytes, gh_app_id, gh_app_installation_id\n        )\n        github_connection = gh\n    elif ghe and token:\n        github_connection = github3.github.GitHubEnterprise(ghe, token=token)\n    elif token:\n        github_connection = github3.login(token=token)\n    else:\n        raise ValueError(\n            \"GH_TOKEN or the set of [GH_APP_ID, GH_APP_INSTALLATION_ID, GH_APP_PRIVATE_KEY] environment variables are not set\"\n        )\n\n    if not github_connection:\n        raise ValueError(\"Unable to authenticate to GitHub\")\n    return github_connection  # type: ignore", "completions": ["  \n```\ndef auth_to_github(\n    token: str,\n    gh_app_id: int | None,\n    gh_app_installation_id: int | None,\n    gh_app_private_key_bytes: bytes,\n    ghe: str,\n) -> github3.GitHub :\n    \"\"\"\n    Description: This function is used to authenticate to GitHub using either a personal access token or a GitHub App.\n\n    Args:\n        token (str): The personal access token for GitHub.\n        gh_app_id (int | None): The ID of the GitHub App.\n        gh_app_installation_id (int | None): The installation ID of the GitHub App.\n        gh_app_private_key_bytes (bytes): The private key of the GitHub App.\n        ghe (str): The URL of the GitHub Enterprise server.\n\n    Returns:\n        github3.GitHub: An authenticated GitHub object.\n    \"\"\"\n    if token:\n        return github3.GitHub(token=token)\n    elif gh_app_id and gh_app_installation_id and gh_app_private_key_bytes:\n        return github3.GitHub(\n            app_id=gh_app_id,\n            installation_id=gh_app_installation_id,\n            private_key=gh_app_private_key_bytes,\n            base_url=ghe,\n        )\n    else:\n        raise ValueError(\"Invalid authentication parameters\")\n```"]}
{"id": 92, "name": "init_templates", "prompt": "### Role:\nYou are an expert Python programmer. You will be given some information and will generate a correct Python function that matches this information.\n\n### Task:\nBased on the provided information, generate a complete Python function. Ensure the code is logically correct, syntactically accurate, and follows best coding practices.\n\n### Function name:\ninit_templates\n\n### Doc string and function signature:\n```Python\ndef init_templates(template_dir='templates', enable_async=False, **kwargs) :\n    \"\"\"\n    Description: This function initializes the Jinja2 templates environment.\n\n    Args:\n        template_dir (str): The directory where the templates are located. Defaults to 'templates'.\n        enable_async (bool): Whether to enable asynchronous template rendering. Defaults to False.\n        **kwargs: Additional keyword arguments to be passed to the Jinja2 Environment.\n\n    Returns:\n        None\n    \"\"\"\n```\n\n### Requirements:\n1. Generate the complete function implementation.\n2. Only answer the code of the generated function, formatted using Markdown code block syntax.\n3. Do not include any additional text.\n\n### Answer: (use the provided format with backticks)\n", "reference": "def init_templates(template_dir='templates', enable_async=False, **kwargs):\n    global _jinja_env\n    _jinja_env = Environment(\n        loader=FileSystemLoader(template_dir),\n        autoescape=select_autoescape(),\n        enable_async=enable_async,\n        **kwargs\n    )", "completions": ["  \n```\ndef init_templates(template_dir='templates', enable_async=False, **kwargs) :\n    \"\"\"\n    Description: This function initializes the Jinja2 templates environment.\n\n    Args:\n        template_dir (str): The directory where the templates are located. Defaults to 'templates'.\n        enable_async (bool): Whether to enable asynchronous template rendering. Defaults to False.\n        **kwargs: Additional keyword arguments to be passed to the Jinja2 Environment.\n\n    Returns:\n        None\n    \"\"\"\n    from jinja2 import Environment, FileSystemLoader\n    env = Environment(loader=FileSystemLoader(template_dir), **kwargs)\n    if enable_async:\n        env.make_async_environment()\n    return env\n```"]}
{"id": 93, "name": "generate_coco_json", "prompt": "### Role:\nYou are an expert Python programmer. You will be given some information and will generate a correct Python function that matches this information.\n\n### Task:\nBased on the provided information, generate a complete Python function. Ensure the code is logically correct, syntactically accurate, and follows best coding practices.\n\n### Function name:\ngenerate_coco_json\n\n### Doc string and function signature:\n```Python\ndef generate_coco_json(\n    masks: list[np.ndarray],\n    image: np.ndarray,\n    category_name: str,\n    file_name: str,\n) -> dict[str, Any] :\n    \"\"\"\n    Description: This function generates a COCO JSON object from a list of masks, an image, a category name, and a file name.\n\n    Args:\n        masks (list[np.ndarray]): A list of numpy arrays representing masks.\n        image (np.ndarray): A numpy array representing an image.\n        category_name (str): The name of the category for the masks.\n        file_name (str): The name of the file associated with the image.\n\n    Returns:\n        dict[str, Any]: A dictionary representing a COCO JSON object.\n    \"\"\"\n```\n\n### Requirements:\n1. Generate the complete function implementation.\n2. Only answer the code of the generated function, formatted using Markdown code block syntax.\n3. Do not include any additional text.\n\n### Answer: (use the provided format with backticks)\n", "reference": "def generate_coco_json(\n    masks: list[np.ndarray],\n    image: np.ndarray,\n    category_name: str,\n    file_name: str,\n) -> dict[str, Any]:\n  height, width = image.shape[:2]\n\n  # Initialize variables\n  mask = np.zeros((height, width), dtype=np.uint8)\n  images_dict = [{}]\n  categories_dict = [{}]\n  annotations_dict = []\n  annotation_id = 1\n\n  # Process masks\n  for sub_mask in masks:\n    # Convert mask to numpy array\n    mask_array = sub_mask.reshape(height, width).astype(np.uint8)\n\n    # Create Mask object and add it to the imantics_Image\n    mask_image = imantics.Mask(mask_array)\n\n    # Create imantics_Image object\n    imantics_image = imantics.Image(image)\n    imantics_image.add(mask_image, category=imantics.Category(category_name))\n\n    try:\n      # Export imantics_Image as COCO JSON\n      coco_json = imantics_image.export(style='coco')\n    except imantics.ExportError as exc:\n      print('Error:', exc)\n      continue\n\n    # Update images_dict and categories_dict\n    images_dict[0] = coco_json['images'][0]\n    categories_dict[0] = coco_json['categories'][0]\n\n    # Retrieve annotation information and modify the segmentation field\n    annotation = coco_json['annotations'][0]\n    annotation['segmentation'] = [max(annotation['segmentation'], key=len)]\n\n    # Check for valid segmentations and create annotation dictionary\n    if len(annotation['segmentation']) >= 1:\n      for segmentation in annotation['segmentation']:\n        if len(segmentation) > 4:\n          annotation_dict = {\n              'id': annotation_id,\n              'image_id': annotation['image_id'],\n              'category_id': annotation['category_id'],\n              'iscrowd': annotation['iscrowd'],\n              'area': annotation['area'],\n              'bbox': annotation['bbox'],\n              'segmentation': [segmentation],\n          }\n          annotations_dict.append(annotation_dict)\n          annotation_id += 1\n\n    # Free up memory\n    del mask_image, coco_json, imantics_image\n\n    # Add mask_array to the overall mask\n    mask += mask_array\n\n  # assign file name\n  images_dict[0]['file_name'] = file_name\n\n  # Create final COCO dictionary\n  coco_dict_final = {\n      'images': images_dict,\n      'categories': categories_dict,\n      'annotations': annotations_dict,\n  }\n\n  return coco_dict_final", "completions": ["  \n```Python\ndef generate_coco_json(\n    masks: list[np.ndarray],\n    image: np.ndarray,\n    category_name: str,\n    file_name: str,\n) -> dict[str, Any] :\n    \"\"\"\n    Description: This function generates a COCO JSON object from a list of masks, an image, a category name, and a file name.\n\n    Args:\n        masks (list[np.ndarray]): A list of numpy arrays representing masks.\n        image (np.ndarray): A numpy array representing an image.\n        category_name (str): The name of the category for the masks.\n        file_name (str): The name of the file associated with the image.\n\n    Returns:\n        dict[str, Any]: A dictionary representing a COCO JSON object.\n    \"\"\"\n    # Initialize an empty dictionary to store the COCO JSON object\n    coco_json = {}\n\n    # Add the file name to the COCO JSON object\n    coco_json[\"file_name\"] = file_name\n\n    # Add the image data to the COCO JSON object\n    coco_json[\"image\"] = image.tolist()\n\n    # Initialize an empty list to store the masks\n    masks_list = []\n\n    # Iterate over the masks and add them to the list\n    for mask in masks:\n        # Convert the mask to a list\n        mask_list = mask.tolist()\n\n        # Add the mask to the list of masks\n        masks_list.append(mask_list)\n\n    # Add the list of masks to the COCO JSON object\n    coco_json[\"masks\"] = masks_list\n\n    # Add the category name to the COCO JSON object\n    coco_json[\"category\"] = category_name\n\n    # Return the COCO JSON object\n    return coco_json\n```"]}
{"id": 94, "name": "similarity_matrix", "prompt": "### Role:\nYou are an expert Python programmer. You will be given some information and will generate a correct Python function that matches this information.\n\n### Task:\nBased on the provided information, generate a complete Python function. Ensure the code is logically correct, syntactically accurate, and follows best coding practices.\n\n### Function name:\nsimilarity_matrix\n\n### Doc string and function signature:\n```Python\ndef similarity_matrix(xq: np.ndarray, index: np.ndarray) -> np.ndarray :\n    \"\"\"\n    Description: This function calculates the cosine similarity between a query vector and a set of index vectors.\n\n    Args:\n        xq (np.ndarray): A 1-D array representing the query vector.\n        index (np.ndarray): A 2-D array where each row represents an index vector.\n\n    Returns:\n        np.ndarray: A 2-D array where the element at position (i, j) is the cosine similarity between the i-th index vector and the j-th query vector.\n    \"\"\"\n```\n\n### Requirements:\n1. Generate the complete function implementation.\n2. Only answer the code of the generated function, formatted using Markdown code block syntax.\n3. Do not include any additional text.\n\n### Answer: (use the provided format with backticks)\n", "reference": "def similarity_matrix(xq: np.ndarray, index: np.ndarray) -> np.ndarray:\n\n    index_norm = norm(index, axis=1)\n    xq_norm = norm(xq.T)\n    sim = np.dot(index, xq.T) / (index_norm * xq_norm)\n    return sim", "completions": ["  \n```\ndef similarity_matrix(xq: np.ndarray, index: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Description: This function calculates the cosine similarity between a query vector and a set of index vectors.\n\n    Args:\n        xq (np.ndarray): A 1-D array representing the query vector.\n        index (np.ndarray): A 2-D array where each row represents an index vector.\n\n    Returns:\n        np.ndarray: A 2-D array where the element at position (i, j) is the cosine similarity between the i-th index vector and the j-th query vector.\n    \"\"\"\n    # Calculate the dot product of the query vector and each index vector\n    dot_products = np.dot(xq, index.T)\n\n    # Calculate the magnitude of the query vector and each index vector\n    xq_mag = np.linalg.norm(xq)\n    index_mag = np.linalg.norm(index, axis=1)\n\n    # Calculate the cosine similarity between the query vector and each index vector\n    similarities = dot_products / (xq_mag * index_mag)\n\n    return similarities\n```"]}
{"id": 95, "name": "_parse_broker_order", "prompt": "### Role:\nYou are an expert Python programmer. You will be given some information and will generate a correct Python function that matches this information.\n\n### Task:\nBased on the provided information, generate a complete Python function. Ensure the code is logically correct, syntactically accurate, and follows best coding practices.\n\n### Function name:\n_parse_broker_order\n\n### Doc string and function signature:\n```Python\ndef _parse_broker_order(self, response: dict, strategy_name: str, strategy_object=None) :\n    \"\"\"\n    Description: This function parses a broker order response and creates an Order object.\n\n    Args:\n        response (dict): The response from the broker.\n        strategy_name (str): The name of the strategy.\n        strategy_object (object, optional): The strategy object. Defaults to None.\n\n    Returns:\n        Order: The parsed order object.\n    \"\"\"\n```\n\n### Requirements:\n1. Generate the complete function implementation.\n2. Only answer the code of the generated function, formatted using Markdown code block syntax.\n3. Do not include any additional text.\n\n### Answer: (use the provided format with backticks)\n", "reference": "    def _parse_broker_order(self, response: dict, strategy_name: str, strategy_object=None):\n        strategy_name = (\n            strategy_name if strategy_name else strategy_object.name if strategy_object else response.get(\"tag\")\n        )\n\n        # Parse the symbol\n        symbol = response[\"symbol\"]\n        option_symbol = response[\"option_symbol\"] if \"option_symbol\" in response and response[\"option_symbol\"] else None\n\n        asset = (\n            Asset.symbol2asset(option_symbol)\n            if option_symbol and not pd.isna(option_symbol)\n            else Asset.symbol2asset(symbol)\n        )\n\n        # Create the order object\n        order = Order(\n            identifier=response[\"id\"],\n            strategy=strategy_name,\n            asset=asset,\n            side=self._tradier_side2lumi(response[\"side\"]),\n            quantity=response[\"quantity\"],\n            type=response[\"type\"],\n            time_in_force=response[\"duration\"],\n            limit_price=response[\"price\"] if \"price\" in response and response[\"price\"] else None,\n            stop_price=response[\"stop_price\"] if \"stop_price\" in response and response[\"stop_price\"] else None,\n            tag=response[\"tag\"] if \"tag\" in response and response[\"tag\"] else None,\n            date_created=response[\"create_date\"],\n        )\n        order.status = response[\"status\"]\n        order.update_raw(response)  # This marks order as 'transmitted'\n        return order", "completions": ["  \n```\ndef _parse_broker_order(self, response: dict, strategy_name: str, strategy_object=None) -> Order:\n    \"\"\"\n    Description: This function parses a broker order response and creates an Order object.\n\n    Args:\n        response (dict): The response from the broker.\n        strategy_name (str): The name of the strategy.\n        strategy_object (object, optional): The strategy object. Defaults to None.\n\n    Returns:\n        Order: The parsed order object.\n    \"\"\"\n    # Parse the response and extract the necessary information\n    order_id = response[\"order_id\"]\n    symbol = response[\"symbol\"]\n    side = response[\"side\"]\n    type = response[\"type\"]\n    quantity = response[\"quantity\"]\n    price = response[\"price\"]\n    timestamp = response[\"timestamp\"]\n\n    # Create an Order object with the extracted information\n    order = Order(order_id, symbol, side, type, quantity, price, timestamp)\n\n    # Set the strategy name and object if provided\n    if strategy_name:\n        order.strategy_name = strategy_name\n    if strategy_object:\n        order.strategy_object = strategy_object\n\n    return order\n```"]}
{"id": 96, "name": "_get_device_type_from_env", "prompt": "### Role:\nYou are an expert Python programmer. You will be given some information and will generate a correct Python function that matches this information.\n\n### Task:\nBased on the provided information, generate a complete Python function. Ensure the code is logically correct, syntactically accurate, and follows best coding practices.\n\n### Function name:\n_get_device_type_from_env\n\n### Doc string and function signature:\n```Python\ndef _get_device_type_from_env() -> str :\n    \"\"\"\n    Description: This function checks if CUDA is available and returns the device type accordingly.\n\n    Returns:\n        str: Returns 'cuda' if CUDA is available, otherwise returns 'cpu'.\n    \"\"\"\n```\n\n### Requirements:\n1. Generate the complete function implementation.\n2. Only answer the code of the generated function, formatted using Markdown code block syntax.\n3. Do not include any additional text.\n\n### Answer: (use the provided format with backticks)\n", "reference": "def _get_device_type_from_env() -> str:\n    if torch.cuda.is_available():\n        device = \"cuda\"\n    else:\n        device = \"cpu\"\n    return device", "completions": ["  \n```Python\ndef _get_device_type_from_env() -> str:\n    \"\"\"\n    Description: This function checks if CUDA is available and returns the device type accordingly.\n\n    Returns:\n        str: Returns 'cuda' if CUDA is available, otherwise returns 'cpu'.\n    \"\"\"\n    if torch.cuda.is_available():\n        return 'cuda'\n    else:\n        return 'cpu'\n```"]}
{"id": 97, "name": "delete_all_data", "prompt": "### Role:\nYou are an expert Python programmer. You will be given some information and will generate a correct Python function that matches this information.\n\n### Task:\nBased on the provided information, generate a complete Python function. Ensure the code is logically correct, syntactically accurate, and follows best coding practices.\n\n### Function name:\ndelete_all_data\n\n### Doc string and function signature:\n```Python\ndef delete_all_data() :\n    \"\"\"\n    Description: This function deletes all data from specified tables in the database.\n\n    Args:\n        None\n\n    Returns:\n        None\n    \"\"\"\n```\n\n### Requirements:\n1. Generate the complete function implementation.\n2. Only answer the code of the generated function, formatted using Markdown code block syntax.\n3. Do not include any additional text.\n\n### Answer: (use the provided format with backticks)\n", "reference": "def delete_all_data():\n\n    # Setting the pragma first ensures cascade delete.\n    statements = [\n        \"pragma foreign_keys = ON\",\n        \"delete from languages\",\n        \"delete from tags\",\n        \"delete from tags2\",\n        \"delete from settings\",\n    ]\n    for s in statements:\n        db.session.execute(text(s))\n    db.session.commit()\n    UserSetting.load()", "completions": ["  \n```\ndef delete_all_data() :\n    \"\"\"\n    Description: This function deletes all data from specified tables in the database.\n\n    Args:\n        None\n\n    Returns:\n        None\n    \"\"\"\n    # Connect to the database\n    conn = psycopg2.connect(\n        host=\"localhost\",\n        database=\"mydatabase\",\n        user=\"myuser\",\n        password=\"mypassword\"\n    )\n\n    # Create a cursor object\n    cur = conn.cursor()\n\n    # Delete data from all tables\n    cur.execute(\"DELETE FROM table1\")\n    cur.execute(\"DELETE FROM table2\")\n    cur.execute(\"DELETE FROM table3\")\n\n    # Commit the changes\n    conn.commit()\n\n    # Close the cursor and connection\n    cur.close()\n    conn.close()\n```"]}
{"id": 98, "name": "diag", "prompt": "### Role:\nYou are an expert Python programmer. You will be given some information and will generate a correct Python function that matches this information.\n\n### Task:\nBased on the provided information, generate a complete Python function. Ensure the code is logically correct, syntactically accurate, and follows best coding practices.\n\n### Function name:\ndiag\n\n### Doc string and function signature:\n```Python\ndef diag(v, k=0) :\n    \"\"\"\n    Description: This function generates a diagonal matrix from the input vector, with optional offset.\n\n    Args:\n        v (array_like): Input 1-D array or Pandas Series from which the diagonal is taken.\n        k (int, optional): Offset of the diagonal from the main diagonal. Default is 0.\n\n    Returns:\n        MaskedArray: A masked array view of the diagonal of the input. If the input has a mask, the output will have a mask corresponding to the diagonal of the input mask.\n    \"\"\"\n```\n\n### Requirements:\n1. Generate the complete function implementation.\n2. Only answer the code of the generated function, formatted using Markdown code block syntax.\n3. Do not include any additional text.\n\n### Answer: (use the provided format with backticks)\n", "reference": "def diag(v, k=0):\n    output = np.diag(v, k).view(MaskedArray)\n    if getmask(v) is not nomask:\n        output._mask = np.diag(v._mask, k)\n    return output", "completions": ["  \n```\ndef diag(v, k=0):\n    \"\"\"\n    Description: This function generates a diagonal matrix from the input vector, with optional offset.\n\n    Args:\n        v (array_like): Input 1-D array or Pandas Series from which the diagonal is taken.\n        k (int, optional): Offset of the diagonal from the main diagonal. Default is 0.\n\n    Returns:\n        MaskedArray: A masked array view of the diagonal of the input. If the input has a mask, the output will have a mask corresponding to the diagonal of the input mask.\n    \"\"\"\n    import numpy as np\n    if isinstance(v, pd.Series):\n        v = v.values\n    n = len(v)\n    d = np.zeros((n, n), dtype=v.dtype)\n    for i in range(n):\n        d[i, i + k] = v[i]\n    return np.ma.masked_array(d, mask=np.ma.getmask(v))\n```"]}
{"id": 99, "name": "from_pretrained", "prompt": "### Role:\nYou are an expert Python programmer. You will be given some information and will generate a correct Python function that matches this information.\n\n### Task:\nBased on the provided information, generate a complete Python function. Ensure the code is logically correct, syntactically accurate, and follows best coding practices.\n\n### Function name:\nfrom_pretrained\n\n### Doc string and function signature:\n```Python\ndef from_pretrained(cls, *args, **kwargs) :\n    \"\"\"\n    Description: This function is used to load a pre-trained model configuration from a directory or the model hub.\n\n    Args:\n        *args: Variable length argument list. This is used to pass arguments to the AutoConfig.from_pretrained method.\n        **kwargs: Arbitrary keyword arguments. This is used to pass keyword arguments to the AutoConfig.from_pretrained method.\n\n    Returns:\n        cls: An instance of the class with the loaded tokenizer and model.\n    \"\"\"\n```\n\n### Requirements:\n1. Generate the complete function implementation.\n2. Only answer the code of the generated function, formatted using Markdown code block syntax.\n3. Do not include any additional text.\n\n### Answer: (use the provided format with backticks)\n", "reference": "    def from_pretrained(cls, *args, **kwargs):\n\n        config = AutoConfig.from_pretrained(*args, **kwargs)\n\n        assert hasattr(config, \"chronos_config\"), \"Not a Chronos config file\"\n\n        chronos_config = ChronosConfig(**config.chronos_config)\n\n        if chronos_config.model_type == \"seq2seq\":\n            inner_model = AutoModelForSeq2SeqLM.from_pretrained(*args, **kwargs)\n        else:\n            assert config.model_type == \"causal\"\n            inner_model = AutoModelForCausalLM.from_pretrained(*args, **kwargs)\n\n        return cls(\n            tokenizer=chronos_config.create_tokenizer(),\n            model=ChronosModel(config=chronos_config, model=inner_model),\n        )", "completions": ["  \n```\ndef from_pretrained(cls, *args, **kwargs) :\n    \"\"\"\n    Description: This function is used to load a pre-trained model configuration from a directory or the model hub.\n\n    Args:\n        *args: Variable length argument list. This is used to pass arguments to the AutoConfig.from_pretrained method.\n        **kwargs: Arbitrary keyword arguments. This is used to pass keyword arguments to the AutoConfig.from_pretrained method.\n\n    Returns:\n        cls: An instance of the class with the loaded tokenizer and model.\n    \"\"\"\n    # Load the tokenizer and model from the pre-trained model configuration\n    tokenizer = AutoTokenizer.from_pretrained(*args, **kwargs)\n    model = AutoModel.from_pretrained(*args, **kwargs)\n\n    # Create an instance of the class with the loaded tokenizer and model\n    instance = cls(tokenizer=tokenizer, model=model)\n\n    return instance\n```"]}
