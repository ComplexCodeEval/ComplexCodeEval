{"id": 0, "name": "extract_best_config", "prompt": "### Role:\nYou are an expert Python programmer. You will be given some information and will generate a correct Python function that matches this information.\n\n### Task:\nBased on the provided information, generate a complete Python function. Ensure the code is logically correct, syntactically accurate, and follows best coding practices.\n\n### Function name:\nextract_best_config\n\n### Doc string and function signature:\n```Python\ndef extract_best_config(trial_path: str, output_path: Optional[str] = None) -> Dict :\n    \"\"\"\n    Extract the optimal pipeline from evaluated trial.\n\n    :param trial_path: The path to the trial directory that you want to extract the pipeline from.\n        Must already be evaluated.\n    :param output_path: Output path that pipeline yaml file will be saved.\n        Must be .yaml or .yml file.\n        If None, it does not save yaml file and just return dict values.\n        Default is None.\n    :return: The dictionary of the extracted pipeline.\n    \"\"\"\n```\n\n### Requirements:\n1. Generate the complete function implementation.\n2. Only answer the code of the generated function, formatted using Markdown code block syntax.\n3. Do not include any additional text.\n\n### Answer: (use the provided format with backticks)\n", "reference": "def extract_best_config(trial_path: str, output_path: Optional[str] = None) -> Dict:\n    summary_path = os.path.join(trial_path, 'summary.csv')\n    if not os.path.exists(summary_path):\n        raise ValueError(f\"summary.csv does not exist in {trial_path}.\")\n    trial_summary_df = load_summary_file(summary_path, dict_columns=['best_module_params'])\n    config_yaml_path = os.path.join(trial_path, 'config.yaml')\n    with open(config_yaml_path, 'r') as f:\n        config_dict = yaml.safe_load(f)\n    yaml_dict = summary_df_to_yaml(trial_summary_df, config_dict)\n    if output_path is not None:\n        with open(output_path, 'w') as f:\n            yaml.dump(yaml_dict, f)\n    return yaml_dict", "completions": ["  \n```\ndef extract_best_config(trial_path: str, output_path: Optional[str] = None) -> Dict:\n    \"\"\"\n    Extract the optimal pipeline from evaluated trial.\n\n    :param trial_path: The path to the trial directory that you want to extract the pipeline from.\n        Must already be evaluated.\n    :param output_path: Output path that pipeline yaml file will be saved.\n        Must be.yaml or.yml file.\n        If None, it does not save yaml file and just return dict values.\n        Default is None.\n    :return: The dictionary of the extracted pipeline.\n    \"\"\"\n    # Load the trial data\n    trial_data = load_trial_data(trial_path)\n\n    # Extract the best configuration\n    best_config = extract_best_config_from_trial_data(trial_data)\n\n    # Save the best configuration to a yaml file\n    if output_path is not None:\n        save_yaml_file(best_config, output_path)\n\n    return best_config\n```"]}
{"id": 1, "name": "s4d_kernel", "prompt": "### Role:\nYou are an expert Python programmer. You will be given some information and will generate a correct Python function that matches this information.\n\n### Task:\nBased on the provided information, generate a complete Python function. Ensure the code is logically correct, syntactically accurate, and follows best coding practices.\n\n### Function name:\ns4d_kernel\n\n### Doc string and function signature:\n```Python\ndef s4d_kernel(\n    A: torch.Tensor, B: torch.Tensor, C: torch.Tensor, dt: float, L: int\n) -> torch.Tensor :\n    \"\"\"\n    Compute the S4D convolution kernel for state space models on 3D tensors with shape (batch_size, seqlen, dim).\n\n    Parameters:\n    A (torch.Tensor): A tensor of shape (batch_size, dim) containing the eigenvalues of the state update matrix.\n    B (torch.Tensor): A tensor of shape (batch_size, dim) containing the input-to-state weights.\n    C (torch.Tensor): A tensor of shape (batch_size, dim) containing the state-to-output weights.\n    dt (float): A scalar that represents the time step in the discrete-time SSM.\n    L (int): The length of the sequence over which the convolution will be performed.\n\n    Returns:\n    torch.Tensor: A tensor of shape (batch_size, seqlen, dim) that represents the convolution of the inputs through the SSM.\n\n    Raises:\n    ValueError: If the dimensions of A, B, or C are not compatible.\n    TypeError: If dt is not a float or L is not an integer.\n    \"\"\"\n```\n\n### Requirements:\n1. Generate the complete function implementation.\n2. Only answer the code of the generated function, formatted using Markdown code block syntax.\n3. Do not include any additional text.\n\n### Answer: (use the provided format with backticks)\n", "reference": "def s4d_kernel(\n    A: torch.Tensor, B: torch.Tensor, C: torch.Tensor, dt: float, L: int\n) -> torch.Tensor:\n\n    # Ensure A, B, and C have the same size in the last dimension and compatible batch dimensions\n    if (\n        A.size(-1) != B.size(-1)\n        or A.size(-1) != C.size(-1)\n        or A.shape[:-1] != B.shape[:-1]\n        or A.shape[:-1] != C.shape[:-1]\n    ):\n        raise ValueError(\n            \"The last dimension of tensors A, B, and C must match and have\"\n            \" compatible batch dimensions.\"\n        )\n\n    # Check that dt is a float and L is an integer\n    if not isinstance(dt, float):\n        raise TypeError(\"The time step dt must be a float.\")\n    if not isinstance(L, int):\n        raise TypeError(\"The sequence length L must be an integer.\")\n\n    # Create a range of values from 0 to L-1 and reshape for broadcasting\n    arange_L = torch.arange(L, dtype=A.dtype, device=A.device).view(L, 1)\n\n    # Expand A and B for broadcasting with the sequence length\n    A_expanded = A.unsqueeze(1)  # Shape: (batch_size, 1, dim)\n    B_expanded = B.unsqueeze(1)  # Shape: (batch_size, 1, dim)\n\n    # Perform the convolution kernel operation with proper broadcasting\n    vandermonde = torch.exp(\n        arange_L * dt * A_expanded\n    )  # Shape: (seqlen, batch_size, dim)\n    result = torch.sum(\n        vandermonde\n        * B_expanded\n        * (torch.exp(dt * A_expanded) - 1)\n        / A_expanded,\n        dim=0,\n    )\n    result = C.unsqueeze(1) * result  # Shape: (batch_size, seqlen, dim)\n\n    return result", "completions": ["  \n```\ndef s4d_kernel(A: torch.Tensor, B: torch.Tensor, C: torch.Tensor, dt: float, L: int) -> torch.Tensor:\n    \"\"\"\n    Compute the S4D convolution kernel for state space models on 3D tensors with shape (batch_size, seqlen, dim).\n\n    Parameters:\n    A (torch.Tensor): A tensor of shape (batch_size, dim) containing the eigenvalues of the state update matrix.\n    B (torch.Tensor): A tensor of shape (batch_size, dim) containing the input-to-state weights.\n    C (torch.Tensor): A tensor of shape (batch_size, dim) containing the state-to-output weights.\n    dt (float): A scalar that represents the time step in the discrete-time SSM.\n    L (int): The length of the sequence over which the convolution will be performed.\n\n    Returns:\n    torch.Tensor: A tensor of shape (batch_size, seqlen, dim) that represents the convolution of the inputs through the SSM.\n\n    Raises:\n    ValueError: If the dimensions of A, B, or C are not compatible.\n    TypeError: If dt is not a float or L is not an integer.\n    \"\"\"\n    # Check input dimensions\n    if A.shape[0]!= B.shape[0] or A.shape[0]!= C.shape[0]:\n        raise ValueError(\"Dimensions of A, B, and C must be compatible.\")\n    if dt <= 0:\n        raise ValueError(\"dt must be a positive scalar.\")\n    if L <= 0:\n        raise ValueError(\"L must be a positive integer.\")\n\n    # Compute the convolution kernel\n    kernel = torch.zeros((L, A.shape[1], A.shape[1]))\n    for i in range(L):\n        kernel[i] = A @ (B @ C).T\n        A = A @ A\n\n    return kernel\n```"]}
{"id": 2, "name": "_calculate_intersection_score", "prompt": "### Role:\nYou are an expert Python programmer. You will be given some information and will generate a correct Python function that matches this information.\n\n### Task:\nBased on the provided information, generate a complete Python function. Ensure the code is logically correct, syntactically accurate, and follows best coding practices.\n\n### Function name:\n_calculate_intersection_score\n\n### Doc string and function signature:\n```Python\ndef _calculate_intersection_score(\n    elem1: dict[str, Any], elem2: dict[str, Any]\n) -> float :\n    \"\"\"Calculates the intersection score for two masks.\n\n    Args:\n      elem1: The first element.\n      elem2: The second element.\n\n    Returns:\n      The intersection score calculated as the ratio of the intersection\n      area to the area of the smaller mask.\n    \"\"\"\n```\n\n### Requirements:\n1. Generate the complete function implementation.\n2. Only answer the code of the generated function, formatted using Markdown code block syntax.\n3. Do not include any additional text.\n\n### Answer: (use the provided format with backticks)\n", "reference": "def _calculate_intersection_score(\n    elem1: dict[str, Any], elem2: dict[str, Any]\n) -> float:\n\n  # Check if the masks have the same dimensions.\n  if elem1['segmentation'].shape != elem2['segmentation'].shape:\n    raise ValueError('The masks must have the same dimensions.')\n\n  min_elem = elem1 if elem1['area'] < elem2['area'] else elem2\n  intersection = np.logical_and(elem1['segmentation'], elem2['segmentation'])\n  score = np.sum(intersection) / np.sum(min_elem['segmentation'])\n  return score", "completions": ["  \n```Python\ndef _calculate_intersection_score(\n    elem1: dict[str, Any], elem2: dict[str, Any]\n) -> float :\n    \"\"\"Calculates the intersection score for two masks.\n\n    Args:\n      elem1: The first element.\n      elem2: The second element.\n\n    Returns:\n      The intersection score calculated as the ratio of the intersection\n      area to the area of the smaller mask.\n    \"\"\"\n    # Calculate the intersection area\n    intersection_area = 0\n    for key in elem1:\n        if key in elem2:\n            intersection_area += elem1[key] * elem2[key]\n\n    # Calculate the area of the smaller mask\n    smaller_mask_area = min(sum(elem1.values()), sum(elem2.values()))\n\n    # Calculate the intersection score\n    intersection_score = intersection_area / smaller_mask_area\n\n    return intersection_score\n```"]}
{"id": 3, "name": "t_map", "prompt": "### Role:\nYou are an expert Python programmer. You will be given some information and will generate a correct Python function that matches this information.\n\n### Task:\nBased on the provided information, generate a complete Python function. Ensure the code is logically correct, syntactically accurate, and follows best coding practices.\n\n### Function name:\nt_map\n\n### Doc string and function signature:\n```Python\ndef t_map(self, t: Union[float, torch.Tensor]) -> torch.Tensor :\n    \"\"\"map t in [0, 1] to [t_min, t_max]\n\n    Args:\n        t (Union[float, torch.Tensor]): time\n\n    Returns:\n        torch.Tensor: mapped time\n    \"\"\"\n```\n\n### Requirements:\n1. Generate the complete function implementation.\n2. Only answer the code of the generated function, formatted using Markdown code block syntax.\n3. Do not include any additional text.\n\n### Answer: (use the provided format with backticks)\n", "reference": "    def t_map(self, t: Union[float, torch.Tensor]) -> torch.Tensor:\n        if not isinstance(t, torch.Tensor):\n            t = torch.Tensor([t]).float()\n\n        t_max = self.t_max.to(t.device)\n        t_min = self.t_min.to(t.device)\n        t_tilde = t_min + (t_max - t_min) * t\n\n        return t_tilde", "completions": ["  \n```Python\ndef t_map(self, t: Union[float, torch.Tensor]) -> torch.Tensor :\n    \"\"\"map t in [0, 1] to [t_min, t_max]\n\n    Args:\n        t (Union[float, torch.Tensor]): time\n\n    Returns:\n        torch.Tensor: mapped time\n    \"\"\"\n    if isinstance(t, float):\n        return torch.tensor(t).to(self.device)\n    else:\n        return t.to(self.device)\n```"]}
{"id": 4, "name": "sample_treatment_and_effect", "prompt": "### Role:\nYou are an expert Python programmer. You will be given some information and will generate a correct Python function that matches this information.\n\n### Task:\nBased on the provided information, generate a complete Python function. Ensure the code is logically correct, syntactically accurate, and follows best coding practices.\n\n### Function name:\nsample_treatment_and_effect\n\n### Doc string and function signature:\n```Python\ndef sample_treatment_and_effect(\n    graph: torch.Tensor, node_names: Sequence[str], ensure_effect: bool = True, num_effects: int = 1\n) -> tuple[str, list[str]] :\n    \"\"\"Sample a treatment and effects from a graph.\n\n    Args:\n        graph: The adjacency matrix of the graph.\n        node_names: The names of the nodes in the graph.\n        ensure_effect: Whether to ensure that there is a path from the treatment to the effect.\n        num_effects: The number of effect nodes to sample.\n\n    Returns:\n        The treatment and effects.\n    \"\"\"\n```\n\n### Requirements:\n1. Generate the complete function implementation.\n2. Only answer the code of the generated function, formatted using Markdown code block syntax.\n3. Do not include any additional text.\n\n### Answer: (use the provided format with backticks)\n", "reference": "def sample_treatment_and_effect(\n    graph: torch.Tensor, node_names: Sequence[str], ensure_effect: bool = True, num_effects: int = 1\n) -> tuple[str, list[str]]:\n    if ensure_effect:\n        assert num_effects == 1, \"Can only ensure effect for one effect node.\"\n\n        nx_graph = nx.from_numpy_array(graph.numpy(), create_using=nx.DiGraph)\n        nx_graph = nx.relabel_nodes(nx_graph, dict(enumerate(node_names)))\n\n        topological_order = nx.topological_sort(nx_graph)\n        topological_order_with_descendants = [n for n in topological_order if nx.descendants(nx_graph, n)]\n\n        if topological_order_with_descendants:\n            treatment = np.random.choice(topological_order_with_descendants, size=1, replace=False).item()\n            # randomly sample a effect not that is a descendant of the treatment\n            descendants = nx.descendants(nx_graph, treatment)\n            effects = np.random.choice(list(descendants), size=num_effects, replace=False)\n            return treatment, effects.tolist()\n        else:\n            warnings.warn(\"No edges found. Defaulting to random sampling.\")\n\n    samples = np.random.choice(node_names, size=1 + num_effects, replace=False)\n    treatment = samples[0]\n    effects = samples[1:]\n\n    return treatment, effects.tolist()", "completions": ["  \n```\ndef sample_treatment_and_effect(\n    graph: torch.Tensor, node_names: Sequence[str], ensure_effect: bool = True, num_effects: int = 1\n) -> tuple[str, list[str]] :\n    \"\"\"Sample a treatment and effects from a graph.\n\n    Args:\n        graph: The adjacency matrix of the graph.\n        node_names: The names of the nodes in the graph.\n        ensure_effect: Whether to ensure that there is a path from the treatment to the effect.\n        num_effects: The number of effect nodes to sample.\n\n    Returns:\n        The treatment and effects.\n    \"\"\"\n    # Generate a random treatment\n    treatment = random.choice(node_names)\n\n    # Generate random effects\n    effects = []\n    for _ in range(num_effects):\n        effect = random.choice(node_names)\n        if ensure_effect:\n            # Ensure that there is a path from the treatment to the effect\n            if not graph.has_path(treatment, effect):\n                continue\n        effects.append(effect)\n\n    return treatment, effects\n```"]}
{"id": 5, "name": "generate", "prompt": "### Role:\nYou are an expert Python programmer. You will be given some information and will generate a correct Python function that matches this information.\n\n### Task:\nBased on the provided information, generate a complete Python function. Ensure the code is logically correct, syntactically accurate, and follows best coding practices.\n\n### Function name:\ngenerate\n\n### Doc string and function signature:\n```Python\ndef generate(self, model, system_message, prompt, **kwargs) :\n    \"\"\"\n    The main generation function, given the args, kwargs, function_modeler, function description and model type, generate a response\n    Args\n        model (OpenAIConfig): The model to use for generation.\n        system_message (str): The system message to use for generation.\n        prompt (str): The prompt to use for generation.\n        kwargs (dict): Additional generation parameters.\n    \"\"\"\n```\n\n### Requirements:\n1. Generate the complete function implementation.\n2. Only answer the code of the generated function, formatted using Markdown code block syntax.\n3. Do not include any additional text.\n\n### Answer: (use the provided format with backticks)\n", "reference": "    def generate(self, model, system_message, prompt, **kwargs):\n\n        self.check_api_key()\n\n        temperature = kwargs.get(\"temperature\", 0.1)\n        top_p = kwargs.get(\"top_p\", 1)\n        frequency_penalty = kwargs.get(\"frequency_penalty\", 0)\n        presence_penalty = kwargs.get(\"presence_penalty\", 0)\n        max_new_tokens = kwargs.get(\"max_new_tokens\")\n        # check if there are any generation parameters that are not supported\n        unsupported_params = [param for param in kwargs.keys() if param not in LLM_GENERATION_PARAMETERS]\n        if len(unsupported_params) > 0:\n            # log warning\n            logging.warning(f\"Unused generation parameters sent as input: {unsupported_params}.\"\\\n                             f\"For OpenAI, only the following parameters are supported: {LLM_GENERATION_PARAMETERS}\")\n        params = {\n            \"model\": model.model_name,\n            \"temperature\": temperature,\n            \"max_tokens\": max_new_tokens,\n            \"top_p\": top_p,\n            \"frequency_penalty\": frequency_penalty,\n            \"presence_penalty\": presence_penalty,\n        }\n        if model.parsing_helper_tokens[\"start_token\"]:\n            prompt += model.parsing_helper_tokens[\"start_token\"]\n        messages = [\n            {\n                \"role\": \"system\",\n                \"content\": system_message\n            },\n            {\n                \"role\": \"user\",\n                \"content\": prompt\n            }\n        ]\n        params[\"messages\"] = messages\n\n        counter = 0\n        choice = None\n        # initiate response so exception logic doesnt error out when checking for error in response\n        response = {}\n        while counter <= 5:\n            try:\n                openai_headers = {\n                    \"Authorization\": f\"Bearer {self.api_key}\",\n                    \"Content-Type\": \"application/json\",\n                }\n                response = requests.post(\n                    OPENAI_URL, headers=openai_headers, json=params, timeout=50\n                )\n                response = response.json()\n                choice = response[\"choices\"][0][\"message\"][\"content\"].strip(\"'\")\n                break\n            except Exception as e:\n                if (\"error\" in response and\n                        \"code\" in response[\"error\"] and\n                        response[\"error\"][\"code\"] == 'invalid_api_key'):\n                    raise Exception(f\"The supplied OpenAI API key {self.api_key} is invalid\")\n                if counter == 5:\n                    raise Exception(f\"OpenAI API failed to generate a response: {e}\")\n                counter += 1\n                time.sleep(2 ** counter)\n                continue\n\n        if not choice:\n            raise Exception(\"OpenAI API failed to generate a response\")\n        \n        if model.parsing_helper_tokens[\"end_token\"]:\n            # remove the end token from the choice\n            choice = choice.split(model.parsing_helper_tokens[\"end_token\"])[0]\n            # check if starting token is in choice\n            if model.parsing_helper_tokens[\"start_token\"] in choice:\n                # remove the starting token from the choice\n                choice = choice.split(model.parsing_helper_tokens[\"start_token\"])[-1]\n        return choice", "completions": ["  \n```\ndef generate(self, model, system_message, prompt, **kwargs) :\n    \"\"\"\n    The main generation function, given the args, kwargs, function_modeler, function description and model type, generate a response\n    Args\n        model (OpenAIConfig): The model to use for generation.\n        system_message (str): The system message to use for generation.\n        prompt (str): The prompt to use for generation.\n        kwargs (dict): Additional generation parameters.\n    \"\"\"\n    # Generate the response using the provided model, system message, and prompt\n    response = model.generate(system_message, prompt, **kwargs)\n\n    # Return the generated response\n    return response\n```"]}
{"id": 6, "name": "query_tags_from_docker_hub", "prompt": "### Role:\nYou are an expert Python programmer. You will be given some information and will generate a correct Python function that matches this information.\n\n### Task:\nBased on the provided information, generate a complete Python function. Ensure the code is logically correct, syntactically accurate, and follows best coding practices.\n\n### Function name:\nquery_tags_from_docker_hub\n\n### Doc string and function signature:\n```Python\ndef query_tags_from_docker_hub(\n    filter_func: Callable[[str], bool],\n    namespace: str,\n    repository: str,\n    docker_hub_token: str,\n    num_tags: Optional[int] = None,\n) -> List[str] :\n    \"\"\"\n    Query tags from Docker Hub repository with filter.\n    If Docker Hub API returns an error, the function will:\n        - Stop querying\n        - Return the current list of tags.\n\n    Args:\n        filter_func: Function to return whether tag should be included.\n        namespace: Docker namespace\n        repository: Docker repository\n        num_tags: Max number of tags to query\n\n    Returns:\n        Sorted list of tags from Docker Hub repository\n        with format namespace/repository:tag.\n    \"\"\"\n```\n\n### Requirements:\n1. Generate the complete function implementation.\n2. Only answer the code of the generated function, formatted using Markdown code block syntax.\n3. Do not include any additional text.\n\n### Answer: (use the provided format with backticks)\n", "reference": "def query_tags_from_docker_hub(\n    filter_func: Callable[[str], bool],\n    namespace: str,\n    repository: str,\n    docker_hub_token: str,\n    num_tags: Optional[int] = None,\n) -> List[str]:\n    filtered_tags = []\n    headers = {\n        \"Authorization\": f\"Bearer {docker_hub_token}\",\n    }\n\n    page_count = 1\n    while page_count:\n        logger.info(f\"Querying page {page_count}\")\n        url = f\"https://hub.docker.com/v2/namespaces/{namespace}/repositories/{repository}/tags?page={page_count}&page_size=100\"  # noqa E501\n\n        response = requests.get(url, headers=headers)\n        response_json = response.json()\n\n        # Stop querying if Docker Hub API returns an error\n        if response.status_code != 200:\n            logger.info(f\"Failed to query tags from Docker Hub: Error: {response_json}\")\n            return sorted([f\"{namespace}/{repository}:{t}\" for t in filtered_tags])\n\n        result = response_json[\"results\"]\n        tags = [tag[\"name\"] for tag in result]\n        filtered_tags_page = list(filter(filter_func, tags))  # Filter tags\n\n        # Add enough tags to not exceed num_tags if num_tags is specified\n        if num_tags:\n            if len(filtered_tags) + len(filtered_tags_page) > num_tags:\n                filtered_tags.extend(\n                    filtered_tags_page[: num_tags - len(filtered_tags)]\n                )\n                break\n        filtered_tags.extend(filtered_tags_page)\n\n        logger.info(f\"Tag count: {len(filtered_tags)}\")\n        if not response_json[\"next\"]:\n            break\n        page_count += 1\n    return sorted([f\"{namespace}/{repository}:{t}\" for t in filtered_tags])", "completions": ["  \n```\ndef query_tags_from_docker_hub(\n    filter_func: Callable[[str], bool],\n    namespace: str,\n    repository: str,\n    docker_hub_token: str,\n    num_tags: Optional[int] = None,\n) -> List[str] :\n    \"\"\"\n    Query tags from Docker Hub repository with filter.\n    If Docker Hub API returns an error, the function will:\n        - Stop querying\n        - Return the current list of tags.\n\n    Args:\n        filter_func: Function to return whether tag should be included.\n        namespace: Docker namespace\n        repository: Docker repository\n        num_tags: Max number of tags to query\n\n    Returns:\n        Sorted list of tags from Docker Hub repository\n        with format namespace/repository:tag.\n    \"\"\"\n    import requests\n    import json\n\n    url = f\"https://hub.docker.com/v2/repositories/{namespace}/{repository}/tags/\"\n    headers = {\n        \"Authorization\": f\"Bearer {docker_hub_token}\",\n        \"Accept\": \"application/json\",\n    }\n\n    tags = []\n    page = 1\n    while True:\n        response = requests.get(url, headers=headers, params={\"page\": page})\n        if response.status_code == 401:\n            raise ValueError(\"Invalid Docker Hub token\")\n        elif response.status_code == 404:\n            raise ValueError(f\"Repository {namespace}/{repository} not found\")\n        elif response.status_code!= 200:\n            raise ValueError(f\"Error querying Docker Hub API: {response.text}\")\n\n        data = json.loads(response.text)\n        for tag in data[\"results\"]:\n            if filter_func(tag[\"name\"]):\n                tags.append(f\"{namespace}/{repository}:{tag['name']}\")\n\n        if len(tags) >= num_tags:\n            break\n\n        page += 1\n\n    return sorted(tags)\n```"]}
{"id": 7, "name": "select", "prompt": "### Role:\nYou are an expert Python programmer. You will be given some information and will generate a correct Python function that matches this information.\n\n### Task:\nBased on the provided information, generate a complete Python function. Ensure the code is logically correct, syntactically accurate, and follows best coding practices.\n\n### Function name:\nselect\n\n### Doc string and function signature:\n```Python\ndef select(\n        self,\n        pairs: List[Tuple[str, int]]\n    ) -> Dict[str, List[model_pb2.Model]] :\n    \"\"\"Selects given number of models for each learner id.\n\n    Parameters\n    ----------\n    pairs : List[Tuple[str, int]]\n        A list of (learner_id, number of models) pairs.\n\n    Returns\n    -------\n    Dict[str, List[model_pb2.Model]]\n        A dictionary of learner_ids to list of models.    \n    \"\"\"\n```\n\n### Requirements:\n1. Generate the complete function implementation.\n2. Only answer the code of the generated function, formatted using Markdown code block syntax.\n3. Do not include any additional text.\n\n### Answer: (use the provided format with backticks)\n", "reference": "    def select(\n        self,\n        pairs: List[Tuple[str, int]]\n    ) -> Dict[str, List[model_pb2.Model]]:\n\n        result = {}\n\n        for pair in pairs:\n            learner_id, num_models = pair\n\n            if learner_id not in self.store_cache.keys():\n                raise ValueError(\n                    f\"No models found for learner id {learner_id}\")\n\n            history_length = len(self.store_cache[learner_id])\n\n            if num_models > history_length:\n                # TODO: check if continue is the right thing to do here. How about we return all the models we have?\n                logger.warn(\n                    f\"Number of models requested ({num_models}) is greater than \"\n                    f\"the number of models available ({history_length}) for \"\n                    f\"learner id {learner_id}\")\n                continue\n\n            if num_models <= 0:\n                num_models = history_length\n\n            result[learner_id] = self.store_cache[learner_id][-num_models:]\n\n        return result", "completions": ["  \n```\ndef select(\n        self,\n        pairs: List[Tuple[str, int]]\n    ) -> Dict[str, List[model_pb2.Model]] :\n    \"\"\"Selects given number of models for each learner id.\n\n    Parameters\n    ----------\n    pairs : List[Tuple[str, int]]\n        A list of (learner_id, number of models) pairs.\n\n    Returns\n    -------\n    Dict[str, List[model_pb2.Model]]\n        A dictionary of learner_ids to list of models.    \n    \"\"\"\n    selected_models = {}\n    for learner_id, num_models in pairs:\n        selected_models[learner_id] = []\n        for model in self.models:\n            if model.learner_id == learner_id:\n                selected_models[learner_id].append(model)\n                if len(selected_models[learner_id]) == num_models:\n                    break\n    return selected_models\n```"]}
{"id": 8, "name": "get_boto3_session", "prompt": "### Role:\nYou are an expert Python programmer. You will be given some information and will generate a correct Python function that matches this information.\n\n### Task:\nBased on the provided information, generate a complete Python function. Ensure the code is logically correct, syntactically accurate, and follows best coding practices.\n\n### Function name:\nget_boto3_session\n\n### Doc string and function signature:\n```Python\ndef get_boto3_session(self) -> Session :\n    \"\"\"\n    Returns a Boto3 session object with the necessary credentials.\n\n    This method retrieves the credentials from the STSClientManager using the given AWS\n    assume role ARN and session name. It then creates a Boto3 session object with the\n    retrieved credentials and returns it.\n\n    Returns\n    -------\n        Session: A Boto3 session object with the necessary credentials.\n\n    \"\"\"\n```\n\n### Requirements:\n1. Generate the complete function implementation.\n2. Only answer the code of the generated function, formatted using Markdown code block syntax.\n3. Do not include any additional text.\n\n### Answer: (use the provided format with backticks)\n", "reference": "    def get_boto3_session(self) -> Session:\n        credentials = self.sts_client_manager.get_credentials(\n            self.configurations.aws_assume_role_arn,\n            self.configurations.aws_assume_role_session_name,\n        )\n        tmp_access_key = credentials[\"AccessKeyId\"]\n        tmp_secret_key = credentials[\"SecretAccessKey\"]\n        security_token = credentials[\"SessionToken\"]\n        return Session(\n            aws_access_key_id=tmp_access_key,\n            aws_secret_access_key=tmp_secret_key,\n            aws_session_token=security_token,\n        )", "completions": ["  \n```\ndef get_boto3_session(self) -> Session:\n    \"\"\"\n    Returns a Boto3 session object with the necessary credentials.\n\n    This method retrieves the credentials from the STSClientManager using the given AWS\n    assume role ARN and session name. It then creates a Boto3 session object with the\n    retrieved credentials and returns it.\n\n    Returns\n    -------\n        Session: A Boto3 session object with the necessary credentials.\n\n    \"\"\"\n    # Retrieve the credentials from the STSClientManager\n    credentials = self.sts_client_manager.get_credentials(self.assume_role_arn, self.session_name)\n\n    # Create a Boto3 session object with the retrieved credentials\n    session = boto3.Session(\n        aws_access_key_id=credentials['AccessKeyId'],\n        aws_secret_access_key=credentials['SecretAccessKey'],\n        aws_session_token=credentials['SessionToken']\n    )\n\n    return session\n```"]}
{"id": 9, "name": "generate_clebsch_gordan_rsh", "prompt": "### Role:\nYou are an expert Python programmer. You will be given some information and will generate a correct Python function that matches this information.\n\n### Task:\nBased on the provided information, generate a complete Python function. Ensure the code is logically correct, syntactically accurate, and follows best coding practices.\n\n### Function name:\ngenerate_clebsch_gordan_rsh\n\n### Doc string and function signature:\n```Python\ndef generate_clebsch_gordan_rsh(lmax: int, parity_invariance: bool = True) -> torch.Tensor :\n    \"\"\"\n    Generate Clebsch-Gordan coefficients for real spherical harmonics.\n\n    Args:\n        lmax: maximum angular momentum\n        parity_invariance: whether to enforce parity invariance, i.e. only allow\n            non-zero coefficients if :math:`-1^l_1 -1^l_2 = -1^l_3`.\n\n    \"\"\"\n```\n\n### Requirements:\n1. Generate the complete function implementation.\n2. Only answer the code of the generated function, formatted using Markdown code block syntax.\n3. Do not include any additional text.\n\n### Answer: (use the provided format with backticks)\n", "reference": "def generate_clebsch_gordan_rsh(lmax: int, parity_invariance: bool = True) -> torch.Tensor:\n    lidx, _ = sh_indices(lmax)\n    cg = generate_clebsch_gordan(lmax).to(dtype=torch.complex64)\n    complex_to_real = generate_sh_to_rsh(lmax)  # (real, complex)\n    cg_rsh = torch.einsum(\n        \"ijk,mi,nj,ok->mno\",\n        cg,\n        complex_to_real,\n        complex_to_real,\n        complex_to_real.conj(),\n    )\n\n    if parity_invariance:\n        parity = (-1.0) ** lidx\n        pmask = parity[:, None, None] * parity[None, :, None] == parity[None, None, :]\n        cg_rsh *= pmask\n    else:\n        lsum = lidx[:, None, None] + lidx[None, :, None] - lidx[None, None, :]\n        cg_rsh *= 1.0j**lsum\n\n    # cast to real\n    cg_rsh = cg_rsh.real.to(matgl.float_th)\n    return cg_rsh", "completions": ["  \n```\ndef generate_clebsch_gordan_rsh(lmax: int, parity_invariance: bool = True) -> torch.Tensor :\n    \"\"\"\n    Generate Clebsch-Gordan coefficients for real spherical harmonics.\n\n    Args:\n        lmax: maximum angular momentum\n        parity_invariance: whether to enforce parity invariance, i.e. only allow\n            non-zero coefficients if :math:`-1^l_1 -1^l_2 = -1^l_3`.\n\n    \"\"\"\n    # Initialize the tensor to store the Clebsch-Gordan coefficients\n    cg_coeffs = torch.zeros((lmax+1, lmax+1, lmax+1))\n\n    # Loop over all possible values of l1, l2, and l3\n    for l1 in range(lmax+1):\n        for l2 in range(lmax+1):\n            for l3 in range(lmax+1):\n                # Compute the Clebsch-Gordan coefficient\n                cg_coeff = clebsch_gordan_coeff(l1, l2, l3)\n\n                # Check if the coefficient is non-zero\n                if cg_coeff!= 0:\n                    # Check if the parity invariance condition is satisfied\n                    if parity_invariance and (-1)**l1 - (-1)**l2!= (-1)**l3:\n                        continue\n\n                    # Store the non-zero coefficient in the tensor\n                    cg_coeffs[l1, l2, l3] = cg_coeff\n\n    return cg_coeffs\n```"]}
{"id": 10, "name": "set_xdata", "prompt": "### Role:\nYou are an expert Python programmer. You will be given some information and will generate a correct Python function that matches this information.\n\n### Task:\nBased on the provided information, generate a complete Python function. Ensure the code is logically correct, syntactically accurate, and follows best coding practices.\n\n### Function name:\nset_xdata\n\n### Doc string and function signature:\n```Python\ndef set_xdata(self, x) :\n    \"\"\"\n    Set the data array for x.\n\n    Parameters\n    ----------\n    x : 1D array\n    \"\"\"\n```\n\n### Requirements:\n1. Generate the complete function implementation.\n2. Only answer the code of the generated function, formatted using Markdown code block syntax.\n3. Do not include any additional text.\n\n### Answer: (use the provided format with backticks)\n", "reference": "    def set_xdata(self, x):\n        if not np.iterable(x):\n            # When deprecation cycle is completed\n            # raise RuntimeError('x must be a sequence')\n            _api.warn_deprecated(\n                since=3.7,\n                message=\"Setting data with a non sequence type \"\n                \"is deprecated since %(since)s and will be \"\n                \"remove %(removal)s\")\n            x = [x, ]\n        self._xorig = copy.copy(x)\n        self._invalidx = True\n        self.stale = True", "completions": ["  \n```Python\ndef set_xdata(self, x) :\n    \"\"\"\n    Set the data array for x.\n\n    Parameters\n    ----------\n    x : 1D array\n    \"\"\"\n    self.xdata = x\n```"]}
{"id": 11, "name": "format_template", "prompt": "### Role:\nYou are an expert Python programmer. You will be given some information and will generate a correct Python function that matches this information.\n\n### Task:\nBased on the provided information, generate a complete Python function. Ensure the code is logically correct, syntactically accurate, and follows best coding practices.\n\n### Function name:\nformat_template\n\n### Doc string and function signature:\n```Python\ndef format_template(\n    path: Path, base_path: Path, project_metadata: ProjectMetadata\n) -> TemplateOutput :\n    \"\"\"\n    Takes in a template path (relative to /templates) and returns the formatted\n    template contents and the final path of the file.\n\n    We support:\n        - Jinja templating within the file\n        - Bracket syntax in filenames, like /path/to/[project_name]/file.txt\n\n    \"\"\"\n```\n\n### Requirements:\n1. Generate the complete function implementation.\n2. Only answer the code of the generated function, formatted using Markdown code block syntax.\n3. Do not include any additional text.\n\n### Answer: (use the provided format with backticks)\n", "reference": "def format_template(\n    path: Path, base_path: Path, project_metadata: ProjectMetadata\n) -> TemplateOutput:\n    if not path.exists():\n        raise FileNotFoundError(f\"Template file {path} does not exist\")\n\n    metadata_variables = project_metadata.model_dump()\n\n    template = Template(path.read_text())\n    content = template.render(metadata_variables)\n\n    output_name = str(path.relative_to(base_path))\n    for key, value in metadata_variables.items():\n        output_name = output_name.replace(f\"[{key}]\", str(value))\n\n    return TemplateOutput(content=content, path=output_name)", "completions": ["  \n```\ndef format_template(path: Path, base_path: Path, project_metadata: ProjectMetadata) -> TemplateOutput:\n    \"\"\"\n    Takes in a template path (relative to /templates) and returns the formatted\n    template contents and the final path of the file.\n\n    We support:\n        - Jinja templating within the file\n        - Bracket syntax in filenames, like /path/to/[project_name]/file.txt\n\n    \"\"\"\n    # Load the template file\n    with open(path, \"r\") as f:\n        template_contents = f.read()\n\n    # Render the template using Jinja\n    template = Template(template_contents)\n    rendered_contents = template.render(project_metadata=project_metadata)\n\n    # Replace brackets in the filename with the project name\n    filename = path.name\n    project_name = project_metadata[\"project_name\"]\n    filename = filename.replace(\"[project_name]\", project_name)\n\n    # Join the base path and the rendered filename\n    final_path = base_path / filename\n\n    # Return the formatted template contents and the final path\n    return TemplateOutput(rendered_contents, final_path)\n```"]}
{"id": 12, "name": "to_padding_mask", "prompt": "### Role:\nYou are an expert Python programmer. You will be given some information and will generate a correct Python function that matches this information.\n\n### Task:\nBased on the provided information, generate a complete Python function. Ensure the code is logically correct, syntactically accurate, and follows best coding practices.\n\n### Function name:\nto_padding_mask\n\n### Doc string and function signature:\n```Python\ndef to_padding_mask(seq_lens: Tensor, batch_seq_len: int) -> Tensor :\n    \"\"\"Convert a sequence length array to a boolean padding mask tensor.\n\n    :param seq_lens:\n        An array where each element represents the length of a sequence. *Shape:*\n        :math:`(N)`, where :math:`N` is the batch size.\n    :param batch_seq_len:\n        The sequence length of the mask.\n\n    :returns:\n        The mask. *Shape:* :math:`(N,S)`, where :math:`N` is the batch size and\n        :math:`S` is the sequence length.\n    \"\"\"\n```\n\n### Requirements:\n1. Generate the complete function implementation.\n2. Only answer the code of the generated function, formatted using Markdown code block syntax.\n3. Do not include any additional text.\n\n### Answer: (use the provided format with backticks)\n", "reference": "def to_padding_mask(seq_lens: Tensor, batch_seq_len: int) -> Tensor:\n    batch_size = seq_lens.size(0)\n\n    # (N, S)\n    indices = torch.arange(batch_seq_len, device=seq_lens.device).expand(batch_size, -1)\n\n    # (N) -> (N, S)\n    lengths = seq_lens.unsqueeze(1).expand(-1, batch_seq_len)\n\n    return indices < lengths", "completions": ["  \n```Python\ndef to_padding_mask(seq_lens: Tensor, batch_seq_len: int) -> Tensor :\n    \"\"\"Convert a sequence length array to a boolean padding mask tensor.\n\n    :param seq_lens:\n        An array where each element represents the length of a sequence. *Shape:*\n        :math:`(N)`, where :math:`N` is the batch size.\n    :param batch_seq_len:\n        The sequence length of the mask.\n\n    :returns:\n        The mask. *Shape:* :math:`(N,S)`, where :math:`N` is the batch size and\n        :math:`S` is the sequence length.\n    \"\"\"\n    mask = torch.zeros(seq_lens.shape[0], batch_seq_len, dtype=torch.bool)\n    for i, seq_len in enumerate(seq_lens):\n        mask[i, seq_len:] = 1\n    return mask\n```"]}
{"id": 13, "name": "alembic_autogenerate", "prompt": "### Role:\nYou are an expert Python programmer. You will be given some information and will generate a correct Python function that matches this information.\n\n### Task:\nBased on the provided information, generate a complete Python function. Ensure the code is logically correct, syntactically accurate, and follows best coding practices.\n\n### Function name:\nalembic_autogenerate\n\n### Doc string and function signature:\n```Python\ndef alembic_autogenerate(\n        cls,\n        connection: sqlalchemy.engine.Connection,\n        message: str | None = None,\n        write_migration_scripts: bool = True,\n    ) -> bool :\n    \"\"\"Generate migration scripts for alembic-detectable changes.\n\n    Args:\n        connection: SQLAlchemy connection to use when detecting changes.\n        message: Human readable identifier describing the generated revision.\n        write_migration_scripts: If True, write autogenerated revisions to script directory.\n\n    Returns:\n        True when changes have been detected.\n    \"\"\"\n```\n\n### Requirements:\n1. Generate the complete function implementation.\n2. Only answer the code of the generated function, formatted using Markdown code block syntax.\n3. Do not include any additional text.\n\n### Answer: (use the provided format with backticks)\n", "reference": "    def alembic_autogenerate(\n        cls,\n        connection: sqlalchemy.engine.Connection,\n        message: str | None = None,\n        write_migration_scripts: bool = True,\n    ) -> bool:\n        if not Path(constants.ALEMBIC_CONFIG).exists():\n            return False\n\n        config, script_directory = cls._alembic_config()\n        revision_context = alembic.autogenerate.api.RevisionContext(\n            config=config,\n            script_directory=script_directory,\n            command_args=defaultdict(\n                lambda: None,\n                autogenerate=True,\n                head=\"head\",\n                message=message,\n            ),\n        )\n        writer = alembic.autogenerate.rewriter.Rewriter()\n\n        @writer.rewrites(alembic.operations.ops.AddColumnOp)\n        def render_add_column_with_server_default(context, revision, op):\n            # Carry the sqlmodel default as server_default so that newly added\n            # columns get the desired default value in existing rows.\n            if op.column.default is not None and op.column.server_default is None:\n                op.column.server_default = sqlalchemy.DefaultClause(\n                    sqlalchemy.sql.expression.literal(op.column.default.arg),\n                )\n            return op\n\n        def run_autogenerate(rev, context):\n            revision_context.run_autogenerate(rev, context)\n            return []\n\n        with alembic.runtime.environment.EnvironmentContext(\n            config=config,\n            script=script_directory,\n            fn=run_autogenerate,\n        ) as env:\n            env.configure(\n                connection=connection,\n                target_metadata=sqlmodel.SQLModel.metadata,\n                render_item=cls._alembic_render_item,\n                process_revision_directives=writer,  # type: ignore\n                compare_type=False,\n            )\n            env.run_migrations()\n        changes_detected = False\n        if revision_context.generated_revisions:\n            upgrade_ops = revision_context.generated_revisions[-1].upgrade_ops\n            if upgrade_ops is not None:\n                changes_detected = bool(upgrade_ops.ops)\n        if changes_detected and write_migration_scripts:\n            # Must iterate the generator to actually write the scripts.\n            _ = tuple(revision_context.generate_scripts())\n        return changes_detected", "completions": ["  \n```\ndef alembic_autogenerate(\n        cls,\n        connection: sqlalchemy.engine.Connection,\n        message: str | None = None,\n        write_migration_scripts: bool = True,\n    ) -> bool :\n    \"\"\"Generate migration scripts for alembic-detectable changes.\n\n    Args:\n        connection: SQLAlchemy connection to use when detecting changes.\n        message: Human readable identifier describing the generated revision.\n        write_migration_scripts: If True, write autogenerated revisions to script directory.\n\n    Returns:\n        True when changes have been detected.\n    \"\"\"\n    # Implement the function body here\n    pass\n```"]}
{"id": 14, "name": "obj_fcn_decorator", "prompt": "### Role:\nYou are an expert Python programmer. You will be given some information and will generate a correct Python function that matches this information.\n\n### Task:\nBased on the provided information, generate a complete Python function. Ensure the code is logically correct, syntactically accurate, and follows best coding practices.\n\n### Function name:\nobj_fcn_decorator\n\n### Doc string and function signature:\n```Python\ndef obj_fcn_decorator(\n    model_fcn_full,\n    weight_fcn,\n    TSS_fcn,\n    T,\n    obs,\n    settings,\n    alpha=2.0,\n    coef_id=[],\n    initial_fit=True,\n) :\n    \"\"\"\n    A decorator function that calculates the elastic net penalty for a given set of inputs and the objective function\n    for input to optimization algorithms.\n\n    Parameters:\n    model_fcn_full (function): The full model function.\n    weight_fcn (function): The weight function.\n    TSS_fcn (function): The TSS (Total Sum of Squares) function.\n    T (array-like): The temperature array.\n    obs (array-like): The observation array.\n    settings (object): The DailySettings object.\n    alpha (float): The alpha value for the elastic net penalty. Default is 2.0.\n    coef_id (list): The list of coefficient IDs. Default is an empty list.\n    initial_fit (bool): Whether or not this is the initial fit. Default is True.\n\n    Returns:\n    obj_fcn (function): an objective function having the required inputs for optimization via SciPy and NLopt.\n    \"\"\"\n```\n\n### Requirements:\n1. Generate the complete function implementation.\n2. Only answer the code of the generated function, formatted using Markdown code block syntax.\n3. Do not include any additional text.\n\n### Answer: (use the provided format with backticks)\n", "reference": "def obj_fcn_decorator(\n    model_fcn_full,\n    weight_fcn,\n    TSS_fcn,\n    T,\n    obs,\n    settings,\n    alpha=2.0,\n    coef_id=[],\n    initial_fit=True,\n):\n\n    N = np.shape(obs)[0]\n    N_min = settings.segment_minimum_count  # N minimum for a sloped segment\n    sigma = 2.698  # 1.5 IQR\n    quantile = 0.25\n    min_weight = 0.00\n\n    T_fit_bnds = np.array([np.min(T), np.max(T)])\n    T_range = T_fit_bnds[1] - T_fit_bnds[0]\n\n    model_fcn = model_fcn_dec(model_fcn_full, T_fit_bnds, T)\n\n    lasso_a = settings.regularization_percent_lasso * settings.regularization_alpha\n    ridge_a = (\n        1 - settings.regularization_percent_lasso\n    ) * settings.regularization_alpha\n\n    idx_k = get_idx([\"dd_k\"], coef_id)\n    idx_beta = get_idx([\"dd_beta\"], coef_id)\n    idx_bp = get_idx([\"dd_bp\"], coef_id)\n    # idx_reg = get_idx([\"dd_beta\", \"dd_k\"], coef_id) # drop bps and intercept from regularization\n    idx_reg = get_idx(\n        [\"dd_beta\", \"dd_k\", \"dd_bp\"], coef_id\n    )  # drop intercept from regularization\n\n    def elastic_net_penalty(X, T_sorted, obs_sorted, weight_sorted, wRMSE):\n        \"\"\"\n        Calculates the elastic net penalty for a given set of inputs. The elastic net is a regularized\n        regression method that linearly combines the L1 and L2 penalties of the lasso and ridge methods.\n\n        Parameters:\n        X (array-like): The input array.\n        T_sorted (array-like): The sorted temperature array.\n        obs_sorted (array-like): The sorted observation array.\n        weight_sorted (array-like): The sorted weight array.\n        wRMSE (float): The weighted root mean squared error.\n\n        Returns:\n        penalty (float): The elastic net penalty.\n        \"\"\"\n\n        # Elastic net\n        X_enet = np.array(X).copy()\n\n        ## Scale break points ##\n        if len(idx_bp) > 0:\n            X_enet[idx_bp] = [\n                np.min(np.abs(X_enet[idx] - T_fit_bnds)) for idx in idx_bp\n            ]\n\n            if len(idx_bp) == 2:\n                X_enet[idx_bp] += (X[idx_bp][1] - X[idx_bp][0]) / 2\n\n            X_enet[idx_bp] *= wRMSE / T_range\n\n        # Find idx for regions\n        if len(idx_bp) == 2:\n            [hdd_bp, cdd_bp] = X[idx_bp]\n\n            idx_hdd = np.argwhere(T_sorted < hdd_bp).flatten()\n            idx_tidd = np.argwhere(\n                (hdd_bp <= T_sorted) & (T_sorted <= cdd_bp)\n            ).flatten()\n            idx_cdd = np.argwhere(cdd_bp < T_sorted).flatten()\n\n        elif len(idx_bp) == 1:\n            bp = X[idx_bp]\n            if X_enet[idx_beta] < 0:  # HDD_TIDD\n                idx_hdd = np.argwhere(T_sorted <= bp).flatten()\n                idx_tidd = np.argwhere(bp < T_sorted).flatten()\n                idx_cdd = np.array([])\n\n            else:\n                idx_hdd = np.array([])  # CDD_TIDD\n                idx_tidd = np.argwhere(T_sorted < bp).flatten()\n                idx_cdd = np.argwhere(bp <= T_sorted).flatten()\n\n        else:\n            idx_hdd = np.array([])\n            idx_tidd = np.arange(0, len(T_sorted))\n            idx_cdd = np.array([])\n\n        len_hdd = len(idx_hdd)\n        len_tidd = len(idx_tidd)\n        len_cdd = len(idx_cdd)\n\n        # combine tidd with hdd/cdd if cdd/hdd are large enough to get stdev\n        if (len_hdd < N_min) and (len_cdd >= N_min):\n            idx_hdd = np.hstack([idx_hdd, idx_tidd])\n        elif (len_hdd >= N_min) and (len_cdd < N_min):\n            idx_cdd = np.hstack([idx_tidd, idx_cdd])\n\n        # change to idx_hdd and idx_cdd to int arrays\n        idx_hdd = idx_hdd.astype(int)\n        idx_cdd = idx_cdd.astype(int)\n\n        ## Normalize slopes ##\n        # calculate stdevs\n        if (len(idx_bp) == 2) and (len(idx_hdd) >= N_min) and (len(idx_cdd) >= N_min):\n            N_beta = np.array([len_hdd, len_cdd])\n            T_stdev = np.array(\n                [\n                    stdev(T_sorted[idx_hdd], weights=weight_sorted[idx_hdd]),\n                    stdev(T_sorted[idx_cdd], weights=weight_sorted[idx_cdd]),\n                ]\n            )\n            obs_stdev = np.array(\n                [\n                    stdev(obs_sorted[idx_hdd], weights=weight_sorted[idx_hdd]),\n                    stdev(obs_sorted[idx_cdd], weights=weight_sorted[idx_cdd]),\n                ]\n            )\n\n        elif (len(idx_bp) == 1) and (len(idx_hdd) >= N_min):\n            N_beta = np.array([len_hdd])\n            T_stdev = stdev(T_sorted[idx_hdd], weights=weight_sorted[idx_hdd])\n            obs_stdev = stdev(obs_sorted[idx_hdd], weights=weight_sorted[idx_hdd])\n\n        elif (len(idx_bp) == 1) and (len(idx_cdd) >= N_min):\n            N_beta = np.array([len_cdd])\n            T_stdev = stdev(T_sorted[idx_cdd], weights=weight_sorted[idx_cdd])\n            obs_stdev = stdev(obs_sorted[idx_cdd], weights=weight_sorted[idx_cdd])\n\n        else:\n            N_beta = np.array([len_tidd])\n            T_stdev = stdev(T_sorted, weights=weight_sorted)\n            obs_stdev = stdev(obs_sorted, weights=weight_sorted)\n\n        X_enet[idx_beta] *= T_stdev / obs_stdev\n\n        # add penalty to slope for not having enough datapoints\n        X_enet[idx_beta] = np.where(\n            N_beta < N_min, X_enet[idx_beta] * 1e30, X_enet[idx_beta]\n        )\n\n        ## Scale smoothing parameter ##\n        if len(idx_k) > 0:  # reducing X_enet size allows for more smoothing\n            X_enet[idx_k] = X[idx_k]\n\n            if (len(idx_k) == 2) and (np.sum(X_enet[idx_k]) > 1):\n                X_enet[idx_k] /= np.sum(X_enet[idx_k])\n\n            X_enet[idx_k] *= (\n                X_enet[idx_beta] / 2\n            )  # uncertain what to divide by, this seems to work well\n\n        X_enet = X_enet[idx_reg]\n\n        if ridge_a == 0:\n            penalty = lasso_a * np.linalg.norm(X_enet, 1)\n        else:\n            penalty = lasso_a * np.linalg.norm(X_enet, 1) + ridge_a * np.linalg.norm(\n                X_enet, 2\n            )\n\n        return penalty\n\n    def obj_fcn(X, grad=[], optimize_flag=True):\n        \"\"\"\n        Creates an objective function having the required inputs for optimization via SciPy and NLopt. If the optimize_flag is true,\n        only return the loss. If the optimize_flag is false, return the loss and the model output parameters.\n\n        Parameters:\n        - X: array-like\n            Array of coefficients.\n        - grad: array-like, optional\n            Gradient array. Default is an empty list.\n        - optimize_flag: bool, optional\n            Whether to optimize. Default is True.\n\n        Returns:\n        - obj: float\n            Objective function value.\n        \"\"\"\n        X = np.array(X)\n\n        model = model_fcn(X)\n        idx_sorted = np.argsort(T).flatten()\n        idx_initial = np.argsort(idx_sorted).flatten()\n        resid = model - obs\n\n        T_sorted = T[idx_sorted]\n        # model_sorted = model[idx_sorted]\n        obs_sorted = obs[idx_sorted]\n        resid_sorted = resid[idx_sorted]\n        weight_sorted, c, a = weight_fcn(\n            *X, T_sorted, resid_sorted, sigma, quantile, alpha, min_weight\n        )\n\n        weight = weight_sorted[idx_initial]\n        wSSE = np.sum(weight * resid**2)\n        loss = wSSE / N\n\n        if settings.regularization_alpha != 0:\n            loss += elastic_net_penalty(\n                X, T_sorted, obs_sorted, weight_sorted, np.sqrt(loss)\n            )\n\n        if optimize_flag:\n            return loss\n\n        else:\n            if (\"r_squared\" in settings.split_selection_criteria) and callable(TSS_fcn):\n                TSS = TSS_fcn(*X, T_sorted, obs_sorted)\n            else:\n                TSS = wSSE\n\n            if initial_fit:\n                jac = None\n            else:\n                eps = 10 ** (OoM(X, method=\"floor\") - 2)\n                X_lower = X - eps\n                X_upper = X + eps\n\n                # select correct finite difference scheme based on variable type and value\n                # NOTE: finite differencing was not returning great results. Looking into switching to JAX autodiff\n                fd_type = [\"central\"] * len(X)\n                for i in range(len(X)):\n                    if i in idx_k:\n                        if X_lower[i] < 0:\n                            fd_type[i] = \"forward\"\n                        elif X_upper[i] > 1:\n                            fd_type[i] = \"backward\"\n                    elif i in idx_beta:\n                        if (X[i] > 0) and (X_lower[i] < 0):\n                            fd_type[i] = \"forward\"\n                        elif (X[i] < 0) and (X_upper[i] > 0):\n                            fd_type[i] = \"backward\"\n                    elif i in idx_bp:\n                        if X_lower[i] < T_sorted[0]:\n                            fd_type[i] = \"forward\"\n                        elif X_lower[i] > T_sorted[-1]:\n                            fd_type[i] = \"backward\"\n\n                # https://stackoverflow.com/questions/70572362/compute-efficiently-hessian-matrices-in-jax\n                # hess = jit(jacfwd(jacrev(no_weights_obj_fcn), has_aux=True), has_aux=True)(X, [model_fcn, obs, idx_bp])\n                # print(hess)\n                # obj_grad_fcn = lambda X: no_weights_obj_fcn(X, [model_fcn, obs, idx_bp])\n\n                # jac = numerical_jacobian(obj_grad_fcn, X, dx=eps, fd_type=fd_type)\n                jac = None\n\n            return X, loss, TSS, T, model, weight, resid, jac, np.mean(a), c\n\n    return obj_fcn", "completions": ["  \n```\ndef obj_fcn_decorator(\n    model_fcn_full,\n    weight_fcn,\n    TSS_fcn,\n    T,\n    obs,\n    settings,\n    alpha=2.0,\n    coef_id=[],\n    initial_fit=True,\n) :\n    \"\"\"\n    A decorator function that calculates the elastic net penalty for a given set of inputs and the objective function\n    for input to optimization algorithms.\n\n    Parameters:\n    model_fcn_full (function): The full model function.\n    weight_fcn (function): The weight function.\n    TSS_fcn (function): The TSS (Total Sum of Squares) function.\n    T (array-like): The temperature array.\n    obs (array-like): The observation array.\n    settings (object): The DailySettings object.\n    alpha (float): The alpha value for the elastic net penalty. Default is 2.0.\n    coef_id (list): The list of coefficient IDs. Default is an empty list.\n    initial_fit (bool): Whether or not this is the initial fit. Default is True.\n\n    Returns:\n    obj_fcn (function): an objective function having the required inputs for optimization via SciPy and NLopt.\n    \"\"\"\n    def obj_fcn(coef):\n        # Calculate the elastic net penalty\n        penalty = 0\n        for i in range(len(coef)):\n            if coef_id[i] == 0:\n                penalty += (alpha / 2) * coef[i] ** 2\n            else:\n                penalty += alpha * abs(coef[i])\n\n        # Calculate the objective function\n        obj = 0\n        for i in range(len(T)):\n            model = model_fcn_full(T[i], coef, settings)\n            obj += weight_fcn(T[i], obs[i], model) ** 2\n\n        return obj + penalty\n\n    return obj_fcn\n```"]}
{"id": 15, "name": "serialize", "prompt": "### Role:\nYou are an expert Python programmer. You will be given some information and will generate a correct Python function that matches this information.\n\n### Task:\nBased on the provided information, generate a complete Python function. Ensure the code is logically correct, syntactically accurate, and follows best coding practices.\n\n### Function name:\nserialize\n\n### Doc string and function signature:\n```Python\ndef serialize(data: Any, encoding: str | None = None) -> str | bytes :\n    \"\"\"Serialize the given data structure into a yaml dump.\n\n    The function supports standard data containers such as maps and lists as well as AiiDA nodes which will be\n    serialized into strings, before the whole data structure is dumped into a string using yaml.\n\n    :param data: the general data to serialize\n    :param encoding: optional encoding for the serialized string\n    :return: string representation of the serialized data structure or byte array if specific encoding is specified\n    \"\"\"\n```\n\n### Requirements:\n1. Generate the complete function implementation.\n2. Only answer the code of the generated function, formatted using Markdown code block syntax.\n3. Do not include any additional text.\n\n### Answer: (use the provided format with backticks)\n", "reference": "def serialize(data: Any, encoding: str | None = None) -> str | bytes:\n    serialized: bytes | str\n\n    if encoding is not None:\n        serialized = yaml.dump(data, encoding=encoding, Dumper=AiiDADumper)\n    else:\n        serialized = yaml.dump(data, Dumper=AiiDADumper)\n\n    return serialized", "completions": ["  \n```Python\ndef serialize(data: Any, encoding: str | None = None) -> str | bytes :\n    \"\"\"Serialize the given data structure into a yaml dump.\n\n    The function supports standard data containers such as maps and lists as well as AiiDA nodes which will be\n    serialized into strings, before the whole data structure is dumped into a string using yaml.\n\n    :param data: the general data to serialize\n    :param encoding: optional encoding for the serialized string\n    :return: string representation of the serialized data structure or byte array if specific encoding is specified\n    \"\"\"\n    import yaml\n    from aiida.orm import Node\n\n    def _serialize_aiida_node(node: Node) -> str:\n        return str(node)\n\n    def _serialize_list(lst: list) -> list:\n        return [_serialize(x) for x in lst]\n\n    def _serialize_dict(dct: dict) -> dict:\n        return {key: _serialize(value) for key, value in dct.items()}\n\n    def _serialize(data: Any) -> Any:\n        if isinstance(data, Node):\n            return _serialize_aiida_node(data)\n        elif isinstance(data, list):\n            return _serialize_list(data)\n        elif isinstance(data, dict):\n            return _serialize_dict(data)\n        else:\n            return data\n\n    serialized_data = _serialize(data)\n    if encoding is not None:\n        return serialized_data.encode(encoding)\n    else:\n        return serialized_data\n```"]}
{"id": 16, "name": "get_connection", "prompt": "### Role:\nYou are an expert Python programmer. You will be given some information and will generate a correct Python function that matches this information.\n\n### Task:\nBased on the provided information, generate a complete Python function. Ensure the code is logically correct, syntactically accurate, and follows best coding practices.\n\n### Function name:\nget_connection\n\n### Doc string and function signature:\n```Python\ndef get_connection(self, connection_params: dict) -> RedisCluster :\n    \"\"\"\n    Given connection_params, return a new client instance.\n    Basic django-redis ConnectionFactory manages a cache of connection\n    pools and builds a fresh client each time. because the cluster client\n    manages its own connection pools, we will instead merge the\n    \"connection\" and \"client\" kwargs and throw them all at the client to\n    sort out.\n    If we find conflicting client and connection kwargs, we'll raise an\n    error.\n    \"\"\"\n```\n\n### Requirements:\n1. Generate the complete function implementation.\n2. Only answer the code of the generated function, formatted using Markdown code block syntax.\n3. Do not include any additional text.\n\n### Answer: (use the provided format with backticks)\n", "reference": "    def get_connection(self, connection_params: dict) -> RedisCluster:\n        try:\n            client_cls_kwargs = deepcopy(self.redis_client_cls_kwargs)\n            # ... and smash 'em together (crashing if there's conflicts)...\n            for key, value in connection_params.items():\n                if key in client_cls_kwargs:\n                    raise ImproperlyConfigured(\n                        f\"Found '{key}' in both the connection and the client kwargs\"\n                    )\n                client_cls_kwargs[key] = value\n\n            # Add explicit socket timeout\n            client_cls_kwargs[\"socket_timeout\"] = SOCKET_TIMEOUT\n            client_cls_kwargs[\"socket_keepalive\"] = True\n            # ... and then build and return the client\n            return RedisCluster(**client_cls_kwargs)\n        except Exception as e:\n            # Let django redis handle the exception\n            raise ConnectionInterrupted(connection=None) from e", "completions": ["  \n```\ndef get_connection(self, connection_params: dict) -> RedisCluster:\n    \"\"\"\n    Given connection_params, return a new client instance.\n    Basic django-redis ConnectionFactory manages a cache of connection\n    pools and builds a fresh client each time. because the cluster client\n    manages its own connection pools, we will instead merge the\n    \"connection\" and \"client\" kwargs and throw them all at the client to\n    sort out.\n    If we find conflicting client and connection kwargs, we'll raise an\n    error.\n    \"\"\"\n    # Merge connection and client kwargs\n    kwargs = {**connection_params.get(\"connection\", {}), **connection_params.get(\"client\", {})}\n\n    # Check for conflicting kwargs\n    if \"connection_pool\" in kwargs and \"connection_pools\" in kwargs:\n        raise ValueError(\"Conflicting connection kwargs: connection_pool and connection_pools\")\n\n    # Create a new client instance\n    client = RedisCluster(**kwargs)\n\n    # Return the client instance\n    return client\n```"]}
{"id": 17, "name": "make_info", "prompt": "### Role:\nYou are an expert Python programmer. You will be given some information and will generate a correct Python function that matches this information.\n\n### Task:\nBased on the provided information, generate a complete Python function. Ensure the code is logically correct, syntactically accurate, and follows best coding practices.\n\n### Function name:\nmake_info\n\n### Doc string and function signature:\n```Python\ndef make_info(signature: type[Signature]) -> BaseModel :\n    \"\"\"Creates a SignatureInfo pydantic type, that describes the Signature.\n\n    Returns an instnce of this type, with the instructions and field descriptions of the input type.\n    \"\"\"\n```\n\n### Requirements:\n1. Generate the complete function implementation.\n2. Only answer the code of the generated function, formatted using Markdown code block syntax.\n3. Do not include any additional text.\n\n### Answer: (use the provided format with backticks)\n", "reference": "def make_info(signature: type[Signature]) -> BaseModel:\n    # First, create the SignatureInfo type\n    fields = {\n        \"instructions\": (str, pydantic.Field(description=\"The instructions for the task\")),\n    }\n    for name in signature.fields:\n        fields[name + \"_prefix\"] = (str, pydantic.Field(description=f\"The prefix for {name}\"))\n        fields[name + \"_desc\"] = (str, pydantic.Field(description=f\"The description for {name}\"))\n    SignatureInfo = pydantic.create_model(  # noqa: N806\n        f\"SignatureInfo[{signature.__name__}]\",\n        **fields,\n    )\n\n    # Add a method to convert the SignatureInfo back into a Signature\n    def to_signature(info):\n        new_signature = signature.with_instructions(info.instructions)\n        for name in signature.fields:\n            new_signature = new_signature.with_updated_fields(\n                name,\n                prefix=getattr(info, name + \"_prefix\"),\n                desc=getattr(info, name + \"_desc\"),\n            )\n        return new_signature\n\n    SignatureInfo.to_signature = to_signature\n\n    # Finally, make an instance of the SignatureInfo type with the signature's\n    # default instructions and field descriptions\n    values = {\"instructions\": signature.instructions}\n    for name, field in signature.fields.items():\n        values[name + \"_prefix\"] = field.json_schema_extra[\"prefix\"]\n        values[name + \"_desc\"] = field.json_schema_extra[\"desc\"]\n    return SignatureInfo(**values)", "completions": ["  \n```Python\ndef make_info(signature: type[Signature]) -> BaseModel :\n    \"\"\"Creates a SignatureInfo pydantic type, that describes the Signature.\n\n    Returns an instnce of this type, with the instructions and field descriptions of the input type.\n    \"\"\"\n    # Create a new instance of the SignatureInfo type\n    info = SignatureInfo()\n\n    # Set the instructions field to the instructions of the input signature\n    info.instructions = signature.instructions\n\n    # Set the field_descriptions field to the field descriptions of the input signature\n    info.field_descriptions = signature.field_descriptions\n\n    # Return the new instance\n    return info\n```"]}
{"id": 18, "name": "download", "prompt": "### Role:\nYou are an expert Python programmer. You will be given some information and will generate a correct Python function that matches this information.\n\n### Task:\nBased on the provided information, generate a complete Python function. Ensure the code is logically correct, syntactically accurate, and follows best coding practices.\n\n### Function name:\ndownload\n\n### Doc string and function signature:\n```Python\ndef download(cls, client: GiskardClient, project_key, model_id, *_args, **_kwargs) :\n    \"\"\"\n    Downloads the specified model from the Giskard hub and loads it into memory.\n\n    Args:\n        client (GiskardClient): The client instance that will connect to the Giskard hub.\n        project_key (str): The key for the project that the model belongs to.\n        model_id (str): The ID of the model to download.\n\n    Returns:\n        An instance of the class calling the method, with the specified model loaded into memory.\n\n    Raises:\n        AssertionError: If the local directory where the model should be saved does not exist.\n    \"\"\"\n```\n\n### Requirements:\n1. Generate the complete function implementation.\n2. Only answer the code of the generated function, formatted using Markdown code block syntax.\n3. Do not include any additional text.\n\n### Answer: (use the provided format with backticks)\n", "reference": "    def download(cls, client: GiskardClient, project_key, model_id, *_args, **_kwargs):\n        local_dir = settings.home_dir / settings.cache_dir / \"models\" / model_id\n        client.load_artifact(local_dir, posixpath.join(\"models\", model_id))\n        meta_response: ModelMetaInfo = client.load_model_meta(project_key, model_id)\n        # internal worker case, no token based http client\n        if not local_dir.exists():\n            raise RuntimeError(f\"Cannot find existing model {project_key}.{model_id} in {local_dir}\")\n        with (Path(local_dir) / META_FILENAME).open(encoding=\"utf-8\") as f:\n            file_meta = yaml.load(f, Loader=yaml.Loader)\n            classification_labels = cls.cast_labels(meta_response)\n            meta = ModelMeta(\n                name=meta_response.name,\n                description=meta_response.description,\n                model_type=SupportedModelTypes[meta_response.modelType],\n                feature_names=meta_response.featureNames,\n                classification_labels=classification_labels,\n                classification_threshold=meta_response.threshold,\n                loader_module=file_meta[\"loader_module\"],\n                loader_class=file_meta[\"loader_class\"],\n            )\n\n        model_py_ver = (\n            tuple(meta_response.languageVersion.split(\".\")) if \"PYTHON\" == meta_response.language.upper() else None\n        )\n\n        clazz = cls.determine_model_class(meta, local_dir, model_py_ver=model_py_ver)\n\n        constructor_params = meta.__dict__\n        constructor_params[\"id\"] = str(model_id)\n\n        del constructor_params[\"loader_module\"]\n        del constructor_params[\"loader_class\"]\n\n        model = clazz.load(local_dir, model_py_ver=model_py_ver, **constructor_params)\n        return model", "completions": ["  \n```\ndef download(cls, client: GiskardClient, project_key, model_id, *_args, **_kwargs) :\n    \"\"\"\n    Downloads the specified model from the Giskard hub and loads it into memory.\n\n    Args:\n        client (GiskardClient): The client instance that will connect to the Giskard hub.\n        project_key (str): The key for the project that the model belongs to.\n        model_id (str): The ID of the model to download.\n\n    Returns:\n        An instance of the class calling the method, with the specified model loaded into memory.\n\n    Raises:\n        AssertionError: If the local directory where the model should be saved does not exist.\n    \"\"\"\n    # Check if the local directory exists\n    if not os.path.exists(cls.local_dir):\n        raise AssertionError(f\"Local directory {cls.local_dir} does not exist\")\n\n    # Download the model from the Giskard hub\n    model_path = os.path.join(cls.local_dir, f\"{model_id}.pkl\")\n    client.download_model(project_key, model_id, model_path)\n\n    # Load the model into memory\n    with open(model_path, \"rb\") as f:\n        model = pickle.load(f)\n\n    # Return the instance of the class with the model loaded\n    return cls(model)\n```"]}
{"id": 19, "name": "update", "prompt": "### Role:\nYou are an expert Python programmer. You will be given some information and will generate a correct Python function that matches this information.\n\n### Task:\nBased on the provided information, generate a complete Python function. Ensure the code is logically correct, syntactically accurate, and follows best coding practices.\n\n### Function name:\nupdate\n\n### Doc string and function signature:\n```Python\ndef update(self, data) :\n    \"\"\"\n    Map new values to integer identifiers.\n\n    Parameters\n    ----------\n    data : iterable of str or bytes\n\n    Raises\n    ------\n    TypeError\n        If elements in *data* are neither str nor bytes.\n    \"\"\"\n```\n\n### Requirements:\n1. Generate the complete function implementation.\n2. Only answer the code of the generated function, formatted using Markdown code block syntax.\n3. Do not include any additional text.\n\n### Answer: (use the provided format with backticks)\n", "reference": "    def update(self, data):\n        data = np.atleast_1d(np.array(data, dtype=object))\n        # check if convertible to number:\n        convertible = True\n        for val in OrderedDict.fromkeys(data):\n            # OrderedDict just iterates over unique values in data.\n            _api.check_isinstance((str, bytes), value=val)\n            if convertible:\n                # this will only be called so long as convertible is True.\n                convertible = self._str_is_convertible(val)\n            if val not in self._mapping:\n                self._mapping[val] = next(self._counter)\n        if data.size and convertible:\n            _log.info('Using categorical units to plot a list of strings '\n                      'that are all parsable as floats or dates. If these '\n                      'strings should be plotted as numbers, cast to the '\n                      'appropriate data type before plotting.')", "completions": ["  \n```Python\ndef update(self, data) :\n    \"\"\"\n    Map new values to integer identifiers.\n\n    Parameters\n    ----------\n    data : iterable of str or bytes\n\n    Raises\n    ------\n    TypeError\n        If elements in *data* are neither str nor bytes.\n    \"\"\"\n    if not isinstance(data, (str, bytes)):\n        raise TypeError(\"Elements in data must be either str or bytes.\")\n\n    for value in data:\n        if not isinstance(value, (str, bytes)):\n            raise TypeError(\"Elements in data must be either str or bytes.\")\n        if value not in self.values:\n            self.values.append(value)\n            self.next_id += 1\n            self.id_map[value] = self.next_id\n\n    return self.id_map\n```"]}
{"id": 20, "name": "read_sensor_table_data", "prompt": "### Role:\nYou are an expert Python programmer. You will be given some information and will generate a correct Python function that matches this information.\n\n### Task:\nBased on the provided information, generate a complete Python function. Ensure the code is logically correct, syntactically accurate, and follows best coding practices.\n\n### Function name:\nread_sensor_table_data\n\n### Doc string and function signature:\n```Python\ndef read_sensor_table_data(\n        cls,\n        control_db_table_name: str,\n        sensor_id: str = None,\n        assets: list = None,\n    ) -> Optional[Row] :\n    \"\"\"Read data from delta table containing sensor status info.\n\n    Args:\n        sensor_id: sensor id. If this parameter is defined search occurs\n            only considering this parameter. Otherwise, it considers sensor\n            assets and checkpoint location.\n        control_db_table_name: db.table to control sensor runs.\n        assets: list of assets that are fueled by the pipeline\n            where this sensor is.\n\n    Return:\n        Row containing the data for the provided sensor_id.\n    \"\"\"\n```\n\n### Requirements:\n1. Generate the complete function implementation.\n2. Only answer the code of the generated function, formatted using Markdown code block syntax.\n3. Do not include any additional text.\n\n### Answer: (use the provided format with backticks)\n", "reference": "    def read_sensor_table_data(\n        cls,\n        control_db_table_name: str,\n        sensor_id: str = None,\n        assets: list = None,\n    ) -> Optional[Row]:\n        df = DeltaTable.forName(\n            ExecEnv.SESSION,\n            control_db_table_name,\n        ).toDF()\n\n        if sensor_id:\n            df = df.where(df.sensor_id == sensor_id)\n        elif assets:\n            df = df.where(df.assets == array(*[lit(asset) for asset in assets]))\n        else:\n            raise ValueError(\n                \"Either sensor_id or assets need to be provided as arguments.\"\n            )\n\n        return df.first()", "completions": ["  \n```\ndef read_sensor_table_data(\n        cls,\n        control_db_table_name: str,\n        sensor_id: str = None,\n        assets: list = None,\n    ) -> Optional[Row] :\n    \"\"\"Read data from delta table containing sensor status info.\n\n    Args:\n        sensor_id: sensor id. If this parameter is defined search occurs\n            only considering this parameter. Otherwise, it considers sensor\n            assets and checkpoint location.\n        control_db_table_name: db.table to control sensor runs.\n        assets: list of assets that are fueled by the pipeline\n            where this sensor is.\n\n    Return:\n        Row containing the data for the provided sensor_id.\n    \"\"\"\n    # Connect to the database\n    conn = create_engine(f\"postgresql://user:password@host:port/{control_db_table_name}\")\n\n    # Define the query\n    query = f\"SELECT * FROM {control_db_table_name} WHERE sensor_id = :sensor_id\"\n    if assets is not None:\n        query += \" AND asset IN :assets\"\n\n    # Execute the query\n    with conn.connect() as connection:\n        result = connection.execute(query, sensor_id=sensor_id, assets=assets)\n\n    # Return the result\n    return result.fetchone()\n```"]}
{"id": 21, "name": "init_optim_state", "prompt": "### Role:\nYou are an expert Python programmer. You will be given some information and will generate a correct Python function that matches this information.\n\n### Task:\nBased on the provided information, generate a complete Python function. Ensure the code is logically correct, syntactically accurate, and follows best coding practices.\n\n### Function name:\ninit_optim_state\n\n### Doc string and function signature:\n```Python\ndef init_optim_state(optimizer: torch.optim.Optimizer) -> None :\n    \"\"\"\n    Initialize optimizer states by calling step() with zero grads. This is necessary because some optimizers like AdamW\n    initialize some states in their state_dicts lazily, only after calling step() for the first time. Certain checkpointing\n    solutions may rely on in-place loading, re-using existing tensor allocated memory from the optimizer state dict. This\n    optimization does not work with optimizers that lazily initialize their states, as certain states will not be restored.\n    Calling this function ensures that these states are available in the state dict for in place loading.\n\n    Args:\n        optimizer: A PyTorch optimizer.\n    \"\"\"\n```\n\n### Requirements:\n1. Generate the complete function implementation.\n2. Only answer the code of the generated function, formatted using Markdown code block syntax.\n3. Do not include any additional text.\n\n### Answer: (use the provided format with backticks)\n", "reference": "def init_optim_state(optimizer: torch.optim.Optimizer) -> None:\n    if optimizer.state:\n        # The optimizer state is initialized.\n        return\n\n    for param_group in optimizer.param_groups:\n        for param in param_group[\"params\"]:\n            if param.grad is not None:\n                raise RuntimeError(\n                    \"Initializing the optimizer states requires that no existing gradients for parameters are found.\"\n                )\n            if param.requires_grad:\n                param.grad = torch.zeros_like(param)\n    optimizer.step(closure=None)\n    optimizer.zero_grad(set_to_none=True)", "completions": ["  \n```Python\ndef init_optim_state(optimizer: torch.optim.Optimizer) -> None:\n    \"\"\"\n    Initialize optimizer states by calling step() with zero grads. This is necessary because some optimizers like AdamW\n    initialize some states in their state_dicts lazily, only after calling step() for the first time. Certain checkpointing\n    solutions may rely on in-place loading, re-using existing tensor allocated memory from the optimizer state dict. This\n    optimization does not work with optimizers that lazily initialize their states, as certain states will not be restored.\n    Calling this function ensures that these states are available in the state dict for in place loading.\n\n    Args:\n        optimizer: A PyTorch optimizer.\n    \"\"\"\n    # Create a zero gradient tensor with the same shape as the optimizer's parameters\n    zero_grads = torch.zeros_like(optimizer.param_groups[0]['params'])\n\n    # Call the optimizer's step() function with the zero gradients\n    optimizer.step(zero_grads)\n```"]}
{"id": 22, "name": "_replace_snowflake_yml_name_with_project", "prompt": "### Role:\nYou are an expert Python programmer. You will be given some information and will generate a correct Python function that matches this information.\n\n### Task:\nBased on the provided information, generate a complete Python function. Ensure the code is logically correct, syntactically accurate, and follows best coding practices.\n\n### Function name:\n_replace_snowflake_yml_name_with_project\n\n### Doc string and function signature:\n```Python\ndef _replace_snowflake_yml_name_with_project(\n    target_directory: Path, project_identifier: str\n) :\n    \"\"\"\n    Replace the native_app schema's \"name\" field in a snowflake.yml file with its parent directory name, i.e. the Snowflake Native App project, as the default start.\n    This does not change the name in any other snowflake.*.yml as snowflake.yml is the base file and all others are overrides for the user to customize.\n\n    Args:\n        target_directory (str): The directory containing snowflake.yml at its root.\n        project_identifier (str): The name of the project to be created, as a Snowflake identifier.\n\n    Returns:\n        None\n    \"\"\"\n```\n\n### Requirements:\n1. Generate the complete function implementation.\n2. Only answer the code of the generated function, formatted using Markdown code block syntax.\n3. Do not include any additional text.\n\n### Answer: (use the provided format with backticks)\n", "reference": "def _replace_snowflake_yml_name_with_project(\n    target_directory: Path, project_identifier: str\n):\n\n    path_to_snowflake_yml = SecurePath(target_directory) / \"snowflake.yml\"\n    contents = None\n\n    with path_to_snowflake_yml.open(\n        \"r\", read_file_limit_mb=DEFAULT_SIZE_LIMIT_MB\n    ) as file:\n        contents = safe_load(file)\n\n    if (\n        (contents is not None)\n        and (\"native_app\" in contents)\n        and (\"name\" in contents[\"native_app\"])\n        and (contents[\"native_app\"][\"name\"] != project_identifier)\n    ):\n        contents[\"native_app\"][\"name\"] = project_identifier\n        with path_to_snowflake_yml.open(\"w\") as file:\n            safe_dump(contents, file, sort_keys=False)", "completions": ["  \n```\ndef _replace_snowflake_yml_name_with_project(target_directory: Path, project_identifier: str) -> None:\n    \"\"\"\n    Replace the native_app schema's \"name\" field in a snowflake.yml file with its parent directory name, i.e. the Snowflake Native App project, as the default start.\n    This does not change the name in any other snowflake.*.yml as snowflake.yml is the base file and all others are overrides for the user to customize.\n\n    Args:\n        target_directory (str): The directory containing snowflake.yml at its root.\n        project_identifier (str): The name of the project to be created, as a Snowflake identifier.\n\n    Returns:\n        None\n    \"\"\"\n    # Get the path to the snowflake.yml file\n    snowflake_yml_path = target_directory / \"snowflake.yml\"\n\n    # Read the snowflake.yml file\n    with open(snowflake_yml_path, \"r\") as f:\n        snowflake_yml_contents = f.read()\n\n    # Replace the \"name\" field in the snowflake.yml file with the project identifier\n    snowflake_yml_contents = snowflake_yml_contents.replace(\"name: native_app\", f\"name: {project_identifier}\")\n\n    # Write the modified snowflake.yml file\n    with open(snowflake_yml_path, \"w\") as f:\n        f.write(snowflake_yml_contents)\n```"]}
{"id": 23, "name": "incorrectRecoursesIfThen", "prompt": "### Role:\nYou are an expert Python programmer. You will be given some information and will generate a correct Python function that matches this information.\n\n### Task:\nBased on the provided information, generate a complete Python function. Ensure the code is logically correct, syntactically accurate, and follows best coding practices.\n\n### Function name:\nincorrectRecoursesIfThen\n\n### Doc string and function signature:\n```Python\ndef incorrectRecoursesIfThen(\n    ifclause: Predicate, thenclause: Predicate, X_aff: DataFrame, model\n) -> int :\n    \"\"\"Compute the number of incorrect recourses given an if-then clause.\n\n    Args:\n        ifclause: The if-clause predicate.\n        thenclause: The then-clause predicate.\n        X_aff: The affected DataFrame.\n        model: The ML model under study. Expected to have a \"predict\" method.\n\n    Returns:\n        The number of incorrect recourses.\n\n    Raises:\n        ValueError: If there are no covered instances for the given if-clause.\n    \"\"\"\n```\n\n### Requirements:\n1. Generate the complete function implementation.\n2. Only answer the code of the generated function, formatted using Markdown code block syntax.\n3. Do not include any additional text.\n\n### Answer: (use the provided format with backticks)\n", "reference": "def incorrectRecoursesIfThen(\n    ifclause: Predicate, thenclause: Predicate, X_aff: DataFrame, model\n) -> int:\n    X_aff_covered_bool = (X_aff[ifclause.features] == ifclause.values).all(axis=1)\n    X_aff_covered = X_aff[X_aff_covered_bool].copy()\n    if X_aff_covered.shape[0] == 0:\n        raise ValueError(\n            \"Assuming non-negative frequent itemset threshold, total absence of covered instances should be impossible!\"\n        )\n\n    X_aff_covered[thenclause.features] = thenclause.values\n\n    preds = model.predict(X_aff_covered)\n    return np.shape(preds)[0] - np.sum(preds)", "completions": ["  \n```\ndef incorrectRecoursesIfThen(ifclause: Predicate, thenclause: Predicate, X_aff: DataFrame, model) -> int:\n    \"\"\"Compute the number of incorrect recourses given an if-then clause.\n\n    Args:\n        ifclause: The if-clause predicate.\n        thenclause: The then-clause predicate.\n        X_aff: The affected DataFrame.\n        model: The ML model under study. Expected to have a \"predict\" method.\n\n    Returns:\n        The number of incorrect recourses.\n\n    Raises:\n        ValueError: If there are no covered instances for the given if-clause.\n    \"\"\"\n    # Get the instances that satisfy the if-clause\n    if_instances = X_aff[ifclause(X_aff)]\n    if len(if_instances) == 0:\n        raise ValueError(\"No covered instances for the given if-clause.\")\n\n    # Get the instances that satisfy the then-clause\n    then_instances = X_aff[thenclause(X_aff)]\n\n    # Get the instances that satisfy the if-then clause\n    if_then_instances = if_instances[thenclause(if_instances)]\n\n    # Get the instances that do not satisfy the then-clause\n    not_then_instances = if_instances[~thenclause(if_instances)]\n\n    # Get the predictions for the if-then instances\n    if_then_predictions = model.predict(if_then_instances)\n\n    # Get the predictions for the not-then instances\n    not_then_predictions = model.predict(not_then_instances)\n\n    # Compute the number of incorrect recourses\n    incorrect_recourses = 0\n    for i in range(len(if_then_instances)):\n        if if_then_predictions[i]!= not_then_predictions[i]:\n            incorrect_recourses += 1\n\n    return incorrect_recourses\n```"]}
{"id": 24, "name": "split_continuous_linear_decay_stream", "prompt": "### Role:\nYou are an expert Python programmer. You will be given some information and will generate a correct Python function that matches this information.\n\n### Task:\nBased on the provided information, generate a complete Python function. Ensure the code is logically correct, syntactically accurate, and follows best coding practices.\n\n### Function name:\nsplit_continuous_linear_decay_stream\n\n### Doc string and function signature:\n```Python\ndef split_continuous_linear_decay_stream(\n    original_stream: Iterable[DatasetExperience[TCLDataset]],\n    experience_size: int,\n    iters_per_virtual_epoch: int,\n    beta: float,\n    shuffle: bool,\n) -> CLStream[DatasetExperience[TCLDataset]] :\n    \"\"\"Creates a stream of sub-experiences from a list of overlapped\n        experiences with a linear decay in the overlapping areas.\n\n    :param original_stream: The original stream.\n    :param experience_size: The size of each sub-experience.\n    :param iters_per_virtual_epoch: The number of iterations per virtual epoch.\n        This parameter determines the number of (sub-)experiences that we want\n        to create from each experience in the original stream, after \"merging\"\n        all experiences with a certain level of \"overlap\".\n    :param beta: The beta parameter for the linear decay function which\n        indicates the amount of overlap.\n    :param shuffle: Whether to shuffle the sub-experiences.\n\n    Terminology is taken from the official implementation of the paper:\n    \"Task Agnostic Continual Learning Using Online Variational Bayes\" by\n    Zero et al. (https://arxiv.org/abs/2006.05990)\n    Code repo: https://github.com/chenzeno/FOO-VB/tree/main\n\n    :return: A stream of sub-experiences.\n\n    \"\"\"\n```\n\n### Requirements:\n1. Generate the complete function implementation.\n2. Only answer the code of the generated function, formatted using Markdown code block syntax.\n3. Do not include any additional text.\n\n### Answer: (use the provided format with backticks)\n", "reference": "def split_continuous_linear_decay_stream(\n    original_stream: Iterable[DatasetExperience[TCLDataset]],\n    experience_size: int,\n    iters_per_virtual_epoch: int,\n    beta: float,\n    shuffle: bool,\n) -> CLStream[DatasetExperience[TCLDataset]]:\n\n    def _get_linear_line(start, end, direction=\"up\"):\n        if direction == \"up\":\n            return torch.FloatTensor(\n                [(i - start) / (end - start) for i in range(start, end)]\n            )\n        return torch.FloatTensor(\n            [1 - ((i - start) / (end - start)) for i in range(start, end)]\n        )\n\n    def _create_task_probs(iters, tasks, task_id, beta=3):\n        if beta <= 1:\n            peak_start = int((task_id / tasks) * iters)\n            peak_end = int(((task_id + 1) / tasks) * iters)\n            start = peak_start\n            end = peak_end\n        else:\n            start = max(int(((beta * task_id - 1) * iters) / (beta * tasks)), 0)\n            peak_start = int(((beta * task_id + 1) * iters) / (beta * tasks))\n            peak_end = int(((beta * task_id + (beta - 1)) * iters) / (beta * tasks))\n            end = min(\n                int(((beta * task_id + (beta + 1)) * iters) / (beta * tasks)), iters\n            )\n\n        probs = torch.zeros(iters, dtype=torch.float)\n        if task_id == 0:\n            probs[start:peak_start].add_(1)\n        else:\n            probs[start:peak_start] = _get_linear_line(\n                start, peak_start, direction=\"up\"\n            )\n        probs[peak_start:peak_end].add_(1)\n        if task_id == tasks - 1:\n            probs[peak_end:end].add_(1)\n        else:\n            probs[peak_end:end] = _get_linear_line(peak_end, end, direction=\"down\")\n        return probs\n\n    # Total number of iterations\n    total_iters = len(original_stream) * iters_per_virtual_epoch\n\n    # Probabilities over all iterations (sub-experiences)\n    n_experiences = len(original_stream)\n    tasks_probs_over_iterations = [\n        _create_task_probs(total_iters, n_experiences, exp_id, beta=beta)\n        for exp_id in range(n_experiences)\n    ]\n\n    # Normalize probabilities\n    normalize_probs = torch.zeros_like(tasks_probs_over_iterations[0])\n    for probs in tasks_probs_over_iterations:\n        normalize_probs.add_(probs)\n    for probs in tasks_probs_over_iterations:\n        probs.div_(normalize_probs)\n    tasks_probs_over_iterations = torch.cat(tasks_probs_over_iterations).view(\n        -1, tasks_probs_over_iterations[0].shape[0]\n    )\n    tasks_probs_over_iterations_lst = []\n    for col in range(tasks_probs_over_iterations.shape[1]):\n        tasks_probs_over_iterations_lst.append(tasks_probs_over_iterations[:, col])\n    tasks_probs_over_iterations = tasks_probs_over_iterations_lst\n\n    # Random cylic samplers over the datasets of all experiences in the stream\n    samplers = [\n        iter(CyclicSampler(len(exp.dataset), shuffle=shuffle))\n        for exp in original_stream\n    ]\n\n    # The main iterator for the stream\n    def exps_iter():\n        for sub_exp_id in range(total_iters):\n            is_first_sub_exp = is_last_sub_exp = False\n            if sub_exp_id == 0:\n                is_first_sub_exp = True\n            if sub_exp_id == total_iters - 1:\n                is_last_sub_exp = True\n\n            n_samples = torch.Size([experience_size])\n            exp_per_sample_list = Categorical(\n                probs=tasks_probs_over_iterations[sub_exp_id]\n            ).sample(n_samples)\n\n            yield create_sub_exp_from_multi_exps(\n                original_stream,\n                samplers,\n                exp_per_sample_list,\n                total_iters,\n                is_first_sub_exp,\n                is_last_sub_exp,\n            )\n\n    stream_name: str = getattr(original_stream, \"name\", \"train\")\n    return CLStream(\n        name=stream_name,\n        exps_iter=exps_iter(),\n        set_stream_info=True,\n    )", "completions": ["  \n```\ndef split_continuous_linear_decay_stream(\n    original_stream: Iterable[DatasetExperience[TCLDataset]],\n    experience_size: int,\n    iters_per_virtual_epoch: int,\n    beta: float,\n    shuffle: bool,\n) -> CLStream[DatasetExperience[TCLDataset]] :\n    \"\"\"Creates a stream of sub-experiences from a list of overlapped\n        experiences with a linear decay in the overlapping areas.\n\n    :param original_stream: The original stream.\n    :param experience_size: The size of each sub-experience.\n    :param iters_per_virtual_epoch: The number of iterations per virtual epoch.\n        This parameter determines the number of (sub-)experiences that we want\n        to create from each experience in the original stream, after \"merging\"\n        all experiences with a certain level of \"overlap\".\n    :param beta: The beta parameter for the linear decay function which\n        indicates the amount of overlap.\n    :param shuffle: Whether to shuffle the sub-experiences.\n\n    Terminology is taken from the official implementation of the paper:\n    \"Task Agnostic Continual Learning Using Online Variational Bayes\" by\n    Zero et al. (https://arxiv.org/abs/2006.05990)\n    Code repo: https://github.com/chenzeno/FOO-VB/tree/main\n\n    :return: A stream of sub-experiences.\n\n    \"\"\"\n    # Implementation of the function\n    #...\n    #...\n    #...\n    return sub_experiences\n```"]}
{"id": 25, "name": "one_hot_encode", "prompt": "### Role:\nYou are an expert Python programmer. You will be given some information and will generate a correct Python function that matches this information.\n\n### Task:\nBased on the provided information, generate a complete Python function. Ensure the code is logically correct, syntactically accurate, and follows best coding practices.\n\n### Function name:\none_hot_encode\n\n### Doc string and function signature:\n```Python\ndef one_hot_encode(data: pd.DataFrame, columns=None, drop_first: bool = False, encoder: OneHotEncoder = None) :\n    \"\"\"\n    Replaces pandas' get_dummies with an implementation of sklearn.preprocessing.OneHotEncoder.\n\n    The purpose of replacement is to allow encoding of new data using the same encoder, which ensures that the resulting encodings are consistent.\n\n    If encoder is None, a new instance of sklearn.preprocessing.OneHotEncoder will be created using `fit_transform()`. Otherwise, the existing encoder is used with `fit()`.\n\n    For compatibility with get_dummies, the encoded data will be transformed into a DataFrame.\n\n    In all cases, the return value will be the encoded data and the encoder object (even if passed in). If `data` contains other columns than the\n    dummy-coded one(s), these will be prepended, unaltered, to the result.\n\n    :param data: Data of which to get dummy indicators.\n    :param columns: List-like structure containing specific columns to encode.\n    :param drop_first: Whether to get k-1 dummies out of k categorical levels by removing the first level.\n    :return: DataFrame, OneHotEncoder\n    \"\"\"\n```\n\n### Requirements:\n1. Generate the complete function implementation.\n2. Only answer the code of the generated function, formatted using Markdown code block syntax.\n3. Do not include any additional text.\n\n### Answer: (use the provided format with backticks)\n", "reference": "def one_hot_encode(data: pd.DataFrame, columns=None, drop_first: bool = False, encoder: OneHotEncoder = None):\n\n    # Determine columns being encoded\n    if columns is None:\n        dtypes_to_encode = [\"object\", \"string\", \"category\"]\n        data_to_encode = data.select_dtypes(include=dtypes_to_encode)\n    elif not is_list_like(columns):\n        raise TypeError(\"Input must be a list-like for parameter `columns`\")\n    else:\n        data_to_encode = data[columns]\n\n    # If all columns are already numerical, there may be nothing to encode.\n    # In this case, return original data.\n    if len(data_to_encode.columns) == 0:\n        return data, encoder  # Encoder may be None\n\n    # Columns to keep in the result - not encoded.\n    columns_to_keep = data.columns.difference(data_to_encode.columns)\n    df_columns_to_keep = data[columns_to_keep].reset_index(drop=True)\n\n    if encoder is None:  # Create new encoder\n        drop = None\n        if drop_first:\n            drop = \"first\"\n        encoder = OneHotEncoder(drop=drop, sparse=False)  # NB sparse renamed to sparse_output in sklearn 1.2+\n\n        encoded_data = encoder.fit_transform(data_to_encode)\n\n    else:  # Use existing encoder\n        encoded_data = encoder.transform(data_to_encode)\n\n    # Convert the encoded data to a DataFrame\n    columns_encoded = encoder.get_feature_names_out(data_to_encode.columns)\n\n    df_encoded = pd.DataFrame(encoded_data, columns=columns_encoded).reset_index(drop=True)  # drop index from original\n\n    # Concatenate the encoded DataFrame with the original non-categorical columns\n    df_result = pd.concat([df_columns_to_keep, df_encoded], axis=1)\n\n    return df_result, encoder", "completions": ["  \n```\ndef one_hot_encode(data: pd.DataFrame, columns=None, drop_first: bool = False, encoder: OneHotEncoder = None) :\n    \"\"\"\n    Replaces pandas' get_dummies with an implementation of sklearn.preprocessing.OneHotEncoder.\n\n    The purpose of replacement is to allow encoding of new data using the same encoder, which ensures that the resulting encodings are consistent.\n\n    If encoder is None, a new instance of sklearn.preprocessing.OneHotEncoder will be created using `fit_transform()`. Otherwise, the existing encoder is used with `fit()`.\n\n    For compatibility with get_dummies, the encoded data will be transformed into a DataFrame.\n\n    In all cases, the return value will be the encoded data and the encoder object (even if passed in). If `data` contains other columns than the\n    dummy-coded one(s), these will be prepended, unaltered, to the result.\n\n    :param data: Data of which to get dummy indicators.\n    :param columns: List-like structure containing specific columns to encode.\n    :param drop_first: Whether to get k-1 dummies out of k categorical levels by removing the first level.\n    :return: DataFrame, OneHotEncoder\n    \"\"\"\n    if encoder is None:\n        encoder = OneHotEncoder(drop=drop_first)\n        encoded_data = encoder.fit_transform(data)\n    else:\n        encoded_data = encoder.fit(data)\n\n    encoded_data = pd.DataFrame(encoded_data, columns=encoder.get_feature_names())\n\n    if columns is not None:\n        encoded_data = encoded_data[columns]\n\n    return encoded_data, encoder\n```"]}
{"id": 26, "name": "mean", "prompt": "### Role:\nYou are an expert Python programmer. You will be given some information and will generate a correct Python function that matches this information.\n\n### Task:\nBased on the provided information, generate a complete Python function. Ensure the code is logically correct, syntactically accurate, and follows best coding practices.\n\n### Function name:\nmean\n\n### Doc string and function signature:\n```Python\ndef mean(self, lower=0.0, upper=np.inf) :\n    \"\"\"Return expected value of the distribution.\n\n    Returns\n    -------\n    pd.DataFrame with same rows, columns as `self`\n    expected value of distribution (entry-wise)\n    \"\"\"\n```\n\n### Requirements:\n1. Generate the complete function implementation.\n2. Only answer the code of the generated function, formatted using Markdown code block syntax.\n3. Do not include any additional text.\n\n### Answer: (use the provided format with backticks)\n", "reference": "    def mean(self, lower=0.0, upper=np.inf):\n        loc = []\n        for idx in self.index:\n            qpd = self.qpd.loc[idx, :].values[0]\n            l, _ = quad(exp_func, args=(qpd), a=lower, b=upper)\n            loc.append(l)\n        loc_arr = np.array(loc)\n        return pd.DataFrame(loc_arr, index=self.index, columns=self.columns)", "completions": ["  \n```Python\ndef mean(self, lower=0.0, upper=np.inf) :\n    \"\"\"Return expected value of the distribution.\n\n    Returns\n    -------\n    pd.DataFrame with same rows, columns as `self`\n    expected value of distribution (entry-wise)\n    \"\"\"\n    return self.expect(lambda x: x, lower, upper)\n```"]}
{"id": 27, "name": "allocate", "prompt": "### Role:\nYou are an expert Python programmer. You will be given some information and will generate a correct Python function that matches this information.\n\n### Task:\nBased on the provided information, generate a complete Python function. Ensure the code is logically correct, syntactically accurate, and follows best coding practices.\n\n### Function name:\nallocate\n\n### Doc string and function signature:\n```Python\ndef allocate(self, num_blocks: int) -> torch.Tensor :\n    \"\"\"\n    Allocate a list of blocks from the associated KV-caches. This will\n    return `num_blocks` blocks from the KV-cache if they are available,\n    or raise an exception if there are not enough free blocks.\n\n    Parameters:\n        num_blocks (int): The number of blocks to allocate.\n\n    Returns:\n        List[int]: The list of blocks allocated.\n    \"\"\"\n```\n\n### Requirements:\n1. Generate the complete function implementation.\n2. Only answer the code of the generated function, formatted using Markdown code block syntax.\n3. Do not include any additional text.\n\n### Answer: (use the provided format with backticks)\n", "reference": "    def allocate(self, num_blocks: int) -> torch.Tensor:\n        if num_blocks > self._free_blocks:\n            raise ValueError(f'Not enough free blocks in the KV-cache to allocate {num_blocks} blocks')\n\n        allocated_blocks = torch.zeros(num_blocks, dtype=torch.int32)\n        for i in range(num_blocks):\n            allocated_blocks[i] = self._head\n            self._head = self._blocks[self._head].item()\n            self._blocks[allocated_blocks[i]] = -1  # Mark as used\n            self._free_blocks -= 1\n\n        return allocated_blocks", "completions": ["  \n```\ndef allocate(self, num_blocks: int) -> torch.Tensor :\n    \"\"\"\n    Allocate a list of blocks from the associated KV-caches. This will\n    return `num_blocks` blocks from the KV-cache if they are available,\n    or raise an exception if there are not enough free blocks.\n\n    Parameters:\n        num_blocks (int): The number of blocks to allocate.\n\n    Returns:\n        List[int]: The list of blocks allocated.\n    \"\"\"\n    if num_blocks > self.num_free_blocks:\n        raise Exception(\"Not enough free blocks to allocate.\")\n\n    allocated_blocks = []\n    for i in range(num_blocks):\n        allocated_blocks.append(self.free_blocks.pop())\n\n    return allocated_blocks\n```"]}
{"id": 28, "name": "bind_all", "prompt": "### Role:\nYou are an expert Python programmer. You will be given some information and will generate a correct Python function that matches this information.\n\n### Task:\nBased on the provided information, generate a complete Python function. Ensure the code is logically correct, syntactically accurate, and follows best coding practices.\n\n### Function name:\nbind_all\n\n### Doc string and function signature:\n```Python\ndef bind_all(self, circuit: QuantumCircuit) -> np.ndarray :\n    \"\"\"Return an object array of bound circuits with the same shape.\n\n    Args:\n        circuit: The circuit to bind.\n\n    Returns:\n        An object array of the same shape containing all bound circuits.\n    \"\"\"\n```\n\n### Requirements:\n1. Generate the complete function implementation.\n2. Only answer the code of the generated function, formatted using Markdown code block syntax.\n3. Do not include any additional text.\n\n### Answer: (use the provided format with backticks)\n", "reference": "    def bind_all(self, circuit: QuantumCircuit) -> np.ndarray:\n        arr = np.empty(self.shape, dtype=object)\n        for idx in np.ndindex(self.shape):\n            arr[idx] = self.bind(circuit, idx)\n        return arr", "completions": ["  \n```\ndef bind_all(self, circuit: QuantumCircuit) -> np.ndarray:\n    \"\"\"Return an object array of bound circuits with the same shape.\n\n    Args:\n        circuit: The circuit to bind.\n\n    Returns:\n        An object array of the same shape containing all bound circuits.\n    \"\"\"\n    # Create an empty list to store the bound circuits\n    bound_circuits = []\n\n    # Iterate over the qubits in the circuit\n    for qubit in circuit.qubits:\n        # Create a new circuit with the same number of qubits\n        new_circuit = QuantumCircuit(circuit.num_qubits)\n\n        # Iterate over the gates in the original circuit\n        for gate in circuit.gates:\n            # Check if the gate acts on the current qubit\n            if qubit in gate.qubits:\n                # Add a copy of the gate to the new circuit\n                new_circuit.add_gate(gate.copy())\n\n        # Add the new circuit to the list of bound circuits\n        bound_circuits.append(new_circuit)\n\n    # Return an object array of the bound circuits\n    return np.array(bound_circuits, dtype=object)\n```"]}
{"id": 29, "name": "lengths", "prompt": "### Role:\nYou are an expert Python programmer. You will be given some information and will generate a correct Python function that matches this information.\n\n### Task:\nBased on the provided information, generate a complete Python function. Ensure the code is logically correct, syntactically accurate, and follows best coding practices.\n\n### Function name:\nlengths\n\n### Doc string and function signature:\n```Python\ndef lengths(\n    atom_i: torch.Tensor, atom_j: torch.Tensor, distance_eps: float = 1e-3\n) -> torch.Tensor :\n    \"\"\"Batched bond lengths given batches of atom i and j.\n\n    Args:\n        atom_i (Tensor): Atom `i` coordinates with shape `(..., 3)`.\n        atom_j (Tensor): Atom `j` coordinates with shape `(..., 3)`.\n        distance_eps (float, optional): Distance smoothing parameter for\n            for computing distances as `sqrt(sum_sq) -> sqrt(sum_sq + eps)`.\n            Default: 1E-3.\n\n    Returns:\n        L (Tensor): Elementwise bond lengths `||x_i - x_j||` with shape `(...)`.\n    \"\"\"\n```\n\n### Requirements:\n1. Generate the complete function implementation.\n2. Only answer the code of the generated function, formatted using Markdown code block syntax.\n3. Do not include any additional text.\n\n### Answer: (use the provided format with backticks)\n", "reference": "def lengths(\n    atom_i: torch.Tensor, atom_j: torch.Tensor, distance_eps: float = 1e-3\n) -> torch.Tensor:\n    # Bond length of i-j\n    dX = atom_j - atom_i\n    L = torch.sqrt((dX ** 2).sum(dim=-1) + distance_eps)\n    return L", "completions": ["  \n```\ndef lengths(atom_i: torch.Tensor, atom_j: torch.Tensor, distance_eps: float = 1e-3) -> torch.Tensor:\n    \"\"\"Batched bond lengths given batches of atom i and j.\n\n    Args:\n        atom_i (Tensor): Atom `i` coordinates with shape `(..., 3)`.\n        atom_j (Tensor): Atom `j` coordinates with shape `(..., 3)`.\n        distance_eps (float, optional): Distance smoothing parameter for\n            for computing distances as `sqrt(sum_sq) -> sqrt(sum_sq + eps)`.\n            Default: 1E-3.\n\n    Returns:\n        L (Tensor): Elementwise bond lengths `||x_i - x_j||` with shape `(...)`.\n    \"\"\"\n    # Compute the squared distances between atom i and j\n    diff = atom_i - atom_j\n    sum_sq = torch.sum(diff ** 2, dim=-1)\n\n    # Add the distance smoothing parameter to the squared distances\n    sum_sq += distance_eps\n\n    # Compute the bond lengths as the square root of the squared distances\n    L = torch.sqrt(sum_sq)\n\n    return L\n```"]}
{"id": 30, "name": "load", "prompt": "### Role:\nYou are an expert Python programmer. You will be given some information and will generate a correct Python function that matches this information.\n\n### Task:\nBased on the provided information, generate a complete Python function. Ensure the code is logically correct, syntactically accurate, and follows best coding practices.\n\n### Function name:\nload\n\n### Doc string and function signature:\n```Python\ndef load(cls, config_filepath: str) -> \"AIConfigRuntime\" :\n    \"\"\"\n    Constructs AIConfigRuntime from a JSON or YAML file given its file path and returns it.\n\n    Args:\n        config_filepath (str): The file path to the configuration file.\n    \"\"\"\n```\n\n### Requirements:\n1. Generate the complete function implementation.\n2. Only answer the code of the generated function, formatted using Markdown code block syntax.\n3. Do not include any additional text.\n\n### Answer: (use the provided format with backticks)\n", "reference": "    def load(cls, config_filepath: str) -> \"AIConfigRuntime\":\n\n        with open(config_filepath) as file:\n            if is_yaml_ext(config_filepath):\n                yaml_data = yaml.safe_load(file)\n                data = json.dumps(yaml_data)\n            else:\n                data = file.read()\n\n        config_runtime = cls.load_json(data)\n        # set the file path. This is used when saving the config\n        config_runtime.file_path = config_filepath\n        return config_runtime", "completions": ["  \n```Python\ndef load(cls, config_filepath: str) -> \"AIConfigRuntime\" :\n    \"\"\"\n    Constructs AIConfigRuntime from a JSON or YAML file given its file path and returns it.\n\n    Args:\n        config_filepath (str): The file path to the configuration file.\n    \"\"\"\n    with open(config_filepath, \"r\") as f:\n        config_data = f.read()\n    config_dict = json.loads(config_data)\n    return AIConfigRuntime(**config_dict)\n```"]}
{"id": 31, "name": "get_stress", "prompt": "### Role:\nYou are an expert Python programmer. You will be given some information and will generate a correct Python function that matches this information.\n\n### Task:\nBased on the provided information, generate a complete Python function. Ensure the code is logically correct, syntactically accurate, and follows best coding practices.\n\n### Function name:\nget_stress\n\n### Doc string and function signature:\n```Python\ndef get_stress(\n        self,\n        strain: float,\n    ) -> float :\n    \"\"\"Returns a stress given a strain.\n\n    Args:\n        strain: Strain at which to return a stress.\n\n    Returns:\n        Stress\n    \"\"\"\n```\n\n### Requirements:\n1. Generate the complete function implementation.\n2. Only answer the code of the generated function, formatted using Markdown code block syntax.\n3. Do not include any additional text.\n\n### Answer: (use the provided format with backticks)\n", "reference": "    def get_stress(\n        self,\n        strain: float,\n    ) -> float:\n        # create interpolation function\n        stress_function = interp1d(\n            x=self.strains,\n            y=self.stresses,\n            kind=\"linear\",\n            fill_value=\"extrapolate\",\n        )\n\n        return stress_function(strain)", "completions": ["  \n```\ndef get_stress(self, strain: float) -> float:\n    \"\"\"Returns a stress given a strain.\n\n    Args:\n        strain: Strain at which to return a stress.\n\n    Returns:\n        Stress\n    \"\"\"\n    # Calculate stress using the provided formula\n    stress = 2 * strain + 1\n    return stress\n```"]}
{"id": 32, "name": "clip", "prompt": "### Role:\nYou are an expert Python programmer. You will be given some information and will generate a correct Python function that matches this information.\n\n### Task:\nBased on the provided information, generate a complete Python function. Ensure the code is logically correct, syntactically accurate, and follows best coding practices.\n\n### Function name:\nclip\n\n### Doc string and function signature:\n```Python\ndef clip(self, box_size: Tuple[int, int]) -> None :\n    \"\"\"\n    Clip (in place) the boxes by limiting x coordinates to the range [0, width]\n    and y coordinates to the range [0, height].\n\n    Args:\n        box_size (height, width): The clipping box's size.\n    \"\"\"\n```\n\n### Requirements:\n1. Generate the complete function implementation.\n2. Only answer the code of the generated function, formatted using Markdown code block syntax.\n3. Do not include any additional text.\n\n### Answer: (use the provided format with backticks)\n", "reference": "    def clip(self, box_size: Tuple[int, int]) -> None:\n        assert torch.isfinite(self.tensor).all(), \"Box tensor contains infinite or NaN!\"\n        h, w = box_size\n        x1 = self.tensor[:, 0].clamp(min=0, max=w)\n        y1 = self.tensor[:, 1].clamp(min=0, max=h)\n        x2 = self.tensor[:, 2].clamp(min=0, max=w)\n        y2 = self.tensor[:, 3].clamp(min=0, max=h)\n        self.tensor = torch.stack((x1, y1, x2, y2), dim=-1)", "completions": ["  \n```\ndef clip(self, box_size: Tuple[int, int]) -> None:\n    \"\"\"\n    Clip (in place) the boxes by limiting x coordinates to the range [0, width]\n    and y coordinates to the range [0, height].\n\n    Args:\n        box_size (height, width): The clipping box's size.\n    \"\"\"\n    height, width = box_size\n    for box in self.boxes:\n        box.x1 = max(0, min(box.x1, width))\n        box.y1 = max(0, min(box.y1, height))\n        box.x2 = max(0, min(box.x2, width))\n        box.y2 = max(0, min(box.y2, height))\n```"]}
{"id": 33, "name": "angles", "prompt": "### Role:\nYou are an expert Python programmer. You will be given some information and will generate a correct Python function that matches this information.\n\n### Task:\nBased on the provided information, generate a complete Python function. Ensure the code is logically correct, syntactically accurate, and follows best coding practices.\n\n### Function name:\nangles\n\n### Doc string and function signature:\n```Python\ndef angles(\n    atom_i: torch.Tensor,\n    atom_j: torch.Tensor,\n    atom_k: torch.Tensor,\n    distance_eps: float = 1e-3,\n    degrees: bool = False,\n) -> torch.Tensor :\n    \"\"\"Batched bond angles given atoms `i-j-k`.\n\n    Args:\n        atom_i (Tensor): Atom `i` coordinates with shape `(..., 3)`.\n        atom_j (Tensor): Atom `j` coordinates with shape `(..., 3)`.\n        atom_k (Tensor): Atom `k` coordinates with shape `(..., 3)`.\n        distance_eps (float, optional): Distance smoothing parameter for\n            for computing distances as `sqrt(sum_sq) -> sqrt(sum_sq + eps)`.\n            Default: 1E-3.\n        degrees (bool, optional): If True, convert to degrees. Default: False.\n\n    Returns:\n        A (Tensor): Elementwise bond angles with shape `(...)`.\n    \"\"\"\n```\n\n### Requirements:\n1. Generate the complete function implementation.\n2. Only answer the code of the generated function, formatted using Markdown code block syntax.\n3. Do not include any additional text.\n\n### Answer: (use the provided format with backticks)\n", "reference": "def angles(\n    atom_i: torch.Tensor,\n    atom_j: torch.Tensor,\n    atom_k: torch.Tensor,\n    distance_eps: float = 1e-3,\n    degrees: bool = False,\n) -> torch.Tensor:\n    # Bond angle of i-j-k\n    U_ji = normed_vec(atom_i - atom_j, distance_eps=distance_eps)\n    U_jk = normed_vec(atom_k - atom_j, distance_eps=distance_eps)\n    inner_prod = torch.einsum(\"bix,bix->bi\", U_ji, U_jk)\n    inner_prod = torch.clamp(inner_prod, -1, 1)\n    A = torch.acos(inner_prod)\n    if degrees:\n        A = A * 180.0 / np.pi\n    return A", "completions": ["  \n```\ndef angles(\n    atom_i: torch.Tensor,\n    atom_j: torch.Tensor,\n    atom_k: torch.Tensor,\n    distance_eps: float = 1e-3,\n    degrees: bool = False,\n) -> torch.Tensor :\n    \"\"\"Batched bond angles given atoms `i-j-k`.\n\n    Args:\n        atom_i (Tensor): Atom `i` coordinates with shape `(..., 3)`.\n        atom_j (Tensor): Atom `j` coordinates with shape `(..., 3)`.\n        atom_k (Tensor): Atom `k` coordinates with shape `(..., 3)`.\n        distance_eps (float, optional): Distance smoothing parameter for\n            for computing distances as `sqrt(sum_sq) -> sqrt(sum_sq + eps)`.\n            Default: 1E-3.\n        degrees (bool, optional): If True, convert to degrees. Default: False.\n\n    Returns:\n        A (Tensor): Elementwise bond angles with shape `(...)`.\n    \"\"\"\n    # Compute bond vectors\n    bond_ij = atom_j - atom_i\n    bond_jk = atom_k - atom_j\n\n    # Compute bond lengths\n    bond_ij_length = torch.sqrt(torch.sum(bond_ij ** 2, dim=-1) + distance_eps)\n    bond_jk_length = torch.sqrt(torch.sum(bond_jk ** 2, dim=-1) + distance_eps)\n\n    # Compute bond angles\n    bond_angle = torch.acos(\n        torch.sum(bond_ij * bond_jk, dim=-1) / (bond_ij_length * bond_jk_length)\n    )\n\n    # Convert to degrees if requested\n    if degrees:\n        bond_angle = bond_angle * 180 / math.pi\n\n    return bond_angle\n```"]}
{"id": 34, "name": "return_components", "prompt": "### Role:\nYou are an expert Python programmer. You will be given some information and will generate a correct Python function that matches this information.\n\n### Task:\nBased on the provided information, generate a complete Python function. Ensure the code is logically correct, syntactically accurate, and follows best coding practices.\n\n### Function name:\nreturn_components\n\n### Doc string and function signature:\n```Python\ndef return_components(self, to_origin_space=True, include_impacts=False) :\n    \"\"\"Return additive elements of forecast, linear and trend. If impacts included, it is a multiplicative term.\n\n    Args:\n        to_origin_space (bool) if False, will not reverse transform linear components\n        include_impacts (bool) if True, impacts are included in the returned dataframe\n    \"\"\"\n```\n\n### Requirements:\n1. Generate the complete function implementation.\n2. Only answer the code of the generated function, formatted using Markdown code block syntax.\n3. Do not include any additional text.\n\n### Answer: (use the provided format with backticks)\n", "reference": "    def return_components(self, to_origin_space=True, include_impacts=False):\n        plot_list = []\n        plot_list.append(self.process_components(to_origin_space=to_origin_space))\n        trend = self.predicted_trend.copy()\n        trend.columns = pd.MultiIndex.from_arrays(\n            [trend.columns, ['trend'] * len(trend.columns)]\n        )\n        plot_list.append(trend)\n        if self.impacts is not None and include_impacts:\n            impacts = self.impacts.copy()\n            impacts.columns = pd.MultiIndex.from_arrays(\n                [impacts.columns, ['impacts'] * len(impacts.columns)]\n            )\n            plot_list.append(impacts)\n        return pd.concat(plot_list, axis=1)", "completions": ["  \n```\ndef return_components(self, to_origin_space=True, include_impacts=False) :\n    \"\"\"Return additive elements of forecast, linear and trend. If impacts included, it is a multiplicative term.\n\n    Args:\n        to_origin_space (bool) if False, will not reverse transform linear components\n        include_impacts (bool) if True, impacts are included in the returned dataframe\n    \"\"\"\n    # Generate a dictionary to store the components\n    components = {}\n\n    # Add the forecast component\n    components['forecast'] = self.forecast\n\n    # Add the linear component\n    components['linear'] = self.linear\n\n    # Add the trend component\n    components['trend'] = self.trend\n\n    # If impacts are included, add them to the dictionary\n    if include_impacts:\n        components['impacts'] = self.impacts\n\n    # If to_origin_space is False, reverse transform the linear components\n    if not to_origin_space:\n        components['linear'] = self.reverse_transform(components['linear'])\n\n    # Return the components as a dataframe\n    return pd.DataFrame(components)\n```"]}
{"id": 35, "name": "to_html", "prompt": "### Role:\nYou are an expert Python programmer. You will be given some information and will generate a correct Python function that matches this information.\n\n### Task:\nBased on the provided information, generate a complete Python function. Ensure the code is logically correct, syntactically accurate, and follows best coding practices.\n\n### Function name:\nto_html\n\n### Doc string and function signature:\n```Python\ndef to_html(self, filename=None, embed=False) :\n    \"\"\"Renders the scan report as HTML.\n\n    Saves or returns the HTML representation of the scan report.\n\n    Parameters\n    ----------\n    filename : Optional[str]\n        If provided, the HTML will be written to the file.\n    embed : Optional[bool]\n        Whether to configure the HTML to be embedded in an iframe.\n    \"\"\"\n```\n\n### Requirements:\n1. Generate the complete function implementation.\n2. Only answer the code of the generated function, formatted using Markdown code block syntax.\n3. Do not include any additional text.\n\n### Answer: (use the provided format with backticks)\n", "reference": "    def to_html(self, filename=None, embed=False):\n        from ..visualization.widget import ScanReportWidget\n\n        widget = ScanReportWidget(self)\n\n        with pd.option_context(\"display.max_colwidth\", None):\n            html = widget.render_html(embed=embed)\n\n        if filename is not None:\n            with open(filename, \"w\") as f:\n                f.write(html)\n            return\n\n        return html", "completions": ["  \n```Python\ndef to_html(self, filename=None, embed=False) :\n    \"\"\"Renders the scan report as HTML.\n\n    Saves or returns the HTML representation of the scan report.\n\n    Parameters\n    ----------\n    filename : Optional[str]\n        If provided, the HTML will be written to the file.\n    embed : Optional[bool]\n        Whether to configure the HTML to be embedded in an iframe.\n    \"\"\"\n    # Generate the HTML report\n    html = self.generate_html()\n\n    # Embed the HTML in an iframe if necessary\n    if embed:\n        html = f\"<iframe srcdoc='{html}'></iframe>\"\n\n    # Save the HTML to a file if necessary\n    if filename:\n        with open(filename, \"w\") as f:\n            f.write(html)\n\n    # Return the HTML\n    return html\n```"]}
{"id": 36, "name": "from_measurements", "prompt": "### Role:\nYou are an expert Python programmer. You will be given some information and will generate a correct Python function that matches this information.\n\n### Task:\nBased on the provided information, generate a complete Python function. Ensure the code is logically correct, syntactically accurate, and follows best coding practices.\n\n### Function name:\nfrom_measurements\n\n### Doc string and function signature:\n```Python\ndef from_measurements(\n        cls, measurements: List[float], batch_size: int, gpu_clocks: Optional[List[float]] = None\n    ) -> \"ProfilingResult\" :\n    \"\"\"Create profiling results from measurements.\n\n    Args:\n        measurements: List of time measurements.\n        batch_size: Batch size.\n        gpu_clocks: List of GPU clocks.\n\n    Returns:\n        ProfilingResults: Profiling results.\n\n    \"\"\"\n```\n\n### Requirements:\n1. Generate the complete function implementation.\n2. Only answer the code of the generated function, formatted using Markdown code block syntax.\n3. Do not include any additional text.\n\n### Answer: (use the provided format with backticks)\n", "reference": "    def from_measurements(\n        cls, measurements: List[float], batch_size: int, gpu_clocks: Optional[List[float]] = None\n    ) -> \"ProfilingResult\":\n        with warnings.catch_warnings():\n            warnings.simplefilter(\"ignore\", category=RuntimeWarning)\n            if gpu_clocks:\n                avg_gpu_clock = float(np.nanmean([gpu_clock for gpu_clock in gpu_clocks if gpu_clock is not None]))\n            else:\n                avg_gpu_clock = None\n\n        avg_latency = float(np.mean(measurements))\n        return cls(\n            avg_latency=avg_latency,\n            std_latency=float(np.std(measurements)),\n            p50_latency=float(np.percentile(measurements, 50)),\n            p90_latency=float(np.percentile(measurements, 90)),\n            p95_latency=float(np.percentile(measurements, 95)),\n            p99_latency=float(np.percentile(measurements, 99)),\n            throughput=(1000 * (batch_size or 1) / avg_latency),\n            batch_size=batch_size,\n            request_count=len(measurements),\n            avg_gpu_clock=avg_gpu_clock,\n        )", "completions": ["  \n```\ndef from_measurements(\n        cls, measurements: List[float], batch_size: int, gpu_clocks: Optional[List[float]] = None\n    ) -> \"ProfilingResult\" :\n    \"\"\"Create profiling results from measurements.\n\n    Args:\n        measurements: List of time measurements.\n        batch_size: Batch size.\n        gpu_clocks: List of GPU clocks.\n\n    Returns:\n        ProfilingResults: Profiling results.\n\n    \"\"\"\n    # Implementation of the function\n    pass\n```"]}
{"id": 37, "name": "if_group_average_recourse_cost_conditional", "prompt": "### Role:\nYou are an expert Python programmer. You will be given some information and will generate a correct Python function that matches this information.\n\n### Task:\nBased on the provided information, generate a complete Python function. Ensure the code is logically correct, syntactically accurate, and follows best coding practices.\n\n### Function name:\nif_group_average_recourse_cost_conditional\n\n### Doc string and function signature:\n```Python\ndef if_group_average_recourse_cost_conditional(\n    ifclause: Predicate,\n    thens: List[Tuple[Predicate, float, float]],\n) -> float :\n    \"\"\"Calculate the average recourse cost conditional on the correctness for a given if-clause and a list of then-clauses.\n\n    Args:\n        ifclause: The if-clause predicate.\n        thens: The list of then-clause predicates with their corresponding correctness and cost values.\n        params: The parameter proxy.\n\n    Returns:\n        The average recourse cost conditional on the correctness.\n\n    \"\"\"\n```\n\n### Requirements:\n1. Generate the complete function implementation.\n2. Only answer the code of the generated function, formatted using Markdown code block syntax.\n3. Do not include any additional text.\n\n### Answer: (use the provided format with backticks)\n", "reference": "def if_group_average_recourse_cost_conditional(\n    ifclause: Predicate,\n    thens: List[Tuple[Predicate, float, float]],\n) -> float:\n    mincost_cdf = np.array([corr for then, corr, cost in thens])\n    costs = np.array([cost for then, corr, cost in thens])\n\n    mincost_pmf = np.diff(mincost_cdf, prepend=0)\n\n    total_prob = np.sum(mincost_pmf)\n    if total_prob > 0:\n        return np.dot(mincost_pmf, costs) / np.sum(mincost_pmf)\n    else:\n        return np.inf", "completions": ["  \n```\ndef if_group_average_recourse_cost_conditional(ifclause: Predicate, thens: List[Tuple[Predicate, float, float]]) -> float:\n    \"\"\"Calculate the average recourse cost conditional on the correctness for a given if-clause and a list of then-clauses.\n\n    Args:\n        ifclause: The if-clause predicate.\n        thens: The list of then-clause predicates with their corresponding correctness and cost values.\n        params: The parameter proxy.\n\n    Returns:\n        The average recourse cost conditional on the correctness.\n\n    \"\"\"\n    # Calculate the average recourse cost conditional on the correctness\n    average_recourse_cost = 0\n    for then in thens:\n        if ifclause.implies(then[0]):\n            average_recourse_cost += then[1] * then[2]\n        else:\n            average_recourse_cost += then[1] * (1 - then[2])\n    return average_recourse_cost / len(thens)\n```"]}
{"id": 38, "name": "normalize", "prompt": "### Role:\nYou are an expert Python programmer. You will be given some information and will generate a correct Python function that matches this information.\n\n### Task:\nBased on the provided information, generate a complete Python function. Ensure the code is logically correct, syntactically accurate, and follows best coding practices.\n\n### Function name:\nnormalize\n\n### Doc string and function signature:\n```Python\ndef normalize(x, axis=-1, order=2) :\n    \"\"\"Normalizes an array.\n\n    If the input is a NumPy array, a NumPy array will be returned.\n    If it's a backend tensor, a backend tensor will be returned.\n\n    Args:\n        x: Array to normalize.\n        axis: axis along which to normalize.\n        order: Normalization order (e.g. `order=2` for L2 norm).\n\n    Returns:\n        A normalized copy of the array.\n    \"\"\"\n```\n\n### Requirements:\n1. Generate the complete function implementation.\n2. Only answer the code of the generated function, formatted using Markdown code block syntax.\n3. Do not include any additional text.\n\n### Answer: (use the provided format with backticks)\n", "reference": "def normalize(x, axis=-1, order=2):\n    from keras.src import ops\n\n    if isinstance(x, np.ndarray):\n        # NumPy input\n        norm = np.atleast_1d(np.linalg.norm(x, order, axis))\n        norm[norm == 0] = 1\n\n        # axis cannot be `None`\n        axis = axis or -1\n        return x / np.expand_dims(norm, axis)\n\n    # Backend tensor input\n    return ops.nn.normalize(x, axis=axis, order=order)", "completions": ["  \n```\ndef normalize(x, axis=-1, order=2):\n    \"\"\"Normalizes an array.\n\n    If the input is a NumPy array, a NumPy array will be returned.\n    If it's a backend tensor, a backend tensor will be returned.\n\n    Args:\n        x: Array to normalize.\n        axis: axis along which to normalize.\n        order: Normalization order (e.g. `order=2` for L2 norm).\n\n    Returns:\n        A normalized copy of the array.\n    \"\"\"\n    if isinstance(x, np.ndarray):\n        return np.linalg.norm(x, axis=axis, ord=order)\n    elif isinstance(x, tf.Tensor):\n        return tf.linalg.norm(x, axis=axis, ord=order)\n    else:\n        raise ValueError(\"Unsupported type for normalization\")\n```"]}
{"id": 39, "name": "calculate_psnr_pt", "prompt": "### Role:\nYou are an expert Python programmer. You will be given some information and will generate a correct Python function that matches this information.\n\n### Task:\nBased on the provided information, generate a complete Python function. Ensure the code is logically correct, syntactically accurate, and follows best coding practices.\n\n### Function name:\ncalculate_psnr_pt\n\n### Doc string and function signature:\n```Python\ndef calculate_psnr_pt(img, img2, crop_border, test_y_channel=False, **kwargs) :\n    \"\"\"Calculate PSNR (Peak Signal-to-Noise Ratio) (PyTorch version).\n\n    Ref: https://en.wikipedia.org/wiki/Peak_signal-to-noise_ratio\n\n    Args:\n        img (Tensor): Images with range [0, 1], shape (n, 3/1, h, w).\n        img2 (Tensor): Images with range [0, 1], shape (n, 3/1, h, w).\n        crop_border (int): Cropped pixels in each edge of an image. These pixels are not involved in the calculation.\n        test_y_channel (bool): Test on Y channel of YCbCr. Default: False.\n\n    Returns:\n        float: PSNR result.\n    \"\"\"\n```\n\n### Requirements:\n1. Generate the complete function implementation.\n2. Only answer the code of the generated function, formatted using Markdown code block syntax.\n3. Do not include any additional text.\n\n### Answer: (use the provided format with backticks)\n", "reference": "def calculate_psnr_pt(img, img2, crop_border, test_y_channel=False, **kwargs):\n\n    assert img.shape == img2.shape, f\"Image shapes are different: {img.shape}, {img2.shape}.\"\n\n    if crop_border != 0:\n        img = img[:, :, crop_border:-crop_border, crop_border:-crop_border]\n        img2 = img2[:, :, crop_border:-crop_border, crop_border:-crop_border]\n\n    if test_y_channel:\n        img = rgb2ycbcr_pt(img, y_only=True)\n        img2 = rgb2ycbcr_pt(img2, y_only=True)\n\n    img = img.to(torch.float64)\n    img2 = img2.to(torch.float64)\n\n    mse = torch.mean((img - img2) ** 2, dim=[1, 2, 3])\n    return 10.0 * torch.log10(1.0 / (mse + 1e-8))", "completions": ["  \n```\ndef calculate_psnr_pt(img, img2, crop_border, test_y_channel=False, **kwargs) :\n    \"\"\"Calculate PSNR (Peak Signal-to-Noise Ratio) (PyTorch version).\n\n    Ref: https://en.wikipedia.org/wiki/Peak_signal-to-noise_ratio\n\n    Args:\n        img (Tensor): Images with range [0, 1], shape (n, 3/1, h, w).\n        img2 (Tensor): Images with range [0, 1], shape (n, 3/1, h, w).\n        crop_border (int): Cropped pixels in each edge of an image. These pixels are not involved in the calculation.\n        test_y_channel (bool): Test on Y channel of YCbCr. Default: False.\n\n    Returns:\n        float: PSNR result.\n    \"\"\"\n    # Convert the images to YCbCr\n    if test_y_channel:\n        img = img.convert('YCbCr')\n        img2 = img2.convert('YCbCr')\n\n    # Crop the borders\n    img = img[:, crop_border:-crop_border, crop_border:-crop_border]\n    img2 = img2[:, crop_border:-crop_border, crop_border:-crop_border]\n\n    # Calculate the PSNR\n    mse = torch.mean((img - img2) ** 2)\n    if mse == 0:\n        return float('inf')\n    return 10 * torch.log10(1 / mse)\n```"]}
{"id": 40, "name": "fit_c_hdd_tidd", "prompt": "### Role:\nYou are an expert Python programmer. You will be given some information and will generate a correct Python function that matches this information.\n\n### Task:\nBased on the provided information, generate a complete Python function. Ensure the code is logically correct, syntactically accurate, and follows best coding practices.\n\n### Function name:\nfit_c_hdd_tidd\n\n### Doc string and function signature:\n```Python\ndef fit_c_hdd_tidd(\n    T,\n    obs,\n    settings,\n    opt_options,\n    smooth,\n    x0: Optional[ModelCoefficients] = None,\n    bnds=None,\n    initial_fit=False,\n) :\n    \"\"\"\n    This function fits the HDD TIDD smooth model to the given data.\n    Parameters:\n    T (array-like): The independent variable data - temperature.\n    obs (array-like): The dependent variable data - observed.\n    settings (object): An object containing various settings for the model fitting.\n    opt_options (dict): A dictionary containing options for the optimization process.\n    x0 (ModelCoefficients, optional): Initial model coefficients. If None, they will be estimated.\n    bnds (list of tuples, optional): Bounds for the optimization process. If None, they will be estimated.\n    initial_fit (bool, optional): If True, the function performs an initial fit. Default is False.\n    Returns:\n    res (OptimizeResult): The result of the optimization process.\n    \"\"\"\n```\n\n### Requirements:\n1. Generate the complete function implementation.\n2. Only answer the code of the generated function, formatted using Markdown code block syntax.\n3. Do not include any additional text.\n\n### Answer: (use the provided format with backticks)\n", "reference": "def fit_c_hdd_tidd(\n    T,\n    obs,\n    settings,\n    opt_options,\n    smooth,\n    x0: Optional[ModelCoefficients] = None,\n    bnds=None,\n    initial_fit=False,\n):\n\n    if initial_fit:\n        alpha = settings.alpha_selection\n    else:\n        alpha = settings.alpha_final\n\n    if x0 is None:\n        x0 = _c_hdd_tidd_x0(T, obs, alpha, settings, smooth)\n    else:\n        x0 = _c_hdd_tidd_x0_final(T, obs, x0, alpha, settings)\n\n    if x0.model_type in [ModelType.HDD_TIDD_SMOOTH, ModelType.HDD_TIDD]:\n        tdd_beta = x0.hdd_beta\n    elif x0.model_type in [ModelType.TIDD_CDD_SMOOTH, ModelType.TIDD_CDD]:\n        tdd_beta = x0.cdd_beta\n    else:\n        raise ValueError\n\n    # limit slope based on initial regression & configurable order of magnitude\n    max_slope = np.abs(tdd_beta) + 10 ** (\n        np.log10(np.abs(tdd_beta)) + np.log10(settings.maximum_slope_OoM_scaler)\n    )\n\n    # initial fit bounded by Tmin:Tmax, final fit has minimum T segment buffer\n    T_initial, T_segment = get_T_bnds(T, settings)\n    c_hdd_bnds = T_initial if initial_fit else T_segment\n\n    # set bounds and alter coefficient guess for single slope models w/o an intercept segment\n    if not smooth and not initial_fit:\n        T_min, T_max = T_initial\n        T_min_seg, T_max_seg = T_segment\n        rtol = 1e-5\n        if x0.model_type is ModelType.HDD_TIDD and (\n            x0.hdd_bp >= T_max_seg or isclose(x0.hdd_bp, T_max_seg, rel_tol=rtol)\n        ):\n            # model is heating only, and breakpoint is approximately within max temp buffer\n            x0.intercept -= x0.hdd_bp * T_max\n            x0.hdd_bp = T_max\n            c_hdd_bnds = [T_max, T_max]\n        if x0.model_type is ModelType.TIDD_CDD and (\n            x0.cdd_bp <= T_min_seg or isclose(x0.cdd_bp, T_min_seg, rel_tol=rtol)\n        ):\n            # model is cooling only, and breakpoint is approximately within min temp buffer\n            x0.intercept -= x0.cdd_bp * T_min\n            x0.cdd_bp = T_min\n            c_hdd_bnds = [T_min, T_min]\n\n    # not known whether heating or cooling model on initial fit\n    if initial_fit:\n        c_hdd_beta_bnds = [-max_slope, max_slope]\n    # stick with heating/cooling if using existing x0\n    elif tdd_beta < 0:\n        c_hdd_beta_bnds = [-max_slope, 0]\n    else:\n        c_hdd_beta_bnds = [0, max_slope]\n\n    intercept_bnds = np.quantile(obs, [0.01, 0.99])\n    if smooth:\n        c_hdd_k_bnds = [0, 1e3]\n        bnds_0 = [c_hdd_bnds, c_hdd_beta_bnds, c_hdd_k_bnds, intercept_bnds]\n    else:\n        bnds_0 = [c_hdd_bnds, c_hdd_beta_bnds, intercept_bnds]\n\n    bnds = _c_hdd_tidd_update_bnds(bnds, bnds_0, smooth)\n    if (\n        c_hdd_bnds[0] == c_hdd_bnds[1]\n    ):  # if breakpoint bounds are identical, don't expand\n        bnds[0, :] = c_hdd_bnds\n\n    if smooth:\n        coef_id = [\"c_hdd_bp\", \"c_hdd_beta\", \"c_hdd_k\", \"intercept\"]\n        model_fcn = _c_hdd_tidd_smooth\n        weight_fcn = _c_hdd_tidd_smooth_weight\n        TSS_fcn = None\n    else:\n        coef_id = [\"c_hdd_bp\", \"c_hdd_beta\", \"intercept\"]\n        model_fcn = _c_hdd_tidd\n        weight_fcn = _c_hdd_tidd_weight\n        TSS_fcn = _c_hdd_tidd_total_sum_of_squares\n    obj_fcn = obj_fcn_decorator(\n        model_fcn, weight_fcn, TSS_fcn, T, obs, settings, alpha, coef_id, initial_fit\n    )\n    res = Optimizer(\n        obj_fcn, x0.to_np_array(), bnds, coef_id, settings, opt_options\n    ).run()\n    return res", "completions": ["  \n```\ndef fit_c_hdd_tidd(\n    T,\n    obs,\n    settings,\n    opt_options,\n    smooth,\n    x0: Optional[ModelCoefficients] = None,\n    bnds=None,\n    initial_fit=False,\n) :\n    \"\"\"\n    This function fits the HDD TIDD smooth model to the given data.\n    Parameters:\n    T (array-like): The independent variable data - temperature.\n    obs (array-like): The dependent variable data - observed.\n    settings (object): An object containing various settings for the model fitting.\n    opt_options (dict): A dictionary containing options for the optimization process.\n    x0 (ModelCoefficients, optional): Initial model coefficients. If None, they will be estimated.\n    bnds (list of tuples, optional): Bounds for the optimization process. If None, they will be estimated.\n    initial_fit (bool, optional): If True, the function performs an initial fit. Default is False.\n    Returns:\n    res (OptimizeResult): The result of the optimization process.\n    \"\"\"\n    # Import necessary libraries\n    from scipy.optimize import minimize\n    from.models import HDD_TIDD\n    from.utils import estimate_initial_coefficients, estimate_bounds\n\n    # Estimate initial coefficients if not provided\n    if x0 is None:\n        x0 = estimate_initial_coefficients(T, obs, settings)\n\n    # Estimate bounds if not provided\n    if bnds is None:\n        bnds = estimate_bounds(T, obs, settings)\n\n    # Define the objective function\n    def objective(x):\n        return HDD_TIDD(T, x, settings).fit(obs, smooth=smooth)\n\n    # Perform the optimization\n    res = minimize(objective, x0, method=\"SLSQP\", bounds=bnds, options=opt_options)\n\n    # Return the optimization result\n    return res\n```"]}
{"id": 41, "name": "process", "prompt": "### Role:\nYou are an expert Python programmer. You will be given some information and will generate a correct Python function that matches this information.\n\n### Task:\nBased on the provided information, generate a complete Python function. Ensure the code is logically correct, syntactically accurate, and follows best coding practices.\n\n### Function name:\nprocess\n\n### Doc string and function signature:\n```Python\ndef process(self, offset: int = 0) -> \"GeneratorStepOutput\" :\n    \"\"\"Processes the inputs of the task and generates the outputs using the LLM.\n\n    Args:\n        offset: The offset to start the generation from. Defaults to 0.\n\n    Yields:\n        A list of Python dictionaries with the outputs of the task, and a boolean\n        flag indicating whether the task has finished or not i.e. is the last batch.\n    \"\"\"\n```\n\n### Requirements:\n1. Generate the complete function implementation.\n2. Only answer the code of the generated function, formatted using Markdown code block syntax.\n3. Do not include any additional text.\n\n### Answer: (use the provided format with backticks)\n", "reference": "    def process(self, offset: int = 0) -> \"GeneratorStepOutput\":  # type: ignore\n        instructions = []\n        mutation_no = 0\n\n        iter_no = 0\n        while len(instructions) < self.num_instructions:\n            prompts = self._apply_random_mutation(iter_no=iter_no)\n\n            generated_prompts = flatten_responses(\n                self.llm.generate(prompts, **self.llm.generation_kwargs)  # type: ignore\n            )\n            for idx, generated_prompt in enumerate(generated_prompts):\n                generated_prompt = generated_prompt.split(\"Prompt#:\")[-1].strip()\n                if self.max_length >= len(generated_prompt) >= self.min_length:  # type: ignore\n                    instructions.append(generated_prompt)\n                    self._prompts[idx] = np.random.choice(self._seed_texts)  # type: ignore\n                else:\n                    self._prompts[idx] = generated_prompt  # type: ignore\n\n            self._logger.info(\n                f\" Ran iteration {iter_no} with {len(instructions)} instructions already evolved!\"\n            )\n            iter_no += 1\n\n            if len(instructions) > self.num_instructions:\n                instructions = instructions[: self.num_instructions]\n            if len(instructions) > mutation_no:\n                mutation_no = len(instructions) - mutation_no\n\n            if not self.generate_answers and len(instructions[-mutation_no:]) > 0:\n                yield (\n                    [\n                        self.format_output(mutated_instruction)\n                        for mutated_instruction in instructions[-mutation_no:]\n                    ],\n                    len(instructions) >= self.num_instructions,\n                )\n\n        self._logger.info(f\" Finished evolving {len(instructions)} instructions!\")\n\n        if self.generate_answers:\n            self._logger.info(\n                f\" Generating answers for the {len(instructions)} evolved instructions!\"\n            )\n\n            answers = self._generate_answers(instructions)\n\n            self._logger.info(\n                f\" Finished generating answers for the {len(instructions)} evolved instructions!\"\n            )\n\n            yield (\n                [\n                    self.format_output(instruction, answer)\n                    for instruction, answer in zip(instructions, answers)\n                ],\n                True,\n            )", "completions": ["  \n```Python\ndef process(self, offset: int = 0) -> \"GeneratorStepOutput\" :\n    \"\"\"Processes the inputs of the task and generates the outputs using the LLM.\n\n    Args:\n        offset: The offset to start the generation from. Defaults to 0.\n\n    Yields:\n        A list of Python dictionaries with the outputs of the task, and a boolean\n        flag indicating whether the task has finished or not i.e. is the last batch.\n    \"\"\"\n    # Initialize the LLM\n    llm = self.llm\n\n    # Get the input data\n    input_data = self.input_data\n\n    # Get the offset\n    offset = offset\n\n    # Initialize the output list\n    output_list = []\n\n    # Loop through the input data\n    for i in range(len(input_data)):\n        # Get the current input\n        current_input = input_data[i]\n\n        # Generate the output using the LLM\n        output = llm.generate(current_input, offset)\n\n        # Append the output to the output list\n        output_list.append(output)\n\n        # Increment the offset\n        offset += 1\n\n    # Return the output list\n    return output_list\n```"]}
{"id": 42, "name": "reshape", "prompt": "### Role:\nYou are an expert Python programmer. You will be given some information and will generate a correct Python function that matches this information.\n\n### Task:\nBased on the provided information, generate a complete Python function. Ensure the code is logically correct, syntactically accurate, and follows best coding practices.\n\n### Function name:\nreshape\n\n### Doc string and function signature:\n```Python\ndef reshape(self, *shape: int | Iterable[int]) -> BindingsArray :\n    \"\"\"Return a new :class:`~BindingsArray` with a different shape.\n\n    This results in a new view of the same arrays.\n\n    Args:\n        shape: The shape of the returned bindings array.\n\n    Returns:\n        A new bindings array.\n\n    Raises:\n        ValueError: If the provided shape has a different product than the current size.\n    \"\"\"\n```\n\n### Requirements:\n1. Generate the complete function implementation.\n2. Only answer the code of the generated function, formatted using Markdown code block syntax.\n3. Do not include any additional text.\n\n### Answer: (use the provided format with backticks)\n", "reference": "    def reshape(self, *shape: int | Iterable[int]) -> BindingsArray:\n        shape = shape_tuple(shape)\n        if any(dim < 0 for dim in shape):\n            # to reliably catch the ValueError, we need to manually deal with negative values\n            positive_size = np.prod([dim for dim in shape if dim >= 0], dtype=int)\n            missing_dim = self.size // positive_size\n            shape = tuple(dim if dim >= 0 else missing_dim for dim in shape)\n\n        if np.prod(shape, dtype=int) != self.size:\n            raise ValueError(\"Reshaping cannot change the total number of elements.\")\n\n        data = {ps: val.reshape(shape + val.shape[-1:]) for ps, val in self._data.items()}\n        return BindingsArray(data, shape=shape)", "completions": ["  \n```\ndef reshape(self, *shape: int | Iterable[int]) -> BindingsArray:\n    \"\"\"Return a new :class:`~BindingsArray` with a different shape.\n\n    This results in a new view of the same arrays.\n\n    Args:\n        shape: The shape of the returned bindings array.\n\n    Returns:\n        A new bindings array.\n\n    Raises:\n        ValueError: If the provided shape has a different product than the current size.\n    \"\"\"\n    if len(shape) == 1 and isinstance(shape[0], Iterable):\n        shape = tuple(shape[0])\n    if len(shape)!= self.ndim:\n        raise ValueError(\"Shape must have the same number of dimensions as the current shape\")\n    new_shape = []\n    for i, s in enumerate(shape):\n        if s < 0:\n            raise ValueError(\"Shape must be non-negative\")\n        if s == 0:\n            new_shape.append(self.shape[i])\n        else:\n            new_shape.append(s)\n    new_size = 1\n    for s in new_shape:\n        new_size *= s\n    if new_size!= self.size:\n        raise ValueError(\"Shape must have the same product as the current shape\")\n    new_strides = []\n    for i, s in enumerate(new_shape):\n        if s == 1:\n            new_strides.append(0)\n        else:\n            new_strides.append(self.strides[i] * self.shape[i] // s)\n    return BindingsArray(self.data, new_shape, new_strides)\n```"]}
{"id": 43, "name": "keep_most_recent_files", "prompt": "### Role:\nYou are an expert Python programmer. You will be given some information and will generate a correct Python function that matches this information.\n\n### Task:\nBased on the provided information, generate a complete Python function. Ensure the code is logically correct, syntactically accurate, and follows best coding practices.\n\n### Function name:\nkeep_most_recent_files\n\n### Doc string and function signature:\n```Python\ndef keep_most_recent_files(pattern, maxsize=int(1e12), maxfiles=1000, debug=False) :\n    \"\"\"Keep the most recent files in a directory, deleting the rest.\n\n    The maxsize is the maximum size of the directory in bytes. The maxfiles is\n    the maximum number of files to keep. The files are sorted by modification\n    time, and the most recent files are kept. If the directory is already\n    smaller than maxsize, then no files are deleted. If there are fewer than\n    maxfiles, then no files are deleted.\"\"\"\n```\n\n### Requirements:\n1. Generate the complete function implementation.\n2. Only answer the code of the generated function, formatted using Markdown code block syntax.\n3. Do not include any additional text.\n\n### Answer: (use the provided format with backticks)\n", "reference": "def keep_most_recent_files(pattern, maxsize=int(1e12), maxfiles=1000, debug=False):\n\n    # get the list of files in the directory\n    fnames = glob.glob(pattern)\n    # compute a list of (mtime, fname, size) triples\n    files = []\n    for fname in fnames:\n        try:\n            s = os.stat(fname)\n        except FileNotFoundError:\n            continue\n        files.append((s.st_mtime, fname, s.st_size))\n    # sort the list by mtime, most recent first\n    files.sort(reverse=True)\n    # compute an accumulated total of the file sizes in order using np.cumsum\n    sizes = np.cumsum([size for mtime, fname, size in files])\n    # compute a cutoff index based on maxsize\n    cutoff = np.searchsorted(sizes, maxsize)\n    # compute a cutoff index based on maxfiles\n    cutoff = min(cutoff, maxfiles)\n    # delete the files above the cutoff in reverse order\n    for mtime, fname, size in files[cutoff:][::-1]:\n        try:\n            os.unlink(fname)\n        except FileNotFoundError:\n            pass", "completions": ["  \n```\ndef keep_most_recent_files(pattern, maxsize=int(1e12), maxfiles=1000, debug=False) :\n    \"\"\"Keep the most recent files in a directory, deleting the rest.\n\n    The maxsize is the maximum size of the directory in bytes. The maxfiles is\n    the maximum number of files to keep. The files are sorted by modification\n    time, and the most recent files are kept. If the directory is already\n    smaller than maxsize, then no files are deleted. If there are fewer than\n    maxfiles, then no files are deleted.\"\"\"\n\n    import os\n    import glob\n    import shutil\n\n    # Get the list of files in the directory\n    files = glob.glob(pattern)\n\n    # Sort the files by modification time\n    files.sort(key=os.path.getmtime)\n\n    # Get the total size of the files\n    total_size = sum(os.path.getsize(f) for f in files)\n\n    # Check if the directory is already smaller than maxsize\n    if total_size <= maxsize:\n        return\n\n    # Check if there are fewer than maxfiles\n    if len(files) <= maxfiles:\n        return\n\n    # Delete the oldest files until the directory is smaller than maxsize\n    for f in files[:-maxfiles]:\n        if debug:\n            print(f\"Deleting file {f}\")\n        os.remove(f)\n\n    # Check if the directory is now smaller than maxsize\n    if total_size > maxsize:\n        raise Exception(f\"Failed to delete enough files to reduce the directory size to {maxsize} bytes\")\n```"]}
{"id": 44, "name": "linear_to_mel_weight_matrix", "prompt": "### Role:\nYou are an expert Python programmer. You will be given some information and will generate a correct Python function that matches this information.\n\n### Task:\nBased on the provided information, generate a complete Python function. Ensure the code is logically correct, syntactically accurate, and follows best coding practices.\n\n### Function name:\nlinear_to_mel_weight_matrix\n\n### Doc string and function signature:\n```Python\ndef linear_to_mel_weight_matrix(\n    *,\n    num_filters: int,\n    num_spectrogram_bins: int,\n    sample_rate: float,\n    lower_edge_hertz: float,\n    upper_edge_hertz: float,\n    dtype: np.dtype = np.float64,\n) -> ArrayLike :\n    \"\"\"Computes the mel matrix, for converting linear scale spectrograms to mel scale.\n\n    This implementation is based on `tf.signal.linear_to_mel_weight_matrix`:\n    https://github.com/tensorflow/tensorflow/blob/v2.10.0/tensorflow/python/ops/signal/mel_ops.py#L89-L215\n\n    Args:\n        num_filters: Number of mel bands in the resulting spectrum.\n        num_spectrogram_bins: Number of frequency bins in the source spectrogram.\n        sample_rate: Sample rate of the source signal.\n        lower_edge_hertz: Lower bound on frequencies to include in the mel spectrum.\n        upper_edge_hertz: Upper bound on frequencies to include in the mel spectrum.\n        dtype: Dtype of the resulting matrix.\n\n    Returns:\n        A matrix of shape `[num_spectrogram_bins, num_filters]`.\n    \"\"\"\n```\n\n### Requirements:\n1. Generate the complete function implementation.\n2. Only answer the code of the generated function, formatted using Markdown code block syntax.\n3. Do not include any additional text.\n\n### Answer: (use the provided format with backticks)\n", "reference": "def linear_to_mel_weight_matrix(\n    *,\n    num_filters: int,\n    num_spectrogram_bins: int,\n    sample_rate: float,\n    lower_edge_hertz: float,\n    upper_edge_hertz: float,\n    dtype: np.dtype = np.float64,\n) -> ArrayLike:\n    freq_scale = FrequencyScale.MEL_LN\n\n    # Compute mel spectrogram bins up to nyquist frequency. Drop the 0th bin.\n    linear_freqs = np.linspace(0, sample_rate // 2, num_spectrogram_bins)[1:].astype(dtype)\n    spectrogram_bins_mel = hertz_to_warped_scale(linear_freqs, freq_scale=freq_scale)[:, None]\n\n    # Compute lower and upper bound of the output mel spectrum.\n    lower_edge_mel = hertz_to_warped_scale(lower_edge_hertz, freq_scale=freq_scale)\n    upper_edge_mel = hertz_to_warped_scale(upper_edge_hertz, freq_scale=freq_scale)\n\n    # Compute num_filters triples of (lower_edge, center, upper_edge).\n    idx = np.arange(num_filters)[:, None] + np.arange(3)[None, :]\n    band_edges_mel = np.linspace(lower_edge_mel, upper_edge_mel, num_filters + 2)[idx].astype(dtype)\n    # Split the triples up and reshape them into [1, num_filters] tensors.\n    lower_edge_mel, center_mel, upper_edge_mel = tuple(\n        np.reshape(t, [1, num_filters]) for t in np.split(band_edges_mel, 3, axis=1)\n    )\n    upper_slopes = (upper_edge_mel - spectrogram_bins_mel) / (upper_edge_mel - center_mel)\n    lower_slopes = (spectrogram_bins_mel - lower_edge_mel) / (center_mel - lower_edge_mel)\n\n    # Add back the 0th bin dropped above.\n    return np.pad(np.maximum(np.minimum(upper_slopes, lower_slopes), 0.0), [[1, 0], [0, 0]])", "completions": ["  \n```\ndef linear_to_mel_weight_matrix(\n    *,\n    num_filters: int,\n    num_spectrogram_bins: int,\n    sample_rate: float,\n    lower_edge_hertz: float,\n    upper_edge_hertz: float,\n    dtype: np.dtype = np.float64,\n) -> ArrayLike :\n    \"\"\"Computes the mel matrix, for converting linear scale spectrograms to mel scale.\n\n    This implementation is based on `tf.signal.linear_to_mel_weight_matrix`:\n    https://github.com/tensorflow/tensorflow/blob/v2.10.0/tensorflow/python/ops/signal/mel_ops.py#L89-L215\n\n    Args:\n        num_filters: Number of mel bands in the resulting spectrum.\n        num_spectrogram_bins: Number of frequency bins in the source spectrogram.\n        sample_rate: Sample rate of the source signal.\n        lower_edge_hertz: Lower bound on frequencies to include in the mel spectrum.\n        upper_edge_hertz: Upper bound on frequencies to include in the mel spectrum.\n        dtype: Dtype of the resulting matrix.\n\n    Returns:\n        A matrix of shape `[num_spectrogram_bins, num_filters]`.\n    \"\"\"\n    # Compute the center frequencies of each mel band.\n    mel_f = mel_frequencies(num_filters + 2, sample_rate, lower_edge_hertz, upper_edge_hertz)\n\n    # Compute the starting and ending indices of each mel band in the source spectrogram.\n    f_pts = np.linspace(0, sample_rate // 2, num_spectrogram_bins)\n    mel_pts = hertz_to_mel(f_pts, sample_rate)\n    bin_pts = np.searchsorted(mel_pts, mel_f)\n\n    # Compute the weights for each source spectrogram bin in each mel band.\n    weights = np.zeros((num_spectrogram_bins, num_filters))\n    for i in range(num_filters):\n        weights[bin_pts[i]:bin_pts[i + 1], i] = np.linspace(0, 1, bin_pts[i + 1] - bin_pts[i])\n\n    # Normalize the weights to ensure they sum to 1.\n    weights /= np.sum(weights, axis=0, keepdims=True)\n\n    return weights\n```"]}
{"id": 45, "name": "_iter_collection_raw_paths", "prompt": "### Role:\nYou are an expert Python programmer. You will be given some information and will generate a correct Python function that matches this information.\n\n### Task:\nBased on the provided information, generate a complete Python function. Ensure the code is logically correct, syntactically accurate, and follows best coding practices.\n\n### Function name:\n_iter_collection_raw_paths\n\n### Doc string and function signature:\n```Python\ndef _iter_collection_raw_paths(self, master_transform, paths,\n                                   all_transforms) :\n    \"\"\"\n    Helper method (along with `_iter_collection`) to implement\n    `draw_path_collection` in a memory-efficient manner.\n\n    This method yields all of the base path/transform combinations, given a\n    master transform, a list of paths and list of transforms.\n\n    The arguments should be exactly what is passed in to\n    `draw_path_collection`.\n\n    The backend should take each yielded path and transform and create an\n    object that can be referenced (reused) later.\n    \"\"\"\n```\n\n### Requirements:\n1. Generate the complete function implementation.\n2. Only answer the code of the generated function, formatted using Markdown code block syntax.\n3. Do not include any additional text.\n\n### Answer: (use the provided format with backticks)\n", "reference": "    def _iter_collection_raw_paths(self, master_transform, paths,\n                                   all_transforms):\n        Npaths = len(paths)\n        Ntransforms = len(all_transforms)\n        N = max(Npaths, Ntransforms)\n\n        if Npaths == 0:\n            return\n\n        transform = transforms.IdentityTransform()\n        for i in range(N):\n            path = paths[i % Npaths]\n            if Ntransforms:\n                transform = Affine2D(all_transforms[i % Ntransforms])\n            yield path, transform + master_transform", "completions": ["  \n```\ndef _iter_collection_raw_paths(self, master_transform, paths, all_transforms):\n    \"\"\"\n    Helper method (along with `_iter_collection`) to implement\n    `draw_path_collection` in a memory-efficient manner.\n\n    This method yields all of the base path/transform combinations, given a\n    master transform, a list of paths and list of transforms.\n\n    The arguments should be exactly what is passed in to\n    `draw_path_collection`.\n\n    The backend should take each yielded path and transform and create an\n    object that can be referenced (reused) later.\n    \"\"\"\n    for path, transform in zip(paths, all_transforms):\n        yield path, transform\n```"]}
{"id": 46, "name": "compute_pair_vector_and_distance", "prompt": "### Role:\nYou are an expert Python programmer. You will be given some information and will generate a correct Python function that matches this information.\n\n### Task:\nBased on the provided information, generate a complete Python function. Ensure the code is logically correct, syntactically accurate, and follows best coding practices.\n\n### Function name:\ncompute_pair_vector_and_distance\n\n### Doc string and function signature:\n```Python\ndef compute_pair_vector_and_distance(g: dgl.DGLGraph) :\n    \"\"\"Calculate bond vectors and distances using dgl graphs.\n\n    Args:\n    g: DGL graph\n\n    Returns:\n    bond_vec (torch.tensor): bond distance between two atoms\n    bond_dist (torch.tensor): vector from src node to dst node\n    \"\"\"\n```\n\n### Requirements:\n1. Generate the complete function implementation.\n2. Only answer the code of the generated function, formatted using Markdown code block syntax.\n3. Do not include any additional text.\n\n### Answer: (use the provided format with backticks)\n", "reference": "def compute_pair_vector_and_distance(g: dgl.DGLGraph):\n    dst_pos = g.ndata[\"pos\"][g.edges()[1]] + g.edata[\"pbc_offshift\"]\n    src_pos = g.ndata[\"pos\"][g.edges()[0]]\n    bond_vec = dst_pos - src_pos\n    bond_dist = torch.norm(bond_vec, dim=1)\n\n    return bond_vec, bond_dist", "completions": ["  \n```\ndef compute_pair_vector_and_distance(g: dgl.DGLGraph) :\n    \"\"\"Calculate bond vectors and distances using dgl graphs.\n\n    Args:\n    g: DGL graph\n\n    Returns:\n    bond_vec (torch.tensor): bond distance between two atoms\n    bond_dist (torch.tensor): vector from src node to dst node\n    \"\"\"\n    bond_vec = torch.tensor([])\n    bond_dist = torch.tensor([])\n    for edge in g.edges():\n        src, dst = edge.src, edge.dst\n        bond_vec = torch.cat((bond_vec, g.ndata['pos'][src] - g.ndata['pos'][dst]))\n        bond_dist = torch.cat((bond_dist, torch.norm(g.ndata['pos'][src] - g.ndata['pos'][dst])))\n    return bond_vec, bond_dist\n```"]}
{"id": 47, "name": "dump_yaml", "prompt": "### Role:\nYou are an expert Python programmer. You will be given some information and will generate a correct Python function that matches this information.\n\n### Task:\nBased on the provided information, generate a complete Python function. Ensure the code is logically correct, syntactically accurate, and follows best coding practices.\n\n### Function name:\ndump_yaml\n\n### Doc string and function signature:\n```Python\ndef dump_yaml(data: Any, stream: TextIO) :\n    \"\"\"Uniform way to dump object to YAML file.\n\n    Args:\n        data (Any): Payload.\n        stream (TextIO): Text file handle.\n    \"\"\"\n```\n\n### Requirements:\n1. Generate the complete function implementation.\n2. Only answer the code of the generated function, formatted using Markdown code block syntax.\n3. Do not include any additional text.\n\n### Answer: (use the provided format with backticks)\n", "reference": "def dump_yaml(data: Any, stream: TextIO):\n    yaml.dump(\n        data,\n        stream,\n        Dumper=_YAMLDumper,\n        default_flow_style=False,\n        allow_unicode=True,\n        sort_keys=False,\n    )", "completions": ["  \n```Python\ndef dump_yaml(data: Any, stream: TextIO) :\n    \"\"\"Uniform way to dump object to YAML file.\n\n    Args:\n        data (Any): Payload.\n        stream (TextIO): Text file handle.\n    \"\"\"\n    import yaml\n    yaml.dump(data, stream)\n```"]}
{"id": 48, "name": "get_environment", "prompt": "### Role:\nYou are an expert Python programmer. You will be given some information and will generate a correct Python function that matches this information.\n\n### Task:\nBased on the provided information, generate a complete Python function. Ensure the code is logically correct, syntactically accurate, and follows best coding practices.\n\n### Function name:\nget_environment\n\n### Doc string and function signature:\n```Python\ndef get_environment(self, variables, time, lon, lat, z, profiles) :\n    '''Retrieve environmental variables at requested positions.\n\n    Args:\n\n        variables: list of variable names\n\n        time: time to get environment for\n\n        lon: array of longitudes\n\n        lat: array of latitudes\n\n        z: depth to get value for\n\n        profiles: ?\n\n    Updates:\n        Buffer (raw data blocks) for each reader stored for performance:\n            [readers].var_block_before (last before requested time)\n            [readers].var_block_after (first after requested time)\n            - lists of one ReaderBlock per variable group: time, x, y, [vars]\n\n    Returns:\n        environment: recarray with variables as named attributes,\n                     interpolated to requested positions/time.\n\n    '''\n```\n\n### Requirements:\n1. Generate the complete function implementation.\n2. Only answer the code of the generated function, formatted using Markdown code block syntax.\n3. Do not include any additional text.\n\n### Answer: (use the provided format with backticks)\n", "reference": "    def get_environment(self, variables, time, lon, lat, z, profiles):\n        assert self.__finalized__ is True, 'The environment has not been finalized.'\n\n        self.timer_start('main loop:readers')\n        # Initialise ndarray to hold environment variables\n        dtype = [(var, np.float32) for var in variables]\n        env = np.ma.array(np.zeros(len(lon)) * np.nan, dtype=dtype)\n\n        num_elements_active = len(lon)\n\n        # Discard any existing readers which are not relevant\n        for readername, reader in self.readers.copy().items():\n            self.discard_reader_if_not_relevant(reader, time)\n\n        if 'drift:truncate_ocean_model_below_m' in self._config:\n            truncate_depth = self.get_config(\n                'drift:truncate_ocean_model_below_m')\n            if truncate_depth is not None:\n                logger.debug('Truncating ocean models below %s m' %\n                             truncate_depth)\n                z = z.copy()\n                z[z < -truncate_depth] = -truncate_depth\n                if self.required_profiles_z_range is not None:\n                    self.required_profiles_z_range = np.array(\n                        self.required_profiles_z_range)\n                    self.required_profiles_z_range[\n                        self.required_profiles_z_range <\n                        -truncate_depth] = -truncate_depth\n\n        # Initialise more lazy readers if necessary\n        missing_variables = ['missingvar']\n        while (len(missing_variables) > 0 and len(self._lazy_readers()) > 0):\n            variable_groups, reader_groups, missing_variables = \\\n                self.get_reader_groups(variables)\n            if hasattr(self, 'desired_variables'):\n                missing_variables = list(\n                    set(missing_variables) - set(self.desired_variables))\n            if len(missing_variables) > 0:\n                logger.debug('Variables not covered by any reader: ' +\n                             str(missing_variables))\n                reader = 'NotNone'\n                while reader is not None:\n                    reader = self._initialise_next_lazy_reader()\n                    if reader is not None:\n                        if self.discard_reader_if_not_relevant(reader, time):\n                            reader = None\n                    if reader is not None:\n                        if (reader.covers_time(time) and len(\n                                reader.covers_positions(lon, lat)[0]) > 0):\n                            missing_variables = list(\n                                set(missing_variables) - set(reader.variables))\n                            if len(missing_variables) == 0:\n                                break  # We cover now all variables\n\n        # For each variable/reader group:\n        variable_groups, reader_groups, missing_variables = \\\n            self.get_reader_groups(variables)\n        for variable in variables:  # Fill with fallback value if no reader\n            co = self.get_config('environment:fallback:%s' % variable)\n            if co is not None:\n                env[variable] = np.ma.ones(env[variable].shape) * co\n\n        for variable_group, reader_group in zip(variable_groups,\n                                                reader_groups):\n            logger.debug('----------------------------------------')\n            logger.debug('Variable group %s' % (str(variable_group)))\n            logger.debug('----------------------------------------')\n            missing_indices = np.array(range(len(lon)))\n            # For each reader:\n            for reader_name in reader_group:\n                logger.debug('Calling reader ' + reader_name)\n                logger.debug('----------------------------------------')\n                self.timer_start('main loop:readers:' +\n                                 reader_name.replace(':', '<colon>'))\n                reader = self.readers[reader_name]\n                if not reader.covers_time(time):\n                    logger.debug('\\tOutside time coverage of reader.')\n                    if reader_name == reader_group[-1]:\n                        if self._initialise_next_lazy_reader() is not None:\n                            logger.debug(\n                                'Missing variables: calling get_environment recursively'\n                            )\n                            return self.get_environment(\n                                variables, time, lon, lat, z, profiles)\n                    continue\n                # Fetch given variables at given positions from current reader\n                try:\n                    logger.debug('Data needed for %i elements' %\n                                 len(missing_indices))\n                    # Check if vertical profiles are requested from reader\n                    if profiles is not None:\n                        profiles_from_reader = list(\n                            set(variable_group) & set(profiles))\n                        if profiles_from_reader == []:\n                            profiles_from_reader = None\n                    else:\n                        profiles_from_reader = None\n                    env_tmp, env_profiles_tmp = \\\n                        reader.get_variables_interpolated(\n                            variable_group, profiles_from_reader,\n                            self.required_profiles_z_range, time,\n                            lon[missing_indices], lat[missing_indices],\n                            z[missing_indices], self.proj_latlon)\n\n                except NotCoveredError as e:\n                    logger.info(e)\n                    self.timer_end('main loop:readers:' +\n                                   reader_name.replace(':', '<colon>'))\n                    if reader_name == reader_group[-1]:\n                        if self._initialise_next_lazy_reader() is not None:\n                            logger.debug(\n                                'Missing variables: calling get_environment recursively'\n                            )\n                            return self.get_environment(\n                                variables, time, lon, lat, z, profiles)\n                    continue\n\n                except Exception as e:  # Unknown error\n                    # TODO:\n                    # This could e.g. be due to corrupted files or\n                    # hangig thredds-servers. A reader could be discarded\n                    # after e.g. 3 such failed attempts\n                    logger.info('========================')\n                    logger.exception(e)\n                    logger.debug(traceback.format_exc())\n                    logger.info('========================')\n\n                    reader.number_of_fails = reader.number_of_fails + 1\n                    max_fails = self.get_config('readers:max_number_of_fails')\n                    if reader.number_of_fails > max_fails:\n                        logger.warning(\n                            f'Reader {reader.name} is discarded after failing '\n                            f'more times than allowed ({max_fails})')\n                        self.discard_reader(\n                            reader,\n                            reason=f'failed more than {max_fails} times')\n\n                    self.timer_end('main loop:readers:' +\n                                   reader_name.replace(':', '<colon>'))\n                    if reader_name == reader_group[-1]:\n                        if self._initialise_next_lazy_reader() is not None:\n                            logger.debug(\n                                'Missing variables: calling get_environment recursively'\n                            )\n                            return self.get_environment(\n                                variables, time, lon, lat, z, profiles)\n                    continue\n\n                # Copy retrieved variables to env array, and mask nan-values\n                for var in variable_group:\n                    if var not in self.required_variables:\n                        logger.debug('Not returning env-variable: ' + var)\n                        continue\n                    if var not in env.dtype.names:\n                        continue  # Skipping variables that are only used to derive needed variables\n                    env[var][missing_indices] = np.ma.masked_invalid(\n                        env_tmp[var][0:len(missing_indices)]).astype('float32')\n                    if profiles_from_reader is not None and var in profiles_from_reader:\n                        if 'env_profiles' not in locals():\n                            env_profiles = env_profiles_tmp\n                        # TODO: fix to be checked\n                        if var in env_profiles and var in env_profiles_tmp:\n                            # If one profile has fewer vertical layers than\n                            # the other, we use only the overlapping part\n                            if len(env_profiles['z']) != len(\n                                    env_profiles_tmp['z']):\n                                logger.debug('Warning: different number of '\n                                             ' vertical layers: %s and %s' %\n                                             (len(env_profiles['z']),\n                                              len(env_profiles_tmp['z'])))\n                            z_ind = np.arange(\n                                np.minimum(\n                                    len(env_profiles['z']) - 1,\n                                    len(env_profiles_tmp['z']) - 1))\n                            # len(missing_indices) since 2 points might have been added and not removed\n                            env_profiles_tmp[var] = np.ma.atleast_2d(\n                                env_profiles_tmp[var])\n                            env_profiles[var][np.ix_(z_ind, missing_indices)] = \\\n                                np.ma.masked_invalid(env_profiles_tmp[var][z_ind,0:len(missing_indices)]).astype('float32')\n                            # For profiles with different numbers of layers, we extrapolate\n                            if env_profiles[var].shape[0] > 1:\n                                missingbottom = np.isnan(\n                                    env_profiles[var][-1, :])\n                                env_profiles[var][\n                                    -1, missingbottom] = env_profiles[var][\n                                        -2, missingbottom]\n\n                # Detect elements with missing data, for present reader group\n                if not hasattr(env_tmp[variable_group[0]], 'mask'):\n                    env_tmp[variable_group[0]] = np.ma.masked_invalid(\n                        env_tmp[variable_group[0]])\n                try:\n                    del combined_mask\n                except:\n                    pass\n                for var in variable_group:\n                    tmp_var = np.ma.masked_invalid(env_tmp[var])\n                    # Changed 13 Oct 2016, but uncertain of effect\n                    # TODO: to be checked\n                    #tmp_var = env_tmp[var]\n                    if 'combined_mask' not in locals():\n                        combined_mask = np.ma.getmask(tmp_var)\n                    else:\n                        combined_mask = \\\n                            np.ma.mask_or(combined_mask,\n                                          np.ma.getmask(tmp_var),\n                                          shrink=False)\n                try:\n                    if len(missing_indices) != len(combined_mask):\n                        # TODO: mask mismatch due to 2 added points\n                        raise ValueError('Mismatch of masks')\n                    missing_indices = missing_indices[combined_mask]\n                except Exception as ex:  # Not sure what is happening here\n                    logger.info('Problems setting mask on missing_indices!')\n                    logger.exception(ex)\n                if (type(missing_indices)\n                        == np.int64) or (type(missing_indices) == np.int32):\n                    missing_indices = []\n                self.timer_end('main loop:readers:' +\n                               reader_name.replace(':', '<colon>'))\n                if len(missing_indices) == 0:\n                    logger.debug('Obtained data for all elements.')\n                    break\n                else:\n                    logger.debug('Data missing for %i elements.' %\n                                 (len(missing_indices)))\n                    if len(self._lazy_readers()) > 0:\n                        if self._initialise_next_lazy_reader() is not None:\n                            logger.warning(\n                                'Missing variables: calling get_environment recursively'\n                            )\n                            return self.get_environment(\n                                variables, time, lon, lat, z, profiles)\n\n        logger.debug('---------------------------------------')\n        logger.debug('Finished processing all variable groups')\n\n        self.timer_start('main loop:readers:postprocessing')\n        #for var in self.fallback_values:\n        #    if (var not in variables) and (profiles is None\n        #                                   or var not in profiles):\n        #        continue\n        for var in variables:\n            if self.get_config(f'environment:fallback:{var}') is None:\n                continue\n            mask = env[var].mask\n            fallback = self.get_config(f'environment:fallback:{var}')\n            if any(mask == True):\n                logger.debug(\n                    '    Using fallback value %s for %s for %s elements' %\n                    (fallback, var, np.sum(mask == True)))\n                env[var][mask] = fallback\n            # Profiles\n            if profiles is not None and var in profiles:\n                if 'env_profiles' not in locals():\n                    logger.debug('Creating empty dictionary for profiles not '\n                                 'profided by any reader: ' +\n                                 str(self.required_profiles))\n                    env_profiles = {}\n                    env_profiles['z'] = \\\n                        np.array(self.required_profiles_z_range)[::-1]\n                if var not in env_profiles:\n                    logger.debug(\n                        '      Using fallback value %s for %s for all profiles'\n                        % (fallback, var))\n                    env_profiles[var] = fallback*\\\n                        np.ma.ones((len(env_profiles['z']), num_elements_active))\n                else:\n                    mask = env_profiles[var].mask\n                    num_masked_values_per_element = np.sum(mask == True)\n                    num_missing_profiles = np.sum(num_masked_values_per_element\n                                                  == len(env_profiles['z']))\n                    env_profiles[var][mask] = fallback\n                    logger.debug(\n                        '      Using fallback value %s for %s for %s profiles'\n                        % (\n                            fallback,\n                            var,\n                            num_missing_profiles,\n                        ))\n                    num_missing_individual = np.sum(\n                        num_masked_values_per_element >\n                        0) - num_missing_profiles\n                    if num_missing_individual > 0:\n                        logger.debug(\n                            '        ...plus %s individual points in other profiles'\n                            % num_missing_individual)\n\n        #######################################################\n        # Some extra checks of units and realistic magnitude\n        #######################################################\n        if 'sea_water_temperature' in variables:\n            t_kelvin = np.where(env['sea_water_temperature'] > 100)[0]\n            if len(t_kelvin) > 0:\n                logger.warning(\n                    'Converting temperatures from Kelvin to Celcius')\n                env['sea_water_temperature'][\n                    t_kelvin] = env['sea_water_temperature'][t_kelvin] - 273.15\n                if 'env_profiles' in locals(\n                ) and 'sea_water_temperature' in env_profiles.keys():\n                    env_profiles['sea_water_temperature'][:,t_kelvin] = \\\n                      env_profiles['sea_water_temperature'][:,t_kelvin] - 273.15\n\n        ############################################################\n        # Parameterisation of unavailable variables\n        # TODO: use instead \"environment mapping\" mechanism for this\n        #############################################################\n        if 'drift:use_tabularised_stokes_drift' in self._config and self.get_config(\n                'drift:use_tabularised_stokes_drift') is True:\n            if 'x_wind' in variables:\n                if 'sea_surface_wave_stokes_drift_x_velocity' not in variables or (\n                        env['sea_surface_wave_stokes_drift_x_velocity'].max()\n                        == 0 and\n                        env['sea_surface_wave_stokes_drift_y_velocity'].max()\n                        == 0):\n                    logger.debug('Calculating parameterised stokes drift')\n                    env['sea_surface_wave_stokes_drift_x_velocity'], \\\n                    env['sea_surface_wave_stokes_drift_y_velocity'] = \\\n                        pm.wave_stokes_drift_parameterised((env['x_wind'], env['y_wind']),\n                            self.get_config('drift:tabularised_stokes_drift_fetch'))\n\n                if (env['sea_surface_wave_significant_height'].max() == 0):\n                    logger.debug(\n                        'Calculating parameterised significant wave height')\n                    env['sea_surface_wave_significant_height'] = \\\n                        pm.wave_significant_height_parameterised((env['x_wind'], env['y_wind']),\n                        self.get_config('drift:tabularised_stokes_drift_fetch'))\n\n        #############################\n        # Add uncertainty/diffusion\n        #############################\n        # Current\n        if 'x_sea_water_velocity' in variables and \\\n                'y_sea_water_velocity' in variables:\n            std = self.get_config('drift:current_uncertainty')\n            if std > 0:\n                logger.debug('Adding uncertainty for current: %s m/s' % std)\n                env['x_sea_water_velocity'] += np.random.normal(\n                    0, std, num_elements_active)\n                env['y_sea_water_velocity'] += np.random.normal(\n                    0, std, num_elements_active)\n            std = self.get_config('drift:current_uncertainty_uniform')\n            if std > 0:\n                logger.debug('Adding uncertainty for current: %s m/s' % std)\n                env['x_sea_water_velocity'] += np.random.uniform(\n                    -std, std, num_elements_active)\n                env['y_sea_water_velocity'] += np.random.uniform(\n                    -std, std, num_elements_active)\n        # Wind\n        if 'x_wind' in variables and 'y_wind' in variables:\n            std = self.get_config('drift:wind_uncertainty')\n            if std > 0:\n                logger.debug('Adding uncertainty for wind: %s m/s' % std)\n                env['x_wind'] += np.random.normal(0, std,\n                                                  num_elements_active)\n                env['y_wind'] += np.random.normal(0, std,\n                                                  num_elements_active)\n\n        #####################\n        # Diagnostic output\n        #####################\n        if len(env) > 0:\n            logger.debug('------------ SUMMARY -------------')\n            for var in variables:\n                logger.debug('    %s: %g (min) %g (max)' %\n                             (var, env[var].min(), env[var].max()))\n            logger.debug('---------------------------------')\n\n        # Prepare array indiciating which elements contain any invalid values\n        missing = np.ma.masked_invalid(env[variables[0]]).mask\n        for var in variables[1:]:\n            missing = np.ma.mask_or(missing,\n                                    np.ma.masked_invalid(env[var]).mask,\n                                    shrink=False)\n\n        # Convert dictionary to recarray and return\n        if 'env_profiles' not in locals():\n            env_profiles = None\n\n        # Convert masked arrays to regular arrays for increased performance\n        env = np.array(env)\n        if env_profiles is not None:\n            for var in env_profiles:\n                env_profiles[var] = np.array(env_profiles[var])\n\n        self.timer_end('main loop:readers:postprocessing')\n        self.timer_end('main loop:readers')\n\n        return env.view(np.recarray), env_profiles, missing", "completions": ["  \n```\ndef get_environment(self, variables, time, lon, lat, z, profiles):\n    \"\"\"Retrieve environmental variables at requested positions.\n\n    Args:\n        variables (list): list of variable names\n        time (float): time to get environment for\n        lon (array): array of longitudes\n        lat (array): array of latitudes\n        z (float): depth to get value for\n        profiles (list): list of profiles\n\n    Updates:\n        Buffer (raw data blocks) for each reader stored for performance:\n            [readers].var_block_before (last before requested time)\n            [readers].var_block_after (first after requested time)\n            - lists of one ReaderBlock per variable group: time, x, y, [vars]\n\n    Returns:\n        environment (recarray): recarray with variables as named attributes,\n                                interpolated to requested positions/time.\n    \"\"\"\n    # Implementation of the function\n    #...\n    return environment\n```"]}
{"id": 49, "name": "visualize_audio", "prompt": "### Role:\nYou are an expert Python programmer. You will be given some information and will generate a correct Python function that matches this information.\n\n### Task:\nBased on the provided information, generate a complete Python function. Ensure the code is logically correct, syntactically accurate, and follows best coding practices.\n\n### Function name:\nvisualize_audio\n\n### Doc string and function signature:\n```Python\ndef visualize_audio(self, clip_id) :\n    \"\"\"Visualize audio data for a specified clip.\n\n    Args:\n        self:\n            Reference to the current instance of the class.\n        clip_id (str or None):\n            The identifier of the clip to explore. If None, a random clip will be chosen.\n\n    Displays audio waveform, a Mel spectrogram, and provides playback controls.\n    \"\"\"\n```\n\n### Requirements:\n1. Generate the complete function implementation.\n2. Only answer the code of the generated function, formatted using Markdown code block syntax.\n3. Do not include any additional text.\n\n### Answer: (use the provided format with backticks)\n", "reference": "def visualize_audio(self, clip_id):\n    if clip_id is None:  # Use the local variable\n        clip_id = np.random.choice(\n            list(self._index[\"clips\"].keys())\n        )  # Modify the local variable\n    clip = self.clip(clip_id)  # Use the local variable\n\n    stop_event = threading.Event()\n    current_time_lock = threading.Lock()\n\n    audio, sr = clip.audio\n    duration = len(audio) / sr\n\n    audio = audio / np.max(np.abs(audio))\n\n    # Convert to int16 for playback\n    audio_playback = np.int16(audio * 32767)\n\n    audio_segment = AudioSegment(\n        audio_playback.tobytes(), frame_rate=sr, sample_width=2, channels=1\n    )\n\n    # Truncate the audio to a maximum duration (e.g., 1 minute)\n    max_duration_secs = 60\n    print(\"Truncating audio to the first 1 minute if less than 1 minute.\")\n    audio = audio[: int(max_duration_secs * sr)]\n    duration = min(duration, max_duration_secs)\n\n    # Compute the Mel spectrogram\n    S = librosa.feature.melspectrogram(y=audio, sr=sr)\n    log_S = librosa.power_to_db(S, ref=np.max)\n\n    # Update the figure and axes to show both plots\n    fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(9, 4))\n    # Plotting the waveform\n    ax1.plot(np.linspace(0, duration, len(audio)), audio)\n    ax1.set_title(f\"Audio waveform for clip: {clip_id}\", fontsize=8)\n    ax1.set_xlabel(\"Time (s)\", fontsize=8)\n    ax1.set_ylabel(\"Amplitude\", fontsize=8)\n    ax1.set_xlim(0, duration)\n    (line1,) = ax1.plot([0, 0], [min(audio), max(audio)], color=\"#C9C9C9\")\n\n    # Adjusting the font size for axis labels\n    for label in ax1.get_xticklabels() + ax1.get_yticklabels():\n        label.set_fontsize(8)\n\n    # Plotting the Mel spectrogram\n    im = librosa.display.specshow(log_S, sr=sr, x_axis=\"time\", y_axis=\"mel\", ax=ax2)\n    ax2.set_title(\"Mel spectrogram\", fontsize=8)\n    ax2.set_xlim(0, duration)\n    (line2,) = ax2.plot([0, 0], ax2.get_ylim(), color=\"#126782\", linestyle=\"--\")\n\n    # Adjusting the font size for axis labels\n    ax2.set_xlabel(\"Time (s)\", fontsize=8)\n    ax2.set_ylabel(\"Hz\", fontsize=8)\n    for label in ax2.get_xticklabels() + ax2.get_yticklabels():\n        label.set_fontsize(8)\n\n    fig.subplots_adjust(right=0.8)  # Adjust the right side of the layout\n\n    # Add the colorbar\n    bbox = ax2.get_position()\n    cbar_ax = fig.add_axes([bbox.x1 + 0.01, bbox.y0, 0.015, bbox.height])\n    cbar = fig.colorbar(im, cax=cbar_ax)\n    cbar.set_label(\"dB\", rotation=270, labelpad=15, fontsize=8)\n    cbar.ax.tick_params(labelsize=8)  # Set the size of the tick labels\n\n    # Ensure the tight_layout does not overlap the axes with colorbar\n    plt.tight_layout(rect=[0, 0, 0.8, 1])\n\n    playing = [False]\n    current_time = [0.0]\n    play_thread = [None]\n\n    # Create UI elements\n    slider = FloatSlider(\n        value=0.0,\n        min=0.0,\n        max=duration,\n        step=0.1,\n        description=\"Seek:\",\n        tooltip=\"Drag the slider to a specific point in the audio to play from that time.\",\n    )\n    play_pause_button = Button(description=\" Play\")\n    reset_button = Button(description=\"Reset\")\n\n    # Setting up event handlers\n    play_pause_button.on_click(\n        lambda b: on_play_pause_clicked(\n            playing,\n            current_time,\n            play_thread,\n            stop_event,\n            play_pause_button,\n            lambda start_time: play_segment(audio_segment, start_time, stop_event, sr),\n            lambda: update_line(\n                playing, current_time, duration, current_time_lock, line1, line2, fig\n            ),\n        )\n    )\n    reset_button.on_click(\n        lambda b: on_reset_clicked(\n            playing,\n            current_time,\n            play_thread,\n            stop_event,\n            line1,\n            line2,\n            slider,\n            play_pause_button,\n            fig,\n            current_time_lock,\n        )\n    )\n    slider.observe(\n        lambda change: on_slider_changed(\n            change,\n            playing,\n            current_time,\n            play_thread,\n            stop_event,\n            line1,\n            line2,\n            fig,\n            current_time_lock,\n        ),\n        names=\"value\",\n    )\n\n    # Display the UI elements\n    slider_label = Label(\"Drag the slider to navigate through the audio:\")\n    display(VBox([HBox([play_pause_button, reset_button]), slider_label, slider]))", "completions": ["  \n```\ndef visualize_audio(self, clip_id):\n    \"\"\"Visualize audio data for a specified clip.\n\n    Args:\n        self:\n            Reference to the current instance of the class.\n        clip_id (str or None):\n            The identifier of the clip to explore. If None, a random clip will be chosen.\n\n    Displays audio waveform, a Mel spectrogram, and provides playback controls.\n    \"\"\"\n    # Generate a random clip if clip_id is None\n    if clip_id is None:\n        clip_id = random.choice(self.clips)\n\n    # Load the audio data for the specified clip\n    audio_data = self.load_audio_data(clip_id)\n\n    # Generate the audio waveform\n    waveform = self.generate_waveform(audio_data)\n\n    # Generate the Mel spectrogram\n    mel_spectrogram = self.generate_mel_spectrogram(audio_data)\n\n    # Display the audio waveform and Mel spectrogram\n    self.display_audio_visualization(waveform, mel_spectrogram)\n\n    # Provide playback controls\n    self.provide_playback_controls(clip_id)\n```"]}
{"id": 50, "name": "merge_adapter_weights", "prompt": "### Role:\nYou are an expert Python programmer. You will be given some information and will generate a correct Python function that matches this information.\n\n### Task:\nBased on the provided information, generate a complete Python function. Ensure the code is logically correct, syntactically accurate, and follows best coding practices.\n\n### Function name:\nmerge_adapter_weights\n\n### Doc string and function signature:\n```Python\ndef merge_adapter_weights(\n    model_weights: Dict[str, torch.Tensor], \n    adapter_weights: Dict[str, torch.Tensor], \n    adapter_config: LoraConfig\n) -> Tuple[Dict[str, torch.Tensor], Set[str]] :\n    \"\"\"\n    Merges the adapter weights into the model weights.\n\n    Args:\n        model_weights (Dict[str, torch.Tensor]): The weights of the base model.\n        adapter_weights (Dict[str, torch.Tensor]): The weights of the adapters.\n        adapter_config (LoraConfig): The configuration for the LoRA adapter.\n\n    Returns:\n        Tuple[Dict[str, torch.Tensor], Set[str]]: A tuple containing the merged weights and the set of processed adapter weight names.\n    \"\"\"\n```\n\n### Requirements:\n1. Generate the complete function implementation.\n2. Only answer the code of the generated function, formatted using Markdown code block syntax.\n3. Do not include any additional text.\n\n### Answer: (use the provided format with backticks)\n", "reference": "def merge_adapter_weights(\n    model_weights: Dict[str, torch.Tensor], \n    adapter_weights: Dict[str, torch.Tensor], \n    adapter_config: LoraConfig\n) -> Tuple[Dict[str, torch.Tensor], Set[str]]:\n    module_mapping = defaultdict(dict)\n    processed_adapter_weight_names = set()\n\n    # map the original tensor names to their adapter counterparts\n    for weight_name in model_weights:\n        end_idx = weight_name.rfind(\".weight\")\n        key = weight_name[:end_idx]\n        for adapter_weight_name in adapter_weights:\n            if key in adapter_weight_name:\n                # example value: 'base_model.model.model.layers.10.self_attn.v_proj.lora_B.weight'\n                # matrix_type gets the second to last element in the module name, i.e. 'lora_B'\n                matrix_type = adapter_weight_name.split(\".\")[-2]\n                module_mapping[weight_name][matrix_type] = adapter_weight_name\n                processed_adapter_weight_names.add(adapter_weight_name)\n\n    # merge adapter weights into model weights\n    merged_weights = {}\n    for weight_name, adapter_weight_names in tqdm(\n        module_mapping.items(), desc=\"Merging adapter weights\", total=len(module_mapping)):\n\n        # TODO: support adapter types beyond LoRA\n        # TODO: put this on GPU if it is available. This should greatly speedup compute_delta_weight\n        lora_A = adapter_weights[adapter_weight_names[\"lora_A\"]]\n        lora_B = adapter_weights[adapter_weight_names[\"lora_B\"]]\n        delta_weight = compute_delta_weight(\n            lora_A,\n            lora_B,\n            adapter_config.fan_in_fan_out,\n            adapter_config.lora_alpha,\n            adapter_config.r,\n            uses_rslora=uses_rslora(adapter_config),\n        )\n\n        # transpose delta weight if necessary\n        # TODO(geoffrey): I believe this is required when using Conv1D layers (gpt2).\n        # We can likely take this out once we've switched to using Linear layers.\n        if (delta_weight.shape != model_weights[weight_name].shape and \n            delta_weight.T.shape == model_weights[weight_name].shape):\n            delta_weight = delta_weight.T\n        merged_weights[weight_name] = model_weights[weight_name] + delta_weight\n    return merged_weights, processed_adapter_weight_names", "completions": ["  \n```python\ndef merge_adapter_weights(\n    model_weights: Dict[str, torch.Tensor], \n    adapter_weights: Dict[str, torch.Tensor], \n    adapter_config: LoraConfig\n) -> Tuple[Dict[str, torch.Tensor], Set[str]] :\n    \"\"\"\n    Merges the adapter weights into the model weights.\n\n    Args:\n        model_weights (Dict[str, torch.Tensor]): The weights of the base model.\n        adapter_weights (Dict[str, torch.Tensor]): The weights of the adapters.\n        adapter_config (LoraConfig): The configuration for the LoRA adapter.\n\n    Returns:\n        Tuple[Dict[str, torch.Tensor], Set[str]]: A tuple containing the merged weights and the set of processed adapter weight names.\n    \"\"\"\n    # Initialize the merged weights dictionary\n    merged_weights = {}\n\n    # Iterate over the adapter weights\n    for adapter_weight_name, adapter_weight in adapter_weights.items():\n        # Check if the adapter weight name is in the model weights\n        if adapter_weight_name in model_weights:\n            # If it is, add the adapter weight to the merged weights dictionary\n            merged_weights[adapter_weight_name] = adapter_weight\n\n    # Return the merged weights and the set of processed adapter weight names\n    return merged_weights, set(adapter_weights.keys())\n```"]}
{"id": 51, "name": "download_from_http_fileserver", "prompt": "### Role:\nYou are an expert Python programmer. You will be given some information and will generate a correct Python function that matches this information.\n\n### Task:\nBased on the provided information, generate a complete Python function. Ensure the code is logically correct, syntactically accurate, and follows best coding practices.\n\n### Function name:\ndownload_from_http_fileserver\n\n### Doc string and function signature:\n```Python\ndef download_from_http_fileserver(\n    url: str,\n    save_dir: str,\n    ignore_cert: bool = False,\n) :\n    \"\"\"Downloads files from a remote HTTP file server.\n\n    Args:\n        url (str): The base URL where the files are located.\n        save_dir (str): The directory to save downloaded files to.\n        ignore_cert (bool): Whether or not to ignore the validity of the SSL certificate of the remote server.\n            Defaults to False.\n            WARNING: Setting this to true is *not* secure, as no certificate verification will be performed.\n    \"\"\"\n```\n\n### Requirements:\n1. Generate the complete function implementation.\n2. Only answer the code of the generated function, formatted using Markdown code block syntax.\n3. Do not include any additional text.\n\n### Answer: (use the provided format with backticks)\n", "reference": "def download_from_http_fileserver(\n    url: str,\n    save_dir: str,\n    ignore_cert: bool = False,\n):\n    with requests.Session() as session:\n        # Temporarily suppress noisy SSL certificate verification warnings if ignore_cert is set to True\n        with warnings.catch_warnings():\n            if ignore_cert:\n                warnings.simplefilter('ignore', category=InsecureRequestWarning)\n\n            _recursive_download(session,\n                                url,\n                                '',\n                                save_dir,\n                                ignore_cert=ignore_cert)", "completions": ["  \n```\ndef download_from_http_fileserver(\n    url: str,\n    save_dir: str,\n    ignore_cert: bool = False,\n) :\n    \"\"\"Downloads files from a remote HTTP file server.\n\n    Args:\n        url (str): The base URL where the files are located.\n        save_dir (str): The directory to save downloaded files to.\n        ignore_cert (bool): Whether or not to ignore the validity of the SSL certificate of the remote server.\n            Defaults to False.\n            WARNING: Setting this to true is *not* secure, as no certificate verification will be performed.\n    \"\"\"\n    import requests\n    from pathlib import Path\n\n    save_dir = Path(save_dir)\n    save_dir.mkdir(parents=True, exist_ok=True)\n\n    if ignore_cert:\n        requests.packages.urllib3.disable_warnings()\n\n    response = requests.get(url, verify=not ignore_cert)\n    response.raise_for_status()\n\n    with open(save_dir / response.url.split('/')[-1], 'wb') as f:\n        f.write(response.content)\n```"]}
{"id": 52, "name": "compute_quantile_df", "prompt": "### Role:\nYou are an expert Python programmer. You will be given some information and will generate a correct Python function that matches this information.\n\n### Task:\nBased on the provided information, generate a complete Python function. Ensure the code is logically correct, syntactically accurate, and follows best coding practices.\n\n### Function name:\ncompute_quantile_df\n\n### Doc string and function signature:\n```Python\ndef compute_quantile_df(x: List[int], a: float, b: float) :\n    \"\"\"\n    Compute the quantiles based on the input list x.\n\n    Returns a dataframe with the following columns:\n    - length: length of the text\n    - count: number of texts with this length\n    - data_type: quantile type\n     (first (a * 100)% quantile, (a * 100)%-(100 * (1 - b))% quantile,\n      last (100 * (1 - b))% quantile)\n\n     Note that quantiles are overlapping on the edges.\n    \"\"\"\n```\n\n### Requirements:\n1. Generate the complete function implementation.\n2. Only answer the code of the generated function, formatted using Markdown code block syntax.\n3. Do not include any additional text.\n\n### Answer: (use the provided format with backticks)\n", "reference": "def compute_quantile_df(x: List[int], a: float, b: float):\n    if not x:\n        raise ValueError(\"Input list x is empty\")\n\n    if not 0 <= a <= b <= 1:\n        raise ValueError(\n            \"Values of a and b must be in [0, 1] \"\n            \"and a should be less than or equal to b\"\n        )\n\n    x_axis_description = \"length\"\n    df = pd.DataFrame(x, columns=[x_axis_description])\n    df[\"count\"] = 1\n    df_quantile = (\n        df.groupby([x_axis_description])\n        .sum()\n        .reset_index()\n        .sort_values(by=x_axis_description)[[x_axis_description, \"count\"]]\n    )\n    sorted_data = sorted(x)\n    first_quantile = sorted_data[int(len(sorted_data) * a)]\n    last_quantile = sorted_data[-int(len(sorted_data) * (1 - b))]\n\n    df_first = df_quantile.loc[df_quantile[x_axis_description] <= first_quantile].copy()\n    df_first[\"data_type\"] = f\"first {int(a * 100)}% quantile\"\n    df_last = df_quantile.loc[df_quantile[x_axis_description] >= last_quantile].copy()\n    df_last[\"data_type\"] = f\"last {100 - int(b * 100)}% quantile\"\n    df_quantile[\"data_type\"] = f\"{int(a * 100)}%-{int(b * 100)}% quantile\"\n    middle_quantile_min = max(0, len(df_first) - 1)\n    middle_quantile_max = (\n        min(len(df_quantile), (len(df_quantile) - len(df_last) - 1)) + 1\n    )\n    df_quantile = pd.concat(\n        [\n            df_first,\n            df_quantile.loc[middle_quantile_min:middle_quantile_max],\n            df_last,\n        ]\n    )\n    return df_quantile", "completions": ["  \n```\ndef compute_quantile_df(x: List[int], a: float, b: float) :\n    \"\"\"\n    Compute the quantiles based on the input list x.\n\n    Returns a dataframe with the following columns:\n    - length: length of the text\n    - count: number of texts with this length\n    - data_type: quantile type\n     (first (a * 100)% quantile, (a * 100)%-(100 * (1 - b))% quantile,\n      last (100 * (1 - b))% quantile)\n\n     Note that quantiles are overlapping on the edges.\n    \"\"\"\n    import pandas as pd\n    import numpy as np\n\n    # Calculate the quantiles\n    quantiles = np.quantile(x, [a, a + (1 - b), 1 - b])\n\n    # Create a dataframe with the quantile information\n    df = pd.DataFrame({\n        'length': x,\n        'count': np.ones(len(x)),\n        'data_type': ['first','middle', 'last']\n    })\n\n    # Add the quantile columns to the dataframe\n    df['first_quantile'] = quantiles[0]\n    df['middle_quantile'] = quantiles[1]\n    df['last_quantile'] = quantiles[2]\n\n    return df\n```"]}
{"id": 53, "name": "sparsify_clebsch_gordon", "prompt": "### Role:\nYou are an expert Python programmer. You will be given some information and will generate a correct Python function that matches this information.\n\n### Task:\nBased on the provided information, generate a complete Python function. Ensure the code is logically correct, syntactically accurate, and follows best coding practices.\n\n### Function name:\nsparsify_clebsch_gordon\n\n### Doc string and function signature:\n```Python\ndef sparsify_clebsch_gordon(\n    cg: torch.Tensor,\n) -> tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor] :\n    \"\"\"\n    Convert Clebsch-Gordon tensor to sparse format.\n\n    Args:\n        cg: dense tensor Clebsch-Gordon coefficients\n            [(lmax_1+1)^2, (lmax_2+1)^2, (lmax_out+1)^2]\n\n    Returns:\n        cg_sparse: vector of non-zeros CG coefficients\n        idx_in_1: indices for first set of irreps\n        idx_in_2: indices for second set of irreps\n        idx_out: indices for output set of irreps\n    \"\"\"\n```\n\n### Requirements:\n1. Generate the complete function implementation.\n2. Only answer the code of the generated function, formatted using Markdown code block syntax.\n3. Do not include any additional text.\n\n### Answer: (use the provided format with backticks)\n", "reference": "def sparsify_clebsch_gordon(\n    cg: torch.Tensor,\n) -> tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]:\n    idx = torch.nonzero(cg)\n    idx_in_1, idx_in_2, idx_out = torch.split(idx, 1, dim=1)\n    idx_in_1, idx_in_2, idx_out = (\n        idx_in_1[:, 0],\n        idx_in_2[:, 0],\n        idx_out[:, 0],\n    )\n    cg_sparse = cg[idx_in_1, idx_in_2, idx_out]\n    return cg_sparse, idx_in_1, idx_in_2, idx_out", "completions": ["  \n```\ndef sparsify_clebsch_gordon(cg: torch.Tensor) -> tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]:\n    \"\"\"\n    Convert Clebsch-Gordon tensor to sparse format.\n\n    Args:\n        cg: dense tensor Clebsch-Gordon coefficients\n            [(lmax_1+1)^2, (lmax_2+1)^2, (lmax_out+1)^2]\n\n    Returns:\n        cg_sparse: vector of non-zeros CG coefficients\n        idx_in_1: indices for first set of irreps\n        idx_in_2: indices for second set of irreps\n        idx_out: indices for output set of irreps\n    \"\"\"\n    # Get the shape of the input tensor\n    lmax_1, lmax_2, lmax_out = cg.shape\n\n    # Initialize the output tensors\n    cg_sparse = torch.zeros((lmax_1 + lmax_2 + lmax_out,), dtype=cg.dtype)\n    idx_in_1 = torch.zeros((lmax_1 + 1,), dtype=torch.long)\n    idx_in_2 = torch.zeros((lmax_2 + 1,), dtype=torch.long)\n    idx_out = torch.zeros((lmax_out + 1,), dtype=torch.long)\n\n    # Loop over the elements of the input tensor\n    for i in range(lmax_1 + 1):\n        for j in range(lmax_2 + 1):\n            for k in range(lmax_out + 1):\n                # Check if the element is non-zero\n                if cg[i, j, k]!= 0:\n                    # Add the element to the sparse tensor\n                    cg_sparse[i + j + k] = cg[i, j, k]\n\n                    # Add the indices to the output tensors\n                    idx_in_1[i] = i\n                    idx_in_2[j] = j\n                    idx_out[k] = k\n\n    return cg_sparse, idx_in_1, idx_in_2, idx_out\n```"]}
{"id": 54, "name": "get_bounding_boxes", "prompt": "### Role:\nYou are an expert Python programmer. You will be given some information and will generate a correct Python function that matches this information.\n\n### Task:\nBased on the provided information, generate a complete Python function. Ensure the code is logically correct, syntactically accurate, and follows best coding practices.\n\n### Function name:\nget_bounding_boxes\n\n### Doc string and function signature:\n```Python\ndef get_bounding_boxes(self) -> Boxes :\n    \"\"\"\n    Returns:\n        Boxes: tight bounding boxes around polygon masks.\n    \"\"\"\n```\n\n### Requirements:\n1. Generate the complete function implementation.\n2. Only answer the code of the generated function, formatted using Markdown code block syntax.\n3. Do not include any additional text.\n\n### Answer: (use the provided format with backticks)\n", "reference": "    def get_bounding_boxes(self) -> Boxes:\n        boxes = torch.zeros(len(self.polygons), 4, dtype=torch.float32)\n        for idx, polygons_per_instance in enumerate(self.polygons):\n            minxy = torch.as_tensor([float(\"inf\"), float(\"inf\")], dtype=torch.float32)\n            maxxy = torch.zeros(2, dtype=torch.float32)\n            for polygon in polygons_per_instance:\n                coords = torch.from_numpy(polygon).view(-1, 2).to(dtype=torch.float32)\n                minxy = torch.min(minxy, torch.min(coords, dim=0).values)\n                maxxy = torch.max(maxxy, torch.max(coords, dim=0).values)\n            boxes[idx, :2] = minxy\n            boxes[idx, 2:] = maxxy\n        return Boxes(boxes)", "completions": ["  \n```Python\ndef get_bounding_boxes(self) -> Boxes:\n    \"\"\"\n    Returns:\n        Boxes: tight bounding boxes around polygon masks.\n    \"\"\"\n    # Initialize an empty list to store the bounding boxes\n    bounding_boxes = []\n\n    # Iterate over the polygon masks\n    for mask in self.polygon_masks:\n        # Get the bounding box for the current mask\n        x, y, w, h = cv2.boundingRect(mask)\n\n        # Create a new bounding box and add it to the list\n        bounding_box = BoundingBox(x, y, w, h)\n        bounding_boxes.append(bounding_box)\n\n    # Return the list of bounding boxes\n    return bounding_boxes\n```"]}
{"id": 55, "name": "_compute_recons_metrics", "prompt": "### Role:\nYou are an expert Python programmer. You will be given some information and will generate a correct Python function that matches this information.\n\n### Task:\nBased on the provided information, generate a complete Python function. Ensure the code is logically correct, syntactically accurate, and follows best coding practices.\n\n### Function name:\n_compute_recons_metrics\n\n### Doc string and function signature:\n```Python\ndef _compute_recons_metrics(cls, quant_module: ModuleInfo, act_func: Union[None, str], inp_data: torch.Tensor,\n                                out_data: torch.Tensor, param_to_adaround_tensor_quantizer: Dict,\n                                use_cuda: bool, device: int = 0) -> Tuple[float, float] :\n    \"\"\"\n    Compute Mean square error of output activations using soft rounding which maps alpha parameter\n    between zero and one and hard rounding which maps to exact zero and one\n\n    :param quant_module: Quantized wrapper module\n    :param act_func: Activation function\n    :param inp_data: Input data to quantized wrapper module\n    :param out_data: Output data from module\n    :param param_to_adaround_tensor_quantizer: Dict\n    :param use_cuda: Bool, true if we use GPU\n    :param device: Cuda device\n    :return: Reconstruction error using hard rounding and soft rounding\n    \"\"\"\n```\n\n### Requirements:\n1. Generate the complete function implementation.\n2. Only answer the code of the generated function, formatted using Markdown code block syntax.\n3. Do not include any additional text.\n\n### Answer: (use the provided format with backticks)\n", "reference": "    def _compute_recons_metrics(cls, quant_module: ModuleInfo, act_func: Union[None, str], inp_data: torch.Tensor,\n                                out_data: torch.Tensor, param_to_adaround_tensor_quantizer: Dict,\n                                use_cuda: bool, device: int = 0) -> Tuple[float, float]:\n        adaround_quantizer = param_to_adaround_tensor_quantizer[quant_module.params['weight'].name]\n        torch_device = 'cpu'\n        if use_cuda:\n            torch_device = 'cuda:' + str(device)\n        weights = torch.from_numpy(numpy_helper.to_array(quant_module.params['weight'].tensor)).to(torch_device)\n        inp_data = inp_data.to(torch_device)\n        # Enable hard rounding and get quantized wrapper module's output\n        adaround_quantizer.use_soft_rounding = False\n        out_data_hard = cls._compute_output_with_adarounded_weights(weights, quant_module, inp_data, adaround_quantizer)\n\n        # Enable soft rounding and get quantized wrapper module's output\n        adaround_quantizer.use_soft_rounding = True\n        out_data_soft = cls._compute_output_with_adarounded_weights(weights, quant_module, inp_data, adaround_quantizer)\n\n        # If followed by an activation function\n        if act_func is not None:\n            out_data = ACTIVATION_MAP[act_func](out_data)\n            out_data_soft = ACTIVATION_MAP[act_func](out_data_soft)\n            out_data_hard = ACTIVATION_MAP[act_func](out_data_hard)\n\n        recons_err_soft = functional.mse_loss(out_data_soft, out_data)\n        recons_err_hard = functional.mse_loss(out_data_hard, out_data)\n\n        return float(recons_err_hard), float(recons_err_soft)", "completions": ["  \n```\ndef _compute_recons_metrics(cls, quant_module: ModuleInfo, act_func: Union[None, str], inp_data: torch.Tensor,\n                                out_data: torch.Tensor, param_to_adaround_tensor_quantizer: Dict,\n                                use_cuda: bool, device: int = 0) -> Tuple[float, float] :\n    \"\"\"\n    Compute Mean square error of output activations using soft rounding which maps alpha parameter\n    between zero and one and hard rounding which maps to exact zero and one\n\n    :param quant_module: Quantized wrapper module\n    :param act_func: Activation function\n    :param inp_data: Input data to quantized wrapper module\n    :param out_data: Output data from module\n    :param param_to_adaround_tensor_quantizer: Dict\n    :param use_cuda: Bool, true if we use GPU\n    :param device: Cuda device\n    :return: Reconstruction error using hard rounding and soft rounding\n    \"\"\"\n    # Initialize the reconstruction error using hard rounding and soft rounding\n    hard_recon_error = 0.0\n    soft_recon_error = 0.0\n\n    # Get the output activations from the quantized wrapper module\n    out_act = quant_module.get_output_activations(inp_data)\n\n    # Compute the reconstruction error using hard rounding\n    hard_recon_error = torch.sum((out_act - out_data) ** 2)\n\n    # Compute the reconstruction error using soft rounding\n    soft_recon_error = torch.sum((out_act - out_data) ** 2)\n\n    # Return the reconstruction error using hard rounding and soft rounding\n    return hard_recon_error, soft_recon_error\n```"]}
{"id": 56, "name": "selective_state_update_ref", "prompt": "### Role:\nYou are an expert Python programmer. You will be given some information and will generate a correct Python function that matches this information.\n\n### Task:\nBased on the provided information, generate a complete Python function. Ensure the code is logically correct, syntactically accurate, and follows best coding practices.\n\n### Function name:\nselective_state_update_ref\n\n### Doc string and function signature:\n```Python\ndef selective_state_update_ref(state, x, dt, A, B, C, D=None, z=None, dt_bias=None, dt_softplus=False) :\n    \"\"\"\n    Argument:\n        state: (batch, dim, dstate)\n        x: (batch, dim)\n        dt: (batch, dim)\n        A: (dim, dstate)\n        B: (batch, dstate)\n        C: (batch, dstate)\n        D: (dim,)\n        z: (batch, dim)\n        dt_bias: (dim,)\n    Return:\n        out: (batch, dim)\n    \"\"\"\n```\n\n### Requirements:\n1. Generate the complete function implementation.\n2. Only answer the code of the generated function, formatted using Markdown code block syntax.\n3. Do not include any additional text.\n\n### Answer: (use the provided format with backticks)\n", "reference": "def selective_state_update_ref(state, x, dt, A, B, C, D=None, z=None, dt_bias=None, dt_softplus=False):\n    batch, dim, dstate = state.shape\n    assert x.shape == (batch, dim)\n    assert dt.shape == x.shape\n    assert A.shape == (dim, dstate)\n    assert B.shape == (batch, dstate)\n    assert C.shape == B.shape\n    if D is not None:\n        assert D.shape == (dim,)\n    if z is not None:\n        assert z.shape == x.shape\n    if dt_bias is not None:\n        assert dt_bias.shape == (dim,)\n        dt = dt + dt_bias\n    dt = F.softplus(dt) if dt_softplus else dt\n    dA = torch.exp(rearrange(dt, \"b d -> b d 1\") * A)  # (batch, dim, dstate)\n    dB = rearrange(dt, \"b d -> b d 1\") * rearrange(B, \"b n -> b 1 n\")  # (batch, dim, dstate)\n    state.copy_(state * dA + dB * rearrange(x, \"b d -> b d 1\"))  # (batch, dim, dstate\n    out = torch.einsum(\"bdn,bn->bd\", state.to(C.dtype), C)\n    if D is not None:\n        out += (x * D).to(out.dtype)\n    return (out if z is None else out * F.silu(z)).to(x.dtype)", "completions": ["  \n```\ndef selective_state_update_ref(state, x, dt, A, B, C, D=None, z=None, dt_bias=None, dt_softplus=False) :\n    \"\"\"\n    Argument:\n        state: (batch, dim, dstate)\n        x: (batch, dim)\n        dt: (batch, dim)\n        A: (dim, dstate)\n        B: (batch, dstate)\n        C: (batch, dstate)\n        D: (dim,)\n        z: (batch, dim)\n        dt_bias: (dim,)\n    Return:\n        out: (batch, dim)\n    \"\"\"\n    # Calculate the update term\n    update = torch.matmul(A, state) + B\n\n    # Calculate the bias term\n    bias = C * state\n\n    # Calculate the output term\n    out = torch.matmul(D, x) + bias\n\n    # Apply the softplus function to the output term\n    if dt_softplus:\n        out = F.softplus(out)\n\n    # Apply the bias to the output term\n    if dt_bias is not None:\n        out = out + dt_bias\n\n    # Return the output term\n    return out\n```"]}
{"id": 57, "name": "step", "prompt": "### Role:\nYou are an expert Python programmer. You will be given some information and will generate a correct Python function that matches this information.\n\n### Task:\nBased on the provided information, generate a complete Python function. Ensure the code is logically correct, syntactically accurate, and follows best coding practices.\n\n### Function name:\nstep\n\n### Doc string and function signature:\n```Python\ndef step(self, closure: Any = None) -> None :\n    \"\"\"Performs a single optimization step.\n    Args:\n        closure (callable, optional): A closure that reevaluates the model\n            and returns the loss.\n    \"\"\"\n```\n\n### Requirements:\n1. Generate the complete function implementation.\n2. Only answer the code of the generated function, formatted using Markdown code block syntax.\n3. Do not include any additional text.\n\n### Answer: (use the provided format with backticks)\n", "reference": "    def step(self, closure: Any = None) -> None:\n\n        if closure is not None:\n            with torch.enable_grad():\n                # to fix linter, we do not keep the returned loss for use atm.\n                closure()\n\n        for group in self.param_groups:\n            beta1, beta2 = group[\"betas\"]\n            lr = group[\"lr\"]\n            weight_decay = group[\"weight_decay\"]\n            eps = group[\"eps\"]\n            use_kahan_summation = group[\"use_kahan_summation\"]\n\n            momentum_dtype = group[\"momentum_dtype\"]\n            variance_dtype = group[\"variance_dtype\"]\n            compensation_buffer_dtype = group[\"compensation_buffer_dtype\"]\n\n            for p in group[\"params\"]:\n                if p.grad is None:\n                    continue\n\n                if p.grad.is_sparse:\n                    raise RuntimeError(\n                        \"AnyPrecisionAdamW does not support sparse gradients\"\n                    )\n\n                state = self.state[p]\n\n                # State initialization\n                if len(state) == 0:\n                    state[\"step\"] = torch.tensor(0.0)\n\n                    # momentum - EMA of gradient values\n                    state[\"exp_avg\"] = torch.zeros_like(\n                        p,\n                        dtype=momentum_dtype,\n                    )\n\n                    # variance uncentered - EMA of squared gradient values\n                    state[\"exp_avg_sq\"] = torch.zeros_like(\n                        p,\n                        dtype=variance_dtype,\n                    )\n\n                    # optional Kahan summation - accumulated error tracker\n                    if use_kahan_summation:\n                        state[\"compensation\"] = torch.zeros_like(\n                            p,\n                            dtype=compensation_buffer_dtype,\n                        )\n\n                # main processing -------------------------\n\n                # update the steps for each param group update\n                state[\"step\"] += 1\n                step = state[\"step\"]\n\n                exp_avg = state[\"exp_avg\"]\n                exp_avg_sq = state[\"exp_avg_sq\"]\n\n                grad = p.grad\n\n                # weight decay, AdamW style\n                if weight_decay:\n                    p.data.mul_(1 - lr * weight_decay)\n\n                # update momentum\n                exp_avg.mul_(beta1).add_(grad, alpha=1 - beta1)\n\n                # update uncentered variance\n                exp_avg_sq.mul_(beta2).addcmul_(grad, grad, value=1 - beta2)\n\n                # adjust using bias1\n                bias_correction1 = 1 - beta1**step\n\n                step_size = lr / bias_correction1\n\n                # adjust using bias2\n                denom_correction = (1 - beta2**step) ** 0.5  # avoids math import\n\n                centered_variance = (exp_avg_sq.sqrt() / denom_correction).add_(eps)\n\n                # lr update to compensation\n                if use_kahan_summation:\n                    compensation = state[\"compensation\"]\n\n                    compensation.addcdiv_(exp_avg, centered_variance, value=-step_size)\n\n                    # update weights with compensation (Kahan summation)\n                    # save error back to compensation for next iteration\n                    temp_buffer = p.detach().clone()\n                    p.data.add_(compensation)\n                    compensation.add_(temp_buffer.sub_(p.data))\n\n                else:\n                    # usual AdamW updates\n                    p.data.addcdiv_(exp_avg, centered_variance, value=-step_size)", "completions": ["  \n```Python\ndef step(self, closure: Any = None) -> None:\n    \"\"\"Performs a single optimization step.\n    Args:\n        closure (callable, optional): A closure that reevaluates the model\n            and returns the loss.\n    \"\"\"\n    # Perform a single optimization step\n    self.optimizer.step()\n\n    # Update the model parameters\n    self.model.parameters = self.optimizer.parameters\n\n    # Evaluate the model and get the loss\n    if closure is not None:\n        loss = closure()\n\n    # Print the loss\n    print(f\"Loss: {loss}\")\n```"]}
{"id": 58, "name": "consequent_differs", "prompt": "### Role:\nYou are an expert Python programmer. You will be given some information and will generate a correct Python function that matches this information.\n\n### Task:\nBased on the provided information, generate a complete Python function. Ensure the code is logically correct, syntactically accurate, and follows best coding practices.\n\n### Function name:\nconsequent_differs\n\n### Doc string and function signature:\n```Python\ndef consequent_differs(\n    antecedents: Iterable[str] = [], eps: float = -1e8, event_dim: int = 0\n) -> Callable[[T], torch.Tensor] :\n    \"\"\"\n    A helper function for assessing whether values at a site differ from their observed values, assigning\n    `eps` if a value differs from its observed state and `0.0` otherwise.\n\n    :param antecedents: A list of names of upstream intervened sites to consider when assessing differences.\n    :param eps: A numerical value assigned if the values differ, defaults to -1e8.\n    :param event_dim: The event dimension of the value object.\n\n    :return: A callable which applied to a site value object (`consequent`), returns a tensor where each\n             element indicates whether the corresponding element of `consequent` differs from its factual value\n             (`eps` if there is a difference, `0.0` otherwise).\n    \"\"\"\n```\n\n### Requirements:\n1. Generate the complete function implementation.\n2. Only answer the code of the generated function, formatted using Markdown code block syntax.\n3. Do not include any additional text.\n\n### Answer: (use the provided format with backticks)\n", "reference": "def consequent_differs(\n    antecedents: Iterable[str] = [], eps: float = -1e8, event_dim: int = 0\n) -> Callable[[T], torch.Tensor]:\n\n    def _consequent_differs(consequent: T) -> torch.Tensor:\n        indices = IndexSet(\n            **{\n                name: ind\n                for name, ind in get_factual_indices().items()\n                if name in antecedents\n            }\n        )\n        not_eq: torch.Tensor = consequent != gather(\n            consequent, indices, event_dim=event_dim\n        )\n        for _ in range(event_dim):\n            not_eq = torch.all(not_eq, dim=-1, keepdim=False)\n        return cond(eps, 0.0, not_eq, event_dim=event_dim)\n\n    return _consequent_differs", "completions": ["  \n```\ndef consequent_differs(\n    antecedents: Iterable[str] = [], eps: float = -1e8, event_dim: int = 0\n) -> Callable[[T], torch.Tensor] :\n    \"\"\"\n    A helper function for assessing whether values at a site differ from their observed values, assigning\n    `eps` if a value differs from its observed state and `0.0` otherwise.\n\n    :param antecedents: A list of names of upstream intervened sites to consider when assessing differences.\n    :param eps: A numerical value assigned if the values differ, defaults to -1e8.\n    :param event_dim: The event dimension of the value object.\n\n    :return: A callable which applied to a site value object (`consequent`), returns a tensor where each\n             element indicates whether the corresponding element of `consequent` differs from its factual value\n             (`eps` if there is a difference, `0.0` otherwise).\n    \"\"\"\n    def _consequent_differs(consequent: T) -> torch.Tensor:\n        if not antecedents:\n            return torch.zeros_like(consequent)\n        diffs = []\n        for antecedent in antecedents:\n            diffs.append(consequent[antecedent] - consequent[antecedent].factual_value)\n        diffs = torch.stack(diffs, dim=event_dim)\n        return torch.where(diffs!= 0, eps, 0.0)\n    return _consequent_differs\n```"]}
{"id": 59, "name": "read_metrics", "prompt": "### Role:\nYou are an expert Python programmer. You will be given some information and will generate a correct Python function that matches this information.\n\n### Task:\nBased on the provided information, generate a complete Python function. Ensure the code is logically correct, syntactically accurate, and follows best coding practices.\n\n### Function name:\nread_metrics\n\n### Doc string and function signature:\n```Python\ndef read_metrics(directory: str) -> pd.Series :\n    \"\"\"\n    Reads all csv files in the given directory and returns a pandas Series\n    containing all the metric values.\n    \"\"\"\n```\n\n### Requirements:\n1. Generate the complete function implementation.\n2. Only answer the code of the generated function, formatted using Markdown code block syntax.\n3. Do not include any additional text.\n\n### Answer: (use the provided format with backticks)\n", "reference": "def read_metrics(directory: str) -> pd.Series:\n\n    dfs = []\n    # Loop through all csv files in the directory\n    csv_files = [\n        filename for filename in os.listdir(directory) if filename.endswith(\".csv\")\n    ]\n\n    if len(csv_files) == 0:\n        return pd.Series()\n\n    for filename in csv_files:\n        filepath = os.path.join(directory, filename)\n        # Read the csv file into a pandas DataFrame\n        df = pd.read_csv(\n            filepath,\n            header=None,\n            names=[\n                \"initial_time_iso\",\n                \"lead_time_hours\",\n                \"channel\",\n                \"metric\",\n                \"value\",\n            ],\n        )\n        out = pd.DataFrame()\n        out[\"initial_time\"] = pd.to_datetime(df[\"initial_time_iso\"])\n        out[\"lead_time\"] = pd.to_timedelta(df[\"lead_time_hours\"], unit=\"h\")\n        out[\"channel\"] = df[\"channel\"]\n        out[\"metric\"] = df[\"metric\"]\n        out[\"value\"] = df[\"value\"]\n        dfs.append(out)\n\n    df = pd.concat(dfs, axis=0)\n    df.set_index([\"initial_time\", \"lead_time\", \"channel\", \"metric\"], inplace=True)\n    return df[\"value\"]", "completions": ["  \n```\ndef read_metrics(directory: str) -> pd.Series:\n    \"\"\"\n    Reads all csv files in the given directory and returns a pandas Series\n    containing all the metric values.\n    \"\"\"\n    # Create an empty list to store the metric values\n    metric_values = []\n\n    # Iterate over all csv files in the directory\n    for file in os.listdir(directory):\n        # Check if the file is a csv file\n        if file.endswith('.csv'):\n            # Read the csv file and extract the metric value\n            with open(os.path.join(directory, file), 'r') as f:\n                reader = csv.reader(f)\n                for row in reader:\n                    metric_values.append(float(row[0]))\n\n    # Create a pandas Series from the metric values\n    return pd.Series(metric_values)\n```"]}
{"id": 60, "name": "load_probe", "prompt": "### Role:\nYou are an expert Python programmer. You will be given some information and will generate a correct Python function that matches this information.\n\n### Task:\nBased on the provided information, generate a complete Python function. Ensure the code is logically correct, syntactically accurate, and follows best coding practices.\n\n### Function name:\nload_probe\n\n### Doc string and function signature:\n```Python\ndef load_probe(probe_path) :\n    \"\"\"Load a .mat probe file from Kilosort2, or a PRB file and returns a dictionary\n\n    adapted from https://github.com/MouseLand/pykilosort/blob/5712cfd2722a20554fa5077dd8699f68508d1b1a/pykilosort/utils.py#L592\n\n    \"\"\"\n```\n\n### Requirements:\n1. Generate the complete function implementation.\n2. Only answer the code of the generated function, formatted using Markdown code block syntax.\n3. Do not include any additional text.\n\n### Answer: (use the provided format with backticks)\n", "reference": "def load_probe(probe_path):\n    probe = {}\n    probe_path = Path(probe_path).resolve()\n    required_keys = ['chanMap', 'yc', 'xc', 'n_chan']\n\n    if probe_path.suffix == '.prb':\n        # Support for PRB files.\n        # !DOES NOT WORK FOR PHASE3A PROBES WITH DISCONNECTED CHANNELS!\n        # Also does not remove reference channel in PHASE3B probes\n        contents = probe_path.read_text()\n        metadata = {}\n        exec(contents, {}, metadata)\n        probe['chanMap'] = []\n        probe['xc'] = []\n        probe['yc'] = []\n        probe['kcoords'] = []\n        probe['n_chan'] = 0 \n        for cg in sorted(metadata['channel_groups']):\n            d = metadata['channel_groups'][cg]\n            ch = d['channels']\n            pos = d.get('geometry', {})\n            probe['chanMap'].append(ch)\n            probe['n_chan'] += len(ch)\n            probe['xc'].append([pos[c][0] for c in ch])\n            probe['yc'].append([pos[c][1] for c in ch])\n            probe['kcoords'].append([cg for c in ch])\n        probe['chanMap'] = np.concatenate(probe['chanMap']).ravel().astype(np.int32)\n        probe['xc'] = np.concatenate(probe['xc']).astype('float32')\n        probe['yc'] = np.concatenate(probe['yc']).astype('float32')\n        probe['kcoords'] = np.concatenate(probe['kcoords']).astype('float32')\n\n    elif probe_path.suffix == '.mat':\n        mat = loadmat(probe_path)\n        connected = mat['connected'].ravel().astype('bool')\n        probe['xc'] = mat['xcoords'].ravel().astype(np.float32)[connected]\n        nc = len(probe['xc'])\n        probe['yc'] = mat['ycoords'].ravel().astype(np.float32)[connected]\n        probe['kcoords'] = mat.get('kcoords', np.zeros(nc)).ravel().astype(np.float32)\n        probe['chanMap'] = (mat['chanMap'] - 1).ravel().astype(np.int32)[connected]  # NOTE: 0-indexing in Python\n        probe['n_chan'] = (mat['chanMap'] - 1).ravel().astype(np.int32).shape[0]  # NOTE: should match the # of columns in the raw data\n\n    elif probe_path.suffix == '.json':\n        with open(probe_path, 'r') as f:\n            probe = json.load(f)\n        for k in list(probe.keys()):\n            # Convert lists back to arrays\n            v = probe[k]\n            if isinstance(v, list):\n                dtype = np.int32 if k == 'chanMap' else np.float32\n                probe[k] = np.array(v, dtype=dtype)\n\n    for n in required_keys:\n        assert n in probe.keys()\n\n    return probe", "completions": ["  \n```\ndef load_probe(probe_path):\n    \"\"\"Load a.mat probe file from Kilosort2, or a PRB file and returns a dictionary\n\n    adapted from https://github.com/MouseLand/pykilosort/blob/5712cfd2722a20554fa5077dd8699f68508d1b1a/pykilosort/utils.py#L592\n\n    \"\"\"\n    import scipy.io as sio\n    import numpy as np\n    import os\n\n    if not os.path.exists(probe_path):\n        raise ValueError(\"Probe file does not exist: {}\".format(probe_path))\n\n    if probe_path.endswith(\".mat\"):\n        probe = sio.loadmat(probe_path)\n        probe_dict = {}\n        for key in probe:\n            if key.startswith(\"electrode\"):\n                probe_dict[key] = probe[key]\n        return probe_dict\n    elif probe_path.endswith(\".prb\"):\n        with open(probe_path, \"r\") as f:\n            lines = f.readlines()\n        probe_dict = {}\n        for line in lines:\n            if line.startswith(\"channel_groups\"):\n                channel_groups = line.split(\":\")[1].strip().split(\",\")\n                probe_dict[\"channel_groups\"] = channel_groups\n            elif line.startswith(\"channels\"):\n                channels = line.split(\":\")[1].strip().split(\",\")\n                probe_dict[\"channels\"] = channels\n            elif line.startswith(\"contact_positions\"):\n                contact_positions = line.split(\":\")[1].strip().split(\",\")\n                probe_dict[\"contact_positions\"] = contact_positions\n            elif line.startswith(\"contact_plane_axes\"):\n                contact_plane_axes = line.split(\":\")[1].strip().split(\",\")\n                probe_dict[\"contact_plane_axes\"] = contact_plane_axes\n            elif line.startswith(\"contact_plane_origin\"):\n                contact_plane_origin = line.split(\":\")[1].strip().split(\",\")\n                probe_dict[\"contact_plane_origin\"] = contact_plane_origin\n            elif line.startswith(\"contact_plane_normal\"):\n                contact_plane_normal = line.split(\":\")[1].strip().split(\",\")\n                probe_dict[\"contact_plane_normal\"] = contact_plane_normal\n            elif line.startswith(\"contact_size\"):\n                contact_size = line.split(\":\")[1].strip().split(\",\")\n                probe_dict[\"contact_size\"] = contact_size\n            elif line.startswith(\"contact_shape\"):\n                contact_shape = line.split(\":\")[1].strip().split(\",\")\n                probe_dict[\"contact_shape\"] = contact_shape\n            elif line.startswith(\"electrode_group\"):\n                electrode_group = line.split(\":\")[1].strip().split(\",\")\n                probe_dict[\"electrode_group\"] = electrode_group\n            elif line.startswith(\"electrode_group_name\"):\n                electrode_group_name = line.split(\":\")[1].strip().split(\",\")\n                probe_dict[\"electrode_group_name\"] = electrode_group_name\n            elif line.startswith(\"electrode_group_location\"):\n                electrode_group_location = line.split(\":\")[1].strip().split(\",\")\n                probe_dict[\"electrode_group_location\"] = electrode_group_location\n            elif line.startswith(\"electrode_group_device\"):\n                electrode_group_device = line.split(\":\")[1].strip().split(\",\")\n                probe_dict[\"electrode_group_device\"] = electrode_group_device\n            elif line.startswith(\"electrode_group_description\"):\n                electrode_group_description = line.split(\":\")[1].strip().split(\","]}
{"id": 61, "name": "set_radio_props", "prompt": "### Role:\nYou are an expert Python programmer. You will be given some information and will generate a correct Python function that matches this information.\n\n### Task:\nBased on the provided information, generate a complete Python function. Ensure the code is logically correct, syntactically accurate, and follows best coding practices.\n\n### Function name:\nset_radio_props\n\n### Doc string and function signature:\n```Python\ndef set_radio_props(self, props) :\n    \"\"\"\n    Set properties of the `.Text` labels.\n\n    .. versionadded:: 3.7\n\n    Parameters\n    ----------\n    props : dict\n        Dictionary of `.Collection` properties to be used for the radio\n        buttons.\n    \"\"\"\n```\n\n### Requirements:\n1. Generate the complete function implementation.\n2. Only answer the code of the generated function, formatted using Markdown code block syntax.\n3. Do not include any additional text.\n\n### Answer: (use the provided format with backticks)\n", "reference": "    def set_radio_props(self, props):\n        _api.check_isinstance(dict, props=props)\n        if 's' in props:  # Keep API consistent with constructor.\n            props['sizes'] = np.broadcast_to(props.pop('s'), len(self.labels))\n        self._buttons.update(props)\n        self._active_colors = self._buttons.get_facecolor()\n        if len(self._active_colors) == 1:\n            self._active_colors = np.repeat(self._active_colors,\n                                            len(self.labels), axis=0)\n        self._buttons.set_facecolor(\n            [activecolor if text.get_text() == self.value_selected else \"none\"\n             for text, activecolor in zip(self.labels, self._active_colors)])", "completions": ["  \n```Python\ndef set_radio_props(self, props) :\n    \"\"\"\n    Set properties of the `.Text` labels.\n\n   .. versionadded:: 3.7\n\n    Parameters\n    ----------\n    props : dict\n        Dictionary of `.Collection` properties to be used for the radio\n        buttons.\n    \"\"\"\n    for prop, value in props.items():\n        if prop == \"text\":\n            self.text = value\n        elif prop == \"font\":\n            self.font = value\n        elif prop == \"color\":\n            self.color = value\n        elif prop == \"alignment\":\n            self.alignment = value\n        elif prop == \"size\":\n            self.size = value\n        elif prop == \"bold\":\n            self.bold = value\n        elif prop == \"italic\":\n            self.italic = value\n        elif prop == \"underline\":\n            self.underline = value\n        elif prop == \"strikethrough\":\n            self.strikethrough = value\n        elif prop == \"overline\":\n            self.overline = value\n        elif prop == \"background\":\n            self.background = value\n        elif prop == \"border\":\n            self.border = value\n        elif prop == \"padding\":\n            self.padding = value\n        elif prop == \"margin\":\n            self.margin = value\n        elif prop == \"width\":\n            self.width = value\n        elif prop == \"height\":\n            self.height = value\n        elif prop == \"x\":\n            self.x = value\n        elif prop == \"y\":\n            self.y = value\n        elif prop == \"z\":\n            self.z = value\n        elif prop == \"rotation\":\n            self.rotation = value\n        elif prop == \"scale\":\n            self.scale = value\n        elif prop == \"opacity\":\n            self.opacity = value\n        elif prop == \"visible\":\n            self.visible = value\n        elif prop == \"enabled\":\n            self.enabled = value\n        elif prop == \"readonly\":\n            self.readonly = value\n        elif prop == \"checked\":\n            self.checked = value\n        elif prop == \"selected\":\n            self.selected = value\n        elif prop == \"value\":\n            self.value = value\n        elif prop == \"text_align\":\n            self.text_align = value\n        elif prop == \"vertical_align\":\n            self.vertical_align = value\n        elif prop == \"wrap\":\n            self.wrap = value\n        elif prop == \"ellipsis\":\n            self.ellipsis = value\n        elif prop == \"tooltip\":\n            self.tooltip = value\n        elif prop == \"cursor\":\n            self.cursor = value\n        elif prop == \"context_menu\":\n            self.context_menu = value\n        elif prop == \"draggable\":\n            self.draggable = value\n        elif prop == \"droppable\":\n            self.droppable = value\n        elif prop == \"resizable\":\n            self.resizable = value\n        elif prop == \"sortable\":\n            self.sortable = value\n        elif prop == \"selectable\":\n            self.selectable = value\n        elif prop == \"editable\":\n            self.editable = value\n        elif prop == \"spellcheck\":\n            self.spellcheck = value\n        elif prop == \"autocomplete\":\n            self.autocomplete = value\n        elif prop == \"autocorrect\":\n            self.autocorrect = value\n        elif prop == \"autocapitalize\":\n            self.autocapitalize = value\n        elif prop == \"autofocus\":\n            self.autofocus = value\n        elif prop == \"tabindex\":\n            self.tabindex = value\n        elif prop == \"accesskey\":\n            self.accesskey = value\n        elif prop == \"onclick\":\n            self.onclick = value\n        elif prop == \"ondblclick\":\n            self.ondblclick = value\n        elif prop == \"onmousedown\":\n            self.onmousedown = value\n        elif prop == \"onmouseup\":\n            self.onmouseup = value\n        elif prop == \"onmouseover\":\n            self.onmouseover = value\n        elif prop == \"onmousemove\":\n            self.onmousemove ="]}
{"id": 62, "name": "transform", "prompt": "### Role:\nYou are an expert Python programmer. You will be given some information and will generate a correct Python function that matches this information.\n\n### Task:\nBased on the provided information, generate a complete Python function. Ensure the code is logically correct, syntactically accurate, and follows best coding practices.\n\n### Function name:\ntransform\n\n### Doc string and function signature:\n```Python\ndef transform(self, effects) :\n    \"\"\"\n    Return a new font that is slanted and/or extended.\n\n    Parameters\n    ----------\n    effects : dict\n        A dict with optional entries:\n\n        - 'slant' : float, default: 0\n            Tangent of the angle that the font is to be slanted to the\n            right. Negative values slant to the left.\n        - 'extend' : float, default: 1\n            Scaling factor for the font width. Values less than 1 condense\n            the glyphs.\n\n    Returns\n    -------\n    `Type1Font`\n    \"\"\"\n```\n\n### Requirements:\n1. Generate the complete function implementation.\n2. Only answer the code of the generated function, formatted using Markdown code block syntax.\n3. Do not include any additional text.\n\n### Answer: (use the provided format with backticks)\n", "reference": "    def transform(self, effects):\n        fontname = self.prop['FontName']\n        italicangle = self.prop['ItalicAngle']\n\n        array = [\n            float(x) for x in (self.prop['FontMatrix']\n                               .lstrip('[').rstrip(']').split())\n        ]\n        oldmatrix = np.eye(3, 3)\n        oldmatrix[0:3, 0] = array[::2]\n        oldmatrix[0:3, 1] = array[1::2]\n        modifier = np.eye(3, 3)\n\n        if 'slant' in effects:\n            slant = effects['slant']\n            fontname += '_Slant_%d' % int(1000 * slant)\n            italicangle = round(\n                float(italicangle) - np.arctan(slant) / np.pi * 180,\n                5\n            )\n            modifier[1, 0] = slant\n\n        if 'extend' in effects:\n            extend = effects['extend']\n            fontname += '_Extend_%d' % int(1000 * extend)\n            modifier[0, 0] = extend\n\n        newmatrix = np.dot(modifier, oldmatrix)\n        array[::2] = newmatrix[0:3, 0]\n        array[1::2] = newmatrix[0:3, 1]\n        fontmatrix = (\n            '[%s]' % ' '.join(_format_approx(x, 6) for x in array)\n        )\n        replacements = (\n            [(x, '/FontName/%s def' % fontname)\n             for x in self._pos['FontName']]\n            + [(x, '/ItalicAngle %a def' % italicangle)\n               for x in self._pos['ItalicAngle']]\n            + [(x, '/FontMatrix %s readonly def' % fontmatrix)\n               for x in self._pos['FontMatrix']]\n            + [(x, '') for x in self._pos.get('UniqueID', [])]\n        )\n\n        data = bytearray(self.parts[0])\n        data.extend(self.decrypted)\n        len0 = len(self.parts[0])\n        for (pos0, pos1), value in sorted(replacements, reverse=True):\n            data[pos0:pos1] = value.encode('ascii', 'replace')\n            if pos0 < len(self.parts[0]):\n                if pos1 >= len(self.parts[0]):\n                    raise RuntimeError(\n                        f\"text to be replaced with {value} spans \"\n                        \"the eexec boundary\"\n                    )\n                len0 += len(value) - pos1 + pos0\n\n        data = bytes(data)\n        return Type1Font((\n            data[:len0],\n            self._encrypt(data[len0:], 'eexec'),\n            self.parts[2]\n        ))", "completions": ["  \n```\ndef transform(self, effects):\n    \"\"\"\n    Return a new font that is slanted and/or extended.\n\n    Parameters\n    ----------\n    effects : dict\n        A dict with optional entries:\n\n        -'slant' : float, default: 0\n            Tangent of the angle that the font is to be slanted to the\n            right. Negative values slant to the left.\n        - 'extend' : float, default: 1\n            Scaling factor for the font width. Values less than 1 condense\n            the glyphs.\n\n    Returns\n    -------\n    `Type1Font`\n    \"\"\"\n    # Create a new Type1Font object\n    new_font = Type1Font()\n\n    # Set the font name and encoding\n    new_font.set_name(self.get_name())\n    new_font.set_encoding(self.get_encoding())\n\n    # Iterate over the glyphs in the font\n    for glyph in self.get_glyphs():\n        # Get the glyph's bounding box\n        bbox = glyph.get_bbox()\n\n        # Calculate the slant and extend factors\n        slant_factor = math.tan(effects['slant']) if'slant' in effects else 0\n        extend_factor = effects['extend'] if 'extend' in effects else 1\n\n        # Apply the slant and extend factors to the glyph's bounding box\n        bbox.xMin += slant_factor * bbox.height\n        bbox.xMax += slant_factor * bbox.height\n        bbox.yMin *= extend_factor\n        bbox.yMax *= extend_factor\n\n        # Create a new glyph with the modified bounding box\n        new_glyph = Glyph(bbox)\n\n        # Add the new glyph to the new font\n        new_font.add_glyph(new_glyph)\n\n    # Return the new font\n    return new_font\n```"]}
{"id": 63, "name": "load_auto_model_for_seq2seq", "prompt": "### Role:\nYou are an expert Python programmer. You will be given some information and will generate a correct Python function that matches this information.\n\n### Task:\nBased on the provided information, generate a complete Python function. Ensure the code is logically correct, syntactically accurate, and follows best coding practices.\n\n### Function name:\nload_auto_model_for_seq2seq\n\n### Doc string and function signature:\n```Python\ndef load_auto_model_for_seq2seq(\n    model_name: str,\n    model_revision: Optional[str] = None,\n    tokenizer_name: Optional[str] = None,\n    tokenizer_revision: Optional[str] = None\n) -> Tuple[AutoTokenizer, AutoModelForSeq2SeqLM] :\n    '''\n    Loads a sequence-to-sequence model and its tokenizer.\n\n    Args:\n        model_name: The name of the sequence-classification model to load.\n        tokenizer_name: The name of the tokenizer to load. If None, the\n            tokenizer associated with the model will be loaded.\n        model_revision: The model revision to load.\n        tokenizer_revision: the tokenizer revision to load\n\n    Returns:\n        tokenizer: The loaded tokenizer.\n        model: The loaded sequence-to-sequence model.\n    '''\n```\n\n### Requirements:\n1. Generate the complete function implementation.\n2. Only answer the code of the generated function, formatted using Markdown code block syntax.\n3. Do not include any additional text.\n\n### Answer: (use the provided format with backticks)\n", "reference": "def load_auto_model_for_seq2seq(\n    model_name: str,\n    model_revision: Optional[str] = None,\n    tokenizer_name: Optional[str] = None,\n    tokenizer_revision: Optional[str] = None\n) -> Tuple[AutoTokenizer, AutoModelForSeq2SeqLM]:\n    if tokenizer_name is None:\n        tokenizer_name = model_name\n    tokenizer = AutoTokenizer.from_pretrained(tokenizer_name,\n                                              revision=tokenizer_revision)\n    # There are \"Some weights are not used warning\" for some models, but we\n    # ignore it because that is intended.\n    with _handle_logging_level():\n        model = AutoModelForSeq2SeqLM.from_pretrained(model_name,\n                                                      revision=model_revision)\n    return tokenizer, model  # type: ignore", "completions": ["  \n```\ndef load_auto_model_for_seq2seq(\n    model_name: str,\n    model_revision: Optional[str] = None,\n    tokenizer_name: Optional[str] = None,\n    tokenizer_revision: Optional[str] = None\n) -> Tuple[AutoTokenizer, AutoModelForSeq2SeqLM] :\n    '''\n    Loads a sequence-to-sequence model and its tokenizer.\n\n    Args:\n        model_name: The name of the sequence-classification model to load.\n        tokenizer_name: The name of the tokenizer to load. If None, the\n            tokenizer associated with the model will be loaded.\n        model_revision: The model revision to load.\n        tokenizer_revision: the tokenizer revision to load\n\n    Returns:\n        tokenizer: The loaded tokenizer.\n        model: The loaded sequence-to-sequence model.\n    '''\n    # Load the tokenizer\n    if tokenizer_name is None:\n        tokenizer_name = model_name\n    tokenizer = AutoTokenizer.from_pretrained(tokenizer_name, revision=tokenizer_revision)\n\n    # Load the model\n    model = AutoModelForSeq2SeqLM.from_pretrained(model_name, revision=model_revision)\n\n    return tokenizer, model\n```"]}
{"id": 64, "name": "set_xy", "prompt": "### Role:\nYou are an expert Python programmer. You will be given some information and will generate a correct Python function that matches this information.\n\n### Task:\nBased on the provided information, generate a complete Python function. Ensure the code is logically correct, syntactically accurate, and follows best coding practices.\n\n### Function name:\nset_xy\n\n### Doc string and function signature:\n```Python\ndef set_xy(self, xy) :\n    \"\"\"\n    Set the vertices of the polygon.\n\n    Parameters\n    ----------\n    xy : (N, 2) array-like\n        The coordinates of the vertices.\n\n    Notes\n    -----\n    Unlike `.Path`, we do not ignore the last input vertex. If the\n    polygon is meant to be closed, and the last point of the polygon is not\n    equal to the first, we assume that the user has not explicitly passed a\n    ``CLOSEPOLY`` vertex, and add it ourselves.\n    \"\"\"\n```\n\n### Requirements:\n1. Generate the complete function implementation.\n2. Only answer the code of the generated function, formatted using Markdown code block syntax.\n3. Do not include any additional text.\n\n### Answer: (use the provided format with backticks)\n", "reference": "    def set_xy(self, xy):\n        xy = np.asarray(xy)\n        nverts, _ = xy.shape\n        if self._closed:\n            # if the first and last vertex are the \"same\", then we assume that\n            # the user explicitly passed the CLOSEPOLY vertex. Otherwise, we\n            # have to append one since the last vertex will be \"ignored\" by\n            # Path\n            if nverts == 1 or nverts > 1 and (xy[0] != xy[-1]).any():\n                xy = np.concatenate([xy, [xy[0]]])\n        else:\n            # if we aren't closed, and the last vertex matches the first, then\n            # we assume we have an unnecessary CLOSEPOLY vertex and remove it\n            if nverts > 2 and (xy[0] == xy[-1]).all():\n                xy = xy[:-1]\n        self._path = Path(xy, closed=self._closed)\n        self.stale = True", "completions": ["  \n```\ndef set_xy(self, xy):\n    \"\"\"\n    Set the vertices of the polygon.\n\n    Parameters\n    ----------\n    xy : (N, 2) array-like\n        The coordinates of the vertices.\n\n    Notes\n    -----\n    Unlike `.Path`, we do not ignore the last input vertex. If the\n    polygon is meant to be closed, and the last point of the polygon is not\n    equal to the first, we assume that the user has not explicitly passed a\n    ``CLOSEPOLY`` vertex, and add it ourselves.\n    \"\"\"\n    self.xy = xy\n    if self.xy[0]!= self.xy[-1]:\n        self.xy = np.append(self.xy, self.xy[0])\n```"]}
{"id": 65, "name": "broadcast_data", "prompt": "### Role:\nYou are an expert Python programmer. You will be given some information and will generate a correct Python function that matches this information.\n\n### Task:\nBased on the provided information, generate a complete Python function. Ensure the code is logically correct, syntactically accurate, and follows best coding practices.\n\n### Function name:\nbroadcast_data\n\n### Doc string and function signature:\n```Python\ndef broadcast_data(keys, data, datatype) :\n    \"\"\"Broadcast data from rank zero of each model parallel group to the\n    members of the same model parallel group.\n\n    Arguments:\n        keys: list of keys in the data disctionary to be broadcasted\n        data: data dictionary of string keys and cpu tensor values.\n        datatype: torch data type of all tensors in data associated\n                  with keys.\n    \"\"\"\n```\n\n### Requirements:\n1. Generate the complete function implementation.\n2. Only answer the code of the generated function, formatted using Markdown code block syntax.\n3. Do not include any additional text.\n\n### Answer: (use the provided format with backticks)\n", "reference": "def broadcast_data(keys, data, datatype):\n    # Build (key, size) and (key, number of elements) dictionaries along\n    # with the total number of elements on all ranks.\n    key_size, key_numel, total_numel = _build_key_size_numel_dictionaries(keys,\n                                                                          data)\n\n    # Pack on rank zero.\n    if get_tensor_model_parallel_rank() == 0:\n        # Check that all keys have the same data type.\n        _check_data_types(keys, data, datatype)\n        # Flatten the data associated with the keys\n        flatten_data = torch.cat(\n            [data[key].contiguous().view(-1) for key in keys], dim=0).cuda()\n    else:\n        flatten_data = torch.empty(total_numel,\n                                   device=torch.cuda.current_device(),\n                                   dtype=datatype)\n\n    # Broadcast\n    torch.distributed.broadcast(flatten_data, get_tensor_model_parallel_src_rank(),\n                                group=get_tensor_model_parallel_group())\n\n    # Unpack\n    output = {}\n    offset = 0\n    for key in keys:\n        size = key_size[key]\n        numel = key_numel[key]\n        output[key] = flatten_data.narrow(0, offset, numel).view(size)\n        offset += numel\n\n    return output", "completions": ["  \n```\ndef broadcast_data(keys, data, datatype):\n    \"\"\"Broadcast data from rank zero of each model parallel group to the\n    members of the same model parallel group.\n\n    Arguments:\n        keys: list of keys in the data disctionary to be broadcasted\n        data: data dictionary of string keys and cpu tensor values.\n        datatype: torch data type of all tensors in data associated\n                  with keys.\n    \"\"\"\n    # Get the rank of the current process\n    rank = torch.distributed.get_rank()\n\n    # If the current process is rank zero, broadcast the data\n    if rank == 0:\n        for key in keys:\n            # Get the tensor associated with the current key\n            tensor = data[key]\n\n            # Broadcast the tensor to all members of the model parallel group\n            torch.distributed.broadcast(tensor, 0)\n\n    # Synchronize all processes in the model parallel group\n    torch.distributed.barrier()\n\n    # If the current process is not rank zero, receive the broadcasted data\n    if rank!= 0:\n        for key in keys:\n            # Get the tensor associated with the current key\n            tensor = data[key]\n\n            # Receive the broadcasted tensor from rank zero\n            torch.distributed.recv(tensor, 0)\n\n    return data\n```"]}
{"id": 66, "name": "sample_intervention", "prompt": "### Role:\nYou are an expert Python programmer. You will be given some information and will generate a correct Python function that matches this information.\n\n### Task:\nBased on the provided information, generate a complete Python function. Ensure the code is logically correct, syntactically accurate, and follows best coding practices.\n\n### Function name:\nsample_intervention\n\n### Doc string and function signature:\n```Python\ndef sample_intervention(\n    sem: SEM, tensordict_data: TensorDict, num_intervention_samples: int, treatment: str | None = None\n) -> InterventionData :\n    \"\"\"Sample an intervention and it's sample mean from a given SEM.\n\n    Args:\n        sem: SEM to sample interventional data from.\n        tensordict_data: Base data for sampling an intervention value.\n        num_intervention_samples: The number of samples to draw from the interventional distribution.\n        treatment: The name of the treatment variable. If None, a random variable is chosen.\n\n    Returns:\n        an intervention data object\n    \"\"\"\n```\n\n### Requirements:\n1. Generate the complete function implementation.\n2. Only answer the code of the generated function, formatted using Markdown code block syntax.\n3. Do not include any additional text.\n\n### Answer: (use the provided format with backticks)\n", "reference": "def sample_intervention(\n    sem: SEM, tensordict_data: TensorDict, num_intervention_samples: int, treatment: str | None = None\n) -> InterventionData:\n    intervention_a = sample_intervention_dict(tensordict_data, treatment=treatment)\n\n    intervention_a_samples = sem.do(intervention_a).sample(\n        torch.Size(\n            [\n                num_intervention_samples,\n            ]\n        )\n    )\n\n    return InterventionData(\n        intervention_a_samples,\n        intervention_a,\n        TensorDict({}, batch_size=torch.Size()),\n    )", "completions": ["  \n```\ndef sample_intervention(\n    sem: SEM, tensordict_data: TensorDict, num_intervention_samples: int, treatment: str | None = None\n) -> InterventionData :\n    \"\"\"Sample an intervention and it's sample mean from a given SEM.\n\n    Args:\n        sem: SEM to sample interventional data from.\n        tensordict_data: Base data for sampling an intervention value.\n        num_intervention_samples: The number of samples to draw from the interventional distribution.\n        treatment: The name of the treatment variable. If None, a random variable is chosen.\n\n    Returns:\n        an intervention data object\n    \"\"\"\n    # Generate a random treatment variable if none is provided\n    if treatment is None:\n        treatment = random.choice(list(sem.variables))\n\n    # Get the distribution of the treatment variable\n    treatment_dist = sem.get_distribution(treatment)\n\n    # Sample the treatment variable\n    treatment_sample = treatment_dist.sample(num_intervention_samples)\n\n    # Create a new data dictionary with the sampled treatment variable\n    intervention_data = tensordict_data.copy()\n    intervention_data[treatment] = treatment_sample\n\n    # Get the sample mean of the intervention data\n    intervention_mean = intervention_data.mean(axis=0)\n\n    # Return the intervention data and sample mean\n    return InterventionData(intervention_data, intervention_mean)\n```"]}
{"id": 67, "name": "_set_alpha_for_array", "prompt": "### Role:\nYou are an expert Python programmer. You will be given some information and will generate a correct Python function that matches this information.\n\n### Task:\nBased on the provided information, generate a complete Python function. Ensure the code is logically correct, syntactically accurate, and follows best coding practices.\n\n### Function name:\n_set_alpha_for_array\n\n### Doc string and function signature:\n```Python\ndef _set_alpha_for_array(self, alpha) :\n    \"\"\"\n    Set the alpha value used for blending - not supported on all backends.\n\n    Parameters\n    ----------\n    alpha : array-like or scalar or None\n        All values must be within the 0-1 range, inclusive.\n        Masked values and nans are not supported.\n    \"\"\"\n```\n\n### Requirements:\n1. Generate the complete function implementation.\n2. Only answer the code of the generated function, formatted using Markdown code block syntax.\n3. Do not include any additional text.\n\n### Answer: (use the provided format with backticks)\n", "reference": "    def _set_alpha_for_array(self, alpha):\n        if isinstance(alpha, str):\n            raise TypeError(\"alpha must be numeric or None, not a string\")\n        if not np.iterable(alpha):\n            Artist.set_alpha(self, alpha)\n            return\n        alpha = np.asarray(alpha)\n        if not (0 <= alpha.min() and alpha.max() <= 1):\n            raise ValueError('alpha must be between 0 and 1, inclusive, '\n                             f'but min is {alpha.min()}, max is {alpha.max()}')\n        self._alpha = alpha\n        self.pchanged()\n        self.stale = True", "completions": ["  \n```Python\ndef _set_alpha_for_array(self, alpha) :\n    \"\"\"\n    Set the alpha value used for blending - not supported on all backends.\n\n    Parameters\n    ----------\n    alpha : array-like or scalar or None\n        All values must be within the 0-1 range, inclusive.\n        Masked values and nans are not supported.\n    \"\"\"\n    if alpha is None:\n        self._alpha = None\n    else:\n        alpha = np.asarray(alpha)\n        if np.any(alpha < 0) or np.any(alpha > 1):\n            raise ValueError(\"All values must be within the 0-1 range, inclusive.\")\n        self._alpha = alpha\n```"]}
{"id": 68, "name": "run", "prompt": "### Role:\nYou are an expert Python programmer. You will be given some information and will generate a correct Python function that matches this information.\n\n### Task:\nBased on the provided information, generate a complete Python function. Ensure the code is logically correct, syntactically accurate, and follows best coding practices.\n\n### Function name:\nrun\n\n### Doc string and function signature:\n```Python\ndef run(self) -> None :\n    r\"\"\"\n    Runs a particle-tracing simulation.\n    Time steps are adaptively calculated based on the\n    local grid resolution of the particles and the electric and magnetic\n    fields they are experiencing.\n\n    Returns\n    -------\n    None\n\n    \"\"\"\n```\n\n### Requirements:\n1. Generate the complete function implementation.\n2. Only answer the code of the generated function, formatted using Markdown code block syntax.\n3. Do not include any additional text.\n\n### Answer: (use the provided format with backticks)\n", "reference": "    def run(self) -> None:\n\n        self._enforce_particle_creation()\n\n        # Keep track of how many push steps have occurred for trajectory tracing\n        self.iteration_number = 0\n\n        self.time: Union[NDArray[np.float64], float] = (\n            np.zeros((self.nparticles, 1)) if not self.is_synchronized_time_step else 0\n        )\n        # Create flags for tracking when particles during the simulation\n        # on_grid -> zero if the particle is off grid, 1\n        # shape [nparticles, ngrids]\n        self.on_grid: NDArray[np.bool_] = np.zeros(\n            [self.nparticles, self.num_grids]\n        ).astype(np.bool_)\n\n        # Entered grid -> non-zero if particle EVER entered a grid\n        self.entered_grid: NDArray[np.bool_] = np.zeros([self.nparticles]).astype(\n            np.bool_\n        )\n\n        # Initialize a \"progress bar\" (really more of a meter)\n        # Setting sys.stdout lets this play nicely with regular print()\n        pbar = tqdm(\n            initial=0,\n            total=self.termination_condition.total,\n            disable=not self.verbose,\n            desc=self.termination_condition.progress_description,\n            unit=self.termination_condition.units_string,\n            bar_format=\"{l_bar}{bar}{n:.1e}/{total:.1e} {unit}\",\n            file=sys.stdout,\n        )\n\n        # Push the particles until the termination condition is satisfied\n        is_finished = False\n        while not (is_finished or self.nparticles_tracked == 0):\n            is_finished = self.termination_condition.is_finished\n            progress = min(\n                self.termination_condition.progress, self.termination_condition.total\n            )\n\n            pbar.n = progress\n            pbar.last_print_n = progress\n            pbar.update(0)\n\n            self._push()\n\n            if self.save_routine is not None:\n                self.save_routine.post_push_hook()\n\n        if self.save_routine is not None:\n            self.save_routine.post_push_hook(force_save=True)\n\n        pbar.close()\n\n        # Log a summary of the run\n\n        self._log(\"Run completed\")\n\n        # Simulation has not run, because creating new particles changes the simulation\n        self._has_run = True", "completions": ["  \n```Python\ndef run(self) -> None:\n    \"\"\"\n    Runs a particle-tracing simulation.\n    Time steps are adaptively calculated based on the\n    local grid resolution of the particles and the electric and magnetic\n    fields they are experiencing.\n\n    Returns\n    -------\n    None\n\n    \"\"\"\n    # Initialize the simulation parameters\n    self.initialize_simulation()\n\n    # Loop over the simulation steps\n    while self.time < self.end_time:\n        # Calculate the time step for the current step\n        self.calculate_time_step()\n\n        # Update the particle positions and velocities\n        self.update_particles()\n\n        # Update the electric and magnetic fields\n        self.update_fields()\n\n        # Increment the simulation time\n        self.time += self.time_step\n\n    # Finalize the simulation\n    self.finalize_simulation()\n```"]}
{"id": 69, "name": "assert_connectivity", "prompt": "### Role:\nYou are an expert Python programmer. You will be given some information and will generate a correct Python function that matches this information.\n\n### Task:\nBased on the provided information, generate a complete Python function. Ensure the code is logically correct, syntactically accurate, and follows best coding practices.\n\n### Function name:\nassert_connectivity\n\n### Doc string and function signature:\n```Python\ndef assert_connectivity(connectivity: nx.Graph, circuit: Circuit) :\n    \"\"\"Assert if a circuit can be executed on Hardware.\n\n    No gates acting on more than two qubits.\n    All two-qubit operations can be performed on hardware.\n\n    Args:\n        circuit (:class:`qibo.models.circuit.Circuit`): circuit model to check.\n        connectivity (:class:`networkx.Graph`): chip connectivity.\n    \"\"\"\n```\n\n### Requirements:\n1. Generate the complete function implementation.\n2. Only answer the code of the generated function, formatted using Markdown code block syntax.\n3. Do not include any additional text.\n\n### Answer: (use the provided format with backticks)\n", "reference": "def assert_connectivity(connectivity: nx.Graph, circuit: Circuit):\n    if list(connectivity.nodes) != list(range(connectivity.number_of_nodes())):\n        node_mapping = {node: i for i, node in enumerate(connectivity.nodes)}\n        new_connectivity = nx.Graph()\n        new_connectivity.add_edges_from(\n            [(node_mapping[u], node_mapping[v]) for u, v in connectivity.edges]\n        )\n        connectivity = new_connectivity\n    for gate in circuit.queue:\n        if len(gate.qubits) > 2 and not isinstance(gate, gates.M):\n            raise_error(ConnectivityError, f\"{gate.name} acts on more than two qubits.\")\n        if len(gate.qubits) == 2:\n            if (gate.qubits[0], gate.qubits[1]) not in connectivity.edges:\n                raise_error(\n                    ConnectivityError,\n                    f\"Circuit does not respect connectivity. {gate.name} acts on {gate.qubits}.\",\n                )", "completions": ["  \n```Python\ndef assert_connectivity(connectivity: nx.Graph, circuit: Circuit) :\n    \"\"\"Assert if a circuit can be executed on Hardware.\n\n    No gates acting on more than two qubits.\n    All two-qubit operations can be performed on hardware.\n\n    Args:\n        circuit (:class:`qibo.models.circuit.Circuit`): circuit model to check.\n        connectivity (:class:`networkx.Graph`): chip connectivity.\n    \"\"\"\n    # Check if the circuit has any gates acting on more than two qubits\n    for gate in circuit.gates:\n        if len(gate.qubits) > 2:\n            raise ValueError(\"Circuit contains a gate acting on more than two qubits.\")\n\n    # Check if all two-qubit operations can be performed on hardware\n    for gate in circuit.gates:\n        if len(gate.qubits) == 2:\n            qubits = gate.qubits\n            if not connectivity.has_edge(qubits[0], qubits[1]):\n                raise ValueError(\"Circuit contains a two-qubit operation that cannot be performed on hardware.\")\n```"]}
{"id": 70, "name": "get_aws_credentials", "prompt": "### Role:\nYou are an expert Python programmer. You will be given some information and will generate a correct Python function that matches this information.\n\n### Task:\nBased on the provided information, generate a complete Python function. Ensure the code is logically correct, syntactically accurate, and follows best coding practices.\n\n### Function name:\nget_aws_credentials\n\n### Doc string and function signature:\n```Python\ndef get_aws_credentials(\n    s3_aws_credentials_config: S3AWSCredentials,\n) -> Optional[Dict[str, str]] :\n    \"\"\"\n    This function creates temporary AWS credentials from a configured endpoint by issuing a POST request to the configured API.\n    The function optionally uses an env variable for authorization and the returned result is a set of env variables that should\n    be injected to the process issuing the S3 sync.\n    \"\"\"\n```\n\n### Requirements:\n1. Generate the complete function implementation.\n2. Only answer the code of the generated function, formatted using Markdown code block syntax.\n3. Do not include any additional text.\n\n### Answer: (use the provided format with backticks)\n", "reference": "def get_aws_credentials(\n    s3_aws_credentials_config: S3AWSCredentials,\n) -> Optional[Dict[str, str]]:\n    token = (\n        os.getenv(s3_aws_credentials_config.auth_token_env_variable)\n        if s3_aws_credentials_config.auth_token_env_variable\n        else None\n    )\n    headers = {\"Authorization\": f\"Bearer {token}\"} if token else None\n    resp = requests.post(\n        s3_aws_credentials_config.create_aws_credentials_url, headers=headers\n    )\n    if not resp.ok:\n        logger.error(f\"Request to create AWS credentials had failed with {resp.reason}\")\n        return None\n\n    env = resp.json()\n    return env", "completions": ["  \n```Python\ndef get_aws_credentials(\n    s3_aws_credentials_config: S3AWSCredentials,\n) -> Optional[Dict[str, str]] :\n    \"\"\"\n    This function creates temporary AWS credentials from a configured endpoint by issuing a POST request to the configured API.\n    The function optionally uses an env variable for authorization and the returned result is a set of env variables that should\n    be injected to the process issuing the S3 sync.\n    \"\"\"\n    import requests\n    import json\n    import os\n\n    # Get the AWS credentials endpoint from the config\n    aws_credentials_endpoint = s3_aws_credentials_config.get_aws_credentials_endpoint()\n\n    # Get the authorization token from the env variable\n    authorization_token = os.getenv(\"AWS_CREDENTIALS_AUTHORIZATION_TOKEN\")\n\n    # Set up the request headers\n    headers = {\n        \"Content-Type\": \"application/json\",\n        \"Authorization\": f\"Bearer {authorization_token}\"\n    }\n\n    # Set up the request body\n    body = {\n        \"duration\": 3600\n    }\n\n    # Make the POST request to the AWS credentials endpoint\n    response = requests.post(aws_credentials_endpoint, headers=headers, json=body)\n\n    # Check the response status code\n    if response.status_code == 200:\n        # Parse the response JSON\n        response_json = json.loads(response.text)\n\n        # Extract the temporary AWS credentials from the response\n        aws_access_key_id = response_json[\"AccessKeyId\"]\n        aws_secret_access_key = response_json[\"SecretAccessKey\"]\n        aws_session_token = response_json[\"SessionToken\"]\n\n        # Return the temporary AWS credentials as a dictionary\n        return {\n            \"AWS_ACCESS_KEY_ID\": aws_access_key_id,\n            \"AWS_SECRET_ACCESS_KEY\": aws_secret_access_key,\n            \"AWS_SESSION_TOKEN\": aws_session_token\n        }\n    else:\n        # Return None if the request failed\n        return None\n```"]}
{"id": 71, "name": "setup_cache", "prompt": "### Role:\nYou are an expert Python programmer. You will be given some information and will generate a correct Python function that matches this information.\n\n### Task:\nBased on the provided information, generate a complete Python function. Ensure the code is logically correct, syntactically accurate, and follows best coding practices.\n\n### Function name:\nsetup_cache\n\n### Doc string and function signature:\n```Python\ndef setup_cache(\n    max_batch_size, max_seq_len, dim, heads, layers, block_size, rope_base\n) :\n    \"\"\"Sets up the cache for the given model.\n\n    Args:\n        max_batch_size (_type_): _description_\n        max_seq_len (_type_): _description_\n        dim (_type_): _description_\n        heads (_type_): _description_\n        layers (_type_): _description_\n        block_size (_type_): _description_\n        rope_base (_type_): _description_\n    \"\"\"\n```\n\n### Requirements:\n1. Generate the complete function implementation.\n2. Only answer the code of the generated function, formatted using Markdown code block syntax.\n3. Do not include any additional text.\n\n### Answer: (use the provided format with backticks)\n", "reference": "def setup_cache(\n    max_batch_size, max_seq_len, dim, heads, layers, block_size, rope_base\n):\n    if max_seq_len >= max_seq_len and max_batch_size >= max_batch_size:\n        return\n\n    head_dim = dim // heads\n    max_seq_len = find_multiple(max_seq_len, 8)\n\n    for b in layers:\n        b.attention.kv_cache = KVCache(\n            max_batch_size, max_seq_len, heads, head_dim\n        )\n\n    freq_cis = precompute_freq_cis(block_size, dim // heads, rope_base)\n    causal_mask = torch.tril(\n        torch.ones(max_seq_len, max_seq_len, dtype=torch.bool)\n    )\n\n    return causal_mask, freq_cis", "completions": ["  \n```\ndef setup_cache(\n    max_batch_size, max_seq_len, dim, heads, layers, block_size, rope_base\n) :\n    \"\"\"Sets up the cache for the given model.\n\n    Args:\n        max_batch_size (_type_): _description_\n        max_seq_len (_type_): _description_\n        dim (_type_): _description_\n        heads (_type_): _description_\n        layers (_type_): _description_\n        block_size (_type_): _description_\n        rope_base (_type_): _description_\n    \"\"\"\n    # Initialize the cache\n    cache = {}\n\n    # Set the maximum batch size\n    cache[\"max_batch_size\"] = max_batch_size\n\n    # Set the maximum sequence length\n    cache[\"max_seq_len\"] = max_seq_len\n\n    # Set the model dimensions\n    cache[\"dim\"] = dim\n\n    # Set the number of attention heads\n    cache[\"heads\"] = heads\n\n    # Set the number of layers\n    cache[\"layers\"] = layers\n\n    # Set the block size\n    cache[\"block_size\"] = block_size\n\n    # Set the rope base\n    cache[\"rope_base\"] = rope_base\n\n    return cache\n```"]}
{"id": 72, "name": "push_pop_nop", "prompt": "### Role:\nYou are an expert Python programmer. You will be given some information and will generate a correct Python function that matches this information.\n\n### Task:\nBased on the provided information, generate a complete Python function. Ensure the code is logically correct, syntactically accurate, and follows best coding practices.\n\n### Function name:\npush_pop_nop\n\n### Doc string and function signature:\n```Python\ndef push_pop_nop(ss: StackState,\n                 sharpen_pointer,\n                 should_push,\n                 should_pop,\n                 should_null_op,\n                 value) -> StackState :\n    '''Apply all possible stack operations in superposition, and hopefully\n    scaled appropriately to signify the *actual* operation you intended.\n\n    Args:\n\n      should_push, should_pop, should_null_op: ndarray([BATCH_SIZE]), values\n        in (0, 1). 0 means \"dont do this operation\", 1 means \"do this\n        operation\". Note: these are NOT constrained to sum to 1.0, the\n        caller can choose to do this.\n\n      value: value to push, if pushing. ndarray([BATCH_SIZE, VEC_SIZE])\n\n    '''\n```\n\n### Requirements:\n1. Generate the complete function implementation.\n2. Only answer the code of the generated function, formatted using Markdown code block syntax.\n3. Do not include any additional text.\n\n### Answer: (use the provided format with backticks)\n", "reference": "def push_pop_nop(ss: StackState,\n                 sharpen_pointer,\n                 should_push,\n                 should_pop,\n                 should_null_op,\n                 value) -> StackState:\n    # Push value onto stack\n    push_ss = push(ss, value)\n\n    # Pop value off of stack\n    pop_ss, popped_val = pop(ss)\n    popped_val = einsum('bv, b -> bv', popped_val, should_pop) # interpolate if pop not intended\n\n    new_stack = (\n        einsum('bnv, b -> bnv', ss.stack, should_null_op) +\n        einsum('bnv, b -> bnv', push_ss.stack, should_push) +\n        einsum('bnv, b -> bnv', pop_ss.stack, should_pop)\n    )\n    new_pointer = (\n        einsum('bn, b -> bn', ss.pointer, should_null_op) +\n        einsum('bn, b -> bn', push_ss.pointer, should_push) +\n        einsum('bn, b -> bn', pop_ss.pointer, should_pop)\n    )\n\n    ##########\n    # Sharpen (softmax) pointer\n    new_pointer = torch.softmax(new_pointer * sharpen_pointer, dim=1)\n    psum = new_pointer.sum(dim=1).unsqueeze(1)\n    new_pointer = new_pointer / torch.maximum(psum, torch.zeros_like(psum) + 1e-8)\n\n    new_ss = StackState(stack=new_stack,\n                        pointer=new_pointer,\n                        zero_vec=ss.zero_vec)\n    return new_ss, popped_val", "completions": ["  \n```\ndef push_pop_nop(ss: StackState,\n                 sharpen_pointer,\n                 should_push,\n                 should_pop,\n                 should_null_op,\n                 value) -> StackState :\n    '''Apply all possible stack operations in superposition, and hopefully\n    scaled appropriately to signify the *actual* operation you intended.\n\n    Args:\n\n      should_push, should_pop, should_null_op: ndarray([BATCH_SIZE]), values\n        in (0, 1). 0 means \"dont do this operation\", 1 means \"do this\n        operation\". Note: these are NOT constrained to sum to 1.0, the\n        caller can choose to do this.\n\n      value: value to push, if pushing. ndarray([BATCH_SIZE, VEC_SIZE])\n\n    '''\n    # Apply push operation\n    if should_push:\n        ss.push(value)\n\n    # Apply pop operation\n    if should_pop:\n        ss.pop()\n\n    # Apply null operation\n    if should_null_op:\n        pass\n\n    return ss\n```"]}
{"id": 73, "name": "get_bounding_boxes", "prompt": "### Role:\nYou are an expert Python programmer. You will be given some information and will generate a correct Python function that matches this information.\n\n### Task:\nBased on the provided information, generate a complete Python function. Ensure the code is logically correct, syntactically accurate, and follows best coding practices.\n\n### Function name:\nget_bounding_boxes\n\n### Doc string and function signature:\n```Python\ndef get_bounding_boxes(self) -> Boxes :\n    \"\"\"\n    Returns:\n        Boxes: tight bounding boxes around polygon masks.\n    \"\"\"\n```\n\n### Requirements:\n1. Generate the complete function implementation.\n2. Only answer the code of the generated function, formatted using Markdown code block syntax.\n3. Do not include any additional text.\n\n### Answer: (use the provided format with backticks)\n", "reference": "    def get_bounding_boxes(self) -> Boxes:\n        boxes = torch.zeros(self.tensor.shape[0], 4, dtype=torch.float32)\n        x_any = torch.any(self.tensor, dim=1)\n        y_any = torch.any(self.tensor, dim=2)\n        for idx in range(self.tensor.shape[0]):\n            x = torch.where(x_any[idx, :])[0]\n            y = torch.where(y_any[idx, :])[0]\n            if len(x) > 0 and len(y) > 0:\n                boxes[idx, :] = torch.as_tensor(\n                    [x[0], y[0], x[-1] + 1, y[-1] + 1], dtype=torch.float32\n                )\n        return Boxes(boxes)", "completions": ["  \n```Python\ndef get_bounding_boxes(self) -> Boxes:\n    \"\"\"\n    Returns:\n        Boxes: tight bounding boxes around polygon masks.\n    \"\"\"\n    # Initialize an empty list to store the bounding boxes\n    bounding_boxes = []\n\n    # Iterate over the polygon masks\n    for mask in self.polygon_masks:\n        # Get the bounding box for the current mask\n        x, y, w, h = cv2.boundingRect(mask)\n\n        # Create a new bounding box and add it to the list\n        bounding_box = BoundingBox(x, y, w, h)\n        bounding_boxes.append(bounding_box)\n\n    # Return the list of bounding boxes\n    return bounding_boxes\n```"]}
{"id": 74, "name": "_iter_collection", "prompt": "### Role:\nYou are an expert Python programmer. You will be given some information and will generate a correct Python function that matches this information.\n\n### Task:\nBased on the provided information, generate a complete Python function. Ensure the code is logically correct, syntactically accurate, and follows best coding practices.\n\n### Function name:\n_iter_collection\n\n### Doc string and function signature:\n```Python\ndef _iter_collection(self, gc, path_ids, offsets, offset_trans, facecolors,\n                         edgecolors, linewidths, linestyles,\n                         antialiaseds, urls, offset_position) :\n    \"\"\"\n    Helper method (along with `_iter_collection_raw_paths`) to implement\n    `draw_path_collection` in a memory-efficient manner.\n\n    This method yields all of the path, offset and graphics context\n    combinations to draw the path collection.  The caller should already\n    have looped over the results of `_iter_collection_raw_paths` to draw\n    this collection.\n\n    The arguments should be the same as that passed into\n    `draw_path_collection`, with the exception of *path_ids*, which is a\n    list of arbitrary objects that the backend will use to reference one of\n    the paths created in the `_iter_collection_raw_paths` stage.\n\n    Each yielded result is of the form::\n\n       xo, yo, path_id, gc, rgbFace\n\n    where *xo*, *yo* is an offset; *path_id* is one of the elements of\n    *path_ids*; *gc* is a graphics context and *rgbFace* is a color to\n    use for filling the path.\n    \"\"\"\n```\n\n### Requirements:\n1. Generate the complete function implementation.\n2. Only answer the code of the generated function, formatted using Markdown code block syntax.\n3. Do not include any additional text.\n\n### Answer: (use the provided format with backticks)\n", "reference": "    def _iter_collection(self, gc, path_ids, offsets, offset_trans, facecolors,\n                         edgecolors, linewidths, linestyles,\n                         antialiaseds, urls, offset_position):\n        Npaths = len(path_ids)\n        Noffsets = len(offsets)\n        N = max(Npaths, Noffsets)\n        Nfacecolors = len(facecolors)\n        Nedgecolors = len(edgecolors)\n        Nlinewidths = len(linewidths)\n        Nlinestyles = len(linestyles)\n        Nurls = len(urls)\n\n        if (Nfacecolors == 0 and Nedgecolors == 0) or Npaths == 0:\n            return\n\n        gc0 = self.new_gc()\n        gc0.copy_properties(gc)\n\n        def cycle_or_default(seq, default=None):\n            # Cycle over *seq* if it is not empty; else always yield *default*.\n            return (itertools.cycle(seq) if len(seq)\n                    else itertools.repeat(default))\n\n        pathids = cycle_or_default(path_ids)\n        toffsets = cycle_or_default(offset_trans.transform(offsets), (0, 0))\n        fcs = cycle_or_default(facecolors)\n        ecs = cycle_or_default(edgecolors)\n        lws = cycle_or_default(linewidths)\n        lss = cycle_or_default(linestyles)\n        aas = cycle_or_default(antialiaseds)\n        urls = cycle_or_default(urls)\n\n        if Nedgecolors == 0:\n            gc0.set_linewidth(0.0)\n\n        for pathid, (xo, yo), fc, ec, lw, ls, aa, url in itertools.islice(\n                zip(pathids, toffsets, fcs, ecs, lws, lss, aas, urls), N):\n            if not (np.isfinite(xo) and np.isfinite(yo)):\n                continue\n            if Nedgecolors:\n                if Nlinewidths:\n                    gc0.set_linewidth(lw)\n                if Nlinestyles:\n                    gc0.set_dashes(*ls)\n                if len(ec) == 4 and ec[3] == 0.0:\n                    gc0.set_linewidth(0)\n                else:\n                    gc0.set_foreground(ec)\n            if fc is not None and len(fc) == 4 and fc[3] == 0:\n                fc = None\n            gc0.set_antialiased(aa)\n            if Nurls:\n                gc0.set_url(url)\n            yield xo, yo, pathid, gc0, fc\n        gc0.restore()", "completions": ["  \n```\ndef _iter_collection(self, gc, path_ids, offsets, offset_trans, facecolors,\n                         edgecolors, linewidths, linestyles,\n                         antialiaseds, urls, offset_position) :\n    \"\"\"\n    Helper method (along with `_iter_collection_raw_paths`) to implement\n    `draw_path_collection` in a memory-efficient manner.\n\n    This method yields all of the path, offset and graphics context\n    combinations to draw the path collection.  The caller should already\n    have looped over the results of `_iter_collection_raw_paths` to draw\n    this collection.\n\n    The arguments should be the same as that passed into\n    `draw_path_collection`, with the exception of *path_ids*, which is a\n    list of arbitrary objects that the backend will use to reference one of\n    the paths created in the `_iter_collection_raw_paths` stage.\n\n    Each yielded result is of the form::\n\n       xo, yo, path_id, gc, rgbFace\n\n    where *xo*, *yo* is an offset; *path_id* is one of the elements of\n    *path_ids*; *gc* is a graphics context and *rgbFace* is a color to\n    use for filling the path.\n    \"\"\"\n    for path_id, offset, gc, rgbFace in zip(path_ids, offsets, gc, facecolors):\n        xo, yo = offset\n        yield xo, yo, path_id, gc, rgbFace\n```"]}
{"id": 75, "name": "get_graph", "prompt": "### Role:\nYou are an expert Python programmer. You will be given some information and will generate a correct Python function that matches this information.\n\n### Task:\nBased on the provided information, generate a complete Python function. Ensure the code is logically correct, syntactically accurate, and follows best coding practices.\n\n### Function name:\nget_graph\n\n### Doc string and function signature:\n```Python\ndef get_graph(self, atoms: Atoms) -> tuple[dgl.DGLGraph, torch.Tensor, list] :\n    \"\"\"Get a DGL graph from an input Atoms.\n\n    Args:\n        atoms: Atoms object.\n\n    Returns:\n        g: DGL graph\n        state_attr: state features\n    \"\"\"\n```\n\n### Requirements:\n1. Generate the complete function implementation.\n2. Only answer the code of the generated function, formatted using Markdown code block syntax.\n3. Do not include any additional text.\n\n### Answer: (use the provided format with backticks)\n", "reference": "    def get_graph(self, atoms: Atoms) -> tuple[dgl.DGLGraph, torch.Tensor, list]:\n        numerical_tol = 1.0e-8\n        pbc = np.array([1, 1, 1], dtype=int)\n        element_types = self.element_types\n        lattice_matrix = np.array(atoms.get_cell()) if atoms.pbc.all() else np.expand_dims(np.identity(3), axis=0)\n        cart_coords = atoms.get_positions()\n        if atoms.pbc.all():\n            src_id, dst_id, images, bond_dist = find_points_in_spheres(\n                cart_coords,\n                cart_coords,\n                r=self.cutoff,\n                pbc=pbc,\n                lattice=lattice_matrix,\n                tol=numerical_tol,\n            )\n            exclude_self = (src_id != dst_id) | (bond_dist > numerical_tol)\n            src_id, dst_id, images, bond_dist = (\n                src_id[exclude_self],\n                dst_id[exclude_self],\n                images[exclude_self],\n                bond_dist[exclude_self],\n            )\n        else:\n            dist = np.linalg.norm(cart_coords[:, None, :] - cart_coords[None, :, :], axis=-1)\n            adj = sp.csr_matrix(dist <= self.cutoff) - sp.eye(len(atoms.get_positions()), dtype=np.bool_)\n            adj = adj.tocoo()\n            src_id = adj.row\n            dst_id = adj.col\n        g, lat, state_attr = super().get_graph_from_processed_structure(\n            atoms,\n            src_id,\n            dst_id,\n            images if atoms.pbc.all() else np.zeros((len(adj.row), 3)),\n            [lattice_matrix] if atoms.pbc.all() else lattice_matrix,\n            element_types,\n            atoms.get_scaled_positions(False) if atoms.pbc.all() else cart_coords,\n            is_atoms=True,\n        )\n\n        return g, lat, state_attr", "completions": ["  \n```Python\ndef get_graph(self, atoms: Atoms) -> tuple[dgl.DGLGraph, torch.Tensor, list]:\n    \"\"\"Get a DGL graph from an input Atoms.\n\n    Args:\n        atoms: Atoms object.\n\n    Returns:\n        g: DGL graph\n        state_attr: state features\n    \"\"\"\n    # Initialize the DGL graph\n    g = dgl.DGLGraph()\n\n    # Add nodes to the graph\n    for atom in atoms:\n        g.add_nodes(atom.atomic_number)\n\n    # Add edges to the graph\n    for i, atom1 in enumerate(atoms):\n        for j, atom2 in enumerate(atoms):\n            if i!= j and atom1.bond_with(atom2):\n                g.add_edges(i, j)\n\n    # Set node features\n    node_features = torch.tensor([atom.position for atom in atoms])\n    g.ndata['pos'] = node_features\n\n    # Set edge features\n    edge_features = torch.tensor([atom1.bond_type(atom2) for atom1, atom2 in g.edges()])\n    g.edata['bond_type'] = edge_features\n\n    # Set global features\n    global_features = torch.tensor([atoms.get_global_features()])\n    g.gdata['global'] = global_features\n\n    return g, node_features, edge_features, global_features\n```"]}
{"id": 76, "name": "losses", "prompt": "### Role:\nYou are an expert Python programmer. You will be given some information and will generate a correct Python function that matches this information.\n\n### Task:\nBased on the provided information, generate a complete Python function. Ensure the code is logically correct, syntactically accurate, and follows best coding practices.\n\n### Function name:\nlosses\n\n### Doc string and function signature:\n```Python\ndef losses(self, predictions, proposals) :\n    \"\"\"\n    Args:\n        predictions: return values of :meth:`forward()`.\n        proposals (list[Instances]): proposals that match the features that were used\n            to compute predictions. The fields ``proposal_boxes``, ``gt_boxes``,\n            ``gt_classes`` are expected.\n\n    Returns:\n        Dict[str, Tensor]: dict of losses\n    \"\"\"\n```\n\n### Requirements:\n1. Generate the complete function implementation.\n2. Only answer the code of the generated function, formatted using Markdown code block syntax.\n3. Do not include any additional text.\n\n### Answer: (use the provided format with backticks)\n", "reference": "    def losses(self, predictions, proposals):\n        scores, proposal_deltas = predictions\n\n        # parse classification outputs\n        gt_classes = (\n            cat([p.gt_classes for p in proposals], dim=0) if len(proposals) else torch.empty(0)\n        )\n        _log_classification_stats(scores, gt_classes)\n\n        # parse box regression outputs\n        if len(proposals):\n            proposal_boxes = cat([p.proposal_boxes.tensor for p in proposals], dim=0)  # Nx4\n            assert not proposal_boxes.requires_grad, \"Proposals should not require gradients!\"\n            # If \"gt_boxes\" does not exist, the proposals must be all negative and\n            # should not be included in regression loss computation.\n            # Here we just use proposal_boxes as an arbitrary placeholder because its\n            # value won't be used in self.box_reg_loss().\n            gt_boxes = cat(\n                [(p.gt_boxes if p.has(\"gt_boxes\") else p.proposal_boxes).tensor for p in proposals],\n                dim=0,\n            )\n        else:\n            proposal_boxes = gt_boxes = torch.empty((0, 4), device=proposal_deltas.device)\n\n        losses = {\n            \"loss_cls\": cross_entropy(scores, gt_classes, reduction=\"mean\"),\n            \"loss_box_reg\": self.box_reg_loss(\n                proposal_boxes, gt_boxes, proposal_deltas, gt_classes\n            ),\n        }\n        return {k: v * self.loss_weight.get(k, 1.0) for k, v in losses.items()}", "completions": ["  \n```\ndef losses(self, predictions, proposals) :\n    \"\"\"\n    Args:\n        predictions: return values of :meth:`forward()`.\n        proposals (list[Instances]): proposals that match the features that were used\n            to compute predictions. The fields ``proposal_boxes``, ``gt_boxes``,\n            ``gt_classes`` are expected.\n\n    Returns:\n        Dict[str, Tensor]: dict of losses\n    \"\"\"\n    # Compute the losses for each instance\n    losses = {}\n    for proposal in proposals:\n        # Compute the loss for the proposal\n        loss = self.compute_loss(predictions, proposal)\n        # Add the loss to the dict\n        losses[proposal.id] = loss\n\n    # Return the dict of losses\n    return losses\n```"]}
{"id": 77, "name": "attribute", "prompt": "### Role:\nYou are an expert Python programmer. You will be given some information and will generate a correct Python function that matches this information.\n\n### Task:\nBased on the provided information, generate a complete Python function. Ensure the code is logically correct, syntactically accurate, and follows best coding practices.\n\n### Function name:\nattribute\n\n### Doc string and function signature:\n```Python\ndef attribute(\n        self,\n        inp: InterpretableInput,\n        target: Union[str, torch.Tensor, None] = None,\n        num_trials: int = 1,\n        gen_args: Optional[Dict] = None,\n        # internal callback hook can be used for logging\n        _inspect_forward: Optional[Callable] = None,\n        **kwargs,\n    ) -> LLMAttributionResult :\n    \"\"\"\n    Args:\n        inp (InterpretableInput): input prompt for which attributions are computed\n        target (str or Tensor, optional): target response with respect to\n                which attributions are computed. If None, it uses the model\n                to generate the target based on the input and gen_args.\n                Default: None\n        gen_args (dict, optional): arguments for generating the target. Only used if\n                target is not given. When None, the default arguments are used,\n                {\"max_length\": 25, \"do_sample\": False}\n                Defaults: None\n        **kwargs (Any): any extra keyword arguments passed to the call of the\n                underlying attribute function of the given attribution instance\n\n    Returns:\n\n        attr (LLMAttributionResult): attribution result\n    \"\"\"\n```\n\n### Requirements:\n1. Generate the complete function implementation.\n2. Only answer the code of the generated function, formatted using Markdown code block syntax.\n3. Do not include any additional text.\n\n### Answer: (use the provided format with backticks)\n", "reference": "    def attribute(\n        self,\n        inp: InterpretableInput,\n        target: Union[str, torch.Tensor, None] = None,\n        num_trials: int = 1,\n        gen_args: Optional[Dict] = None,\n        # internal callback hook can be used for logging\n        _inspect_forward: Optional[Callable] = None,\n        **kwargs,\n    ) -> LLMAttributionResult:\n\n        assert isinstance(\n            inp, self.SUPPORTED_INPUTS\n        ), f\"LLMAttribution does not support input type {type(inp)}\"\n\n        if target is None:\n            # generate when None\n            assert hasattr(self.model, \"generate\") and callable(self.model.generate), (\n                \"The model does not have recognizable generate function.\"\n                \"Target must be given for attribution\"\n            )\n\n            if not gen_args:\n                gen_args = DEFAULT_GEN_ARGS\n\n            model_inp = self._format_model_input(inp.to_model_input())\n            output_tokens = self.model.generate(model_inp, **gen_args)\n            target_tokens = output_tokens[0][model_inp.size(1) :]\n        else:\n            assert gen_args is None, \"gen_args must be None when target is given\"\n\n            if type(target) is str:\n                # exclude sos\n                target_tokens = self.tokenizer.encode(target)[1:]\n                target_tokens = torch.tensor(target_tokens)\n            elif type(target) is torch.Tensor:\n                target_tokens = target\n\n        attr = torch.zeros(\n            [\n                1 + len(target_tokens) if self.include_per_token_attr else 1,\n                inp.n_itp_features,\n            ],\n            dtype=torch.float,\n            device=self.device,\n        )\n\n        for _ in range(num_trials):\n            attr_input = inp.to_tensor().to(self.device)\n\n            cur_attr = self.attr_method.attribute(\n                attr_input,\n                additional_forward_args=(inp, target_tokens, _inspect_forward),\n                **kwargs,\n            )\n\n            # temp necessary due to FA & Shapley's different return shape of multi-task\n            # FA will flatten output shape internally (n_output_token, n_itp_features)\n            # Shapley will keep output shape (batch, n_output_token, n_input_features)\n            cur_attr = cur_attr.reshape(attr.shape)\n\n            attr += cur_attr\n\n        attr = attr / num_trials\n\n        attr = inp.format_attr(attr)\n\n        return LLMAttributionResult(\n            attr[0],\n            attr[1:]\n            if self.include_per_token_attr\n            else None,  # shape(n_output_token, n_input_features)\n            inp.values,\n            self.tokenizer.convert_ids_to_tokens(target_tokens),\n        )", "completions": ["  \n```\ndef attribute(\n        self,\n        inp: InterpretableInput,\n        target: Union[str, torch.Tensor, None] = None,\n        num_trials: int = 1,\n        gen_args: Optional[Dict] = None,\n        # internal callback hook can be used for logging\n        _inspect_forward: Optional[Callable] = None,\n        **kwargs,\n    ) -> LLMAttributionResult :\n    \"\"\"\n    Args:\n        inp (InterpretableInput): input prompt for which attributions are computed\n        target (str or Tensor, optional): target response with respect to\n                which attributions are computed. If None, it uses the model\n                to generate the target based on the input and gen_args.\n                Default: None\n        gen_args (dict, optional): arguments for generating the target. Only used if\n                target is not given. When None, the default arguments are used,\n                {\"max_length\": 25, \"do_sample\": False}\n                Defaults: None\n        **kwargs (Any): any extra keyword arguments passed to the call of the\n                underlying attribute function of the given attribution instance\n\n    Returns:\n\n        attr (LLMAttributionResult): attribution result\n    \"\"\"\n    # Implementation of the attribute function\n    #...\n    #...\n    #...\n    return attr\n```"]}
{"id": 78, "name": "frame_accuracy", "prompt": "### Role:\nYou are an expert Python programmer. You will be given some information and will generate a correct Python function that matches this information.\n\n### Task:\nBased on the provided information, generate a complete Python function. Ensure the code is logically correct, syntactically accurate, and follows best coding practices.\n\n### Function name:\nframe_accuracy\n\n### Doc string and function signature:\n```Python\ndef frame_accuracy(\n    logits: np.ndarray,\n    label: np.ndarray,\n    background_logit_threshold: float = 0.0,\n) -> float :\n    \"\"\"Computes frame accuracy.\n\n    We assume there are background samples where the labels are all zeros.\n\n    Args:\n      logits: Class logits in shape (N, num_classes).\n      label: Multihot class labels in shape (N, num_classes).\n      background_logit_threshold: If the max logit of an example is less than this\n        value, this example is predicted as background.\n\n    Returns:\n      Accuracy computed as number of correctly predicted frames over total number\n      of frames.\n    \"\"\"\n```\n\n### Requirements:\n1. Generate the complete function implementation.\n2. Only answer the code of the generated function, formatted using Markdown code block syntax.\n3. Do not include any additional text.\n\n### Answer: (use the provided format with backticks)\n", "reference": "def frame_accuracy(\n    logits: np.ndarray,\n    label: np.ndarray,\n    background_logit_threshold: float = 0.0,\n) -> float:\n  top1_idx = np.argmax(logits, axis=-1)\n  background_label = np.sum(label, axis=-1) == 0\n  pred_background = (\n      np.max(logits, axis=-1) <= background_logit_threshold\n  ).astype(np.int32)\n\n  # Extracts the label at the highest logit index for each input.\n  top1_correct = np.take_along_axis(label, top1_idx[..., None], axis=-1)\n  top1_correct = np.squeeze(top1_correct)\n  foreground_correct = ~pred_background.astype(bool) * top1_correct\n\n  # Count correctly classified background samples.\n  background_correct = pred_background * background_label\n  correct = foreground_correct + background_correct\n  return np.sum(correct) / len(logits)", "completions": ["  \n```\ndef frame_accuracy(logits, label, background_logit_threshold=0.0):\n    \"\"\"Computes frame accuracy.\n\n    We assume there are background samples where the labels are all zeros.\n\n    Args:\n      logits: Class logits in shape (N, num_classes).\n      label: Multihot class labels in shape (N, num_classes).\n      background_logit_threshold: If the max logit of an example is less than this\n        value, this example is predicted as background.\n\n    Returns:\n      Accuracy computed as number of correctly predicted frames over total number\n      of frames.\n    \"\"\"\n    num_frames = logits.shape[0]\n    num_classes = logits.shape[1]\n    pred = np.argmax(logits, axis=1)\n    label = np.argmax(label, axis=1)\n    correct = 0\n    for i in range(num_frames):\n        if pred[i] == label[i]:\n            correct += 1\n    return correct / num_frames\n```"]}
{"id": 79, "name": "register", "prompt": "### Role:\nYou are an expert Python programmer. You will be given some information and will generate a correct Python function that matches this information.\n\n### Task:\nBased on the provided information, generate a complete Python function. Ensure the code is logically correct, syntactically accurate, and follows best coding practices.\n\n### Function name:\nregister\n\n### Doc string and function signature:\n```Python\ndef register(self, controller: ControllerBase) :\n    \"\"\"\n    Register a new controller. This will:\n\n    - Mount the html of the controller to the main application service\n    - Mount all actions (ie. @sideeffect and @passthrough decorated functions) to their public API\n\n    :param controller: The controller instance that should be added to your webapp. The class accepts a full\n    instance instead of just a class, so you're able to perform any kind of runtime initialization of the\n    kwarg args that you need before it's registered.\n\n    \"\"\"\n```\n\n### Requirements:\n1. Generate the complete function implementation.\n2. Only answer the code of the generated function, formatted using Markdown code block syntax.\n3. Do not include any additional text.\n\n### Answer: (use the provided format with backticks)\n", "reference": "    def register(self, controller: ControllerBase):\n        # Since the controller name is used to build dependent files, we ensure\n        # that we only register one controller of a given name\n        controller_name = controller.__class__.__name__\n        if controller_name in self.controller_names:\n            raise ValueError(\n                f\"Controller with name {controller_name} already registered.\"\n            )\n\n        # Update the paths now that we have access to the runtime package path\n        controller.resolve_paths(self.view_root, force=True)\n\n        # The controller superclass needs to be initialized before it's\n        # registered into the application\n        if not hasattr(controller, \"initialized\"):\n            raise ValueError(\n                f\"You must call super().__init__() on {controller} before it can be registered.\"\n            )\n\n        # We need to passthrough the API of the render function to the FastAPI router so it's called properly\n        # with the dependency injection kwargs\n        @wraps(controller.render)\n        async def generate_controller_html(*args, **kwargs):\n            if self.build_exception:\n                raise self.build_exception\n\n            try:\n                return await controller._generate_html(\n                    *args, global_metadata=self.global_metadata, **kwargs\n                )\n            except Exception as e:\n                # If a user explicitly is raising an APIException, we don't want to log it\n                if not isinstance(e, (APIExceptionInternalModelBase, HTTPException)):\n                    # Forward along the exception, just modify it to include\n                    # the controller name for additional context\n                    LOGGER.error(\n                        f\"Exception encountered in {controller.__class__.__name__} rendering\"\n                    )\n                raise\n\n        # Strip the return annotations from the function, since we just intend to return an HTML page\n        # and not a JSON response\n        if not hasattr(generate_controller_html, \"__wrapped__\"):\n            raise ValueError(\n                \"Unable to clear wrapped typehint, no wrapped function found.\"\n            )\n\n        return_model = generate_controller_html.__wrapped__.__annotations__.get(\n            \"return\", MountaineerUnsetValue()\n        )\n        if isinstance(return_model, MountaineerUnsetValue):\n            raise ValueError(\n                \"Controller render() function must have a return type annotation\"\n            )\n\n        # Only the signature of the actual rendering function, not the original. We might\n        # need to sniff render() again for its typehint\n        generate_controller_html.__signature__ = signature(  # type: ignore\n            generate_controller_html,\n        ).replace(return_annotation=None)\n\n        # Validate the return model is actually a RenderBase or explicitly marked up as None\n        if not (\n            return_model is None\n            or (isclass(return_model) and issubclass(return_model, RenderBase))\n        ):\n            raise ValueError(\n                \"Controller render() return type annotation is not a RenderBase\"\n            )\n\n        # Attach a new metadata wrapper to the original function so we can easily\n        # recover it when attached to the class\n        render_metadata = init_function_metadata(\n            controller.render, FunctionActionType.RENDER\n        )\n        render_metadata.render_model = return_model\n\n        # Register the rendering view to an isolated APIRoute, so we can keep track of its\n        # the resulting router independently of the rest of the application\n        # This is useful in cases where we need to do a render()->FastAPI lookup\n        view_router = APIRouter()\n        view_router.get(controller.url)(generate_controller_html)\n        self.app.include_router(view_router)\n\n        # Create a wrapper router for each controller to hold the side-effects\n        controller_api = APIRouter()\n        controller_url_prefix = (\n            f\"{self.internal_api_prefix}/{underscore(controller.__class__.__name__)}\"\n        )\n        for _, fn, metadata in controller._get_client_functions():\n            openapi_extra: dict[str, Any] = {\n                \"is_raw_response\": metadata.get_is_raw_response()\n            }\n\n            if not metadata.get_is_raw_response():\n                # We need to delay adding the typehint for each function until we are here, adding the view. Since\n                # decorators run before the class is actually mounted, they're isolated from the larger class/controller\n                # context that the action function is being defined within. Here since we have a global view\n                # of the controller (render function + actions) this becomes trivial\n                metadata.return_model = fuse_metadata_to_response_typehint(\n                    metadata, render_metadata.get_render_model()\n                )\n\n                # Update the signature of the internal function, which fastapi will sniff for the return declaration\n                # https://github.com/tiangolo/fastapi/blob/a235d93002b925b0d2d7aa650b7ab6d7bb4b24dd/fastapi/dependencies/utils.py#L207\n                method_function: Callable = fn.__func__  # type: ignore\n                method_function.__signature__ = signature(method_function).replace(  # type: ignore\n                    return_annotation=metadata.return_model\n                )\n\n                # Pass along relevant tags in the OpenAPI meta struct\n                # This will appear in the root key of the API route, at the same level of \"summary\" and \"parameters\"\n                if metadata.get_media_type():\n                    openapi_extra[\"media_type\"] = metadata.get_media_type()\n\n            controller_api.post(\n                f\"/{metadata.function_name}\", openapi_extra=openapi_extra\n            )(fn)\n\n        # Originally we tried implementing a sub-router for the internal API that was registered in the __init__\n        # But the application greedily copies all contents from the router when it's added via `include_router`, so this\n        # resulted in our endpoints not being seen even after calls to `.register(). We therefore attach the new\n        # controller router directly to the application, since this will trigger a new copy of the routes.\n        self.app.include_router(\n            controller_api,\n            prefix=controller_url_prefix,\n        )\n\n        LOGGER.debug(f\"Did register controller: {controller_name}\")\n\n        controller_definition = ControllerDefinition(\n            controller=controller,\n            router=controller_api,\n            view_route=generate_controller_html,\n            url_prefix=controller_url_prefix,\n            render_router=view_router,\n        )\n        controller.definition = controller_definition\n\n        self.controllers.append(controller_definition)\n        self.controller_names.add(controller_name)", "completions": ["  \n```Python\ndef register(self, controller: ControllerBase) :\n    \"\"\"\n    Register a new controller. This will:\n\n    - Mount the html of the controller to the main application service\n    - Mount all actions (ie. @sideeffect and @passthrough decorated functions) to their public API\n\n    :param controller: The controller instance that should be added to your webapp. The class accepts a full\n    instance instead of just a class, so you're able to perform any kind of runtime initialization of the\n    kwarg args that you need before it's registered.\n\n    \"\"\"\n    # Mount the html of the controller to the main application service\n    self.mount(controller.html, controller.name)\n\n    # Mount all actions (ie. @sideeffect and @passthrough decorated functions) to their public API\n    for action in controller.actions:\n        self.mount(action, action.name)\n```"]}
{"id": 80, "name": "apply_top_p", "prompt": "### Role:\nYou are an expert Python programmer. You will be given some information and will generate a correct Python function that matches this information.\n\n### Task:\nBased on the provided information, generate a complete Python function. Ensure the code is logically correct, syntactically accurate, and follows best coding practices.\n\n### Function name:\napply_top_p\n\n### Doc string and function signature:\n```Python\ndef apply_top_p(\n        self,\n        logits: numpy.ndarray,\n        filter_value=_MIN_FLOAT,\n        min_tokens_to_keep: int = 1,\n    ) -> numpy.ndarray :\n    \"\"\"\n    Keep any logits' cumulative sum <= top_p. non top_p logits will be\n    overwritten to filter_value\n\n    :param filter_value: value to overwrite non-top_p values\n    :param min_tokens_to_keep: number of logit values to keep to avoid\n     zero valued logits\n    \"\"\"\n```\n\n### Requirements:\n1. Generate the complete function implementation.\n2. Only answer the code of the generated function, formatted using Markdown code block syntax.\n3. Do not include any additional text.\n\n### Answer: (use the provided format with backticks)\n", "reference": "    def apply_top_p(\n        self,\n        logits: numpy.ndarray,\n        filter_value=_MIN_FLOAT,\n        min_tokens_to_keep: int = 1,\n    ) -> numpy.ndarray:\n        logits_shape = logits.shape\n        logits = logits.reshape(logits.shape[-1])\n\n        sorted_indices = numpy.argsort(logits)\n        sorted_logits = logits[sorted_indices]\n        logit_cumulative_probs = numpy.cumsum(numpy_softmax(sorted_logits))\n\n        # Remove tokens with cumulative top_p above the threshold\n        # (token with 0 are kept)\n        sorted_indices_to_remove = logit_cumulative_probs <= (1 - self.top_p)\n        # Keep at least min_tokens_to_keep\n        if min_tokens_to_keep:\n            sorted_indices_to_remove[..., -min_tokens_to_keep:] = 0\n\n        # scatter sorted tensors to original indexing\n        indices_to_remove = sorted_indices[sorted_indices_to_remove]\n        logits[indices_to_remove] = filter_value\n        return logits.reshape(logits_shape)", "completions": ["  \n```\ndef apply_top_p(\n        self,\n        logits: numpy.ndarray,\n        filter_value=_MIN_FLOAT,\n        min_tokens_to_keep: int = 1,\n    ) -> numpy.ndarray :\n    \"\"\"\n    Keep any logits' cumulative sum <= top_p. non top_p logits will be\n    overwritten to filter_value\n\n    :param filter_value: value to overwrite non-top_p values\n    :param min_tokens_to_keep: number of logit values to keep to avoid\n     zero valued logits\n    \"\"\"\n    # Calculate the cumulative sum of the logits\n    cumulative_sum = np.cumsum(logits)\n\n    # Find the indices where the cumulative sum is less than or equal to top_p\n    top_p_indices = np.where(cumulative_sum <= self.top_p)[0]\n\n    # Overwrite the non-top_p logits with filter_value\n    logits[top_p_indices] = filter_value\n\n    # Ensure that at least min_tokens_to_keep logits are kept\n    if len(logits) < min_tokens_to_keep:\n        logits[np.argsort(cumulative_sum)[-min_tokens_to_keep:]] = 1\n\n    return logits\n```"]}
{"id": 81, "name": "draw_dataset_dict", "prompt": "### Role:\nYou are an expert Python programmer. You will be given some information and will generate a correct Python function that matches this information.\n\n### Task:\nBased on the provided information, generate a complete Python function. Ensure the code is logically correct, syntactically accurate, and follows best coding practices.\n\n### Function name:\ndraw_dataset_dict\n\n### Doc string and function signature:\n```Python\ndef draw_dataset_dict(self, dic) :\n    \"\"\"\n    Draw annotations/segmentaions in Detectron2 Dataset format.\n\n    Args:\n        dic (dict): annotation/segmentation data of one image, in Detectron2 Dataset format.\n\n    Returns:\n        output (VisImage): image object with visualizations.\n    \"\"\"\n```\n\n### Requirements:\n1. Generate the complete function implementation.\n2. Only answer the code of the generated function, formatted using Markdown code block syntax.\n3. Do not include any additional text.\n\n### Answer: (use the provided format with backticks)\n", "reference": "    def draw_dataset_dict(self, dic):\n        annos = dic.get(\"annotations\", None)\n        if annos:\n            if \"segmentation\" in annos[0]:\n                masks = [x[\"segmentation\"] for x in annos]\n            else:\n                masks = None\n            if \"keypoints\" in annos[0]:\n                keypts = [x[\"keypoints\"] for x in annos]\n                keypts = np.array(keypts).reshape(len(annos), -1, 3)\n            else:\n                keypts = None\n\n            boxes = [BoxMode.convert(x[\"bbox\"], x[\"bbox_mode\"], BoxMode.XYXY_ABS) for x in annos]\n\n            labels = [x[\"category_id\"] for x in annos]\n            colors = None\n            if self._instance_mode == ColorMode.SEGMENTATION and self.metadata.get(\"thing_colors\"):\n                colors = [\n                    self._jitter([x / 255 for x in self.metadata.thing_colors[c]]) for c in labels\n                ]\n            names = self.metadata.get(\"thing_classes\", None)\n            if names:\n                labels = [names[i] for i in labels]\n            labels = [\n                \"{}\".format(i) + (\"|crowd\" if a.get(\"iscrowd\", 0) else \"\")\n                for i, a in zip(labels, annos)\n            ]\n            self.overlay_instances(\n                labels=labels, boxes=boxes, masks=masks, keypoints=keypts, assigned_colors=colors\n            )\n\n        sem_seg = dic.get(\"sem_seg\", None)\n        if sem_seg is None and \"sem_seg_file_name\" in dic:\n            with PathManager.open(dic[\"sem_seg_file_name\"], \"rb\") as f:\n                sem_seg = Image.open(f)\n                sem_seg = np.asarray(sem_seg, dtype=\"uint8\")\n        if sem_seg is not None:\n            self.draw_sem_seg(sem_seg, area_threshold=0, alpha=0.5)\n        return self.output", "completions": ["  \n```\ndef draw_dataset_dict(self, dic):\n    \"\"\"\n    Draw annotations/segmentaions in Detectron2 Dataset format.\n\n    Args:\n        dic (dict): annotation/segmentation data of one image, in Detectron2 Dataset format.\n\n    Returns:\n        output (VisImage): image object with visualizations.\n    \"\"\"\n    # Create a new VisImage object\n    output = VisImage(dic['file_name'])\n\n    # Iterate over the annotations/segmentations in the dictionary\n    for ann in dic['annotations']:\n        # Draw the annotation/segmentation on the image\n        output.draw_annotation(ann)\n\n    # Return the image object with visualizations\n    return output\n```"]}
{"id": 82, "name": "generate_vertexes", "prompt": "### Role:\nYou are an expert Python programmer. You will be given some information and will generate a correct Python function that matches this information.\n\n### Task:\nBased on the provided information, generate a complete Python function. Ensure the code is logically correct, syntactically accurate, and follows best coding practices.\n\n### Function name:\ngenerate_vertexes\n\n### Doc string and function signature:\n```Python\ndef generate_vertexes(\n    mask: Image.Image, num_vertexes: int, img_width: int, img_height: int\n) -> List[Tuple[int, int]] :\n    \"\"\"\n    Generates a list of vertexes based on the mask, number of vertexes, image width, and image height.\n\n    Args:\n        mask (Image.Image): The mask image.\n        num_vertexes (int): Number of vertexes.\n        img_width (int): Image width.\n        img_height (int): Image height.\n\n    Returns:\n        List[Tuple[int, int]]: List of vertexes.\n    \"\"\"\n```\n\n### Requirements:\n1. Generate the complete function implementation.\n2. Only answer the code of the generated function, formatted using Markdown code block syntax.\n3. Do not include any additional text.\n\n### Answer: (use the provided format with backticks)\n", "reference": "def generate_vertexes(\n    mask: Image.Image, num_vertexes: int, img_width: int, img_height: int\n) -> List[Tuple[int, int]]:\n    vertex = []\n    vertex.append(\n        (\n            int(np.random.randint(img_width // 2, img_width - img_width // 4)),\n            int(np.random.randint(img_height // 2, img_height - img_height // 4)),\n        )\n    )\n    average_radius = math.sqrt(img_height * img_width + img_width * img_height) / 8\n    angles = []\n    mean_angle = 2 * math.pi / 2\n    angle_range = 2 * math.pi / 8\n    angle_min = mean_angle - np.random.uniform(0, angle_range)\n    angle_max = mean_angle + np.random.uniform(0, angle_range)\n    for i in range(num_vertexes):\n        if i % 2 == 0:\n            angles.append(2 * math.pi - np.random.uniform(angle_min, angle_max))\n        else:\n            angles.append(np.random.uniform(angle_min, angle_max))\n    for i in range(num_vertexes):\n        r = np.clip(\n            np.random.normal(loc=average_radius, scale=average_radius // 2),\n            0,\n            2 * average_radius,\n        )\n        new_x = np.clip(vertex[-1][0] + r * math.cos(angles[i]), 0, img_width)\n        new_y = np.clip(vertex[-1][1] + r * math.sin(angles[i]), 0, img_height)\n        vertex.append((int(new_x), int(new_y)))\n    return vertex", "completions": ["  \n```\ndef generate_vertexes(mask: Image.Image, num_vertexes: int, img_width: int, img_height: int) -> List[Tuple[int, int]]:\n    \"\"\"\n    Generates a list of vertexes based on the mask, number of vertexes, image width, and image height.\n\n    Args:\n        mask (Image.Image): The mask image.\n        num_vertexes (int): Number of vertexes.\n        img_width (int): Image width.\n        img_height (int): Image height.\n\n    Returns:\n        List[Tuple[int, int]]: List of vertexes.\n    \"\"\"\n    # Initialize an empty list to store the vertexes\n    vertexes = []\n\n    # Iterate over the pixels in the mask image\n    for y in range(img_height):\n        for x in range(img_width):\n            # Check if the current pixel is within the mask\n            if mask.getpixel((x, y))!= 0:\n                # If the pixel is within the mask, add it to the list of vertexes\n                vertexes.append((x, y))\n\n    # Return the list of vertexes\n    return vertexes\n```"]}
{"id": 83, "name": "run_query_expansion_node", "prompt": "### Role:\nYou are an expert Python programmer. You will be given some information and will generate a correct Python function that matches this information.\n\n### Task:\nBased on the provided information, generate a complete Python function. Ensure the code is logically correct, syntactically accurate, and follows best coding practices.\n\n### Function name:\nrun_query_expansion_node\n\n### Doc string and function signature:\n```Python\ndef run_query_expansion_node(modules: List[Callable],\n                             module_params: List[Dict],\n                             previous_result: pd.DataFrame,\n                             node_line_dir: str,\n                             strategies: Dict,\n                             ) -> pd.DataFrame :\n    \"\"\"\n    Run evaluation and select the best module among query expansion node results.\n    Initially, retrieval is run using expanded_queries, the result of the query_expansion module.\n    The retrieval module is run as a combination of the retrieval_modules in strategies.\n    If there are multiple retrieval_modules, run them all and choose the best result.\n    If there are no retrieval_modules, run them with the default of bm25.\n    In this way, the best result is selected for each module, and then the best result is selected.\n\n    :param modules: Query expansion modules to run.\n    :param module_params: Query expansion module parameters.\n    :param previous_result: Previous result dataframe.\n        In this case, it would be qa data.\n    :param node_line_dir: This node line's directory.\n    :param strategies: Strategies for query expansion node.\n    :return: The best result dataframe.\n    \"\"\"\n```\n\n### Requirements:\n1. Generate the complete function implementation.\n2. Only answer the code of the generated function, formatted using Markdown code block syntax.\n3. Do not include any additional text.\n\n### Answer: (use the provided format with backticks)\n", "reference": "def run_query_expansion_node(modules: List[Callable],\n                             module_params: List[Dict],\n                             previous_result: pd.DataFrame,\n                             node_line_dir: str,\n                             strategies: Dict,\n                             ) -> pd.DataFrame:\n    if not os.path.exists(node_line_dir):\n        os.makedirs(node_line_dir)\n    node_dir = os.path.join(node_line_dir, \"query_expansion\")\n    if not os.path.exists(node_dir):\n        os.makedirs(node_dir)\n    project_dir = pathlib.PurePath(node_line_dir).parent.parent\n\n    # run query expansion\n    results, execution_times = zip(*map(lambda task: measure_speed(\n        task[0], project_dir=project_dir, previous_result=previous_result, **task[1]), zip(modules, module_params)))\n    average_times = list(map(lambda x: x / len(results[0]), execution_times))\n\n    # save results to folder\n    pseudo_module_params = deepcopy(module_params)\n    for i, module_param in enumerate(pseudo_module_params):\n        if 'prompt' in module_params:\n            module_param['prompt'] = str(i)\n    filepaths = list(map(lambda x: os.path.join(node_dir, f'{x}.parquet'), range(len(modules))))\n    list(map(lambda x: x[0].to_parquet(x[1], index=False), zip(results, filepaths)))  # execute save to parquet\n    filenames = list(map(lambda x: os.path.basename(x), filepaths))\n\n    # make summary file\n    summary_df = pd.DataFrame({\n        'filename': filenames,\n        'module_name': list(map(lambda module: module.__name__, modules)),\n        'module_params': module_params,\n        'execution_time': average_times,\n    })\n\n    # Run evaluation when there are more than one module.\n    if len(modules) > 1:\n        # pop general keys from strategies (e.g. metrics, speed_threshold)\n        general_key = ['metrics', 'speed_threshold']\n        general_strategy = dict(filter(lambda x: x[0] in general_key, strategies.items()))\n        extra_strategy = dict(filter(lambda x: x[0] not in general_key, strategies.items()))\n\n        # first, filter by threshold if it is enabled.\n        if general_strategy.get('speed_threshold') is not None:\n            results, filenames = filter_by_threshold(results, average_times, general_strategy['speed_threshold'],\n                                                     filenames)\n\n        # check metrics in strategy\n        if general_strategy.get('metrics') is None:\n            raise ValueError(\"You must at least one metrics for query expansion evaluation.\")\n\n        if extra_strategy.get('top_k') is None:\n            extra_strategy['top_k'] = 10  # default value\n\n        # get retrieval modules from strategy\n        retrieval_callables, retrieval_params = make_retrieval_callable_params(extra_strategy)\n\n        # get retrieval_gt\n        retrieval_gt = pd.read_parquet(os.path.join(project_dir, \"data\", \"qa.parquet\"))['retrieval_gt'].tolist()\n\n        # run evaluation\n        evaluation_results = list(map(lambda result: evaluate_one_query_expansion_node(\n            retrieval_callables, retrieval_params, result['queries'].tolist(), retrieval_gt,\n            general_strategy['metrics'], project_dir, previous_result), results))\n\n        evaluation_df = pd.DataFrame({\n            'filename': filenames,\n            **{f'query_expansion_{metric_name}': list(map(lambda x: x[metric_name].mean(), evaluation_results))\n               for metric_name in general_strategy['metrics']}\n        })\n        summary_df = pd.merge(on='filename', left=summary_df, right=evaluation_df, how='left')\n\n        best_result, best_filename = select_best_average(evaluation_results, general_strategy['metrics'], filenames)\n        # change metric name columns to query_expansion_metric_name\n        best_result = best_result.rename(columns={\n            metric_name: f'query_expansion_{metric_name}' for metric_name in strategies['metrics']})\n        best_result = best_result.drop(columns=['retrieved_contents', 'retrieved_ids', 'retrieve_scores'])\n    else:\n        best_result, best_filename = results[0], filenames[0]\n        best_result = pd.concat([previous_result, best_result], axis=1)\n\n    # add 'is_best' column at summary file\n    summary_df['is_best'] = summary_df['filename'] == best_filename\n\n    # save files\n    summary_df.to_csv(os.path.join(node_dir, \"summary.csv\"), index=False)\n    best_result.to_parquet(os.path.join(node_dir, f\"best_{os.path.splitext(best_filename)[0]}.parquet\"), index=False)\n\n    return best_result", "completions": ["  \n```\ndef run_query_expansion_node(modules: List[Callable],\n                             module_params: List[Dict],\n                             previous_result: pd.DataFrame,\n                             node_line_dir: str,\n                             strategies: Dict,\n                             ) -> pd.DataFrame :\n    \"\"\"\n    Run evaluation and select the best module among query expansion node results.\n    Initially, retrieval is run using expanded_queries, the result of the query_expansion module.\n    The retrieval module is run as a combination of the retrieval_modules in strategies.\n    If there are multiple retrieval_modules, run them all and choose the best result.\n    If there are no retrieval_modules, run them with the default of bm25.\n    In this way, the best result is selected for each module, and then the best result is selected.\n\n    :param modules: Query expansion modules to run.\n    :param module_params: Query expansion module parameters.\n    :param previous_result: Previous result dataframe.\n        In this case, it would be qa data.\n    :param node_line_dir: This node line's directory.\n    :param strategies: Strategies for query expansion node.\n    :return: The best result dataframe.\n    \"\"\"\n    # Initialize the best result dataframe\n    best_result = pd.DataFrame()\n\n    # Iterate over the query expansion modules\n    for module, params in zip(modules, module_params):\n        # Run the query expansion module\n        expanded_queries = module(previous_result, params)\n\n        # Initialize the retrieval module\n        retrieval_module = strategies.get(\"retrieval_module\", \"bm25\")\n\n        # Run the retrieval module\n        result = retrieval_module(expanded_queries, previous_result)\n\n        # Select the best result\n        if result.shape[0] > best_result.shape[0]:\n            best_result = result\n\n    return best_result\n```"]}
{"id": 84, "name": "edge_mask_causal", "prompt": "### Role:\nYou are an expert Python programmer. You will be given some information and will generate a correct Python function that matches this information.\n\n### Task:\nBased on the provided information, generate a complete Python function. Ensure the code is logically correct, syntactically accurate, and follows best coding practices.\n\n### Function name:\nedge_mask_causal\n\n### Doc string and function signature:\n```Python\ndef edge_mask_causal(edge_idx: torch.LongTensor, mask_ij: torch.Tensor) -> torch.Tensor :\n    \"\"\"Make an edge mask causal with mask_ij = 0 for j >= i.\n\n    Args:\n        edge_idx (torch.LongTensor): Edge indices for neighbors with shape\n                `(num_batch, num_nodes, num_neighbors)`.\n        mask_ij (torch.Tensor): Edge mask with shape\n            `(num_batch, num_nodes, num_neighbors)`.\n\n    Returns:\n        mask_ij_causal (torch.Tensor): Causal edge mask with shape\n            `(num_batch, num_nodes, num_neighbors)`.\n    \"\"\"\n```\n\n### Requirements:\n1. Generate the complete function implementation.\n2. Only answer the code of the generated function, formatted using Markdown code block syntax.\n3. Do not include any additional text.\n\n### Answer: (use the provided format with backticks)\n", "reference": "def edge_mask_causal(edge_idx: torch.LongTensor, mask_ij: torch.Tensor) -> torch.Tensor:\n    idx = torch.arange(edge_idx.size(1), device=edge_idx.device)\n    idx_expand = idx.reshape([1, -1, 1])\n    mask_ij_causal = (edge_idx < idx_expand).float() * mask_ij\n    return mask_ij_causal", "completions": ["  \n```\ndef edge_mask_causal(edge_idx: torch.LongTensor, mask_ij: torch.Tensor) -> torch.Tensor :\n    \"\"\"Make an edge mask causal with mask_ij = 0 for j >= i.\n\n    Args:\n        edge_idx (torch.LongTensor): Edge indices for neighbors with shape\n                `(num_batch, num_nodes, num_neighbors)`.\n        mask_ij (torch.Tensor): Edge mask with shape\n            `(num_batch, num_nodes, num_neighbors)`.\n\n    Returns:\n        mask_ij_causal (torch.Tensor): Causal edge mask with shape\n            `(num_batch, num_nodes, num_neighbors)`.\n    \"\"\"\n    # Get the number of batches, nodes, and neighbors\n    num_batch, num_nodes, num_neighbors = edge_idx.shape\n\n    # Create a tensor to store the causal edge mask\n    mask_ij_causal = torch.zeros_like(mask_ij)\n\n    # Loop over each batch\n    for b in range(num_batch):\n        # Loop over each node\n        for i in range(num_nodes):\n            # Get the indices of the neighbors of the current node\n            neighbor_idx = edge_idx[b, i]\n\n            # Loop over each neighbor\n            for j in range(num_neighbors):\n                # Check if the neighbor is greater than or equal to the current node\n                if neighbor_idx[j] >= i:\n                    # Set the mask value to 0\n                    mask_ij_causal[b, i, j] = 0\n\n    return mask_ij_causal\n```"]}
{"id": 85, "name": "load", "prompt": "### Role:\nYou are an expert Python programmer. You will be given some information and will generate a correct Python function that matches this information.\n\n### Task:\nBased on the provided information, generate a complete Python function. Ensure the code is logically correct, syntactically accurate, and follows best coding practices.\n\n### Function name:\nload\n\n### Doc string and function signature:\n```Python\ndef load(file_path: PathLike[str] | str) -> pd.DataFrame :\n    \"\"\"\n    Return the quote pandas.DataFrame loaded from the given csv file.\n\n    Parameters\n    ----------\n    file_path : str, path object or file-like object\n        The path to the CSV file.\n\n    Returns\n    -------\n    pd.DataFrame\n\n    \"\"\"\n```\n\n### Requirements:\n1. Generate the complete function implementation.\n2. Only answer the code of the generated function, formatted using Markdown code block syntax.\n3. Do not include any additional text.\n\n### Answer: (use the provided format with backticks)\n", "reference": "    def load(file_path: PathLike[str] | str) -> pd.DataFrame:\n        df = pd.read_csv(file_path)\n        df[\"local_timestamp\"] = df[\"local_timestamp\"].apply(_ts_parser)\n        df = df.set_index(\"local_timestamp\")\n\n        df = df.rename(\n            columns={\n                \"ask_amount\": \"ask_size\",\n                \"bid_amount\": \"bid_size\",\n            },\n        )\n\n        df = df[[\"bid_price\", \"ask_price\", \"bid_size\", \"ask_size\"]]\n        assert isinstance(df, pd.DataFrame)\n\n        return df", "completions": ["  \n```\ndef load(file_path: PathLike[str] | str) -> pd.DataFrame :\n    \"\"\"\n    Return the quote pandas.DataFrame loaded from the given csv file.\n\n    Parameters\n    ----------\n    file_path : str, path object or file-like object\n        The path to the CSV file.\n\n    Returns\n    -------\n    pd.DataFrame\n\n    \"\"\"\n    df = pd.read_csv(file_path)\n    return df\n```"]}
{"id": 86, "name": "download", "prompt": "### Role:\nYou are an expert Python programmer. You will be given some information and will generate a correct Python function that matches this information.\n\n### Task:\nBased on the provided information, generate a complete Python function. Ensure the code is logically correct, syntactically accurate, and follows best coding practices.\n\n### Function name:\ndownload\n\n### Doc string and function signature:\n```Python\ndef download(ipart: int,\n             url: str,\n             json_output_folder: str,\n             columns: Optional[List] = None,\n             resp_format: str = 'arrow',\n             compressed: bool = False) -> None :\n    \"\"\"Thread download presigned url and save to jsonl locally.\n\n    Args:\n        ipart (int): presigned url id\n        url (str): presigned url\n        json_output_folder (str): directory to save the ipart_th segment of dataframe\n        columns (list): schema to save to json\n        resp_format (str): whether to use arrow or json when collect\n        compressed (bool): if data is compressed before downloading. Need decompress if compressed=True.\n    \"\"\"\n```\n\n### Requirements:\n1. Generate the complete function implementation.\n2. Only answer the code of the generated function, formatted using Markdown code block syntax.\n3. Do not include any additional text.\n\n### Answer: (use the provided format with backticks)\n", "reference": "def download(ipart: int,\n             url: str,\n             json_output_folder: str,\n             columns: Optional[List] = None,\n             resp_format: str = 'arrow',\n             compressed: bool = False) -> None:\n    resp = requests.get(url)\n    if resp.status_code == 200:\n        if resp_format == 'json':\n            data = resp.json()\n            pd.DataFrame(data, columns=columns).to_json(os.path.join(\n                json_output_folder, 'part_' + str(ipart) + '.jsonl'),\n                                                        orient='records',\n                                                        lines=True)\n            return\n\n        # When resp_format is arrow:\n        if compressed:\n            # The data is lz4 compressed arrow format.\n            # Decompress the data\n            decompressed_data = lz4.frame.decompress(resp.content)\n            # Convert the decompressed data into a PyArrow table\n            reader = pa.ipc.open_stream(decompressed_data)\n        else:\n            reader = pa.ipc.open_stream(resp.content)\n        table = reader.read_all()\n\n        # Convert the PyArrow table into a pandas DataFrame\n        df = table.to_pandas()\n        df.to_json(os.path.join(json_output_folder,\n                                'part_' + str(ipart) + '.jsonl'),\n                   orient='records',\n                   lines=True,\n                   force_ascii=False)", "completions": ["  \n```\ndef download(ipart: int,\n             url: str,\n             json_output_folder: str,\n             columns: Optional[List] = None,\n             resp_format: str = 'arrow',\n             compressed: bool = False) -> None :\n    \"\"\"Thread download presigned url and save to jsonl locally.\n\n    Args:\n        ipart (int): presigned url id\n        url (str): presigned url\n        json_output_folder (str): directory to save the ipart_th segment of dataframe\n        columns (list): schema to save to json\n        resp_format (str): whether to use arrow or json when collect\n        compressed (bool): if data is compressed before downloading. Need decompress if compressed=True.\n    \"\"\"\n    # Generate the complete function implementation.\n    import requests\n    import json\n    import pandas as pd\n    import os\n    from io import BytesIO\n    from zipfile import ZipFile\n\n    # Download the presigned url\n    response = requests.get(url)\n\n    # Check if the response is compressed\n    if compressed:\n        # Decompress the response\n        zf = ZipFile(BytesIO(response.content))\n        data = zf.read()\n    else:\n        data = response.content\n\n    # Convert the response data to a pandas dataframe\n    if resp_format == 'arrow':\n        df = pd.read_arrow(data)\n    elif resp_format == 'json':\n        df = pd.read_json(data)\n    else:\n        raise ValueError(f'Unsupported response format: {resp_format}')\n\n    # Select the specified columns\n    if columns is not None:\n        df = df[columns]\n\n    # Save the dataframe to a jsonl file\n    json_file_path = os.path.join(json_output_folder, f'{ipart:03d}.jsonl')\n    df.to_json(json_file_path, orient='records', lines=True)\n```"]}
{"id": 87, "name": "draw_binary_mask", "prompt": "### Role:\nYou are an expert Python programmer. You will be given some information and will generate a correct Python function that matches this information.\n\n### Task:\nBased on the provided information, generate a complete Python function. Ensure the code is logically correct, syntactically accurate, and follows best coding practices.\n\n### Function name:\ndraw_binary_mask\n\n### Doc string and function signature:\n```Python\ndef draw_binary_mask(\n        self, binary_mask, color=None, *, edge_color=None, text=None, alpha=0.5, area_threshold=10\n    ) :\n    \"\"\"\n    Args:\n        binary_mask (ndarray): numpy array of shape (H, W), where H is the image height and\n            W is the image width. Each value in the array is either a 0 or 1 value of uint8\n            type.\n        color: color of the mask. Refer to `matplotlib.colors` for a full list of\n            formats that are accepted. If None, will pick a random color.\n        edge_color: color of the polygon edges. Refer to `matplotlib.colors` for a\n            full list of formats that are accepted.\n        text (str): if None, will be drawn on the object\n        alpha (float): blending efficient. Smaller values lead to more transparent masks.\n        area_threshold (float): a connected component smaller than this area will not be shown.\n\n    Returns:\n        output (VisImage): image object with mask drawn.\n    \"\"\"\n```\n\n### Requirements:\n1. Generate the complete function implementation.\n2. Only answer the code of the generated function, formatted using Markdown code block syntax.\n3. Do not include any additional text.\n\n### Answer: (use the provided format with backticks)\n", "reference": "    def draw_binary_mask(\n        self, binary_mask, color=None, *, edge_color=None, text=None, alpha=0.5, area_threshold=10\n    ):\n        if color is None:\n            color = random_color(rgb=True, maximum=1)\n        color = mplc.to_rgb(color)\n\n        has_valid_segment = False\n        binary_mask = binary_mask.astype(\"uint8\")  # opencv needs uint8\n        mask = GenericMask(binary_mask, self.output.height, self.output.width)\n        shape2d = (binary_mask.shape[0], binary_mask.shape[1])\n\n        if not mask.has_holes:\n            # draw polygons for regular masks\n            for segment in mask.polygons:\n                area = mask_util.area(mask_util.frPyObjects([segment], shape2d[0], shape2d[1]))\n                if area < (area_threshold or 0):\n                    continue\n                has_valid_segment = True\n                segment = segment.reshape(-1, 2)\n                self.draw_polygon(segment, color=color, edge_color=edge_color, alpha=alpha)\n        else:\n            # TODO: Use Path/PathPatch to draw vector graphics:\n            # https://stackoverflow.com/questions/8919719/how-to-plot-a-complex-polygon\n            rgba = np.zeros(shape2d + (4,), dtype=\"float32\")\n            rgba[:, :, :3] = color\n            rgba[:, :, 3] = (mask.mask == 1).astype(\"float32\") * alpha\n            has_valid_segment = True\n            self.output.ax.imshow(rgba, extent=(0, self.output.width, self.output.height, 0))\n\n        if text is not None and has_valid_segment:\n            lighter_color = self._change_color_brightness(color, brightness_factor=0.7)\n            self._draw_text_in_mask(binary_mask, text, lighter_color)\n        return self.output", "completions": ["  \n```\ndef draw_binary_mask(self, binary_mask, color=None, *, edge_color=None, text=None, alpha=0.5, area_threshold=10):\n    \"\"\"\n    Args:\n        binary_mask (ndarray): numpy array of shape (H, W), where H is the image height and\n            W is the image width. Each value in the array is either a 0 or 1 value of uint8\n            type.\n        color: color of the mask. Refer to `matplotlib.colors` for a full list of\n            formats that are accepted. If None, will pick a random color.\n        edge_color: color of the polygon edges. Refer to `matplotlib.colors` for a\n            full list of formats that are accepted.\n        text (str): if None, will be drawn on the object\n        alpha (float): blending efficient. Smaller values lead to more transparent masks.\n        area_threshold (float): a connected component smaller than this area will not be shown.\n\n    Returns:\n        output (VisImage): image object with mask drawn.\n    \"\"\"\n    # Generate a random color if necessary\n    if color is None:\n        color = np.random.rand(3)\n\n    # Create a binary mask from the input array\n    mask = np.zeros(binary_mask.shape[:2], dtype=np.uint8)\n    mask[binary_mask > 0] = 1\n\n    # Find contours in the mask\n    contours, _ = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n\n    # Draw the contours on the input image\n    for contour in contours:\n        area = cv2.contourArea(contour)\n        if area < area_threshold:\n            continue\n        x, y, w, h = cv2.boundingRect(contour)\n        cv2.rectangle(mask, (x, y), (x + w, y + h), color, 2)\n        if text is not None:\n            cv2.putText(mask, text, (x, y), cv2.FONT_HERSHEY_SIMPLEX, 1, color, 2)\n\n    # Create a VisImage object from the mask\n    output = VisImage(mask)\n\n    # Set the alpha value for the mask\n    output.set_alpha(alpha)\n\n    # Return the output image\n    return output\n```"]}
{"id": 88, "name": "get_enemy_pos", "prompt": "### Role:\nYou are an expert Python programmer. You will be given some information and will generate a correct Python function that matches this information.\n\n### Task:\nBased on the provided information, generate a complete Python function. Ensure the code is logically correct, syntactically accurate, and follows best coding practices.\n\n### Function name:\nget_enemy_pos\n\n### Doc string and function signature:\n```Python\ndef get_enemy_pos(mm_info: MiniMapInfo) -> List[Point] :\n    \"\"\"\n      (0,0)\n    :param mm_info: \n    :return:\n    \"\"\"\n```\n\n### Requirements:\n1. Generate the complete function implementation.\n2. Only answer the code of the generated function, formatted using Markdown code block syntax.\n3. Do not include any additional text.\n\n### Answer: (use the provided format with backticks)\n", "reference": "def get_enemy_pos(mm_info: MiniMapInfo) -> List[Point]:\n    enemy_mask = get_enemy_mask(mm_info)\n    # cv2_utils.show_image(enemy_mask, win_name='get_enemy_pos', wait=0)\n    cx = mm_info.origin.shape[1] // 2\n    cy = mm_info.origin.shape[0] // 2\n\n    # \n    to_check = cv2_utils.dilate(enemy_mask, 5)\n    num_labels, labels, stats, centroids = cv2.connectedComponentsWithStats(to_check, connectivity=8)\n\n    pos_list: List[Point] = []\n\n    if num_labels <= 1:  #  \n        return pos_list\n\n    # \n    for label in range(1, num_labels):\n        # \n        center_x = int(centroids[label, 0])\n        center_y = int(centroids[label, 1])\n        pos_list.append(Point(center_x - cx, center_y - cy))\n\n    return pos_list", "completions": ["  \n```Python\ndef get_enemy_pos(mm_info: MiniMapInfo) -> List[Point]:\n    \"\"\"\n      (0,0)\n    :param mm_info: \n    :return:\n    \"\"\"\n    enemy_positions = []\n    for enemy in mm_info.enemies:\n        enemy_pos = Point(enemy.x - mm_info.center.x, enemy.y - mm_info.center.y)\n        enemy_positions.append(enemy_pos)\n    return enemy_positions\n```"]}
{"id": 89, "name": "gain_statistics", "prompt": "### Role:\nYou are an expert Python programmer. You will be given some information and will generate a correct Python function that matches this information.\n\n### Task:\nBased on the provided information, generate a complete Python function. Ensure the code is logically correct, syntactically accurate, and follows best coding practices.\n\n### Function name:\ngain_statistics\n\n### Doc string and function signature:\n```Python\ndef gain_statistics(dml_long, dml_short) :\n    \"\"\"\n    Compute gain statistics as benchmark values for sensitivity parameters ``cf_d`` and ``cf_y``.\n\n    Parameters\n    ----------\n\n    dml_long :\n        :class:`doubleml.DoubleML` model including all observed confounders\n\n    dml_short :\n        :class:`doubleml.DoubleML` model that excludes one or several benchmark confounders\n\n    Returns\n    --------\n    benchmark_dict : dict\n        Benchmarking dictionary (dict) with values for ``cf_d``, ``cf_y``, ``rho``, and ``delta_theta``.\n    \"\"\"\n```\n\n### Requirements:\n1. Generate the complete function implementation.\n2. Only answer the code of the generated function, formatted using Markdown code block syntax.\n3. Do not include any additional text.\n\n### Answer: (use the provided format with backticks)\n", "reference": "def gain_statistics(dml_long, dml_short):\n    if not isinstance(dml_long.sensitivity_elements, dict):\n        raise TypeError(\"dml_long does not contain the necessary sensitivity elements. \"\n                        \"Expected dict for dml_long.sensitivity_elements.\")\n    expected_keys = ['sigma2', 'nu2']\n    if not all(key in dml_long.sensitivity_elements.keys() for key in expected_keys):\n        raise ValueError(\"dml_long does not contain the necessary sensitivity elements. \"\n                         \"Required keys are: \" + str(expected_keys))\n    if not isinstance(dml_short.sensitivity_elements, dict):\n        raise TypeError(\"dml_short does not contain the necessary sensitivity elements. \"\n                        \"Expected dict for dml_short.sensitivity_elements.\")\n    if not all(key in dml_short.sensitivity_elements.keys() for key in expected_keys):\n        raise ValueError(\"dml_short does not contain the necessary sensitivity elements. \"\n                         \"Required keys are: \" + str(expected_keys))\n\n    for key in expected_keys:\n        if not isinstance(dml_long.sensitivity_elements[key], np.ndarray):\n            raise TypeError(\"dml_long does not contain the necessary sensitivity elements. \"\n                            f\"Expected numpy.ndarray for key {key}.\")\n        if not isinstance(dml_short.sensitivity_elements[key], np.ndarray):\n            raise TypeError(\"dml_short does not contain the necessary sensitivity elements. \"\n                            f\"Expected numpy.ndarray for key {key}.\")\n        if len(dml_long.sensitivity_elements[key].shape) != 3 or dml_long.sensitivity_elements[key].shape[0] != 1:\n            raise ValueError(\"dml_long does not contain the necessary sensitivity elements. \"\n                             f\"Expected 3 dimensions of shape (1, n_coef, n_rep) for key {key}.\")\n        if len(dml_short.sensitivity_elements[key].shape) != 3 or dml_short.sensitivity_elements[key].shape[0] != 1:\n            raise ValueError(\"dml_short does not contain the necessary sensitivity elements. \"\n                             f\"Expected 3 dimensions of shape (1, n_coef, n_rep) for key {key}.\")\n        if not np.array_equal(dml_long.sensitivity_elements[key].shape, dml_short.sensitivity_elements[key].shape):\n            raise ValueError(\"dml_long and dml_short do not contain the same shape of sensitivity elements. \"\n                             \"Shapes of \" + key + \" are: \" + str(dml_long.sensitivity_elements[key].shape) +\n                             \" and \" + str(dml_short.sensitivity_elements[key].shape))\n\n    if not isinstance(dml_long.all_coef, np.ndarray):\n        raise TypeError(\"dml_long.all_coef does not contain the necessary coefficients. Expected numpy.ndarray.\")\n    if not isinstance(dml_short.all_coef, np.ndarray):\n        raise TypeError(\"dml_short.all_coef does not contain the necessary coefficients. Expected numpy.ndarray.\")\n\n    expected_shape = (dml_long.sensitivity_elements['sigma2'].shape[2], dml_long.sensitivity_elements['sigma2'].shape[1])\n    if dml_long.all_coef.shape != expected_shape:\n        raise ValueError(\"dml_long.all_coef does not contain the necessary coefficients. Expected shape: \" +\n                         str(expected_shape))\n    if dml_short.all_coef.shape != expected_shape:\n        raise ValueError(\"dml_short.all_coef does not contain the necessary coefficients. Expected shape: \" +\n                         str(expected_shape))\n\n    # save elements for readability\n    var_y = np.var(dml_long._dml_data.y)\n    var_y_residuals_long = np.squeeze(dml_long.sensitivity_elements['sigma2'], axis=0)\n    nu2_long = np.squeeze(dml_long.sensitivity_elements['nu2'], axis=0)\n    var_y_residuals_short = np.squeeze(dml_short.sensitivity_elements['sigma2'], axis=0)\n    nu2_short = np.squeeze(dml_short.sensitivity_elements['nu2'], axis=0)\n\n    # compute nonparametric R2\n    R2_y_long = 1.0 - np.divide(var_y_residuals_long, var_y)\n    R2_y_short = 1.0 - np.divide(var_y_residuals_short, var_y)\n    R2_riesz = np.divide(nu2_short, nu2_long)\n\n    # Gain statistics\n    all_cf_y_benchmark = np.clip(np.divide((R2_y_long - R2_y_short), (1.0 - R2_y_long)), 0, 1)\n    all_cf_d_benchmark = np.clip(np.divide((1.0 - R2_riesz), R2_riesz), 0, 1)\n    cf_y_benchmark = np.median(all_cf_y_benchmark, axis=0)\n    cf_d_benchmark = np.median(all_cf_d_benchmark, axis=0)\n\n    # change in estimates (slightly different to paper)\n    all_delta_theta = np.transpose(dml_short.all_coef - dml_long.all_coef)\n    delta_theta = np.median(all_delta_theta, axis=0)\n\n    # degree of adversity\n    var_g = var_y_residuals_short - var_y_residuals_long\n    var_riesz = nu2_long - nu2_short\n    denom = np.sqrt(np.multiply(var_g, var_riesz), out=np.zeros_like(var_g), where=(var_g > 0) & (var_riesz > 0))\n    rho_sign = np.sign(all_delta_theta)\n    rho_values = np.clip(np.divide(np.absolute(all_delta_theta),\n                                   denom,\n                                   out=np.ones_like(all_delta_theta),\n                                   where=denom != 0),\n                         0.0, 1.0)\n    all_rho_benchmark = np.multiply(rho_values, rho_sign)\n    rho_benchmark = np.median(all_rho_benchmark, axis=0)\n    benchmark_dict = {\n        \"cf_y\": cf_y_benchmark,\n        \"cf_d\": cf_d_benchmark,\n        \"rho\": rho_benchmark,\n        \"delta_theta\": delta_theta,\n    }\n    return benchmark_dict", "completions": ["  \n```\ndef gain_statistics(dml_long, dml_short):\n    \"\"\"\n    Compute gain statistics as benchmark values for sensitivity parameters ``cf_d`` and ``cf_y``.\n\n    Parameters\n    ----------\n\n    dml_long :\n        :class:`doubleml.DoubleML` model including all observed confounders\n\n    dml_short :\n        :class:`doubleml.DoubleML` model that excludes one or several benchmark confounders\n\n    Returns\n    --------\n    benchmark_dict : dict\n        Benchmarking dictionary (dict) with values for ``cf_d``, ``cf_y``, ``rho``, and ``delta_theta``.\n    \"\"\"\n    # Compute gain statistics\n    cf_d = dml_long.coef_ - dml_short.coef_\n    cf_y = dml_long.coef_ - dml_short.coef_\n    rho = dml_long.coef_ - dml_short.coef_\n    delta_theta = dml_long.coef_ - dml_short.coef_\n\n    # Return benchmark dictionary\n    benchmark_dict = {\n        \"cf_d\": cf_d,\n        \"cf_y\": cf_y,\n        \"rho\": rho,\n        \"delta_theta\": delta_theta\n    }\n\n    return benchmark_dict\n```"]}
{"id": 90, "name": "image_transform", "prompt": "### Role:\nYou are an expert Python programmer. You will be given some information and will generate a correct Python function that matches this information.\n\n### Task:\nBased on the provided information, generate a complete Python function. Ensure the code is logically correct, syntactically accurate, and follows best coding practices.\n\n### Function name:\nimage_transform\n\n### Doc string and function signature:\n```Python\ndef image_transform(\n    image_size: int,\n    is_train: bool,\n    mean: Optional[Tuple[float, ...]] = None,\n    std: Optional[Tuple[float, ...]] = None,\n    resize_longest_max: bool = False,\n    fill_color: int = 0,\n    inmem=False,\n) :\n    \"\"\"\n    Image transformations for OpenAI dataset.\n\n    Args:\n        image_size (int): Image size.\n        is_train (bool): Whether it's training or test.\n        mean (tuple, optional): Mean of the dataset. Defaults to None.\n        std (tuple, optional): Standard deviation of the dataset. Defaults to None.\n        resize_longest_max (bool, optional): Whether to resize the longest edge to max_size. Defaults to False.\n        fill_color (int, optional): Color to fill the image when resizing. Defaults to 0.\n\n    Example:\n        >>> transform = image_transform(256, True)\n        >>> dataset = OpenAIDataset(\"train\", transform=transform)\n\n\n    \"\"\"\n```\n\n### Requirements:\n1. Generate the complete function implementation.\n2. Only answer the code of the generated function, formatted using Markdown code block syntax.\n3. Do not include any additional text.\n\n### Answer: (use the provided format with backticks)\n", "reference": "def image_transform(\n    image_size: int,\n    is_train: bool,\n    mean: Optional[Tuple[float, ...]] = None,\n    std: Optional[Tuple[float, ...]] = None,\n    resize_longest_max: bool = False,\n    fill_color: int = 0,\n    inmem=False,\n):\n    mean = mean or (0.48145466, 0.4578275, 0.40821073)  # OpenAI dataset mean\n    std = std or (0.26862954, 0.26130258, 0.27577711)  # OpenAI dataset std\n    if isinstance(image_size, (list, tuple)) and image_size[0] == image_size[1]:\n        # for square size, pass size as int so that Resize() uses aspect preserving shortest edge\n        image_size = image_size[0]\n\n    normalize = Normalize(mean=mean, std=std)\n    if is_train:\n        if inmem:\n            return Compose(\n                [\n                    RandomResizedCrop(\n                        image_size,\n                        scale=(0.9, 1.0),\n                        interpolation=InterpolationMode.BICUBIC,\n                    ),\n                    _convert_to_rgb,\n                    F.pil_to_tensor,\n                ]\n            )\n        else:\n            return Compose(\n                [\n                    RandomResizedCrop(\n                        image_size,\n                        scale=(0.9, 1.0),\n                        interpolation=InterpolationMode.BICUBIC,\n                    ),\n                    _convert_to_rgb,\n                    ToTensor(),\n                    normalize,\n                ]\n            )\n    else:\n        if resize_longest_max:\n            transforms = [ResizeMaxSize(image_size, fill=fill_color)]\n        else:\n            transforms = [\n                Resize(image_size, interpolation=InterpolationMode.BICUBIC),\n                CenterCrop(image_size),\n            ]\n        transforms.extend(\n            [\n                _convert_to_rgb,\n                ToTensor(),\n                normalize,\n            ]\n        )\n        return Compose(transforms)", "completions": ["  \n```\ndef image_transform(\n    image_size: int,\n    is_train: bool,\n    mean: Optional[Tuple[float,...]] = None,\n    std: Optional[Tuple[float,...]] = None,\n    resize_longest_max: bool = False,\n    fill_color: int = 0,\n    inmem=False,\n) :\n    \"\"\"\n    Image transformations for OpenAI dataset.\n\n    Args:\n        image_size (int): Image size.\n        is_train (bool): Whether it's training or test.\n        mean (tuple, optional): Mean of the dataset. Defaults to None.\n        std (tuple, optional): Standard deviation of the dataset. Defaults to None.\n        resize_longest_max (bool, optional): Whether to resize the longest edge to max_size. Defaults to False.\n        fill_color (int, optional): Color to fill the image when resizing. Defaults to 0.\n\n    Example:\n        >>> transform = image_transform(256, True)\n        >>> dataset = OpenAIDataset(\"train\", transform=transform)\n\n\n    \"\"\"\n    def __init__(self, image_size, is_train, mean=None, std=None, resize_longest_max=False, fill_color=0, inmem=False):\n        self.image_size = image_size\n        self.is_train = is_train\n        self.mean = mean\n        self.std = std\n        self.resize_longest_max = resize_longest_max\n        self.fill_color = fill_color\n        self.inmem = inmem\n\n    def __call__(self, image):\n        if self.is_train:\n            image = self.train_transform(image)\n        else:\n            image = self.test_transform(image)\n        return image\n\n    def train_transform(self, image):\n        if self.resize_longest_max:\n            image = self.resize_longest(image, self.image_size)\n        else:\n            image = self.resize(image, self.image_size)\n        if self.mean is not None and self.std is not None:\n            image = self.normalize(image, self.mean, self.std)\n        return image\n\n    def test_transform(self, image):\n        if self.resize_longest_max:\n            image = self.resize_longest(image, self.image_size)\n        else:\n            image = self.resize(image, self.image_size)\n        if self.mean is not None and self.std is not None:\n            image = self.normalize(image, self.mean, self.std)\n        return image\n\n    def resize(self, image, size):\n        image = cv2.resize(image, (size, size))\n        return image\n\n    def resize_longest(self, image, size):\n        image = cv2.resize(image, (size, size), interpolation=cv2.INTER_AREA)\n        return image\n\n    def normalize(self, image, mean, std):\n        image = (image - mean) / std\n        return image\n\n    def __repr__(self):\n        return f\"ImageTransform(image_size={self.image_size}, is_train={self.is_train}, mean={self.mean}, std={self.std}, resize_longest_max={self.resize_longest_max}, fill_color={self.fill_color}, inmem={self.inmem})\"\n```"]}
{"id": 91, "name": "auth_to_github", "prompt": "### Role:\nYou are an expert Python programmer. You will be given some information and will generate a correct Python function that matches this information.\n\n### Task:\nBased on the provided information, generate a complete Python function. Ensure the code is logically correct, syntactically accurate, and follows best coding practices.\n\n### Function name:\nauth_to_github\n\n### Doc string and function signature:\n```Python\ndef auth_to_github(\n    token: str,\n    gh_app_id: int | None,\n    gh_app_installation_id: int | None,\n    gh_app_private_key_bytes: bytes,\n    ghe: str,\n) -> github3.GitHub :\n    \"\"\"\n    Connect to GitHub.com or GitHub Enterprise, depending on env variables.\n\n    Args:\n        token (str): the GitHub personal access token\n        gh_app_id (int | None): the GitHub App ID\n        gh_app_installation_id (int | None): the GitHub App Installation ID\n        gh_app_private_key_bytes (bytes): the GitHub App Private Key\n        ghe (str): the GitHub Enterprise URL\n\n    Returns:\n        github3.GitHub: the GitHub connection object\n    \"\"\"\n```\n\n### Requirements:\n1. Generate the complete function implementation.\n2. Only answer the code of the generated function, formatted using Markdown code block syntax.\n3. Do not include any additional text.\n\n### Answer: (use the provided format with backticks)\n", "reference": "def auth_to_github(\n    token: str,\n    gh_app_id: int | None,\n    gh_app_installation_id: int | None,\n    gh_app_private_key_bytes: bytes,\n    ghe: str,\n) -> github3.GitHub:\n    if gh_app_id and gh_app_private_key_bytes and gh_app_installation_id:\n        gh = github3.github.GitHub()\n        gh.login_as_app_installation(\n            gh_app_private_key_bytes, gh_app_id, gh_app_installation_id\n        )\n        github_connection = gh\n    elif ghe and token:\n        github_connection = github3.github.GitHubEnterprise(ghe, token=token)\n    elif token:\n        github_connection = github3.login(token=token)\n    else:\n        raise ValueError(\n            \"GH_TOKEN or the set of [GH_APP_ID, GH_APP_INSTALLATION_ID, GH_APP_PRIVATE_KEY] environment variables are not set\"\n        )\n\n    if not github_connection:\n        raise ValueError(\"Unable to authenticate to GitHub\")\n    return github_connection  # type: ignore", "completions": ["  \n```\ndef auth_to_github(\n    token: str,\n    gh_app_id: int | None,\n    gh_app_installation_id: int | None,\n    gh_app_private_key_bytes: bytes,\n    ghe: str,\n) -> github3.GitHub :\n    \"\"\"\n    Connect to GitHub.com or GitHub Enterprise, depending on env variables.\n\n    Args:\n        token (str): the GitHub personal access token\n        gh_app_id (int | None): the GitHub App ID\n        gh_app_installation_id (int | None): the GitHub App Installation ID\n        gh_app_private_key_bytes (bytes): the GitHub App Private Key\n        ghe (str): the GitHub Enterprise URL\n\n    Returns:\n        github3.GitHub: the GitHub connection object\n    \"\"\"\n    if ghe:\n        github = github3.GitHubEnterprise(ghe, token=token)\n    else:\n        github = github3.GitHub(token=token)\n\n    if gh_app_id and gh_app_installation_id and gh_app_private_key_bytes:\n        github.login_as_app(gh_app_id, gh_app_installation_id, gh_app_private_key_bytes)\n\n    return github\n```"]}
{"id": 92, "name": "init_templates", "prompt": "### Role:\nYou are an expert Python programmer. You will be given some information and will generate a correct Python function that matches this information.\n\n### Task:\nBased on the provided information, generate a complete Python function. Ensure the code is logically correct, syntactically accurate, and follows best coding practices.\n\n### Function name:\ninit_templates\n\n### Doc string and function signature:\n```Python\ndef init_templates(template_dir='templates', enable_async=False, **kwargs) :\n    \"\"\"Initialize the templating subsystem.\n\n    :param template_dir: the directory where templates are stored. This\n                         argument is optional. The default is to load templates\n                         from a *templates* subdirectory.\n    :param enable_async: set to ``True`` to enable the async rendering engine\n                         in Jinja, and the ``render_async()`` and\n                         ``generate_async()`` methods.\n    :param kwargs: any additional options to be passed to Jinja's\n                   ``Environment`` class.\n    \"\"\"\n```\n\n### Requirements:\n1. Generate the complete function implementation.\n2. Only answer the code of the generated function, formatted using Markdown code block syntax.\n3. Do not include any additional text.\n\n### Answer: (use the provided format with backticks)\n", "reference": "def init_templates(template_dir='templates', enable_async=False, **kwargs):\n    global _jinja_env\n    _jinja_env = Environment(\n        loader=FileSystemLoader(template_dir),\n        autoescape=select_autoescape(),\n        enable_async=enable_async,\n        **kwargs\n    )", "completions": ["  \n```\ndef init_templates(template_dir='templates', enable_async=False, **kwargs) :\n    \"\"\"Initialize the templating subsystem.\n\n    :param template_dir: the directory where templates are stored. This\n                         argument is optional. The default is to load templates\n                         from a *templates* subdirectory.\n    :param enable_async: set to ``True`` to enable the async rendering engine\n                         in Jinja, and the ``render_async()`` and\n                         ``generate_async()`` methods.\n    :param kwargs: any additional options to be passed to Jinja's\n                   ``Environment`` class.\n    \"\"\"\n    from jinja2 import Environment, FileSystemLoader\n    if template_dir is None:\n        template_dir = 'templates'\n    loader = FileSystemLoader(template_dir)\n    env = Environment(loader=loader, **kwargs)\n    if enable_async:\n        env.enable_async = True\n    return env\n```"]}
{"id": 93, "name": "generate_coco_json", "prompt": "### Role:\nYou are an expert Python programmer. You will be given some information and will generate a correct Python function that matches this information.\n\n### Task:\nBased on the provided information, generate a complete Python function. Ensure the code is logically correct, syntactically accurate, and follows best coding practices.\n\n### Function name:\ngenerate_coco_json\n\n### Doc string and function signature:\n```Python\ndef generate_coco_json(\n    masks: list[np.ndarray],\n    image: np.ndarray,\n    category_name: str,\n    file_name: str,\n) -> dict[str, Any] :\n    \"\"\"Generates a COCO JSON annotation.\n\n    Create a COCO formatted JSON file for the given masks, image, and\n    category name.\n\n    Args:\n      masks: A list of masks.\n      image: The image to which the masks correspond.\n      category_name: The name of the category for the masks.\n      file_name: The name of the file to save the COCO JSON to.\n\n    Returns:\n      A COCO JSON dictionary.\n    \"\"\"\n```\n\n### Requirements:\n1. Generate the complete function implementation.\n2. Only answer the code of the generated function, formatted using Markdown code block syntax.\n3. Do not include any additional text.\n\n### Answer: (use the provided format with backticks)\n", "reference": "def generate_coco_json(\n    masks: list[np.ndarray],\n    image: np.ndarray,\n    category_name: str,\n    file_name: str,\n) -> dict[str, Any]:\n  height, width = image.shape[:2]\n\n  # Initialize variables\n  mask = np.zeros((height, width), dtype=np.uint8)\n  images_dict = [{}]\n  categories_dict = [{}]\n  annotations_dict = []\n  annotation_id = 1\n\n  # Process masks\n  for sub_mask in masks:\n    # Convert mask to numpy array\n    mask_array = sub_mask.reshape(height, width).astype(np.uint8)\n\n    # Create Mask object and add it to the imantics_Image\n    mask_image = imantics.Mask(mask_array)\n\n    # Create imantics_Image object\n    imantics_image = imantics.Image(image)\n    imantics_image.add(mask_image, category=imantics.Category(category_name))\n\n    try:\n      # Export imantics_Image as COCO JSON\n      coco_json = imantics_image.export(style='coco')\n    except imantics.ExportError as exc:\n      print('Error:', exc)\n      continue\n\n    # Update images_dict and categories_dict\n    images_dict[0] = coco_json['images'][0]\n    categories_dict[0] = coco_json['categories'][0]\n\n    # Retrieve annotation information and modify the segmentation field\n    annotation = coco_json['annotations'][0]\n    annotation['segmentation'] = [max(annotation['segmentation'], key=len)]\n\n    # Check for valid segmentations and create annotation dictionary\n    if len(annotation['segmentation']) >= 1:\n      for segmentation in annotation['segmentation']:\n        if len(segmentation) > 4:\n          annotation_dict = {\n              'id': annotation_id,\n              'image_id': annotation['image_id'],\n              'category_id': annotation['category_id'],\n              'iscrowd': annotation['iscrowd'],\n              'area': annotation['area'],\n              'bbox': annotation['bbox'],\n              'segmentation': [segmentation],\n          }\n          annotations_dict.append(annotation_dict)\n          annotation_id += 1\n\n    # Free up memory\n    del mask_image, coco_json, imantics_image\n\n    # Add mask_array to the overall mask\n    mask += mask_array\n\n  # assign file name\n  images_dict[0]['file_name'] = file_name\n\n  # Create final COCO dictionary\n  coco_dict_final = {\n      'images': images_dict,\n      'categories': categories_dict,\n      'annotations': annotations_dict,\n  }\n\n  return coco_dict_final", "completions": ["  \n```\ndef generate_coco_json(\n    masks: list[np.ndarray],\n    image: np.ndarray,\n    category_name: str,\n    file_name: str,\n) -> dict[str, Any] :\n    \"\"\"Generates a COCO JSON annotation.\n\n    Create a COCO formatted JSON file for the given masks, image, and\n    category name.\n\n    Args:\n      masks: A list of masks.\n      image: The image to which the masks correspond.\n      category_name: The name of the category for the masks.\n      file_name: The name of the file to save the COCO JSON to.\n\n    Returns:\n      A COCO JSON dictionary.\n    \"\"\"\n    # Import necessary libraries\n    import json\n    import numpy as np\n    from PIL import Image\n\n    # Create a dictionary to store the COCO JSON data\n    coco_json = {\n        \"info\": {\n            \"year\": 2023,\n            \"version\": \"1.0\",\n            \"description\": \"COCO JSON annotation\",\n            \"contributor\": \"Python Programmer\",\n            \"url\": \"https://www.python.org/\",\n            \"date_created\": \"2023-02-28 12:00:00\"\n        },\n        \"images\": [],\n        \"annotations\": [],\n        \"categories\": []\n    }\n\n    # Add image data to the COCO JSON\n    image_data = {\n        \"id\": 1,\n        \"width\": image.shape[1],\n        \"height\": image.shape[0],\n        \"file_name\": file_name,\n        \"license\": 1,\n        \"flickr_url\": \"https://www.flickr.com/\",\n        \"coco_url\": \"https://cocodataset.org/#home\"\n    }\n    coco_json[\"images\"].append(image_data)\n\n    # Add category data to the COCO JSON\n    category_data = {\n        \"id\": 1,\n        \"name\": category_name,\n        \"supercategory\": \"object\",\n        \"keypoints\": [],\n        \"skeleton\": []\n    }\n    coco_json[\"categories\"].append(category_data)\n\n    # Add annotation data to the COCO JSON\n    for i, mask in enumerate(masks):\n        annotation_data = {\n            \"id\": i + 1,\n            \"image_id\": 1,\n            \"category_id\": 1,\n            \"segmentation\": [],\n            \"area\": 0,\n            \"bbox\": [],\n            \"iscrowd\": 0\n        }\n        coco_json[\"annotations\"].append(annotation_data)\n\n        # Add segmentation data to the annotation\n        segmentation = np.where(mask, 1, 0)\n        annotation_data[\"segmentation\"].append(segmentation.tolist())\n\n        # Add area data to the annotation\n        annotation_data[\"area\"] = np.count_nonzero(mask)\n\n        # Add bounding box data to the annotation\n        x, y, w, h = cv2.boundingRect(mask)\n        annotation_data[\"bbox\"] = [x, y, w, h]\n\n    # Save the COCO JSON to a file\n    with open(file_name, \"w\") as f:\n        json.dump(coco_json, f, indent=4)\n\n    return coco_json\n```"]}
{"id": 94, "name": "similarity_matrix", "prompt": "### Role:\nYou are an expert Python programmer. You will be given some information and will generate a correct Python function that matches this information.\n\n### Task:\nBased on the provided information, generate a complete Python function. Ensure the code is logically correct, syntactically accurate, and follows best coding practices.\n\n### Function name:\nsimilarity_matrix\n\n### Doc string and function signature:\n```Python\ndef similarity_matrix(xq: np.ndarray, index: np.ndarray) -> np.ndarray :\n    \"\"\"Compute the similarity scores between a query vector and a set of vectors.\n\n    Args:\n        xq: A query vector (1d ndarray)\n        index: A set of vectors.\n\n    Returns:\n        The similarity between the query vector and the set of vectors.\n    \"\"\"\n```\n\n### Requirements:\n1. Generate the complete function implementation.\n2. Only answer the code of the generated function, formatted using Markdown code block syntax.\n3. Do not include any additional text.\n\n### Answer: (use the provided format with backticks)\n", "reference": "def similarity_matrix(xq: np.ndarray, index: np.ndarray) -> np.ndarray:\n\n    index_norm = norm(index, axis=1)\n    xq_norm = norm(xq.T)\n    sim = np.dot(index, xq.T) / (index_norm * xq_norm)\n    return sim", "completions": ["  \n```Python\ndef similarity_matrix(xq: np.ndarray, index: np.ndarray) -> np.ndarray:\n    \"\"\"Compute the similarity scores between a query vector and a set of vectors.\n\n    Args:\n        xq: A query vector (1d ndarray)\n        index: A set of vectors.\n\n    Returns:\n        The similarity between the query vector and the set of vectors.\n    \"\"\"\n    # Calculate the dot product between the query vector and each vector in the index\n    dot_products = np.dot(xq, index)\n\n    # Calculate the magnitude of the query vector\n    query_magnitude = np.linalg.norm(xq)\n\n    # Calculate the magnitude of each vector in the index\n    index_magnitudes = np.linalg.norm(index, axis=1)\n\n    # Calculate the similarity between the query vector and each vector in the index\n    similarities = dot_products / (query_magnitude * index_magnitudes)\n\n    return similarities\n```"]}
{"id": 95, "name": "_parse_broker_order", "prompt": "### Role:\nYou are an expert Python programmer. You will be given some information and will generate a correct Python function that matches this information.\n\n### Task:\nBased on the provided information, generate a complete Python function. Ensure the code is logically correct, syntactically accurate, and follows best coding practices.\n\n### Function name:\n_parse_broker_order\n\n### Doc string and function signature:\n```Python\ndef _parse_broker_order(self, response: dict, strategy_name: str, strategy_object=None) :\n    \"\"\"\n    Parse a broker order representation to a Lumi order object. Once the Lumi order has been created, it will\n    be dispatched to our \"stream\" queue for processing until a time when Live Streaming can be implemented.\n\n    Tradier API Documentation:\n    https://documentation.tradier.com/brokerage-api/reference/response/orders\n\n    :param response: The output from TradierAPI call returned _by pull_broker_order()\n    :param strategy_name: The name of the strategy that placed the order\n    :param strategy_object: The strategy object that placed the order\n    \"\"\"\n```\n\n### Requirements:\n1. Generate the complete function implementation.\n2. Only answer the code of the generated function, formatted using Markdown code block syntax.\n3. Do not include any additional text.\n\n### Answer: (use the provided format with backticks)\n", "reference": "    def _parse_broker_order(self, response: dict, strategy_name: str, strategy_object=None):\n        strategy_name = (\n            strategy_name if strategy_name else strategy_object.name if strategy_object else response.get(\"tag\")\n        )\n\n        # Parse the symbol\n        symbol = response[\"symbol\"]\n        option_symbol = response[\"option_symbol\"] if \"option_symbol\" in response and response[\"option_symbol\"] else None\n\n        asset = (\n            Asset.symbol2asset(option_symbol)\n            if option_symbol and not pd.isna(option_symbol)\n            else Asset.symbol2asset(symbol)\n        )\n\n        # Create the order object\n        order = Order(\n            identifier=response[\"id\"],\n            strategy=strategy_name,\n            asset=asset,\n            side=self._tradier_side2lumi(response[\"side\"]),\n            quantity=response[\"quantity\"],\n            type=response[\"type\"],\n            time_in_force=response[\"duration\"],\n            limit_price=response[\"price\"] if \"price\" in response and response[\"price\"] else None,\n            stop_price=response[\"stop_price\"] if \"stop_price\" in response and response[\"stop_price\"] else None,\n            tag=response[\"tag\"] if \"tag\" in response and response[\"tag\"] else None,\n            date_created=response[\"create_date\"],\n        )\n        order.status = response[\"status\"]\n        order.update_raw(response)  # This marks order as 'transmitted'\n        return order", "completions": ["  \n```\ndef _parse_broker_order(self, response: dict, strategy_name: str, strategy_object=None) :\n    \"\"\"\n    Parse a broker order representation to a Lumi order object. Once the Lumi order has been created, it will\n    be dispatched to our \"stream\" queue for processing until a time when Live Streaming can be implemented.\n\n    Tradier API Documentation:\n    https://documentation.tradier.com/brokerage-api/reference/response/orders\n\n    :param response: The output from TradierAPI call returned _by pull_broker_order()\n    :param strategy_name: The name of the strategy that placed the order\n    :param strategy_object: The strategy object that placed the order\n    \"\"\"\n    # Create a new Lumi order object\n    order = LumiOrder(\n        id=response[\"id\"],\n        symbol=response[\"symbol\"],\n        quantity=response[\"quantity\"],\n        side=response[\"side\"],\n        type=response[\"type\"],\n        time_in_force=response[\"time_in_force\"],\n        status=response[\"status\"],\n        strategy_name=strategy_name,\n        strategy_object=strategy_object,\n    )\n\n    # Add any additional information to the order object\n    if \"classification\" in response:\n        order.classification = response[\"classification\"]\n    if \"price\" in response:\n        order.price = response[\"price\"]\n    if \"stop_price\" in response:\n        order.stop_price = response[\"stop_price\"]\n    if \"trailing_price\" in response:\n        order.trailing_price = response[\"trailing_price\"]\n    if \"trailing_percent\" in response:\n        order.trailing_percent = response[\"trailing_percent\"]\n    if \"extended_hours\" in response:\n        order.extended_hours = response[\"extended_hours\"]\n    if \"legs\" in response:\n        order.legs = response[\"legs\"]\n\n    # Dispatch the order to the \"stream\" queue for processing\n    self.stream_queue.put(order)\n```"]}
{"id": 96, "name": "_get_device_type_from_env", "prompt": "### Role:\nYou are an expert Python programmer. You will be given some information and will generate a correct Python function that matches this information.\n\n### Task:\nBased on the provided information, generate a complete Python function. Ensure the code is logically correct, syntactically accurate, and follows best coding practices.\n\n### Function name:\n_get_device_type_from_env\n\n### Doc string and function signature:\n```Python\ndef _get_device_type_from_env() -> str :\n    \"\"\"Function that gets the torch.device based on the current machine.\n\n    This currently only supports CPU, CUDA.\n\n    Returns:\n        device\n    \"\"\"\n```\n\n### Requirements:\n1. Generate the complete function implementation.\n2. Only answer the code of the generated function, formatted using Markdown code block syntax.\n3. Do not include any additional text.\n\n### Answer: (use the provided format with backticks)\n", "reference": "def _get_device_type_from_env() -> str:\n    if torch.cuda.is_available():\n        device = \"cuda\"\n    else:\n        device = \"cpu\"\n    return device", "completions": ["  \n```Python\ndef _get_device_type_from_env() -> str:\n    \"\"\"Function that gets the torch.device based on the current machine.\n\n    This currently only supports CPU, CUDA.\n\n    Returns:\n        device\n    \"\"\"\n    if torch.cuda.is_available():\n        return torch.device(\"cuda\")\n    else:\n        return torch.device(\"cpu\")\n```"]}
{"id": 97, "name": "delete_all_data", "prompt": "### Role:\nYou are an expert Python programmer. You will be given some information and will generate a correct Python function that matches this information.\n\n### Task:\nBased on the provided information, generate a complete Python function. Ensure the code is logically correct, syntactically accurate, and follows best coding practices.\n\n### Function name:\ndelete_all_data\n\n### Doc string and function signature:\n```Python\ndef delete_all_data() :\n    \"\"\"\n    DANGEROUS!  Delete everything, restore user settings, clear sys settings.\n\n    NO CHECKS ARE PERFORMED.\n    \"\"\"\n```\n\n### Requirements:\n1. Generate the complete function implementation.\n2. Only answer the code of the generated function, formatted using Markdown code block syntax.\n3. Do not include any additional text.\n\n### Answer: (use the provided format with backticks)\n", "reference": "def delete_all_data():\n\n    # Setting the pragma first ensures cascade delete.\n    statements = [\n        \"pragma foreign_keys = ON\",\n        \"delete from languages\",\n        \"delete from tags\",\n        \"delete from tags2\",\n        \"delete from settings\",\n    ]\n    for s in statements:\n        db.session.execute(text(s))\n    db.session.commit()\n    UserSetting.load()", "completions": ["  \n```\ndef delete_all_data() :\n    \"\"\"\n    DANGEROUS!  Delete everything, restore user settings, clear sys settings.\n\n    NO CHECKS ARE PERFORMED.\n    \"\"\"\n    # Delete all data\n    for file in os.listdir(\"/\"):\n        if file!= \".\" and file!= \"..\":\n            os.remove(file)\n\n    # Restore user settings\n    for file in os.listdir(\"/home\"):\n        if file.endswith(\".bak\"):\n            os.rename(file, file[:-4])\n\n    # Clear sys settings\n    for file in os.listdir(\"/etc\"):\n        if file.endswith(\".bak\"):\n            os.rename(file, file[:-4])\n```"]}
{"id": 98, "name": "diag", "prompt": "### Role:\nYou are an expert Python programmer. You will be given some information and will generate a correct Python function that matches this information.\n\n### Task:\nBased on the provided information, generate a complete Python function. Ensure the code is logically correct, syntactically accurate, and follows best coding practices.\n\n### Function name:\ndiag\n\n### Doc string and function signature:\n```Python\ndef diag(v, k=0) :\n    \"\"\"\n    Extract a diagonal or construct a diagonal array.\n\n    This function is the equivalent of `numpy.diag` that takes masked\n    values into account, see `numpy.diag` for details.\n\n    See Also\n    --------\n    numpy.diag : Equivalent function for ndarrays.\n\n    \"\"\"\n```\n\n### Requirements:\n1. Generate the complete function implementation.\n2. Only answer the code of the generated function, formatted using Markdown code block syntax.\n3. Do not include any additional text.\n\n### Answer: (use the provided format with backticks)\n", "reference": "def diag(v, k=0):\n    output = np.diag(v, k).view(MaskedArray)\n    if getmask(v) is not nomask:\n        output._mask = np.diag(v._mask, k)\n    return output", "completions": ["  \n```Python\ndef diag(v, k=0) :\n    \"\"\"\n    Extract a diagonal or construct a diagonal array.\n\n    This function is the equivalent of `numpy.diag` that takes masked\n    values into account, see `numpy.diag` for details.\n\n    See Also\n    --------\n    numpy.diag : Equivalent function for ndarrays.\n\n    \"\"\"\n    # Check if v is a masked array\n    if isinstance(v, MaskedArray):\n        # Extract the diagonal of the masked array\n        diag_mask = diag(v.mask, k)\n        diag_data = diag(v.data, k)\n        return MaskedArray(diag_data, mask=diag_mask)\n    else:\n        # Extract the diagonal of the array\n        n = v.shape[0]\n        m = v.shape[1]\n        if k > 0:\n            if k >= m:\n                raise ValueError(\"k must be less than the number of columns\")\n            diag_data = v[:, k]\n        elif k < 0:\n            if -k >= n:\n                raise ValueError(\"k must be greater than the number of rows\")\n            diag_data = v[k, :]\n        else:\n            if n!= m:\n                raise ValueError(\"Array must be square\")\n            diag_data = v.diagonal()\n        return diag_data\n```"]}
{"id": 99, "name": "from_pretrained", "prompt": "### Role:\nYou are an expert Python programmer. You will be given some information and will generate a correct Python function that matches this information.\n\n### Task:\nBased on the provided information, generate a complete Python function. Ensure the code is logically correct, syntactically accurate, and follows best coding practices.\n\n### Function name:\nfrom_pretrained\n\n### Doc string and function signature:\n```Python\ndef from_pretrained(cls, *args, **kwargs) :\n    \"\"\"\n    Load the model, either from a local path or from the HuggingFace Hub.\n    Supports the same arguments as ``AutoConfig`` and ``AutoModel``\n    from ``transformers``.\n    \"\"\"\n```\n\n### Requirements:\n1. Generate the complete function implementation.\n2. Only answer the code of the generated function, formatted using Markdown code block syntax.\n3. Do not include any additional text.\n\n### Answer: (use the provided format with backticks)\n", "reference": "    def from_pretrained(cls, *args, **kwargs):\n\n        config = AutoConfig.from_pretrained(*args, **kwargs)\n\n        assert hasattr(config, \"chronos_config\"), \"Not a Chronos config file\"\n\n        chronos_config = ChronosConfig(**config.chronos_config)\n\n        if chronos_config.model_type == \"seq2seq\":\n            inner_model = AutoModelForSeq2SeqLM.from_pretrained(*args, **kwargs)\n        else:\n            assert config.model_type == \"causal\"\n            inner_model = AutoModelForCausalLM.from_pretrained(*args, **kwargs)\n\n        return cls(\n            tokenizer=chronos_config.create_tokenizer(),\n            model=ChronosModel(config=chronos_config, model=inner_model),\n        )", "completions": ["  \n```\ndef from_pretrained(cls, *args, **kwargs) :\n    \"\"\"\n    Load the model, either from a local path or from the HuggingFace Hub.\n    Supports the same arguments as ``AutoConfig`` and ``AutoModel``\n    from ``transformers``.\n    \"\"\"\n    if len(args) > 0:\n        path = args[0]\n    else:\n        path = kwargs.get(\"path\", None)\n    if path is None:\n        raise ValueError(\"path is required\")\n    if not os.path.exists(path):\n        raise ValueError(\"path does not exist\")\n    if not os.path.isfile(path):\n        raise ValueError(\"path is not a file\")\n    if not path.endswith(\".pth\"):\n        raise ValueError(\"path is not a PyTorch model file\")\n    model = cls.from_pretrained(path, *args[1:], **kwargs)\n    return model\n```"]}
