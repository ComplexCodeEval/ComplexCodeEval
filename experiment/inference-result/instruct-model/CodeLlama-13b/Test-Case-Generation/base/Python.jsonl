{"id": 0, "prompt": "\nYou are an expert Python programmer. You will be given some information and will generate a correct test case for the given function.\nYour response will only be python test function and without anything else.\n\n### focal function:\n```python\ndef compute_pair_vector_and_distance(g: dgl.DGLGraph):\n    dst_pos = g.ndata[\"pos\"][g.edges()[1]] + g.edata[\"pbc_offshift\"]\n    src_pos = g.ndata[\"pos\"][g.edges()[0]]\n    bond_vec = dst_pos - src_pos\n    bond_dist = torch.norm(bond_vec, dim=1)\n\n    return bond_vec, bond_dist\n```\n###test function signature:\n```python\n\n    def test_compute_angle(self, graph_Mo, graph_CH4):\n```\n### Answer: (use the provided format with backticks)\n", "name": "test_compute_angle", "reference": "\n    def test_compute_angle(self, graph_Mo, graph_CH4):\n        s1, g1, state1 = graph_Mo\n        lattice = torch.tensor(s1.lattice.matrix, dtype=matgl.float_th).unsqueeze(dim=0)\n        g1.edata[\"pbc_offshift\"] = torch.matmul(g1.edata[\"pbc_offset\"], lattice[0])\n        g1.ndata[\"pos\"] = g1.ndata[\"frac_coords\"] @ lattice[0]\n        bv, bd = compute_pair_vector_and_distance(g1)\n        g1.edata[\"bond_vec\"] = bv\n        g1.edata[\"bond_dist\"] = bd\n        cos_loop = _calculate_cos_loop(g1, 4.0)\n\n        line_graph = create_line_graph(g1, 4.0)\n        line_graph.apply_edges(compute_theta_and_phi)\n        np.testing.assert_array_almost_equal(\n            np.sort(np.array(cos_loop)), np.sort(np.array(line_graph.edata[\"cos_theta\"]))\n        )\n\n        # test only compute theta\n        line_graph.apply_edges(partial(compute_theta, directed=False))\n        theta = np.arccos(np.clip(cos_loop, -1.0 + 1e-7, 1.0 - 1e-7))\n        np.testing.assert_array_almost_equal(np.sort(theta), np.sort(np.array(line_graph.edata[\"theta\"])), decimal=4)\n\n        # test only compute theta with cosine\n        _ = line_graph.edata.pop(\"cos_theta\")\n        line_graph.apply_edges(partial(compute_theta, cosine=True, directed=False))\n        np.testing.assert_array_almost_equal(\n            np.sort(np.array(cos_loop)), np.sort(np.array(line_graph.edata[\"cos_theta\"]))\n        )\n\n        s2, g2, state2 = graph_CH4\n        lattice = torch.tensor(np.identity(3), dtype=matgl.float_th).unsqueeze(dim=0)\n        g2.edata[\"pbc_offshift\"] = torch.matmul(g2.edata[\"pbc_offset\"], lattice[0])\n        g2.ndata[\"pos\"] = g2.ndata[\"frac_coords\"] @ lattice[0]\n        bv, bd = compute_pair_vector_and_distance(g2)\n        g2.edata[\"bond_vec\"] = bv\n        g2.edata[\"bond_dist\"] = bd\n        cos_loop = _calculate_cos_loop(g2, 2.0)\n\n        line_graph = create_line_graph(g2, 2.0)\n        line_graph.apply_edges(compute_theta_and_phi)\n        np.testing.assert_array_almost_equal(\n            np.sort(np.array(cos_loop)), np.sort(np.array(line_graph.edata[\"cos_theta\"]))\n        )\n\n        # test only compute theta\n        line_graph.apply_edges(partial(compute_theta, directed=False))\n        np.testing.assert_array_almost_equal(\n            np.sort(np.arccos(np.array(cos_loop))), np.sort(np.array(line_graph.edata[\"theta\"]))\n        )\n\n        # test only compute theta with cosine\n        _ = line_graph.edata.pop(\"cos_theta\")\n        line_graph.apply_edges(partial(compute_theta, cosine=True, directed=False))\n        np.testing.assert_array_almost_equal(\n            np.sort(np.array(cos_loop)), np.sort(np.array(line_graph.edata[\"cos_theta\"]))\n        )", "completions": ["  ```\ndef test_compute_pair_vector_and_distance(self, graph_Mo, graph_CH4):\n    # Test case 1:\n    # Test that the function returns the correct pair vector and distance for a single bond\n    bond_vec, bond_dist = compute_pair_vector_and_distance(graph_Mo)\n    expected_bond_vec = torch.tensor([[1.0, 0.0, 0.0], [0.0, 1.0, 0.0], [0.0, 0.0, 1.0]])\n    expected_bond_dist = torch.tensor([1.0, 1.0, 1.0])\n    self.assertTrue(torch.allclose(bond_vec, expected_bond_vec))\n    self.assertTrue(torch.allclose(bond_dist, expected_bond_dist))\n\n    # Test case 2:\n    # Test that the function returns the correct pair vector and distance for a multiple bonds\n    bond_vec, bond_dist = compute_pair_vector_and_distance(graph_CH4)\n    expected_bond_vec = torch.tensor([[1.0, 0.0, 0.0], [0.0, 1.0, 0.0], [0.0, 0.0, 1.0], [1.0, 0.0, 0.0], [0.0, 1.0, 0.0], [0.0, 0.0, 1.0], [1.0, 0.0, 0.0], [0.0, 1.0, 0.0], [0.0, 0.0, 1.0]])\n    expected_bond_dist = torch.tensor([1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0])\n    self.assertTrue(torch.allclose(bond_vec, expected_bond_vec))\n    self.assertTrue(torch.allclose(bond_dist, expected_bond_dist))\n```"]}
{"id": 1, "prompt": "\nYou are an expert Python programmer. You will be given some information and will generate a correct test case for the given function.\nYour response will only be python test function and without anything else.\n\n### focal function:\n```python\n\n    def _make_schedule(self, targets: List[Task]) -> List[Task]:\n        self.schedule = []\n        self.dependencies = self._build_dependencies(targets)\n\n        edge_tups = []\n        for node in self.dependencies:\n            for dependency in self.dependencies[node]:\n                edge_tups.append((dependency, node))\n\n        for task in targets:\n            # add edges from a dummy node to each target to guarantee\n            # they will be included in the final schedule\n            edge_tups.append((Executor.DUMMY_TASK_VALUE, task))\n\n        def _compare_key(task: Union[Task, str]):\n            if task == Executor.DUMMY_TASK_VALUE:\n                return (\"\", 0)\n            return (\n                task.group_label() or \"\",\n                -task.priority(),\n            )\n\n        graph = networkx.DiGraph(edge_tups)\n        res = [\n            t\n            for t in networkx.lexicographical_topological_sort(graph, key=_compare_key)\n            if t != Executor.DUMMY_TASK_VALUE\n        ]\n        return res\n```\n###test function signature:\n```python\n\n    def test_group_label_with_dependencies(self):\n        # Creating tasks with dependencies and group labels\n```\n### Answer: (use the provided format with backticks)\n", "name": "test_group_label_with_dependencies", "reference": "\n    def test_group_label_with_dependencies(self):\n        # Creating tasks with dependencies and group labels\n        task1 = create_mock_task(\"task1\", result=1, group_label=\"group1\")\n        task2 = create_mock_task(\n            \"task2\", result=2, dependencies={\"task1\": task1}, group_label=\"group2\"\n        )\n        task3 = create_mock_task(\n            \"task3\", result=3, dependencies={\"task2\": task2}, group_label=\"group1\"\n        )\n\n        executor = Executor([task3])\n        schedule = executor._make_schedule([task3])\n        scheduled_labels = [\n            task.group_label() for task in schedule if task.group_label()\n        ]\n\n        # Check if task3 is scheduled after task1 and task2 due to dependency, even though it has the same group label as task1\n        group1_indices = [\n            i for i, label in enumerate(scheduled_labels) if label == \"group1\"\n        ]\n        group2_index = scheduled_labels.index(\"group2\")\n\n        assert (\n            group1_indices[-1] > group2_index\n        ), \"Task with the same group label but later dependency was not scheduled after different group label\"", "completions": ["  ```\ndef test_group_label_with_dependencies(self):\n    # Creating tasks with dependencies and group labels\n    task1 = Task(\"task1\", group_label=\"group1\", priority=1)\n    task2 = Task(\"task2\", group_label=\"group1\", priority=2)\n    task3 = Task(\"task3\", group_label=\"group2\", priority=3)\n    task4 = Task(\"task4\", group_label=\"group2\", priority=4)\n    task5 = Task(\"task5\", group_label=\"group3\", priority=5)\n    task6 = Task(\"task6\", group_label=\"group3\", priority=6)\n\n    # Creating dependencies\n    task1.add_dependency(task2)\n    task2.add_dependency(task3)\n    task3.add_dependency(task4)\n    task4.add_dependency(task5)\n    task5.add_dependency(task6)\n\n    # Creating a list of tasks\n    tasks = [task1, task2, task3, task4, task5, task6]\n\n    # Calling the function to be tested\n    schedule = self._make_schedule(tasks)\n\n    # Asserting the expected output\n    expected_output = [task1, task2, task3, task4, task5, task6]\n    self.assertEqual(schedule, expected_output)\n```"]}
{"id": 2, "prompt": "\nYou are an expert Python programmer. You will be given some information and will generate a correct test case for the given function.\nYour response will only be python test function and without anything else.\n\n### focal function:\n```python\ndef setup_cache(\n    max_batch_size, max_seq_len, dim, heads, layers, block_size, rope_base\n):\n    if max_seq_len >= max_seq_len and max_batch_size >= max_batch_size:\n        return\n\n    head_dim = dim // heads\n    max_seq_len = find_multiple(max_seq_len, 8)\n\n    for b in layers:\n        b.attention.kv_cache = KVCache(\n            max_batch_size, max_seq_len, heads, head_dim\n        )\n\n    freq_cis = precompute_freq_cis(block_size, dim // heads, rope_base)\n    causal_mask = torch.tril(\n        torch.ones(max_seq_len, max_seq_len, dtype=torch.bool)\n    )\n\n    return causal_mask, freq_cis\n```\n###test function signature:\n```python\n\ndef test_setup_cache_max_batch_size_greater_than_max():\n```\n### Answer: (use the provided format with backticks)\n", "name": "test_setup_cache_max_batch_size_greater_than_max", "reference": "\ndef test_setup_cache_max_batch_size_greater_than_max():\n    layers = [\n        Mock(attention=Mock(kw_cache=None)),\n        Mock(attention=Mock(kw_cache=None)),\n    ]\n    max_batch_size = 64\n    max_seq_len = 32\n    heads = 4\n    head_dim = 32\n    block_size = 32\n    rope_base = 1000\n    setup_cache(\n        max_batch_size + 10,\n        max_seq_len,\n        head_dim * heads,\n        heads,\n        layers,\n        block_size,\n        rope_base,\n    )\n    for layer in layers:\n        assert isinstance(layer.attention.kw_cache, KVCache)\n        assert layer.attention.kw_cache.k_cache.shape == torch.Size(\n            [max_batch_size + 10, heads, max_seq_len, head_dim]\n        )\n        assert layer.attention.kw_cache.v_cache.shape == torch.Size(\n            [max_batch_size + 10, heads, max_seq_len, head_dim]\n        )", "completions": ["  ```\ndef test_setup_cache_max_batch_size_greater_than_max():\n    max_batch_size = 10\n    max_seq_len = 100\n    dim = 512\n    heads = 8\n    layers = 12\n    block_size = 1024\n    rope_base = 1024\n\n    causal_mask, freq_cis = setup_cache(\n        max_batch_size, max_seq_len, dim, heads, layers, block_size, rope_base\n    )\n\n    assert causal_mask.shape == (max_seq_len, max_seq_len)\n    assert causal_mask.dtype == torch.bool\n    assert causal_mask.sum() == 0\n\n    assert freq_cis.shape == (max_seq_len, dim // heads)\n    assert freq_cis.dtype == torch.float32\n    assert freq_cis.sum() == 0\n```"]}
{"id": 3, "prompt": "\nYou are an expert Python programmer. You will be given some information and will generate a correct test case for the given function.\nYour response will only be python test function and without anything else.\n\n### focal function:\n```python\n\ndef load_image_from_string(\n    reference: str,\n    max_height: Optional[int] = None,\n    max_width: Optional[int] = None,\n) -> Tuple[str, Optional[float]]:\n    if uri_is_http_link(uri=reference):\n        return load_image_from_url(\n            url=reference, max_height=max_height, max_width=max_width\n        )\n    if os.path.exists(reference):\n        if max_height is None or max_width is None:\n            with open(reference, \"rb\") as f:\n                img_bytes = f.read()\n            img_base64_str = encode_base_64(payload=img_bytes)\n            return img_base64_str, None\n        local_image = cv2.imread(reference)\n        if local_image is None:\n            raise EncodingError(f\"Could not load image from {reference}\")\n        local_image, scaling_factor = resize_opencv_image(\n            image=local_image,\n            max_height=max_height,\n            max_width=max_width,\n        )\n        return numpy_array_to_base64_jpeg(image=local_image), scaling_factor\n    if max_height is not None and max_width is not None:\n        image_bytes = base64.b64decode(reference)\n        image = bytes_to_opencv_image(payload=image_bytes)\n        image, scaling_factor = resize_opencv_image(\n            image=image,\n            max_height=max_height,\n            max_width=max_width,\n        )\n        return numpy_array_to_base64_jpeg(image=image), scaling_factor\n    return reference, None\n```\n###test function signature:\n```python\n\ndef test_load_image_from_string_when_file_to_be_downloaded(\n    requests_get_mock: MagicMock,\n) -> None:\n    # given\n```\n### Answer: (use the provided format with backticks)\n", "name": "test_load_image_from_string_when_file_to_be_downloaded", "reference": "\ndef test_load_image_from_string_when_file_to_be_downloaded(\n    requests_get_mock: MagicMock,\n) -> None:\n    # given\n    response = Response()\n    response.status_code = 200\n    image = np.zeros((128, 128, 3), dtype=np.uint8)\n    _, encoded_image = cv2.imencode(\".jpg\", image)\n    response._content = encoded_image\n    requests_get_mock.return_value = response\n\n    # when\n    serialised_image, scaling_factor = load_image_from_string(\n        reference=\"https://some/file.jpg\"\n    )\n\n    # then\n    recovered_image = base64.b64decode(serialised_image)\n    bytes_array = np.frombuffer(recovered_image, dtype=np.uint8)\n    decoding_result = cv2.imdecode(bytes_array, cv2.IMREAD_UNCHANGED)\n\n    # then\n    assert scaling_factor is None\n    assert decoding_result.shape == image.shape\n    assert np.allclose(decoding_result, image)\n    requests_get_mock.assert_called_once_with(\"https://some/file.jpg\")", "completions": ["  ```\ndef test_load_image_from_string_when_file_to_be_downloaded(requests_get_mock):\n    # given\n    reference = \"https://example.com/image.jpg\"\n    max_height = 100\n    max_width = 100\n\n    # when\n    result = load_image_from_string(reference, max_height, max_width)\n\n    # then\n    assert result == (\"data:image/jpeg;base64,/9j/4AAQSkZJRgABAQEAYABgAAD/2wBDAAoHBwgHBgoICAgLCgoLDhgQDg0NDh0VFhEYIx8lJCIfIiEmKzcvJik0KSEiMEExNDk7Pj4+JS5ESUM8SDc9Pjv/2wBDAQoLCw4NDhwYMCAcIDAwMDAwMDAwMCAcIDAwMDAwMDAwMDAwMDAwMDAwMDAwMDAwMDAwMDAwMDAwMDAwMDAwMDAwMDAwMDAwMDAwMDAwMDAwMDAwMDAwMDAwMDAwMDAwMDAwMDAwMDAwMDAwMDAwMDAwMDAwMDAwMDAwMDAwMDAwMDAwMDAwMDAwMDAwMDAwMDAwMDAwMDAwMDAwMDAwMDAwMDAwMDAwMDAwMDAwMDAwMDAwMDAwMDAwMDAwMDAwMDAwMDAwMDAwMDAwMDAwMDAwMDAwMDAwMDAwMDAwMDAwMDAwMDAwMDAwMDAwMDAwMDAwMDAwMDAwMDAwMDAwMDAwMDAwMDAwMDAwMDAwMDAwMDAwMDAwMDAwMDAwMDAwMDAwMDAwMDAwMDAwMDAwMDAwMDAwMDAwMDAwMDAwMDAwMDAwMDAwMDAwMDAwMDAwMDAwMDAwMDAwMDAwMDAwMDAwMDAwMDAwMDAwMDAwMDAwMDAwMDAwMDAwMDAwMDAwMDAwMDAwMDAwMDAwMDAwMDAwMDAwMDAwMDAwMDAwMDAwMDAwMDAwMDAwMDAwMDAwMDAwMDAwMDAwMDAwMDAwMDAwMDAwMDAwMDAwMDAwMDAwMDAwMDAwMDAwMDAwMDAwMDAwMDAwMDAwMDAwMDAwMDAwMDAwMDAwMDAwMDAwMDAwMDAwMDAwMDAwMDAwMDAwMDAwMDAwMDAwMDAwMDAwMDAwMDAwMDAwMDAwMDAwMDAwMDAwMDAwMDAwMDAwMDAwMDAwMDAwMDAwMDAwMDAwMDAwMDAwMDAwMDAwMDAwMDAwMDAwMDAwMDAwMDAwMDAwMDAwMDAwMDAwMDAwMDAwMDAwMDAwMDAwMDAwMDAwMDAwMDAwMDAwMDAwMDAwMDAwMDAwMDAwMDAwMDAwMDAwMDAwMDAwMDAwMDAwMDAwMDAwMDAwMDAwMDAwMDAwMDAwMDAwMDAwMDAwMDAwMDAwMDAwMDAwMDAwMDAwMDAwMDAwMDAwMDAwMDAwMDAwMDAwMDAwMDAwMDAwMDAwMDAwMDAwMD"]}
{"id": 4, "prompt": "\nYou are an expert Python programmer. You will be given some information and will generate a correct test case for the given function.\nYour response will only be python test function and without anything else.\n\n### focal function:\n```python\ndef assert_connectivity(connectivity: nx.Graph, circuit: Circuit):\n    if list(connectivity.nodes) != list(range(connectivity.number_of_nodes())):\n        node_mapping = {node: i for i, node in enumerate(connectivity.nodes)}\n        new_connectivity = nx.Graph()\n        new_connectivity.add_edges_from(\n            [(node_mapping[u], node_mapping[v]) for u, v in connectivity.edges]\n        )\n        connectivity = new_connectivity\n    for gate in circuit.queue:\n        if len(gate.qubits) > 2 and not isinstance(gate, gates.M):\n            raise_error(ConnectivityError, f\"{gate.name} acts on more than two qubits.\")\n        if len(gate.qubits) == 2:\n            if (gate.qubits[0], gate.qubits[1]) not in connectivity.edges:\n                raise_error(\n                    ConnectivityError,\n                    f\"Circuit does not respect connectivity. {gate.name} acts on {gate.qubits}.\",\n                )\n```\n###test function signature:\n```python\n\ndef test_sabre_random_circuits(n_gates, look, decay, placer, connectivity):\n```\n### Answer: (use the provided format with backticks)\n", "name": "test_sabre_random_circuits", "reference": "\ndef test_sabre_random_circuits(n_gates, look, decay, placer, connectivity):\n    placer = placer(connectivity=connectivity)\n    layout_circ = Circuit(5)\n    initial_layout = placer(layout_circ)\n    router = Sabre(connectivity=connectivity, lookahead=look, decay_lookahead=decay)\n    circuit = generate_random_circuit(nqubits=5, ngates=n_gates)\n    measurement = gates.M(*range(5))\n    circuit.add(measurement)\n    transpiled_circuit, final_qubit_map = router(circuit, initial_layout)\n    assert router.added_swaps >= 0\n    assert_connectivity(connectivity, transpiled_circuit)\n    assert_placement(transpiled_circuit, final_qubit_map)\n    assert n_gates + router.added_swaps + 1 == transpiled_circuit.ngates\n    assert_circuit_equivalence(\n        original_circuit=circuit,\n        transpiled_circuit=transpiled_circuit,\n        final_map=final_qubit_map,\n        initial_map=initial_layout,\n    )\n    assert transpiled_circuit.queue[-1].register_name == measurement.register_name", "completions": ["  ```\ndef test_sabre_random_circuits(n_gates, look, decay, placer, connectivity):\n    circuit = Circuit(n_gates, look, decay, placer)\n    assert_connectivity(connectivity, circuit)\n```"]}
{"id": 5, "prompt": "\nYou are an expert Python programmer. You will be given some information and will generate a correct test case for the given function.\nYour response will only be python test function and without anything else.\n\n### focal function:\n```python\ndef xx_circuit_step(source, strength, target, embodiment):\n\n    permute_source_for_overlap, permute_target_for_overlap = None, None\n\n    # apply all possible reflections, shifts to the source\n    for source_reflection_name in reflection_options:\n        reflected_source_coord, source_reflection, reflection_phase_shift = apply_reflection(\n            source_reflection_name, source\n        )\n        for source_shift_name in shift_options:\n            shifted_source_coord, source_shift, shift_phase_shift = apply_shift(\n                source_shift_name, reflected_source_coord\n            )\n\n            # check for overlap, back out permutation\n            source_shared, target_shared = None, None\n            for i, j in [(0, 0), (0, 1), (0, 2), (1, 0), (1, 1), (1, 2), (2, 0), (2, 1), (2, 2)]:\n\n                if (\n                    abs(np.mod(abs(shifted_source_coord[i] - target[j]), np.pi)) < EPSILON\n                    or abs(np.mod(abs(shifted_source_coord[i] - target[j]), np.pi) - np.pi)\n                    < EPSILON\n                ):\n                    source_shared, target_shared = i, j\n                    break\n            if source_shared is None:\n                continue\n\n            # pick out the other coordinates\n            source_first, source_second = (x for x in [0, 1, 2] if x != source_shared)\n            target_first, target_second = (x for x in [0, 1, 2] if x != target_shared)\n\n            # check for arccos validity\n            r, s, u, v, x, y = decompose_xxyy_into_xxyy_xx(\n                float(target[target_first]),\n                float(target[target_second]),\n                float(shifted_source_coord[source_first]),\n                float(shifted_source_coord[source_second]),\n                float(strength),\n            )\n            if any(math.isnan(val) for val in (r, s, u, v, x, y)):\n                continue\n\n            # OK: this combination of things works.\n            # save the permutation which rotates the shared coordinate into ZZ.\n            permute_source_for_overlap = canonical_rotation_circuit(source_first, source_second)\n            permute_target_for_overlap = canonical_rotation_circuit(target_first, target_second)\n            break\n\n        if permute_source_for_overlap is not None:\n            break\n\n    if permute_source_for_overlap is None:\n        raise QiskitError(\n            \"Error during RZX decomposition: Could not find a suitable Weyl \"\n            f\"reflection to match {source} to {target} along {strength}.\"\n        )\n\n    prefix_circuit, affix_circuit = QuantumCircuit(2), QuantumCircuit(2)\n\n    # the basic formula we're trying to work with is:\n    # target^p_t_f_o =\n    #     rs * (source^s_reflection * s_shift)^p_s_f_o * uv * operation * xy\n    # but we're rearranging it into the form\n    #   target = affix source prefix\n    # and computing just the prefix / affix circuits.\n\n    # the outermost prefix layer comes from the (inverse) target permutation.\n    prefix_circuit.compose(permute_target_for_overlap.inverse(), inplace=True)\n    # the middle prefix layer comes from the local Z rolls.\n    prefix_circuit.rz(2 * x, [0])\n    prefix_circuit.rz(2 * y, [1])\n    prefix_circuit.compose(embodiment, inplace=True)\n    prefix_circuit.rz(2 * u, [0])\n    prefix_circuit.rz(2 * v, [1])\n    # the innermost prefix layer is source_reflection, shifted by source_shift,\n    # finally conjugated by p_s_f_o.\n    prefix_circuit.compose(permute_source_for_overlap, inplace=True)\n    prefix_circuit.compose(source_reflection, inplace=True)\n    prefix_circuit.global_phase += -np.log(reflection_phase_shift).imag\n    prefix_circuit.global_phase += -np.log(shift_phase_shift).imag\n\n    # the affix circuit is constructed in reverse.\n    # first (i.e., innermost), we install the other half of the source transformations and p_s_f_o.\n    affix_circuit.compose(source_reflection.inverse(), inplace=True)\n    affix_circuit.compose(source_shift, inplace=True)\n    affix_circuit.compose(permute_source_for_overlap.inverse(), inplace=True)\n    # then, the other local rolls in the middle.\n    affix_circuit.rz(2 * r, [0])\n    affix_circuit.rz(2 * s, [1])\n    # finally, the other half of the p_t_f_o conjugation.\n    affix_circuit.compose(permute_target_for_overlap, inplace=True)\n\n    return {\"prefix_circuit\": prefix_circuit, \"affix_circuit\": affix_circuit}\n```\n###test function signature:\n```python\n    def test_xx_circuit_step(self):\n```\n### Answer: (use the provided format with backticks)\n", "name": "test_xx_circuit_step", "reference": "    def test_xx_circuit_step(self):\n\n        for _ in range(100):\n            source_coordinate, interaction, target_coordinate = self._generate_xxyy_test_case()\n\n            source_embodiment = qiskit.QuantumCircuit(2)\n            source_embodiment.append(UnitaryGate(canonical_matrix(*source_coordinate)), [0, 1])\n            interaction_embodiment = qiskit.QuantumCircuit(2)\n            interaction_embodiment.append(UnitaryGate(canonical_matrix(*interaction)), [0, 1])\n\n            prefix_circuit, affix_circuit = itemgetter(\"prefix_circuit\", \"affix_circuit\")(\n                xx_circuit_step(\n                    source_coordinate, interaction[0], target_coordinate, interaction_embodiment\n                )\n            )\n\n            target_embodiment = QuantumCircuit(2)\n            target_embodiment.compose(prefix_circuit, inplace=True)\n            target_embodiment.compose(source_embodiment, inplace=True)\n            target_embodiment.compose(affix_circuit, inplace=True)\n            self.assertTrue(\n                np.all(\n                    np.abs(\n                        qiskit.quantum_info.operators.Operator(target_embodiment).data\n                        - canonical_matrix(*target_coordinate)\n                    )\n                    < EPSILON\n                )\n            )", "completions": ["  ```\ndef test_xx_circuit_step():\n    source = np.array([0.5, 0.5, 0.5])\n    strength = 0.5\n    target = np.array([0.5, 0.5, 0.5])\n    embodiment = QuantumCircuit(2)\n    expected_prefix_circuit = QuantumCircuit(2)\n    expected_affix_circuit = QuantumCircuit(2)\n    actual_result = xx_circuit_step(source, strength, target, embodiment)\n    self.assertEqual(actual_result[\"prefix_circuit\"], expected_prefix_circuit)\n    self.assertEqual(actual_result[\"affix_circuit\"], expected_affix_circuit)\n```"]}
{"id": 6, "prompt": "\nYou are an expert Python programmer. You will be given some information and will generate a correct test case for the given function.\nYour response will only be python test function and without anything else.\n\n### focal function:\n```python\n\ndef arange(start, stop=None, step=1, dtype=None):\n    if dtype is None:\n        dtypes_to_resolve = [\n            getattr(start, \"dtype\", type(start)),\n            getattr(step, \"dtype\", type(step)),\n        ]\n        if stop is not None:\n            dtypes_to_resolve.append(getattr(stop, \"dtype\", type(stop)))\n        dtype = dtypes.result_type(*dtypes_to_resolve)\n    dtype = to_torch_dtype(dtype)\n    if stop is None:\n        return torch.arange(end=start, dtype=dtype, device=get_device())\n    return torch.arange(\n        start, stop, step=step, dtype=dtype, device=get_device()\n    )\n```\n###test function signature:\n```python\n\n    def test_einsum(self):\n```\n### Answer: (use the provided format with backticks)\n", "name": "test_einsum", "reference": "\n    def test_einsum(self):\n        x = np.arange(24).reshape([2, 3, 4]).astype(\"float32\")\n        y = np.arange(24).reshape([2, 4, 3]).astype(\"float32\")\n        self.assertAllClose(\n            knp.einsum(\"ijk,lkj->il\", x, y),\n            np.einsum(\"ijk,lkj->il\", x, y),\n        )\n        self.assertAllClose(\n            knp.einsum(\"ijk,ikj->i\", x, y),\n            np.einsum(\"ijk,ikj->i\", x, y),\n        )\n        self.assertAllClose(\n            knp.einsum(\"i...,j...k->...ijk\", x, y),\n            np.einsum(\"i..., j...k->...ijk\", x, y),\n        )\n        self.assertAllClose(knp.einsum(\",ijk\", 5, y), np.einsum(\",ijk\", 5, y))\n\n        self.assertAllClose(\n            knp.Einsum(\"ijk,lkj->il\")(x, y),\n            np.einsum(\"ijk,lkj->il\", x, y),\n        )\n        self.assertAllClose(\n            knp.Einsum(\"ijk,ikj->i\")(x, y),\n            np.einsum(\"ijk,ikj->i\", x, y),\n        )\n        self.assertAllClose(\n            knp.Einsum(\"i...,j...k->...ijk\")(x, y),\n            np.einsum(\"i...,j...k->...ijk\", x, y),\n        )\n        self.assertAllClose(knp.Einsum(\",ijk\")(5, y), np.einsum(\",ijk\", 5, y))", "completions": ["  ```\ndef test_einsum():\n    # Test case 1:\n    start = 0\n    stop = 10\n    step = 1\n    dtype = None\n    expected_result = torch.arange(start, stop, step=step, dtype=dtype, device=get_device())\n    result = arange(start, stop, step, dtype)\n    self.assertEqual(result, expected_result)\n\n    # Test case 2:\n    start = 0\n    stop = 10\n    step = 2\n    dtype = None\n    expected_result = torch.arange(start, stop, step=step, dtype=dtype, device=get_device())\n    result = arange(start, stop, step, dtype)\n    self.assertEqual(result, expected_result)\n\n    # Test case 3:\n    start = 0\n    stop = 10\n    step = 1\n    dtype = torch.float32\n    expected_result = torch.arange(start, stop, step=step, dtype=dtype, device=get_device())\n    result = arange(start, stop, step, dtype)\n    self.assertEqual(result, expected_result)\n\n    # Test case 4:\n    start = 0\n    stop = 10\n    step = 2\n    dtype = torch.float32\n    expected_result = torch.arange(start, stop, step=step, dtype=dtype, device=get_device())\n    result = arange(start, stop, step, dtype)\n    self.assertEqual(result, expected_result)\n```"]}
{"id": 7, "prompt": "\nYou are an expert Python programmer. You will be given some information and will generate a correct test case for the given function.\nYour response will only be python test function and without anything else.\n\n### focal function:\n```python\ndef spectrogram(\n    waveform: np.ndarray,\n    window: np.ndarray,\n    frame_length: int,\n    hop_length: int,\n    fft_length: Optional[int] = None,\n    power: Optional[float] = 1.0,\n    center: bool = True,\n    pad_mode: str = \"reflect\",\n    onesided: bool = True,\n    preemphasis: Optional[float] = None,\n    mel_filters: Optional[np.ndarray] = None,\n    mel_floor: float = 1e-10,\n    log_mel: Optional[str] = None,\n    reference: float = 1.0,\n    min_value: float = 1e-10,\n    db_range: Optional[float] = None,\n    remove_dc_offset: Optional[bool] = None,\n    dtype: np.dtype = np.float32,\n) -> np.ndarray:\n    window_length = len(window)\n\n    if fft_length is None:\n        fft_length = frame_length\n\n    if frame_length > fft_length:\n        raise ValueError(f\"frame_length ({frame_length}) may not be larger than fft_length ({fft_length})\")\n\n    if window_length != frame_length:\n        raise ValueError(f\"Length of the window ({window_length}) must equal frame_length ({frame_length})\")\n\n    if hop_length <= 0:\n        raise ValueError(\"hop_length must be greater than zero\")\n\n    if waveform.ndim != 1:\n        raise ValueError(f\"Input waveform must have only one dimension, shape is {waveform.shape}\")\n\n    if np.iscomplexobj(waveform):\n        raise ValueError(\"Complex-valued input waveforms are not currently supported\")\n\n    if power is None and mel_filters is not None:\n        raise ValueError(\n            \"You have provided `mel_filters` but `power` is `None`. Mel spectrogram computation is not yet supported for complex-valued spectrogram.\"\n            \"Specify `power` to fix this issue.\"\n        )\n\n    # center pad the waveform\n    if center:\n        padding = [(int(frame_length // 2), int(frame_length // 2))]\n        waveform = np.pad(waveform, padding, mode=pad_mode)\n\n    # promote to float64, since np.fft uses float64 internally\n    waveform = waveform.astype(np.float64)\n    window = window.astype(np.float64)\n\n    # split waveform into frames of frame_length size\n    num_frames = int(1 + np.floor((waveform.size - frame_length) / hop_length))\n\n    num_frequency_bins = (fft_length // 2) + 1 if onesided else fft_length\n    spectrogram = np.empty((num_frames, num_frequency_bins), dtype=np.complex64)\n\n    # rfft is faster than fft\n    fft_func = np.fft.rfft if onesided else np.fft.fft\n    buffer = np.zeros(fft_length)\n\n    timestep = 0\n    for frame_idx in range(num_frames):\n        buffer[:frame_length] = waveform[timestep : timestep + frame_length]\n\n        if remove_dc_offset:\n            buffer[:frame_length] = buffer[:frame_length] - buffer[:frame_length].mean()\n\n        if preemphasis is not None:\n            buffer[1:frame_length] -= preemphasis * buffer[: frame_length - 1]\n            buffer[0] *= 1 - preemphasis\n\n        buffer[:frame_length] *= window\n\n        spectrogram[frame_idx] = fft_func(buffer)\n        timestep += hop_length\n\n    # note: ** is much faster than np.power\n    if power is not None:\n        spectrogram = np.abs(spectrogram, dtype=np.float64) ** power\n\n    spectrogram = spectrogram.T\n\n    if mel_filters is not None:\n        spectrogram = np.maximum(mel_floor, np.dot(mel_filters.T, spectrogram))\n\n    if power is not None and log_mel is not None:\n        if log_mel == \"log\":\n            spectrogram = np.log(spectrogram)\n        elif log_mel == \"log10\":\n            spectrogram = np.log10(spectrogram)\n        elif log_mel == \"dB\":\n            if power == 1.0:\n                spectrogram = amplitude_to_db(spectrogram, reference, min_value, db_range)\n            elif power == 2.0:\n                spectrogram = power_to_db(spectrogram, reference, min_value, db_range)\n            else:\n                raise ValueError(f\"Cannot use log_mel option '{log_mel}' with power {power}\")\n        else:\n            raise ValueError(f\"Unknown log_mel option: {log_mel}\")\n\n        spectrogram = np.asarray(spectrogram, dtype)\n\n    return spectrogram\n```\n###test function signature:\n```python\n\n    def test_spectrogram_center_padding(self):\n```\n### Answer: (use the provided format with backticks)\n", "name": "test_spectrogram_center_padding", "reference": "\n    def test_spectrogram_center_padding(self):\n        waveform = self._load_datasamples(1)[0]\n\n        spec = spectrogram(\n            waveform,\n            window_function(512, \"hann\"),\n            frame_length=512,\n            hop_length=128,\n            center=True,\n            pad_mode=\"reflect\",\n        )\n        self.assertEqual(spec.shape, (257, 732))\n\n        # fmt: off\n        expected = np.array([\n            0.1287945 , 0.12792738, 0.08311573, 0.03155122, 0.02470202,\n            0.00727857, 0.00910694, 0.00686163, 0.01238981, 0.01473668,\n            0.00336144, 0.00370314, 0.00600871, 0.01120164, 0.01942998,\n            0.03132008, 0.0232842 , 0.01124642, 0.02754783, 0.02423725,\n            0.00147893, 0.00038027, 0.00112299, 0.00596233, 0.00571529,\n            0.02084235, 0.0231855 , 0.00810006, 0.01837943, 0.00651339,\n            0.00093931, 0.00067426, 0.01058399, 0.01270507, 0.00151734,\n            0.00331913, 0.00302416, 0.01081792, 0.00754549, 0.00148963,\n            0.00111943, 0.00152573, 0.00608017, 0.01749986, 0.01205949,\n            0.0143082 , 0.01910573, 0.00413786, 0.03916619, 0.09873404,\n            0.08302026, 0.02673891, 0.00401255, 0.01397392, 0.00751862,\n            0.01024884, 0.01544606, 0.00638907, 0.00623633, 0.0085103 ,\n            0.00217659, 0.00276204, 0.00260835, 0.00299299,\n        ])\n        # fmt: on\n        self.assertTrue(np.allclose(spec[:64, 0], expected))\n\n        spec = spectrogram(\n            waveform,\n            window_function(512, \"hann\"),\n            frame_length=512,\n            hop_length=128,\n            center=True,\n            pad_mode=\"constant\",\n        )\n        self.assertEqual(spec.shape, (257, 732))\n\n        # fmt: off\n        expected = np.array([\n            0.06558744, 0.06889656, 0.06263352, 0.04264418, 0.03404115,\n            0.03244197, 0.02279134, 0.01646339, 0.01452216, 0.00826055,\n            0.00062093, 0.0031821 , 0.00419456, 0.00689327, 0.01106367,\n            0.01712119, 0.01721762, 0.00977533, 0.01606626, 0.02275621,\n            0.01727687, 0.00992739, 0.01217688, 0.01049927, 0.01022947,\n            0.01302475, 0.01166873, 0.01081812, 0.01057327, 0.00767912,\n            0.00429567, 0.00089625, 0.00654583, 0.00912084, 0.00700984,\n            0.00225026, 0.00290545, 0.00667712, 0.00730663, 0.00410813,\n            0.00073102, 0.00219296, 0.00527618, 0.00996585, 0.01123781,\n            0.00872816, 0.01165121, 0.02047945, 0.03681747, 0.0514379 ,\n            0.05137928, 0.03960042, 0.02821562, 0.01813349, 0.01201322,\n            0.01260964, 0.00900654, 0.00207905, 0.00456714, 0.00850599,\n            0.00788239, 0.00664407, 0.00824227, 0.00628301,\n        ])\n        # fmt: on\n        self.assertTrue(np.allclose(spec[:64, 0], expected))\n\n        spec = spectrogram(\n            waveform,\n            window_function(512, \"hann\"),\n            frame_length=512,\n            hop_length=128,\n            center=False,\n        )\n        self.assertEqual(spec.shape, (257, 728))\n\n        # fmt: off\n        expected = np.array([\n            0.00250445, 0.02161521, 0.06232229, 0.04339567, 0.00937727,\n            0.01080616, 0.00248685, 0.0095264 , 0.00727476, 0.0079152 ,\n            0.00839946, 0.00254932, 0.00716622, 0.005559  , 0.00272623,\n            0.00581774, 0.01896395, 0.01829788, 0.01020514, 0.01632692,\n            0.00870888, 0.02065827, 0.0136022 , 0.0132382 , 0.011827  ,\n            0.00194505, 0.0189979 , 0.026874  , 0.02194014, 0.01923883,\n            0.01621437, 0.00661967, 0.00289517, 0.00470257, 0.00957801,\n            0.00191455, 0.00431664, 0.00544359, 0.01126213, 0.00785778,\n            0.00423469, 0.01322504, 0.02226548, 0.02318576, 0.03428908,\n            0.03648811, 0.0202938 , 0.011902  , 0.03226198, 0.06347476,\n            0.01306318, 0.05308729, 0.05474771, 0.03127991, 0.00998512,\n            0.01449977, 0.01272741, 0.00868176, 0.00850386, 0.00313876,\n            0.00811857, 0.00538216, 0.00685749, 0.00535275,\n        ])\n        # fmt: on\n        self.assertTrue(np.allclose(spec[:64, 0], expected))", "completions": ["  ```\ndef test_spectrogram_center_padding():\n    waveform = np.array([1, 2, 3, 4, 5, 6, 7, 8, 9, 10])\n    window = np.array([0.5, 0.5, 0.5, 0.5, 0.5])\n    frame_length = 5\n    hop_length = 2\n    fft_length = 10\n    power = 1.0\n    center = True\n    pad_mode = \"reflect\"\n    onesided = True\n    preemphasis = None\n    mel_filters = None\n    mel_floor = 1e-10\n    log_mel = None\n    reference = 1.0\n    min_value = 1e-10\n    db_range = None\n    remove_dc_offset = None\n    dtype = np.float32\n\n    expected_spectrogram = np.array([[1, 2, 3, 4, 5],\n                                     [3, 4, 5, 6, 7],\n                                     [5, 6, 7, 8, 9],\n                                     [7, 8, 9, 10, 0]])\n\n    spectrogram = spectrogram(waveform, window, frame_length, hop_length, fft_length, power, center, pad_mode, onesided, preemphasis, mel_filters, mel_floor, log_mel, reference, min_value, db_range, remove_dc_offset, dtype)\n\n    np.testing.assert_array_equal(spectrogram, expected_spectrogram)\n```"]}
{"id": 9, "prompt": "\nYou are an expert Python programmer. You will be given some information and will generate a correct test case for the given function.\nYour response will only be python test function and without anything else.\n\n### focal function:\n```python\n\ndef arange(start, stop=None, step=1, dtype=None):\n    if dtype is None:\n        dtypes_to_resolve = [\n            getattr(start, \"dtype\", type(start)),\n            getattr(step, \"dtype\", type(step)),\n        ]\n        if stop is not None:\n            dtypes_to_resolve.append(getattr(stop, \"dtype\", type(stop)))\n        dtype = dtypes.result_type(*dtypes_to_resolve)\n    dtype = to_torch_dtype(dtype)\n    if stop is None:\n        return torch.arange(end=start, dtype=dtype, device=get_device())\n    return torch.arange(\n        start, stop, step=step, dtype=dtype, device=get_device()\n    )\n```\n###test function signature:\n```python\n\n    def test_take_along_axis(self):\n```\n### Answer: (use the provided format with backticks)\n", "name": "test_take_along_axis", "reference": "\n    def test_take_along_axis(self):\n        x = np.arange(24).reshape([1, 2, 3, 4])\n        indices = np.ones([1, 4, 1, 1], dtype=np.int32)\n        self.assertAllClose(\n            knp.take_along_axis(x, indices, axis=1),\n            np.take_along_axis(x, indices, axis=1),\n        )\n        self.assertAllClose(\n            knp.TakeAlongAxis(axis=1)(x, indices),\n            np.take_along_axis(x, indices, axis=1),\n        )\n\n        x = np.arange(12).reshape([1, 1, 3, 4])\n        indices = np.ones([1, 4, 1, 1], dtype=np.int32)\n        self.assertAllClose(\n            knp.take_along_axis(x, indices, axis=2),\n            np.take_along_axis(x, indices, axis=2),\n        )\n        self.assertAllClose(\n            knp.TakeAlongAxis(axis=2)(x, indices),\n            np.take_along_axis(x, indices, axis=2),\n        )\n\n        # Test with axis=None\n        x = np.arange(12).reshape([1, 1, 3, 4])\n        indices = np.array([1, 2, 3], dtype=np.int32)\n        self.assertAllClose(\n            knp.take_along_axis(x, indices, axis=None),\n            np.take_along_axis(x, indices, axis=None),\n        )\n        self.assertAllClose(\n            knp.TakeAlongAxis(axis=None)(x, indices),\n            np.take_along_axis(x, indices, axis=None),\n        )\n\n        # Test with negative indices\n        x = np.arange(12).reshape([1, 1, 3, 4])\n        indices = np.full([1, 4, 1, 1], -1, dtype=np.int32)\n        self.assertAllClose(\n            knp.take_along_axis(x, indices, axis=2),\n            np.take_along_axis(x, indices, axis=2),\n        )\n        self.assertAllClose(\n            knp.TakeAlongAxis(axis=2)(x, indices),\n            np.take_along_axis(x, indices, axis=2),\n        )", "completions": ["  ```\ndef test_take_along_axis(self):\n    # Test case 1:\n    input = torch.arange(1, 10, 1)\n    indices = torch.tensor([[0, 1], [1, 2]])\n    output = torch.take_along_axis(input, indices, axis=0)\n    expected_output = torch.tensor([[1, 2], [2, 3]])\n    self.assertEqual(output, expected_output)\n\n    # Test case 2:\n    input = torch.arange(1, 10, 1)\n    indices = torch.tensor([[0, 1], [1, 2]])\n    output = torch.take_along_axis(input, indices, axis=1)\n    expected_output = torch.tensor([[1, 3], [2, 4]])\n    self.assertEqual(output, expected_output)\n```"]}
{"id": 10, "prompt": "\nYou are an expert Python programmer. You will be given some information and will generate a correct test case for the given function.\nYour response will only be python test function and without anything else.\n\n### focal function:\n```python\ndef compute_correlation(\n    response_level_scores: dict[str, list[dict[str, dict[str, Any]]]],\n    name: str = '',\n) -> dict[str, dict[str, dict[str, float]]]:\n  factscore_metrics = set(list_metrics(response_level_scores[FACTSCORE]))\n  pred_metrics = set(list_metrics(response_level_scores[EVAL_METHOD]))\n  result = {}\n\n  for metric in factscore_metrics.intersection(pred_metrics):\n    factscore_scores = [\n        find_metric(response_level_scores[FACTSCORE][i], metric)\n        for i in range(len(response_level_scores[FACTSCORE]))\n    ]\n    pred_scores = [\n        find_metric(response_level_scores[EVAL_METHOD][i], metric)\n        for i in range(len(response_level_scores[EVAL_METHOD]))\n    ]\n\n    # Remove failed runs\n    for i in range(len(factscore_scores) - 1, -1, -1):\n      if factscore_scores[i] == -1 or pred_scores[i] == -1:\n        factscore_scores.pop(i)\n        pred_scores.pop(i)\n\n    # Cannot calculate correlation with only one data point\n    if len(factscore_scores) <= 1 or len(pred_scores) <= 1:\n      pearson_result, spearman_result = None, None\n    else:\n      pearson_result = stats.pearsonr(factscore_scores, pred_scores)\n      spearman_result = stats.spearmanr(factscore_scores, pred_scores)\n\n    scatter_plot(\n        factscore_scores,\n        pred_scores,\n        f'{name}-{metric}',\n        FACTSCORE,\n        EVAL_METHOD,\n    )\n    result[metric] = {\n        _PEARSON: CorrelationResult(pearson_result).result,\n        _SPEARMAN: CorrelationResult(spearman_result).result,\n    }\n\n  return result\n```\n###test function signature:\n```python\n\n  def test_compute_correlation_one_data_point(\n      self,\n      mock_spearmanr: mock.Mock,\n      mock_pearsonr: mock.Mock,\n      mock_scatter_plot: mock.Mock,\n  ) -> None:\n```\n### Answer: (use the provided format with backticks)\n", "name": "test_compute_correlation_one_data_point", "reference": "\n  def test_compute_correlation_one_data_point(\n      self,\n      mock_spearmanr: mock.Mock,\n      mock_pearsonr: mock.Mock,\n      mock_scatter_plot: mock.Mock,\n  ) -> None:\n    response_level_scores = {\n        correlation_vs_factscore.FACTSCORE: [\n            {correlation_vs_factscore.RAW_STR: {'S': 0, 'N': 1}},\n        ],\n        correlation_vs_factscore.EVAL_METHOD: [\n            {correlation_vs_factscore.RAW_STR: {'S': 0, 'M': 1}},\n        ],\n    }\n    actual_output = correlation_vs_factscore.compute_correlation(\n        response_level_scores=response_level_scores\n    )\n    mock_pearsonr.assert_not_called()\n    mock_spearmanr.assert_not_called()\n    mock_scatter_plot.assert_called()\n    self.assertIsInstance(actual_output, dict)\n    self.assertLen(actual_output, 1)\n    self.assertIn('S', actual_output)\n    self.assertIsInstance(actual_output['S'], dict)\n    self.assertLen(actual_output['S'], 2)\n    self.assertIn('Pearson', actual_output['S'])\n    self.assertIsInstance(actual_output['S']['Pearson'], dict)\n    self.assertIn('correlation', actual_output['S']['Pearson'])\n    self.assertTrue(math.isnan(actual_output['S']['Pearson']['correlation']))\n    self.assertIn('p_value', actual_output['S']['Pearson'])\n    self.assertTrue(math.isnan(actual_output['S']['Pearson']['p_value']))\n    self.assertIn('Spearman', actual_output['S'])\n    self.assertIsInstance(actual_output['S']['Spearman'], dict)\n    self.assertIn('correlation', actual_output['S']['Spearman'])\n    self.assertTrue(math.isnan(actual_output['S']['Spearman']['correlation']))\n    self.assertIn('p_value', actual_output['S']['Spearman'])\n    self.assertTrue(math.isnan(actual_output['S']['Spearman']['p_value']))", "completions": ["  ```\ndef test_compute_correlation_one_data_point(self, mock_spearmanr, mock_pearsonr, mock_scatter_plot):\n    response_level_scores = {\n        FACTSCORE: [\n            {\n                \"metric1\": {\"value\": 1},\n                \"metric2\": {\"value\": 2},\n            },\n            {\n                \"metric1\": {\"value\": 3},\n                \"metric2\": {\"value\": 4},\n            },\n        ],\n        EVAL_METHOD: [\n            {\n                \"metric1\": {\"value\": 1},\n                \"metric2\": {\"value\": 2},\n            },\n            {\n                \"metric1\": {\"value\": 3},\n                \"metric2\": {\"value\": 4},\n            },\n        ],\n    }\n    name = \"test\"\n    mock_spearmanr.return_value = CorrelationResult(None)\n    mock_pearsonr.return_value = CorrelationResult(None)\n    mock_scatter_plot.return_value = None\n    result = compute_correlation(response_level_scores, name)\n    self.assertEqual(result, {})\n```"]}
{"id": 12, "prompt": "\nYou are an expert Python programmer. You will be given some information and will generate a correct test case for the given function.\nYour response will only be python test function and without anything else.\n\n### focal function:\n```python\ndef compute_temperature_features(\n    meter_data_index,\n    temperature_data,\n    heating_balance_points=None,\n    cooling_balance_points=None,\n    data_quality=False,\n    temperature_mean=True,\n    degree_day_method=\"daily\",\n    percent_hourly_coverage_per_day=0.5,\n    percent_hourly_coverage_per_billing_period=0.9,\n    use_mean_daily_values=True,\n    tolerance=None,\n    keep_partial_nan_rows=False,\n):\n    if temperature_data.index.freq != \"H\":\n        raise ValueError(\n            \"temperature_data.index must have hourly frequency (freq='H').\"\n            \" Found: {}\".format(temperature_data.index.freq)\n        )\n\n    if not temperature_data.index.tz:\n        raise ValueError(\n            \"temperature_data.index must be timezone-aware. You can set it with\"\n            \" temperature_data.tz_localize(...).\"\n        )\n\n    if meter_data_index.freq is None and meter_data_index.inferred_freq == \"H\":\n        raise ValueError(\n            \"If you have hourly data explicitly set the frequency\"\n            \" of the dataframe by setting\"\n            \"``meter_data_index.freq =\"\n            \" pd.tseries.frequencies.to_offset('H').\"\n        )\n\n    if not meter_data_index.tz:\n        raise ValueError(\n            \"meter_data_index must be timezone-aware. You can set it with\"\n            \" meter_data.tz_localize(...).\"\n        )\n\n    if meter_data_index.duplicated().any():\n        raise ValueError(\"Duplicates found in input meter trace index.\")\n\n    temp_agg_funcs = []\n    temp_agg_column_renames = {}\n\n    if heating_balance_points is None:\n        heating_balance_points = []\n    if cooling_balance_points is None:\n        cooling_balance_points = []\n\n    if meter_data_index.freq is not None:\n        try:\n            freq_timedelta = pd.Timedelta(meter_data_index.freq)\n        except ValueError:  # freq cannot be converted to timedelta\n            freq_timedelta = None\n    else:\n        freq_timedelta = None\n\n    if tolerance is None:\n        tolerance = freq_timedelta\n\n    if not (heating_balance_points == [] and cooling_balance_points == []):\n        if degree_day_method == \"hourly\":\n            pass\n        elif degree_day_method == \"daily\":\n            if meter_data_index.freq == \"H\":\n                raise ValueError(\n                    \"degree_day_method='daily' must be used with\"\n                    \" daily meter data. Found: 'hourly'\".format(degree_day_method)\n                )\n        else:\n            raise ValueError(\"method not supported: {}\".format(degree_day_method))\n\n    if freq_timedelta == pd.Timedelta(\"1H\"):\n        # special fast route for hourly data.\n        df = temperature_data.to_frame(\"temperature_mean\").reindex(meter_data_index)\n\n        if use_mean_daily_values:\n            n_days = 1\n        else:\n            n_days = 1.0 / 24.0\n\n        df = df.assign(\n            **{\n                \"cdd_{}\".format(bp): np.maximum(df.temperature_mean - bp, 0) * n_days\n                for bp in cooling_balance_points\n            }\n        )\n        df = df.assign(\n            **{\n                \"hdd_{}\".format(bp): np.maximum(bp - df.temperature_mean, 0) * n_days\n                for bp in heating_balance_points\n            }\n        )\n        df = df.assign(\n            n_hours_dropped=df.temperature_mean.isnull().astype(int),\n            n_hours_kept=df.temperature_mean.notnull().astype(int),\n        )\n        # TODO(philngo): bad interface or maybe this is just wrong for some reason?\n        if data_quality:\n            df = df.assign(\n                temperature_null=df.n_hours_dropped,\n                temperature_not_null=df.n_hours_kept,\n            )\n        if not temperature_mean:\n            del df[\"temperature_mean\"]\n    else:\n        # daily/billing route\n        # heating/cooling degree day aggregations. Needed for n_days fields as well.\n        temp_agg_funcs.extend(\n            _degree_day_columns(\n                heating_balance_points=heating_balance_points,\n                cooling_balance_points=cooling_balance_points,\n                degree_day_method=degree_day_method,\n                percent_hourly_coverage_per_day=percent_hourly_coverage_per_day,\n                percent_hourly_coverage_per_billing_period=percent_hourly_coverage_per_billing_period,\n                use_mean_daily_values=use_mean_daily_values,\n            )\n        )\n        temp_agg_column_renames.update(\n            {(\"temp\", \"degree_day_columns\"): \"degree_day_columns\"}\n        )\n\n        if data_quality:\n            temp_agg_funcs.extend(\n                [(\"not_null\", \"count\"), (\"null\", lambda x: x.isnull().sum())]\n            )\n            temp_agg_column_renames.update(\n                {\n                    (\"temp\", \"not_null\"): \"temperature_not_null\",\n                    (\"temp\", \"null\"): \"temperature_null\",\n                }\n            )\n\n        if temperature_mean:\n            temp_agg_funcs.extend([(\"mean\", \"mean\")])\n            temp_agg_column_renames.update({(\"temp\", \"mean\"): \"temperature_mean\"})\n\n        # aggregate temperatures\n        temp_df = temperature_data.to_frame(\"temp\")\n        temp_groups = _matching_groups(meter_data_index, temp_df, tolerance)\n        temp_aggregations = temp_groups.agg({\"temp\": temp_agg_funcs})\n\n        # expand temp aggregations by faking and deleting the `meter_value` column.\n        # I haven't yet figured out a way to avoid this and get the desired\n        # structure and behavior. (philngo)\n        meter_value = pd.DataFrame({\"meter_value\": 0}, index=meter_data_index)\n        df = pd.concat([meter_value, temp_aggregations], axis=1).rename(\n            columns=temp_agg_column_renames\n        )\n        del df[\"meter_value\"]\n\n        if \"degree_day_columns\" in df:\n            if df[\"degree_day_columns\"].dropna().empty:\n                column_defaults = {\n                    column: np.full(df[\"degree_day_columns\"].shape, np.nan)\n                    for column in [\"n_days_dropped\", \"n_days_kept\"]\n                }\n                df = df.drop([\"degree_day_columns\"], axis=1).assign(**column_defaults)\n            else:\n                df = pd.concat(\n                    [\n                        df.drop([\"degree_day_columns\"], axis=1),\n                        df[\"degree_day_columns\"].dropna().apply(pd.Series),\n                    ],\n                    axis=1,\n                )\n\n    if not keep_partial_nan_rows:\n        df = overwrite_partial_rows_with_nan(df)\n\n    # nan last row\n    df = df.iloc[:-1].reindex(df.index)\n    return df\n```\n###test function signature:\n```python\n\ndef test_compute_temperature_features_daily_temp_mean(il_electricity_cdd_hdd_daily):\n```\n### Answer: (use the provided format with backticks)\n", "name": "test_compute_temperature_features_daily_temp_mean", "reference": "\ndef test_compute_temperature_features_daily_temp_mean(il_electricity_cdd_hdd_daily):\n    meter_data = il_electricity_cdd_hdd_daily[\"meter_data\"]\n    temperature_data = il_electricity_cdd_hdd_daily[\"temperature_data\"]\n    df = compute_temperature_features(meter_data.index, temperature_data)\n    assert df.shape == (810, 3)\n    assert list(sorted(df.columns)) == [\n        \"n_days_dropped\",\n        \"n_days_kept\",\n        \"temperature_mean\",\n    ]\n\n    assert round(df.temperature_mean.mean()) == 55.0", "completions": ["  ```\ndef test_compute_temperature_features_daily_temp_mean(il_electricity_cdd_hdd_daily):\n    meter_data_index = il_electricity_cdd_hdd_daily.meter_data_index\n    temperature_data = il_electricity_cdd_hdd_daily.temperature_data\n    heating_balance_points = il_electricity_cdd_hdd_daily.heating_balance_points\n    cooling_balance_points = il_electricity_cdd_hdd_daily.cooling_balance_points\n    data_quality = il_electricity_cdd_hdd_daily.data_quality\n    temperature_mean = il_electricity_cdd_hdd_daily.temperature_mean\n    degree_day_method = il_electricity_cdd_hdd_daily.degree_day_method\n    percent_hourly_coverage_per_day = il_electricity_cdd_hdd_daily.percent_hourly_coverage_per_day\n    percent_hourly_coverage_per_billing_period = il_electricity_cdd_hdd_daily.percent_hourly_coverage_per_billing_period\n    use_mean_daily_values = il_electricity_cdd_hdd_daily.use_mean_daily_values\n    tolerance = il_electricity_cdd_hdd_daily.tolerance\n    keep_partial_nan_rows = il_electricity_cdd_hdd_daily.keep_partial_nan_rows\n\n    expected_output = il_electricity_cdd_hdd_daily.expected_output\n\n    output = compute_temperature_features(\n        meter_data_index,\n        temperature_data,\n        heating_balance_points,\n        cooling_balance_points,\n        data_quality,\n        temperature_mean,\n        degree_day_method,\n        percent_hourly_coverage_per_day,\n        percent_hourly_coverage_per_billing_period,\n        use_mean_daily_values,\n        tolerance,\n        keep_partial_nan_rows,\n    )\n\n    pd.testing.assert_frame_equal(output, expected_output)\n```"]}
{"id": 13, "prompt": "\nYou are an expert Python programmer. You will be given some information and will generate a correct test case for the given function.\nYour response will only be python test function and without anything else.\n\n### focal function:\n```python\ndef power_color(\n    frequency,\n    power,\n    power_err=None,\n    freq_edges=[1 / 256, 1 / 32, 0.25, 2.0, 16.0],\n    df=None,\n    m=1,\n    freqs_to_exclude=None,\n    poisson_power=0,\n    return_log=False,\n):\n    freq_edges = np.asarray(freq_edges)\n    if len(freq_edges) != 5:\n        raise ValueError(\"freq_edges must have 5 elements\")\n\n    frequency = np.asarray(frequency)\n    power = np.asarray(power)\n\n    if df is None:\n        df = np.median(np.diff(frequency))\n    input_frequency_low_edges = frequency - df / 2\n    input_frequency_high_edges = frequency + df / 2\n\n    if freq_edges.min() < input_frequency_low_edges[0]:\n        raise ValueError(\"The minimum frequency is larger than the first frequency edge\")\n    if freq_edges.max() > input_frequency_high_edges[-1]:\n        raise ValueError(\"The maximum frequency is lower than the last frequency edge\")\n\n    if power_err is None:\n        power_err = power / np.sqrt(m)\n    else:\n        power_err = np.asarray(power_err)\n\n    if freqs_to_exclude is not None:\n        if len(np.shape(freqs_to_exclude)) == 1:\n            freqs_to_exclude = [freqs_to_exclude]\n\n        if (\n            not isinstance(freqs_to_exclude, Iterable)\n            or len(np.shape(freqs_to_exclude)) != 2\n            or np.shape(freqs_to_exclude)[1] != 2\n        ):\n            raise ValueError(\"freqs_to_exclude must be of format [[f0, f1], [f2, f3], ...]\")\n        for f0, f1 in freqs_to_exclude:\n            frequency_mask = (input_frequency_low_edges > f0) & (input_frequency_high_edges < f1)\n            idx0, idx1 = np.searchsorted(frequency, [f0, f1])\n            power[frequency_mask] = np.mean([power[idx0], power[idx1]])\n\n    var00, var00_err = integrate_power_in_frequency_range(\n        frequency,\n        power,\n        freq_edges[:2],\n        power_err=power_err,\n        df=df,\n        m=m,\n        poisson_power=poisson_power,\n    )\n    var01, var01_err = integrate_power_in_frequency_range(\n        frequency,\n        power,\n        freq_edges[2:4],\n        power_err=power_err,\n        df=df,\n        m=m,\n        poisson_power=poisson_power,\n    )\n    var10, var10_err = integrate_power_in_frequency_range(\n        frequency,\n        power,\n        freq_edges[1:3],\n        power_err=power_err,\n        df=df,\n        m=m,\n        poisson_power=poisson_power,\n    )\n    var11, var11_err = integrate_power_in_frequency_range(\n        frequency,\n        power,\n        freq_edges[3:5],\n        power_err=power_err,\n        df=df,\n        m=m,\n        poisson_power=poisson_power,\n    )\n    pc0 = var00 / var01\n    pc1 = var10 / var11\n    pc0_err = pc0 * (var00_err / var00 + var01_err / var01)\n    pc1_err = pc1 * (var10_err / var10 + var11_err / var11)\n    if return_log:\n        pc0_err = 1 / pc0 * pc0_err\n        pc1_err = 1 / pc1 * pc1_err\n        pc0 = np.log10(pc0)\n        pc1 = np.log10(pc1)\n    return pc0, pc0_err, pc1, pc1_err\n```\n###test function signature:\n```python\n\n    def test_with_power_err(self):\n```\n### Answer: (use the provided format with backticks)\n", "name": "test_with_power_err", "reference": "\n    def test_with_power_err(self):\n        pc0, pc0_err, pc1, pc1_err = power_color(\n            self.freq,\n            self.power,\n            power_err=self.power / 2,\n        )\n        pc0e, pc0e_err, pc1e, pc1e_err = power_color(\n            self.freq,\n            self.power,\n            power_err=self.power,\n        )\n        assert np.isclose(pc0, 1, atol=0.001)\n        assert np.isclose(pc1, 1, atol=0.001)\n        assert np.isclose(pc0e, 1, atol=0.001)\n        assert np.isclose(pc1e, 1, atol=0.001)\n        assert np.isclose(pc0e_err / pc0_err, 2, atol=0.001)\n        assert np.isclose(pc1e_err / pc1_err, 2, atol=0.001)", "completions": ["  ```\ndef test_with_power_err():\n    frequency = np.array([1, 2, 3, 4, 5])\n    power = np.array([1, 4, 9, 16, 25])\n    power_err = np.array([0.1, 0.2, 0.3, 0.4, 0.5])\n    freq_edges = np.array([1 / 256, 1 / 32, 0.25, 2.0, 16.0])\n    df = 1\n    m = 1\n    freqs_to_exclude = None\n    poisson_power = 0\n    return_log = False\n\n    pc0, pc0_err, pc1, pc1_err = power_color(\n        frequency,\n        power,\n        power_err=power_err,\n        freq_edges=freq_edges,\n        df=df,\n        m=m,\n        freqs_to_exclude=freqs_to_exclude,\n        poisson_power=poisson_power,\n        return_log=return_log,\n    )\n\n    expected_pc0 = 1.0\n    expected_pc0_err = 0.1\n    expected_pc1 = 1.0\n    expected_pc1_err = 0.1\n\n    np.testing.assert_allclose(pc0, expected_pc0, rtol=1e-05, atol=1e-08)\n    np.testing.assert_allclose(pc0_err, expected_pc0_err, rtol=1e-05, atol=1e-08)\n    np.testing.assert_allclose(pc1, expected_pc1, rtol=1e-05, atol=1e-08)\n    np.testing.assert_allclose(pc1_err, expected_pc1_err, rtol=1e-05, atol=1e-08)\n```"]}
{"id": 14, "prompt": "\nYou are an expert Python programmer. You will be given some information and will generate a correct test case for the given function.\nYour response will only be python test function and without anything else.\n\n### focal function:\n```python\n    def apply_top_k(\n        self, logits: numpy.ndarray, filter_value=_MIN_FLOAT\n    ) -> numpy.ndarray:\n        logits_shape = logits.shape\n        logits = logits.reshape(logits.shape[-1])\n        top_k_indices = numpy.argpartition(logits, -self.top_k)[-self.top_k :]\n        logits[~numpy.isin(numpy.arange(len(logits)), top_k_indices)] = filter_value\n\n        return logits.reshape(logits_shape)\n```\n###test function signature:\n```python\n\n    def test_apply_topk(\n        self,\n    ):\n        # logits for opt usually have shape (1,1,51200)\n```\n### Answer: (use the provided format with backticks)\n", "name": "test_apply_topk", "reference": "\n    def test_apply_topk(\n        self,\n    ):\n        # logits for opt usually have shape (1,1,51200)\n        logits = numpy.linspace(0, 1, 11).reshape((1, 1, 11))\n\n        token_generator = TokenGenerator(\n            logits_shape=logits[-1].shape[-1],\n            top_k=3,\n        )\n\n        filter_value = -float(\"Inf\")\n        new_logits = token_generator.apply_top_k(\n            logits.copy(), filter_value=filter_value\n        )\n\n        for _ in range(token_generator.top_k):\n            curr_max, idx = numpy.max(new_logits), numpy.argmax(new_logits)\n            assert curr_max > filter_value\n            new_logits = numpy.delete(new_logits, idx)\n\n        assert numpy.all(new_logits == filter_value)", "completions": ["  ```\ndef test_apply_topk(self):\n    logits = numpy.array([[1, 2, 3, 4, 5]])\n    expected_output = numpy.array([[1, 2, 3, 4, 5]])\n    self.assertEqual(apply_top_k(logits), expected_output)\n ```"]}
{"id": 15, "prompt": "\nYou are an expert Python programmer. You will be given some information and will generate a correct test case for the given function.\nYour response will only be python test function and without anything else.\n\n### focal function:\n```python\n    def run(\n        self,\n        sources: List[Union[str, Path, ByteStream]],\n        meta: Optional[Union[Dict[str, Any], List[Dict[str, Any]]]] = None,\n    ):\n        parser = MarkdownIt(renderer_cls=RendererPlain)\n        if self.table_to_single_line:\n            parser.enable(\"table\")\n\n        documents = []\n        meta_list = normalize_metadata(meta=meta, sources_count=len(sources))\n\n        for source, metadata in tqdm(\n            zip(sources, meta_list),\n            total=len(sources),\n            desc=\"Converting markdown files to Documents\",\n            disable=not self.progress_bar,\n        ):\n            try:\n                bytestream = get_bytestream_from_source(source)\n            except Exception as e:\n                logger.warning(\"Could not read {source}. Skipping it. Error: {error}\", source=source, error=e)\n                continue\n            try:\n                file_content = bytestream.data.decode(\"utf-8\")\n                text = parser.render(file_content)\n            except Exception as conversion_e:\n                logger.warning(\n                    \"Failed to extract text from {source}. Skipping it. Error: {error}\",\n                    source=source,\n                    error=conversion_e,\n                )\n                continue\n\n            merged_metadata = {**bytestream.meta, **metadata}\n            document = Document(content=text, meta=merged_metadata)\n            documents.append(document)\n\n        return {\"documents\": documents}\n```\n###test function signature:\n```python\n\n    def test_run_calls_normalize_metadata(self, test_files_path):\n```\n### Answer: (use the provided format with backticks)\n", "name": "test_run_calls_normalize_metadata", "reference": "\n    def test_run_calls_normalize_metadata(self, test_files_path):\n        bytestream = ByteStream(data=b\"test\", meta={\"author\": \"test_author\", \"language\": \"en\"})\n\n        converter = MarkdownToDocument()\n\n        with patch(\"haystack.components.converters.markdown.normalize_metadata\") as normalize_metadata, patch(\n            \"haystack.components.converters.markdown.MarkdownIt\"\n        ):\n            converter.run(sources=[bytestream, test_files_path / \"markdown\" / \"sample.md\"], meta={\"language\": \"it\"})\n\n        # check that the metadata normalizer is called properly\n        normalize_metadata.assert_called_with(meta={\"language\": \"it\"}, sources_count=2)", "completions": ["  ```\ndef test_run_calls_normalize_metadata(self, test_files_path):\n    with patch(\"normalize_metadata\") as mock_normalize_metadata:\n        run(sources=[\"file1.md\", \"file2.md\"], meta={\"key1\": \"value1\", \"key2\": \"value2\"})\n        mock_normalize_metadata.assert_called_once_with(meta={\"key1\": \"value1\", \"key2\": \"value2\"}, sources_count=2)\n```\nThis test function uses the `patch` context manager to mock the `normalize_metadata` function and verify that it is called with the correct arguments. The `run` function is called with two sources, `file1.md` and `file2.md`, and a metadata dictionary with two key-value pairs. The test verifies that the `normalize_metadata` function is called once with the correct arguments."]}
{"id": 16, "prompt": "\nYou are an expert Python programmer. You will be given some information and will generate a correct test case for the given function.\nYour response will only be python test function and without anything else.\n\n### focal function:\n```python\n    def add_config(config: object) -> None:\n        if hasattr(config, \"kind\"):\n            Config.running_config.classes[config.kind] = config  # type: ignore\n            Config.running_config.types[config.kind] = {}\n            for field in fields(config):  # type: ignore\n                if hasattr(field, \"type\"):\n                    Config.running_config.types[config.kind][field.name] = optional_origin(field.type)\n        else:\n            raise RuntimeError(\"Config must have a 'kind' attribute\")\n```\n###test function signature:\n```python\n\ndef test_config():\n```\n### Answer: (use the provided format with backticks)\n", "name": "test_config", "reference": "\ndef test_config():\n    arg_parser = get_arg_parser()\n    core_add_args(arg_parser)\n    arg_parser.parse_known_args()\n    cfg = Config(\"test\")\n    cfg.add_config(ConfigTest)\n    cfg.init_default_config()\n    assert Config.dict() == {\n        \"configtest\": {\n            \"testvar1\": \"testing123\",\n            \"testvar2\": 12345,\n            \"testvar3\": {\n                \"mydict\": {\"foo\": \"bar\", \"abc\": {\"def\": \"ghi\"}},\n                \"myint\": 0,\n                \"mystr\": \"Hello\",\n            },\n        }\n    }\n    cfg2 = Config(\"test2\")\n    assert cfg.configtest.testvar1 == cfg2.configtest.testvar1\n    assert cfg.configtest.testvar1 == Config.configtest.testvar1\n    assert cfg.configtest.testvar2 == cfg2.configtest.testvar2\n    assert cfg.configtest.testvar3 == cfg2.configtest.testvar3\n    Config.configtest.testvar2 += 1\n    assert cfg.configtest.testvar2 == 12346\n    with pytest.raises(ConfigNotFoundError):\n        Config.does_not_exist.foo = \"bar\"\n    with pytest.raises(ConfigNotFoundError):\n        cfg.does_not_exist.foo = \"bar\"", "completions": ["  ```\ndef test_config():\n    config = Config()\n    config.kind = \"test_kind\"\n    config.field1 = \"test_field1\"\n    config.field2 = \"test_field2\"\n    add_config(config)\n    assert Config.running_config.classes[\"test_kind\"] == config\n    assert Config.running_config.types[\"test_kind\"][\"field1\"] == \"test_field1\"\n    assert Config.running_config.types[\"test_kind\"][\"field2\"] == \"test_field2\"\n```"]}
{"id": 18, "prompt": "\nYou are an expert Python programmer. You will be given some information and will generate a correct test case for the given function.\nYour response will only be python test function and without anything else.\n\n### focal function:\n```python\n\n    def begin_typing(self, x=None):\n        self.capturekeystrokes = True\n        # Disable keypress shortcuts, which may otherwise cause the figure to\n        # be saved, closed, etc., until the user stops typing.  The way to\n        # achieve this depends on whether toolmanager is in use.\n        stack = ExitStack()  # Register cleanup actions when user stops typing.\n        self._on_stop_typing = stack.close\n        toolmanager = getattr(\n            self.ax.figure.canvas.manager, \"toolmanager\", None)\n        if toolmanager is not None:\n            # If using toolmanager, lock keypresses, and plan to release the\n            # lock when typing stops.\n            toolmanager.keypresslock(self)\n            stack.callback(toolmanager.keypresslock.release, self)\n        else:\n            # If not using toolmanager, disable all keypress-related rcParams.\n            # Avoid spurious warnings if keymaps are getting deprecated.\n            with _api.suppress_matplotlib_deprecation_warning():\n                stack.enter_context(mpl.rc_context(\n                    {k: [] for k in mpl.rcParams if k.startswith(\"keymap.\")}))\n```\n###test function signature:\n```python\n\ndef test_TextBox(ax, toolbar):\n    # Avoid \"toolmanager is provisional\" warning.\n```\n### Answer: (use the provided format with backticks)\n", "name": "test_TextBox", "reference": "\ndef test_TextBox(ax, toolbar):\n    # Avoid \"toolmanager is provisional\" warning.\n    plt.rcParams._set(\"toolbar\", toolbar)\n\n    submit_event = mock.Mock(spec=noop, return_value=None)\n    text_change_event = mock.Mock(spec=noop, return_value=None)\n    tool = widgets.TextBox(ax, '')\n    tool.on_submit(submit_event)\n    tool.on_text_change(text_change_event)\n\n    assert tool.text == ''\n\n    do_event(tool, '_click')\n\n    tool.set_val('x**2')\n\n    assert tool.text == 'x**2'\n    assert text_change_event.call_count == 1\n\n    tool.begin_typing()\n    tool.stop_typing()\n\n    assert submit_event.call_count == 2\n\n    do_event(tool, '_click')\n    do_event(tool, '_keypress', key='+')\n    do_event(tool, '_keypress', key='5')\n\n    assert text_change_event.call_count == 3", "completions": ["  ```\ndef test_TextBox(ax, toolbar):\n    # Avoid \"toolmanager is provisional\" warning.\n    with _api.suppress_matplotlib_deprecation_warning():\n        stack = ExitStack()\n        self._on_stop_typing = stack.close\n        toolmanager = getattr(\n            self.ax.figure.canvas.manager, \"toolmanager\", None)\n        if toolmanager is not None:\n            # If using toolmanager, lock keypresses, and plan to release the\n            # lock when typing stops.\n            toolmanager.keypresslock(self)\n            stack.callback(toolmanager.keypresslock.release, self)\n        else:\n            # If not using toolmanager, disable all keypress-related rcParams.\n            # Avoid spurious warnings if keymaps are getting deprecated.\n            with _api.suppress_matplotlib_deprecation_warning():\n                stack.enter_context(mpl.rc_context(\n                    {k: [] for k in mpl.rcParams if k.startswith(\"keymap.\")}))\n```"]}
{"id": 19, "prompt": "\nYou are an expert Python programmer. You will be given some information and will generate a correct test case for the given function.\nYour response will only be python test function and without anything else.\n\n### focal function:\n```python\nasync def read_events(\n    db: PrefectDBInterface,\n    session: AsyncSession,\n    events_filter: EventFilter,\n    limit: \"int | None\" = None,\n    offset: \"int | None\" = None,\n) -> Sequence[ORMEvent]:\n    # Always order by occurred timestamp, with placeholder for order direction\n    order = sa.desc if events_filter.order == EventOrder.DESC else sa.asc\n\n    # Check if distinct fields are provided\n    if distinct_fields := build_distinct_queries(events_filter):\n        # Define window function\n        window_function = (\n            sa.func.row_number()\n            .over(partition_by=distinct_fields, order_by=order(db.Event.occurred))\n            .label(\"row_number\")\n        )\n        # Create a subquery with the window function\n        subquery = (\n            sa.select(db.Event, window_function)\n            .where(\n                sa.and_(\n                    *events_filter.build_where_clauses(db)\n                )  # Ensure the same filters are applied here\n            )\n            .subquery()\n        )\n\n        # Alias the subquery for easier column references\n        aliased_table = aliased(db.Event, subquery)\n\n        # Create the final query from the subquery, filtering to get only rows with row_number = 1\n        select_events_query = sa.select(aliased_table).where(subquery.c.row_number == 1)\n\n        # Order by the occurred timestamp\n        select_events_query = select_events_query.order_by(order(subquery.c.occurred))\n\n    else:\n        # If no distinct fields are provided, create a query for all events\n        select_events_query = sa.select(db.Event).where(\n            sa.and_(*events_filter.build_where_clauses(db))\n        )\n        # Order by the occurred timestamp\n        select_events_query = select_events_query.order_by(order(db.Event.occurred))\n\n    if limit is not None:\n        limit = max(0, min(limit, events_filter.logical_limit))\n        select_events_query = select_events_query.limit(limit=limit)\n    if offset is not None:\n        select_events_query = select_events_query.offset(offset=offset)\n\n    logger.debug(\"Running PostgreSQL query: %s\", select_events_query)\n\n    select_events_query_result = await session.execute(select_events_query)\n    return select_events_query_result.scalars().unique().all()\n```\n###test function signature:\n```python\n\n    async def test_read_events(self, session: AsyncSession, event_1, event_2):\n```\n### Answer: (use the provided format with backticks)\n", "name": "test_read_events", "reference": "\n    async def test_read_events(self, session: AsyncSession, event_1, event_2):\n        async with session as session:\n            events = await read_events(\n                session=session,\n                events_filter=EventFilter(\n                    occurred=EventOccurredFilter(\n                        since=pendulum.now(\"UTC\").subtract(days=1)\n                    ),\n                ),\n            )\n            assert len(events) == 1\n            assert events[0].id == event_1.id\n\n        async with session as session:\n            events = await read_events(\n                session=session,\n                events_filter=EventFilter(\n                    occurred=EventOccurredFilter(\n                        since=pendulum.now(\"UTC\").subtract(days=3),\n                        until=pendulum.now(\"UTC\").subtract(days=1),\n                    ),\n                ),\n            )\n            assert len(events) == 1\n            assert events[0].id == event_2.id\n\n        # Read criteria should apply AND logic\n        # Nothing should be returned for this resource\n        async with session as session:\n            events = await read_events(\n                session=session,\n                events_filter=EventFilter(\n                    resource=EventResourceFilter(id=[\"prefect.garbage.foo\"]),\n                    occurred=EventOccurredFilter(\n                        since=pendulum.now(\"UTC\").subtract(days=1)\n                    ),\n                ),\n            )\n            assert len(events) == 0", "completions": ["  ```\nasync def test_read_events(self, session: AsyncSession, event_1, event_2):\n    # Create a test event filter with distinct fields\n    test_events_filter = EventFilter(\n        distinct_fields=[\"event_type\", \"event_source\"],\n        order=EventOrder.DESC,\n        limit=10,\n        offset=0,\n    )\n\n    # Create a test event with the given event_1 and event_2\n    test_event_1 = ORMEvent(\n        occurred=datetime.now(),\n        event_type=\"test_event_type_1\",\n        event_source=\"test_event_source_1\",\n    )\n    test_event_2 = ORMEvent(\n        occurred=datetime.now(),\n        event_type=\"test_event_type_2\",\n        event_source=\"test_event_source_2\",\n    )\n\n    # Add the test events to the session\n    session.add(test_event_1)\n    session.add(test_event_2)\n\n    # Run the read_events function with the test event filter\n    read_events_result = await read_events(\n        db=PrefectDBInterface(),\n        session=session,\n        events_filter=test_events_filter,\n    )\n\n    # Check that the read_events result contains the test events\n    assert test_event_1 in read_events_result\n    assert test_event_2 in read_events_result\n\n    # Check that the read_events result is ordered by the occurred timestamp\n    assert read_events_result[0].occurred > read_events_result[1].occurred\n```"]}
{"id": 20, "prompt": "\nYou are an expert Python programmer. You will be given some information and will generate a correct test case for the given function.\nYour response will only be python test function and without anything else.\n\n### focal function:\n```python\n    def evaluate(self, points):\n        points = np.atleast_2d(points)\n\n        dim, num_m = np.array(points).shape\n        if dim != self.dim:\n            raise ValueError(\"points have dimension {}, dataset has dimension \"\n                             \"{}\".format(dim, self.dim))\n\n        result = np.zeros(num_m)\n\n        if num_m >= self.num_dp:\n            # there are more points than data, so loop over data\n            for i in range(self.num_dp):\n                diff = self.dataset[:, i, np.newaxis] - points\n                tdiff = np.dot(self.inv_cov, diff)\n                energy = np.sum(diff * tdiff, axis=0) / 2.0\n                result = result + np.exp(-energy)\n        else:\n            # loop over points\n            for i in range(num_m):\n                diff = self.dataset - points[:, i, np.newaxis]\n                tdiff = np.dot(self.inv_cov, diff)\n                energy = np.sum(diff * tdiff, axis=0) / 2.0\n                result[i] = np.sum(np.exp(-energy), axis=0)\n\n        result = result / self.norm_factor\n\n        return result\n```\n###test function signature:\n```python\n    def test_evaluate_diff_dim(self):\n```\n### Answer: (use the provided format with backticks)\n", "name": "test_evaluate_diff_dim", "reference": "    def test_evaluate_diff_dim(self):\n        x1 = np.arange(3, 10, 2)\n        kde = mlab.GaussianKDE(x1)\n        x2 = np.arange(3, 12, 2)\n        y_expected = [\n            0.08797252, 0.11774109, 0.11774109, 0.08797252, 0.0370153\n        ]\n        y = kde.evaluate(x2)\n        np.testing.assert_array_almost_equal(y, y_expected, 7)", "completions": ["  ```\ndef test_evaluate_diff_dim(self):\n    points = np.array([[1, 2, 3], [4, 5, 6]])\n    self.assertRaises(ValueError, self.evaluate, points)\n```"]}
{"id": 22, "prompt": "\nYou are an expert Python programmer. You will be given some information and will generate a correct test case for the given function.\nYour response will only be python test function and without anything else.\n\n### focal function:\n```python\n\ndef get_standard_metadata(\n    pyproject_dict: Mapping[str, Any],\n    settings: ScikitBuildSettings,\n) -> StandardMetadata:\n    new_pyproject_dict = dict(pyproject_dict)\n    # Handle any dynamic metadata\n    for field, provider, config in load_dynamic_metadata(settings.metadata):\n        if provider is None:\n            msg = f\"{field} is missing provider\"\n            raise KeyError(msg)\n        if field not in pyproject_dict.get(\"project\", {}).get(\"dynamic\", []):\n            msg = f\"{field} is not in project.dynamic\"\n            raise KeyError(msg)\n        new_pyproject_dict[\"project\"][field] = provider.dynamic_metadata(field, config)\n        new_pyproject_dict[\"project\"][\"dynamic\"].remove(field)\n\n    metadata = StandardMetadata.from_pyproject(new_pyproject_dict)\n\n    # For scikit-build-core < 0.5, we keep the normalized name for back-compat\n    if settings.minimum_version is not None and settings.minimum_version < Version(\n        \"0.5\"\n    ):\n        metadata.name = metadata.canonical_name\n\n    # The description field is required to be one line. Instead of merging it\n    # or cutting off subsequent lines (setuptools), we throw a nice error.\n    # But we didn't validate before 0.9.\n    if (\n        settings.minimum_version is None or settings.minimum_version >= Version(\"0.9\")\n    ) and \"\\n\" in (metadata.description or \"\"):\n        msg = \"Multiple lines in project.description are not supported; this is supposed to be a one line summary\"\n        raise ValueError(msg)\n\n    return metadata\n```\n###test function signature:\n```python\n\ndef test_plugin_metadata():\n```\n### Answer: (use the provided format with backticks)\n", "name": "test_plugin_metadata", "reference": "\ndef test_plugin_metadata():\n    reason_msg = (\n        \"install hatch-fancy-pypi-readme and setuptools-scm to test the \"\n        \"dynamic metadata plugins\"\n    )\n    pytest.importorskip(\"hatch_fancy_pypi_readme\", reason=reason_msg)\n    pytest.importorskip(\"setuptools_scm\", reason=reason_msg)\n    if shutil.which(\"git\") is None:\n        pytest.skip(\"git is not installed\")\n\n    shutil.copy(\"plugin_project.toml\", \"pyproject.toml\")\n\n    subprocess.run([\"git\", \"init\", \"--initial-branch=main\"], check=True)\n    subprocess.run([\"git\", \"config\", \"user.name\", \"bot\"], check=True)\n    subprocess.run([\"git\", \"config\", \"user.email\", \"bot@scikit-build.org\"], check=True)\n    subprocess.run([\"git\", \"add\", \"pyproject.toml\"], check=True)\n    subprocess.run([\"git\", \"commit\", \"-m\", \"initial commit\"], check=True)\n    subprocess.run([\"git\", \"tag\", \"v0.1.0\", \"-m\", \"initial commint\"], check=True)\n\n    with Path(\"pyproject.toml\").open(\"rb\") as ft:\n        pyproject = tomllib.load(ft)\n    settings_reader = SettingsReader(pyproject, {}, state=\"metadata_wheel\")\n    settings = settings_reader.settings\n\n    settings_reader.validate_may_exit()\n\n    metadata = get_standard_metadata(pyproject, settings)\n\n    assert str(metadata.version) == \"0.1.0\"\n    assert metadata.readme == pyproject_metadata.Readme(\n        \"Fragment #1Fragment #2\", None, \"text/x-rst\"\n    )\n\n    assert set(GetRequires().dynamic_metadata()) == {\n        \"hatch-fancy-pypi-readme>=22.3\",\n        \"setuptools-scm\",\n    }", "completions": ["  ```\ndef test_plugin_metadata():\n    pyproject_dict = {\n        \"project\": {\n            \"dynamic\": [\"description\"],\n            \"description\": \"This is a test project\",\n        }\n    }\n    settings = ScikitBuildSettings(\n        metadata={\n            \"description\": {\n                \"provider\": \"test_provider\",\n                \"config\": {\"test_key\": \"test_value\"},\n            }\n        }\n    )\n    metadata = get_standard_metadata(pyproject_dict, settings)\n    assert metadata.description == \"This is a test project\"\n    assert metadata.dynamic_metadata[\"description\"] == \"test_provider\"\n    assert metadata.dynamic_metadata[\"description\"][\"test_key\"] == \"test_value\"\n```"]}
{"id": 23, "prompt": "\nYou are an expert Python programmer. You will be given some information and will generate a correct test case for the given function.\nYour response will only be python test function and without anything else.\n\n### focal function:\n```python\ndef _get_datetime_format(value):\n    if not isinstance(value, pd.Series):\n        value = pd.Series(value)\n\n    value = value[~value.isna()]\n    value = value.astype(str).to_numpy()\n\n    return _guess_datetime_format_for_array(value)\n```\n###test function signature:\n```python\ndef test__get_datetime_format():\n    # Setup\n```\n### Answer: (use the provided format with backticks)\n", "name": "test__get_datetime_format", "reference": "def test__get_datetime_format():\n    # Setup\n    string_value = '2021-02-02'\n    list_value = [np.nan, '2021-02-02']\n    series_value = pd.Series(['2021-02-02T12:10:59'])\n\n    # Run\n    string_out = _get_datetime_format(string_value)\n    list_out = _get_datetime_format(list_value)\n    series_out = _get_datetime_format(series_value)\n\n    # Assert\n    expected_output = '%Y-%m-%d'\n    assert string_out == expected_output\n    assert list_out == expected_output\n    assert series_out == '%Y-%m-%dT%H:%M:%S'", "completions": ["  ```\ndef test__get_datetime_format():\n    # Setup\n    value = pd.Series([1, 2, 3, 4, 5])\n\n    # Expected result\n    expected_result = \"datetime64[ns]\"\n\n    # Actual result\n    actual_result = _get_datetime_format(value)\n\n    # Assertion\n    assert actual_result == expected_result\n```"]}
{"id": 24, "prompt": "\nYou are an expert Python programmer. You will be given some information and will generate a correct test case for the given function.\nYour response will only be python test function and without anything else.\n\n### focal function:\n```python\n    def load(self) -> None:\n        super().load()\n\n        _path = str(\n            importlib_resources.files(\"distilabel\")\n            / \"steps\"\n            / \"tasks\"\n            / \"templates\"\n            / \"ultrafeedback\"\n            / f\"{self.aspect}.jinja2\"\n        )\n\n        self._template = Template(open(_path).read())\n```\n###test function signature:\n```python\n\n    def test_process_with_complex_aspect(self) -> None:\n```\n### Answer: (use the provided format with backticks)\n", "name": "test_process_with_complex_aspect", "reference": "\n    def test_process_with_complex_aspect(self) -> None:\n        pipeline = Pipeline(name=\"unit-test-pipeline\")\n        llm = UltraFeedbackLLM()\n\n        task = UltraFeedback(\n            name=\"ultrafeedback\",\n            aspect=\"truthfulness\",\n            llm=llm,\n            pipeline=pipeline,\n        )\n        task.load()\n\n        assert next(\n            task.process([{\"instruction\": \"test\", \"generations\": [\"A\", \"B\"]}])\n        ) == [\n            {\n                \"instruction\": \"test\",\n                \"generations\": [\"A\", \"B\"],\n                \"types\": [1, 2],\n                \"rationales\": [\"text\", \"text\"],\n                \"ratings\": [1, 2],\n                \"rationales-for-ratings\": [\"text\", \"text\"],\n                \"model_name\": \"ultrafeedback-model\",\n            }\n        ]", "completions": ["  ```\ndef test_process_with_complex_aspect(self):\n    self.aspect = \"complex\"\n    self.load()\n    self.assertEqual(self._template.render(), \"complex\")\n```"]}
{"id": 25, "prompt": "\nYou are an expert Python programmer. You will be given some information and will generate a correct test case for the given function.\nYour response will only be python test function and without anything else.\n\n### focal function:\n```python\ndef _setup_cuda_device(device: torch.device) -> torch.device:\n    local_rank = _get_local_rank() or 0\n    if device.index is None:\n        device = torch.device(type=\"cuda\", index=local_rank)\n\n    # Ensure index is available before setting device\n    if device.index >= torch.cuda.device_count():\n        raise RuntimeError(\n            \"The local rank is larger than the number of available GPUs.\"\n        )\n\n    torch.cuda.set_device(device)\n    return device\n```\n###test function signature:\n```python\n\n    def test_get_gpu_device(self) -> None:\n```\n### Answer: (use the provided format with backticks)\n", "name": "test_get_gpu_device", "reference": "\n    def test_get_gpu_device(self) -> None:\n        device_idx = torch.cuda.device_count() - 1\n        assert device_idx >= 0\n        with mock.patch.dict(os.environ, {\"LOCAL_RANK\": str(device_idx)}, clear=True):\n            device = get_device()\n            assert device.type == \"cuda\"\n            assert device.index == device_idx\n            assert device.index == torch.cuda.current_device()\n\n            # Test that we raise an error if the device index is specified on distributed runs\n            if device_idx > 0:\n                with pytest.raises(\n                    RuntimeError,\n                    match=f\"Device specified is cuda:0 but was assigned cuda:{device_idx}\",\n                ):\n                    device = get_device(\"cuda:0\")\n\n        invalid_device_idx = device_idx + 10\n        with mock.patch.dict(os.environ, {\"LOCAL_RANK\": str(invalid_device_idx)}):\n            with pytest.raises(\n                RuntimeError,\n                match=\"The local rank is larger than the number of available GPUs\",\n            ):\n                device = get_device(\"cuda\")\n\n        # Test that we fall back to 0 if LOCAL_RANK is not specified\n        device = torch.device(_get_device_type_from_env())\n        device = _setup_cuda_device(device)\n        assert device.type == \"cuda\"\n        assert device.index == 0\n        assert device.index == torch.cuda.current_device()", "completions": ["  ```\ndef test_get_gpu_device(self):\n    device = torch.device(type=\"cuda\", index=0)\n    self.assertEqual(device.index, 0)\n    self.assertEqual(device.type, \"cuda\")\n```"]}
{"id": 26, "prompt": "\nYou are an expert Python programmer. You will be given some information and will generate a correct test case for the given function.\nYour response will only be python test function and without anything else.\n\n### focal function:\n```python\n    def download(cls, client: GiskardClient, project_key, model_id, *_args, **_kwargs):\n        local_dir = settings.home_dir / settings.cache_dir / \"models\" / model_id\n        client.load_artifact(local_dir, posixpath.join(\"models\", model_id))\n        meta_response: ModelMetaInfo = client.load_model_meta(project_key, model_id)\n        # internal worker case, no token based http client\n        if not local_dir.exists():\n            raise RuntimeError(f\"Cannot find existing model {project_key}.{model_id} in {local_dir}\")\n        with (Path(local_dir) / META_FILENAME).open(encoding=\"utf-8\") as f:\n            file_meta = yaml.load(f, Loader=yaml.Loader)\n            classification_labels = cls.cast_labels(meta_response)\n            meta = ModelMeta(\n                name=meta_response.name,\n                description=meta_response.description,\n                model_type=SupportedModelTypes[meta_response.modelType],\n                feature_names=meta_response.featureNames,\n                classification_labels=classification_labels,\n                classification_threshold=meta_response.threshold,\n                loader_module=file_meta[\"loader_module\"],\n                loader_class=file_meta[\"loader_class\"],\n            )\n\n        model_py_ver = (\n            tuple(meta_response.languageVersion.split(\".\")) if \"PYTHON\" == meta_response.language.upper() else None\n        )\n\n        clazz = cls.determine_model_class(meta, local_dir, model_py_ver=model_py_ver)\n\n        constructor_params = meta.__dict__\n        constructor_params[\"id\"] = str(model_id)\n\n        del constructor_params[\"loader_module\"]\n        del constructor_params[\"loader_class\"]\n\n        model = clazz.load(local_dir, model_py_ver=model_py_ver, **constructor_params)\n        return model\n```\n###test function signature:\n```python\n\ndef test_model_download(request):\n```\n### Answer: (use the provided format with backticks)\n", "name": "test_model_download", "reference": "\ndef test_model_download(request):\n    model: BaseModel = request.getfixturevalue(\"german_credit_model\")\n    project_key = str(uuid.uuid4())\n\n    with utils.MockedProjectCacheDir():\n        with utils.MockedClient(mock_all=False) as (client, mr):\n            # The model needs to request files\n            requested_urls = []\n            requested_urls.extend(utils.register_uri_for_model_meta_info(mr, model, project_key))\n            requested_urls.extend(\n                utils.register_uri_for_model_artifact_info(mr, model, project_key, register_file_contents=True)\n            )\n\n            downloaded_model = BaseModel.download(client=client, project_key=project_key, model_id=str(model.id))\n\n            for requested_url in requested_urls:\n                assert utils.is_url_requested(mr.request_history, requested_url)\n\n            assert downloaded_model.id == model.id\n            assert downloaded_model.meta == model.meta", "completions": ["  ```\ndef test_model_download(request):\n    client = GiskardClient()\n    project_key = \"test_project\"\n    model_id = \"test_model\"\n    local_dir = settings.home_dir / settings.cache_dir / \"models\" / model_id\n    client.load_artifact(local_dir, posixpath.join(\"models\", model_id))\n    meta_response = client.load_model_meta(project_key, model_id)\n    classification_labels = ModelDownloader.cast_labels(meta_response)\n    meta = ModelMeta(\n        name=meta_response.name,\n        description=meta_response.description,\n        model_type=SupportedModelTypes[meta_response.modelType],\n        feature_names=meta_response.featureNames,\n        classification_labels=classification_labels,\n        classification_threshold=meta_response.threshold,\n        loader_module=file_meta[\"loader_module\"],\n        loader_class=file_meta[\"loader_class\"],\n    )\n    model_py_ver = (\n        tuple(meta_response.languageVersion.split(\".\")) if \"PYTHON\" == meta_response.language.upper() else None\n    )\n    clazz = ModelDownloader.determine_model_class(meta, local_dir, model_py_ver=model_py_ver)\n    constructor_params = meta.__dict__\n    constructor_params[\"id\"] = str(model_id)\n    del constructor_params[\"loader_module\"]\n    del constructor_params[\"loader_class\"]\n    model = clazz.load(local_dir, model_py_ver=model_py_ver, **constructor_params)\n    return model\n```"]}
{"id": 27, "prompt": "\nYou are an expert Python programmer. You will be given some information and will generate a correct test case for the given function.\nYour response will only be python test function and without anything else.\n\n### focal function:\n```python\ndef columns_equal(\n    col_1: \"pl.Series\",\n    col_2: \"pl.Series\",\n    rel_tol: float = 0,\n    abs_tol: float = 0,\n    ignore_spaces: bool = False,\n    ignore_case: bool = False,\n) -> \"pl.Series\":\n    compare: pl.Series\n    try:\n        compare = pl.Series(\n            np.isclose(col_1, col_2, rtol=rel_tol, atol=abs_tol, equal_nan=True)\n        )\n    except TypeError:\n        try:\n            if col_1.dtype in DATE_TYPE or col_2 in DATE_TYPE:\n                raise TypeError(\"Found date, moving to alternative logic\")\n\n            compare = pl.Series(\n                np.isclose(\n                    col_1.cast(pl.Float64, strict=True),\n                    col_2.cast(pl.Float64, strict=True),\n                    rtol=rel_tol,\n                    atol=abs_tol,\n                    equal_nan=True,\n                )\n            )\n        except (ValueError, TypeError, InvalidOperationError, ComputeError):\n            try:\n                if ignore_spaces:\n                    if str(col_1.dtype) in STRING_TYPE:\n                        col_1 = col_1.str.strip_chars()\n                    if str(col_2.dtype) in STRING_TYPE:\n                        col_2 = col_2.str.strip_chars()\n\n                if ignore_case:\n                    if str(col_1.dtype) in STRING_TYPE:\n                        col_1 = col_1.str.to_uppercase()\n                    if str(col_2.dtype) in STRING_TYPE:\n                        col_2 = col_2.str.to_uppercase()\n\n                if (\n                    str(col_1.dtype) in STRING_TYPE and str(col_2.dtype) in DATE_TYPE\n                ) or (\n                    str(col_1.dtype) in DATE_TYPE and str(col_2.dtype) in STRING_TYPE\n                ):\n                    compare = compare_string_and_date_columns(col_1, col_2)\n                else:\n                    compare = pl.Series(\n                        (col_1.eq_missing(col_2)) | (col_1.is_null() & col_2.is_null())\n                    )\n            except:\n                # Blanket exception should just return all False\n                compare = pl.Series(False * col_1.shape[0])\n    return compare\n```\n###test function signature:\n```python\ndef test_rounded_date_columns():\n```\n### Answer: (use the provided format with backticks)\n", "name": "test_rounded_date_columns", "reference": "def test_rounded_date_columns():\n    df = pl.DataFrame(\n        [\n            {\"a\": \"2017-01-01\", \"b\": \"2017-01-01 00:00:00.000000\", \"exp\": True},\n            {\"a\": \"2017-01-01\", \"b\": \"2017-01-01 00:00:00.123456\", \"exp\": False},\n            {\"a\": \"2017-01-01\", \"b\": \"2017-01-01 00:00:01.000000\", \"exp\": False},\n            {\"a\": \"2017-01-01\", \"b\": \"2017-01-01 00:00:00\", \"exp\": True},\n        ]\n    )\n\n    df = df.with_columns(df[\"a\"].str.to_date().alias(\"a_dt\"))\n    actual = columns_equal(df[\"a_dt\"], df[\"b\"])\n    expected = df[\"exp\"]\n    assert_series_equal(actual, expected, check_names=False)", "completions": ["  ```\ndef test_rounded_date_columns():\n    col_1 = pl.Series([datetime(2022, 1, 1), datetime(2022, 1, 2), datetime(2022, 1, 3)])\n    col_2 = pl.Series([datetime(2022, 1, 1), datetime(2022, 1, 2), datetime(2022, 1, 3)])\n    rel_tol = 0\n    abs_tol = 0\n    ignore_spaces = False\n    ignore_case = False\n    expected = pl.Series([True, True, True])\n    result = columns_equal(col_1, col_2, rel_tol, abs_tol, ignore_spaces, ignore_case)\n    assert result.equals(expected)\n```"]}
{"id": 28, "prompt": "\nYou are an expert Python programmer. You will be given some information and will generate a correct test case for the given function.\nYour response will only be python test function and without anything else.\n\n### focal function:\n```python\ndef annotations_to_instances(annos, image_size, mask_format=\"polygon\"):\n    boxes = [BoxMode.convert(obj[\"bbox\"], obj[\"bbox_mode\"], BoxMode.XYXY_ABS) for obj in annos]\n    target = Instances(image_size)\n    boxes = target.gt_boxes = Boxes(boxes)\n    boxes.clip(image_size)\n\n    classes = [obj[\"category_id\"] for obj in annos]\n    classes = torch.tensor(classes, dtype=torch.int64)\n    target.gt_classes = classes\n\n    if len(annos) and \"segmentation\" in annos[0]:\n        segms = [obj[\"segmentation\"] for obj in annos]\n        if mask_format == \"polygon\":\n            masks = PolygonMasks(segms)\n        else:\n            assert mask_format == \"bitmask\", mask_format\n            masks = []\n            for segm in segms:\n                if isinstance(segm, list):\n                    # polygon\n                    masks.append(polygons_to_bitmask(segm, *image_size))\n                elif isinstance(segm, dict):\n                    # COCO RLE\n                    masks.append(mask_util.decode(segm))\n                elif isinstance(segm, np.ndarray):\n                    assert segm.ndim == 2, \"Expect segmentation of 2 dimensions, got {}.\".format(\n                        segm.ndim\n                    )\n                    # mask array\n                    masks.append(segm)\n                else:\n                    raise ValueError(\n                        \"Cannot convert segmentation of type '{}' to BitMasks!\"\n                        \"Supported types are: polygons as list[list[float] or ndarray],\"\n                        \" COCO-style RLE as a dict, or a full-image segmentation mask \"\n                        \"as a 2D ndarray.\".format(type(segm))\n                    )\n            # torch.from_numpy does not support array with negative stride.\n            masks = BitMasks(\n                torch.stack([torch.from_numpy(np.ascontiguousarray(x)) for x in masks])\n            )\n        target.gt_masks = masks\n\n    if len(annos) and \"keypoints\" in annos[0]:\n        kpts = [obj.get(\"keypoints\", []) for obj in annos]\n        target.gt_keypoints = Keypoints(kpts)\n\n    return target\n```\n###test function signature:\n```python\n\n    def test_transform_simple_annotation(self):\n```\n### Answer: (use the provided format with backticks)\n", "name": "test_transform_simple_annotation", "reference": "\n    def test_transform_simple_annotation(self):\n        transforms = T.TransformList([T.HFlipTransform(400)])\n        anno = {\n            \"bbox\": np.asarray([10, 10, 200, 300]),\n            \"bbox_mode\": BoxMode.XYXY_ABS,\n            \"category_id\": 3,\n            \"segmentation\": [[10, 10, 100, 100, 100, 10], [150, 150, 200, 150, 200, 200]],\n        }\n\n        output = detection_utils.transform_instance_annotations(anno, transforms, (400, 400))\n        self.assertTrue(np.allclose(output[\"bbox\"], [200, 10, 390, 300]))\n        self.assertEqual(len(output[\"segmentation\"]), len(anno[\"segmentation\"]))\n        self.assertTrue(np.allclose(output[\"segmentation\"][0], [390, 10, 300, 100, 300, 10]))\n\n        detection_utils.annotations_to_instances([output, output], (400, 400))", "completions": ["  ```\ndef test_transform_simple_annotation(self):\n    annos = [\n        {\n            \"bbox\": [10, 10, 20, 20],\n            \"bbox_mode\": BoxMode.XYXY_ABS,\n            \"category_id\": 1,\n            \"segmentation\": [\n                [10, 10, 20, 10, 20, 20, 10, 20],\n                [10, 10, 20, 10, 20, 20, 10, 20],\n            ],\n            \"keypoints\": [\n                {\"x\": 15, \"y\": 15, \"v\": 1},\n                {\"x\": 10, \"y\": 20, \"v\": 1},\n            ],\n        }\n    ]\n    image_size = (30, 30)\n    mask_format = \"polygon\"\n    target = annotations_to_instances(annos, image_size, mask_format)\n    self.assertEqual(target.gt_boxes.tensor.tolist(), [[10, 10, 20, 20]])\n    self.assertEqual(target.gt_classes.tolist(), [1])\n    self.assertEqual(target.gt_masks.polygons.tolist(), [[[10, 10], [20, 10], [20, 20], [10, 20]]])\n    self.assertEqual(target.gt_keypoints.tensor.tolist(), [[15, 15], [10, 20]])\n```"]}
{"id": 29, "prompt": "\nYou are an expert Python programmer. You will be given some information and will generate a correct test case for the given function.\nYour response will only be python test function and without anything else.\n\n### focal function:\n```python\ndef assert_connectivity(connectivity: nx.Graph, circuit: Circuit):\n    if list(connectivity.nodes) != list(range(connectivity.number_of_nodes())):\n        node_mapping = {node: i for i, node in enumerate(connectivity.nodes)}\n        new_connectivity = nx.Graph()\n        new_connectivity.add_edges_from(\n            [(node_mapping[u], node_mapping[v]) for u, v in connectivity.edges]\n        )\n        connectivity = new_connectivity\n    for gate in circuit.queue:\n        if len(gate.qubits) > 2 and not isinstance(gate, gates.M):\n            raise_error(ConnectivityError, f\"{gate.name} acts on more than two qubits.\")\n        if len(gate.qubits) == 2:\n            if (gate.qubits[0], gate.qubits[1]) not in connectivity.edges:\n                raise_error(\n                    ConnectivityError,\n                    f\"Circuit does not respect connectivity. {gate.name} acts on {gate.qubits}.\",\n                )\n```\n###test function signature:\n```python\n\ndef test_sabre_simple(seed):\n```\n### Answer: (use the provided format with backticks)\n", "name": "test_sabre_simple", "reference": "\ndef test_sabre_simple(seed):\n    placer = Trivial()\n    circ = Circuit(5)\n    circ.add(gates.CZ(0, 1))\n    initial_layout = placer(circ)\n    router = Sabre(connectivity=star_connectivity(), seed=seed)\n    routed_circuit, final_map = router(circuit=circ, initial_layout=initial_layout)\n    assert router.added_swaps == 1\n    assert final_map == {\"q0\": 2, \"q1\": 1, \"q2\": 0, \"q3\": 3, \"q4\": 4}\n    assert routed_circuit.queue[0].qubits == (0, 2)\n    assert isinstance(routed_circuit.queue[0], gates.SWAP)\n    assert isinstance(routed_circuit.queue[1], gates.CZ)\n    assert_connectivity(circuit=routed_circuit, connectivity=star_connectivity())\n    assert_circuit_equivalence(\n        original_circuit=circ,\n        transpiled_circuit=routed_circuit,\n        final_map=final_map,\n        initial_map=initial_layout,\n    )", "completions": ["  ```\ndef test_sabre_simple(seed):\n    circuit = Circuit()\n    circuit.add_gate(gates.H(0))\n    circuit.add_gate(gates.CNOT(0, 1))\n    circuit.add_gate(gates.H(1))\n    circuit.add_gate(gates.CNOT(1, 2))\n    circuit.add_gate(gates.H(2))\n    circuit.add_gate(gates.CNOT(2, 3))\n    circuit.add_gate(gates.H(3))\n    circuit.add_gate(gates.CNOT(3, 4))\n    circuit.add_gate(gates.H(4))\n    circuit.add_gate(gates.CNOT(4, 5))\n    circuit.add_gate(gates.H(5))\n    circuit.add_gate(gates.CNOT(5, 6))\n    circuit.add_gate(gates.H(6))\n    circuit.add_gate(gates.CNOT(6, 7))\n    circuit.add_gate(gates.H(7))\n    circuit.add_gate(gates.CNOT(7, 8))\n    circuit.add_gate(gates.H(8))\n    circuit.add_gate(gates.CNOT(8, 9))\n    circuit.add_gate(gates.H(9))\n    circuit.add_gate(gates.CNOT(9, 10))\n    circuit.add_gate(gates.H(10))\n    circuit.add_gate(gates.CNOT(10, 11))\n    circuit.add_gate(gates.H(11))\n    circuit.add_gate(gates.CNOT(11, 12))\n    circuit.add_gate(gates.H(12))\n    circuit.add_gate(gates.CNOT(12, 13))\n    circuit.add_gate(gates.H(13))\n    circuit.add_gate(gates.CNOT(13, 14))\n    circuit.add_gate(gates.H(14))\n    circuit.add_gate(gates.CNOT(14, 15))\n    circuit.add_gate(gates.H(15))\n    circuit.add_gate(gates.CNOT(15, 16))\n    circuit.add_gate(gates.H(16))\n    circuit.add_gate(gates.CNOT(16, 17))\n    circuit.add_gate(gates.H(17))\n    circuit.add_gate(gates.CNOT(17, 18))\n    circuit.add_gate(gates.H(18))\n    circuit.add_gate(gates.CNOT(18, 19))\n    circuit.add_gate(gates.H(19))\n    circuit.add_gate(gates.CNOT(19, 20))\n    circuit.add_gate(gates.H(20))\n    circuit.add_gate(gates.CNOT(20, 21))\n    circuit.add_gate(gates.H(21))\n    circuit.add_gate(gates.CNOT(21, 22))\n    circuit.add_gate(gates.H(22))\n    circuit.add_gate(gates.CNOT(22, 23))\n    circuit.add_gate(gates.H(23))\n    circuit.add_gate(gates.CNOT(23, 24))\n    circuit.add_gate(gates.H(24))\n    circuit.add_gate(gates.CNOT(24, 25))\n    circuit.add_gate(gates.H(25))\n    circuit.add_gate(gates.CNOT(25, 26))\n    circuit.add_gate(gates.H(26))\n    circuit.add_gate(gates.CNOT(26, 27))\n    circuit.add_gate(g"]}
{"id": 30, "prompt": "\nYou are an expert Python programmer. You will be given some information and will generate a correct test case for the given function.\nYour response will only be python test function and without anything else.\n\n### focal function:\n```python\n\ndef unique_patterns_2d(\n    agrid: NDArray[np.int64], ksize: int, periodic_input: bool\n) -> tuple[NDArray[np.int64], NDArray[np.int64], NDArray[np.int64]]:\n    assert ksize >= 1\n    if periodic_input:\n        agrid = np.pad(\n            agrid,\n            ((0, ksize - 1), (0, ksize - 1), *(((0, 0),) * (len(agrid.shape) - 2))),\n            mode=\"wrap\",\n        )\n    else:\n        # TODO: implement non-wrapped image handling\n        # a = np.pad(a, ((0,k-1),(0,k-1),*(((0,0),)*(len(a.shape)-2))), mode='constant', constant_values=None)\n        agrid = np.pad(\n            agrid,\n            ((0, ksize - 1), (0, ksize - 1), *(((0, 0),) * (len(agrid.shape) - 2))),\n            mode=\"wrap\",\n        )\n\n    patches: NDArray[np.int64] = np.lib.stride_tricks.as_strided(\n        agrid,\n        (\n            agrid.shape[0] - ksize + 1,\n            agrid.shape[1] - ksize + 1,\n            ksize,\n            ksize,\n            *agrid.shape[2:],\n        ),\n        agrid.strides[:2] + agrid.strides[:2] + agrid.strides[2:],\n        writeable=False,\n    )\n    patch_codes = hash_downto(patches, 2)\n    uc, ui = np.unique(patch_codes, return_index=True)\n    locs = np.unravel_index(ui, patch_codes.shape)\n    up: NDArray[np.int64] = patches[locs[0], locs[1]]\n    ids: NDArray[np.int64] = np.vectorize(\n        {code: ind for ind, code in enumerate(uc)}.get\n    )(patch_codes)\n    return ids, up, patch_codes\n```\n###test function signature:\n```python\n\ndef test_unique_patterns_2d(img_redmaze) -> None:\n```\n### Answer: (use the provided format with backticks)\n", "name": "test_unique_patterns_2d", "reference": "\ndef test_unique_patterns_2d(img_redmaze) -> None:\n    img = img_redmaze\n    tile_size = 1\n    pattern_width = 2\n    _tile_catalog, tile_grid, _code_list, _unique_tiles = wfc_tiles.make_tile_catalog(\n        img, tile_size\n    )\n\n    (\n        _patterns_in_grid,\n        pattern_contents_list,\n        patch_codes,\n    ) = wfc_patterns.unique_patterns_2d(tile_grid, pattern_width, True)\n    assert patch_codes[1][2] == 4867810695119132864\n    assert pattern_contents_list[7][1][1] == 8253868773529191888", "completions": ["  ```\ndef test_unique_patterns_2d(img_redmaze):\n    ksize = 3\n    periodic_input = True\n    ids, up, patch_codes = unique_patterns_2d(img_redmaze, ksize, periodic_input)\n    assert ids.shape == (10, 10)\n    assert up.shape == (10, 10, 3)\n    assert patch_codes.shape == (10, 10, 3)\n    assert np.all(ids >= 0)\n    assert np.all(ids < 10)\n    assert np.all(up >= 0)\n    assert np.all(up < 256)\n    assert np.all(patch_codes >= 0)\n    assert np.all(patch_codes < 256)\n```"]}
{"id": 31, "prompt": "\nYou are an expert Python programmer. You will be given some information and will generate a correct test case for the given function.\nYour response will only be python test function and without anything else.\n\n### focal function:\n```python\ndef serialize(data: Any, encoding: str | None = None) -> str | bytes:\n    serialized: bytes | str\n\n    if encoding is not None:\n        serialized = yaml.dump(data, encoding=encoding, Dumper=AiiDADumper)\n    else:\n        serialized = yaml.dump(data, Dumper=AiiDADumper)\n\n    return serialized\n```\n###test function signature:\n```python\ndef test_mixed_attribute_normal_dict():\n```\n### Answer: (use the provided format with backticks)\n", "name": "test_mixed_attribute_normal_dict", "reference": "def test_mixed_attribute_normal_dict():\n    from aiida.common.extendeddicts import AttributeDict\n\n    # Construct a nested `AttributeDict`, which should make all nested dictionaries `AttributeDicts` recursively\n    dictionary = {'nested': AttributeDict({'dict': 'string', 'value': 1})}\n    attribute_dict = AttributeDict(dictionary)\n\n    # Now add a normal dictionary in the attribute dictionary\n    attribute_dict['nested']['normal'] = {'a': 2}\n\n    serialized = serialize.serialize(attribute_dict)\n    deserialized = serialize.deserialize_unsafe(serialized)\n\n    assert attribute_dict, deserialized", "completions": ["  ```\ndef test_mixed_attribute_normal_dict():\n    data = {\"a\": 1, \"b\": 2, \"c\": 3}\n    serialized = serialize(data, encoding=\"utf-8\")\n    assert serialized == b'a: 1\\nb: 2\\nc: 3\\n'\n```"]}
{"id": 32, "prompt": "\nYou are an expert Python programmer. You will be given some information and will generate a correct test case for the given function.\nYour response will only be python test function and without anything else.\n\n### focal function:\n```python\n\n    def unload_model(self, model_id: str) -> RegisteredModels:\n        self.__ensure_v1_client_mode()\n        de_aliased_model_id = resolve_roboflow_model_alias(model_id=model_id)\n        response = requests.post(\n            f\"{self.__api_url}/model/remove\",\n            json={\n                \"model_id\": de_aliased_model_id,\n            },\n            headers=DEFAULT_HEADERS,\n        )\n        response.raise_for_status()\n        response_payload = response.json()\n        if (\n            de_aliased_model_id == self.__selected_model\n            or model_id == self.__selected_model\n        ):\n            self.__selected_model = None\n        return RegisteredModels.from_dict(response_payload)\n```\n###test function signature:\n```python\n\ndef test_client_unload_single_model_when_successful_response_expected_against_alias(\n    requests_mock: Mocker,\n) -> None:\n    # given\n```\n### Answer: (use the provided format with backticks)\n", "name": "test_client_unload_single_model_when_successful_response_expected_against_alias", "reference": "\ndef test_client_unload_single_model_when_successful_response_expected_against_alias(\n    requests_mock: Mocker,\n) -> None:\n    # given\n    api_url = \"http://some.com\"\n    requests_mock.post(\n        f\"{api_url}/model/remove\",\n        json={\"models\": [{\"model_id\": \"some/1\", \"task_type\": \"classification\"}]},\n    )\n    http_client = InferenceHTTPClient(api_key=\"my-api-key\", api_url=api_url)\n    http_client.select_model(model_id=\"yolov8n-640\")\n\n    # when\n    result = http_client.unload_model(model_id=\"yolov8n-640\")\n\n    # then\n    assert result == RegisteredModels(\n        models=[ModelDescription(model_id=\"some/1\", task_type=\"classification\")]\n    )\n    assert requests_mock.last_request.json() == {\n        \"model_id\": \"coco/3\",\n    }\n    assert (\n        http_client.selected_model is None\n    ), \"Even when alias is in use - selected model should be emptied\"", "completions": ["  ```\ndef test_client_unload_single_model_when_successful_response_expected_against_alias(\n    requests_mock: Mocker,\n) -> None:\n    # given\n    model_id = \"model_1\"\n    de_aliased_model_id = \"model_1\"\n    response_payload = {\n        \"model_id\": de_aliased_model_id,\n        \"status\": \"success\",\n    }\n    requests_mock.post(\n        f\"{self.__api_url}/model/remove\",\n        json=response_payload,\n        headers=DEFAULT_HEADERS,\n    )\n    # when\n    result = self.unload_model(model_id)\n    # then\n    assert result.model_id == de_aliased_model_id\n    assert result.status == \"success\"\n    assert self.__selected_model is None\n```"]}
{"id": 33, "prompt": "\nYou are an expert Python programmer. You will be given some information and will generate a correct test case for the given function.\nYour response will only be python test function and without anything else.\n\n### focal function:\n```python\n\ndef edge_index_to_vector(\n    edge_index: Tensor,\n    size: Tuple[int, int],\n    bipartite: bool,\n    force_undirected: bool = False,\n) -> Tuple[Tensor, int]:\n\n    row, col = edge_index\n\n    if bipartite:  # No need to account for self-loops.\n        idx = (row * size[1]).add_(col)\n        population = size[0] * size[1]\n        return idx, population\n\n    elif force_undirected:\n        assert size[0] == size[1]\n        num_nodes = size[0]\n\n        # We only operate on the upper triangular matrix:\n        mask = row < col\n        row, col = row[mask], col[mask]\n        offset = torch.arange(1, num_nodes, device=row.device).cumsum(0)[row]\n        idx = row.mul_(num_nodes).add_(col).sub_(offset)\n        population = (num_nodes * (num_nodes + 1)) // 2 - num_nodes\n        return idx, population\n\n    else:\n        assert size[0] == size[1]\n        num_nodes = size[0]\n\n        # We remove self-loops as we do not want to take them into account\n        # when sampling negative values.\n        mask = row != col\n        row, col = row[mask], col[mask]\n        col[row < col] -= 1\n        idx = row.mul_(num_nodes - 1).add_(col)\n        population = num_nodes * num_nodes - num_nodes\n        return idx, population\n```\n###test function signature:\n```python\n\ndef test_edge_index_to_vector_and_vice_versa():\n    # Create a fully-connected graph:\n```\n### Answer: (use the provided format with backticks)\n", "name": "test_edge_index_to_vector_and_vice_versa", "reference": "\ndef test_edge_index_to_vector_and_vice_versa():\n    # Create a fully-connected graph:\n    N = 10\n    row = torch.arange(N).view(-1, 1).repeat(1, N).view(-1)\n    col = torch.arange(N).view(1, -1).repeat(N, 1).view(-1)\n    edge_index = torch.stack([row, col], dim=0)\n\n    idx, population = edge_index_to_vector(edge_index, (N, N), bipartite=True)\n    assert population == N * N\n    assert idx.tolist() == list(range(population))\n    edge_index2 = vector_to_edge_index(idx, (N, N), bipartite=True)\n    assert is_undirected(edge_index2)\n    assert edge_index.tolist() == edge_index2.tolist()\n\n    idx, population = edge_index_to_vector(edge_index, (N, N), bipartite=False)\n    assert population == N * N - N\n    assert idx.tolist() == list(range(population))\n    mask = edge_index[0] != edge_index[1]  # Remove self-loops.\n    edge_index2 = vector_to_edge_index(idx, (N, N), bipartite=False)\n    assert is_undirected(edge_index2)\n    assert edge_index[:, mask].tolist() == edge_index2.tolist()\n\n    idx, population = edge_index_to_vector(edge_index, (N, N), bipartite=False,\n                                           force_undirected=True)\n    assert population == (N * (N + 1)) / 2 - N\n    assert idx.tolist() == list(range(population))\n    mask = edge_index[0] != edge_index[1]  # Remove self-loops.\n    edge_index2 = vector_to_edge_index(idx, (N, N), bipartite=False,\n                                       force_undirected=True)\n    assert is_undirected(edge_index2)\n    assert edge_index[:, mask].tolist() == to_undirected(edge_index2).tolist()", "completions": ["  ```\ndef test_edge_index_to_vector_and_vice_versa():\n    # Create a fully-connected graph:\n    size = (5, 5)\n    edge_index = torch.tensor([[0, 1, 2, 3, 4], [1, 2, 3, 4, 0]])\n    bipartite = False\n    force_undirected = False\n\n    # Convert edge_index to vector:\n    vector, population = edge_index_to_vector(edge_index, size, bipartite, force_undirected)\n\n    # Convert vector back to edge_index:\n    edge_index_from_vector = vector_to_edge_index(vector, size, bipartite, force_undirected)\n\n    # Check that the original edge_index and the reconstructed edge_index are the same:\n    assert torch.equal(edge_index, edge_index_from_vector)\n```"]}
{"id": 34, "prompt": "\nYou are an expert Python programmer. You will be given some information and will generate a correct test case for the given function.\nYour response will only be python test function and without anything else.\n\n### focal function:\n```python\ndef spectrogram(\n    waveform: np.ndarray,\n    window: np.ndarray,\n    frame_length: int,\n    hop_length: int,\n    fft_length: Optional[int] = None,\n    power: Optional[float] = 1.0,\n    center: bool = True,\n    pad_mode: str = \"reflect\",\n    onesided: bool = True,\n    preemphasis: Optional[float] = None,\n    mel_filters: Optional[np.ndarray] = None,\n    mel_floor: float = 1e-10,\n    log_mel: Optional[str] = None,\n    reference: float = 1.0,\n    min_value: float = 1e-10,\n    db_range: Optional[float] = None,\n    remove_dc_offset: Optional[bool] = None,\n    dtype: np.dtype = np.float32,\n) -> np.ndarray:\n    window_length = len(window)\n\n    if fft_length is None:\n        fft_length = frame_length\n\n    if frame_length > fft_length:\n        raise ValueError(f\"frame_length ({frame_length}) may not be larger than fft_length ({fft_length})\")\n\n    if window_length != frame_length:\n        raise ValueError(f\"Length of the window ({window_length}) must equal frame_length ({frame_length})\")\n\n    if hop_length <= 0:\n        raise ValueError(\"hop_length must be greater than zero\")\n\n    if waveform.ndim != 1:\n        raise ValueError(f\"Input waveform must have only one dimension, shape is {waveform.shape}\")\n\n    if np.iscomplexobj(waveform):\n        raise ValueError(\"Complex-valued input waveforms are not currently supported\")\n\n    if power is None and mel_filters is not None:\n        raise ValueError(\n            \"You have provided `mel_filters` but `power` is `None`. Mel spectrogram computation is not yet supported for complex-valued spectrogram.\"\n            \"Specify `power` to fix this issue.\"\n        )\n\n    # center pad the waveform\n    if center:\n        padding = [(int(frame_length // 2), int(frame_length // 2))]\n        waveform = np.pad(waveform, padding, mode=pad_mode)\n\n    # promote to float64, since np.fft uses float64 internally\n    waveform = waveform.astype(np.float64)\n    window = window.astype(np.float64)\n\n    # split waveform into frames of frame_length size\n    num_frames = int(1 + np.floor((waveform.size - frame_length) / hop_length))\n\n    num_frequency_bins = (fft_length // 2) + 1 if onesided else fft_length\n    spectrogram = np.empty((num_frames, num_frequency_bins), dtype=np.complex64)\n\n    # rfft is faster than fft\n    fft_func = np.fft.rfft if onesided else np.fft.fft\n    buffer = np.zeros(fft_length)\n\n    timestep = 0\n    for frame_idx in range(num_frames):\n        buffer[:frame_length] = waveform[timestep : timestep + frame_length]\n\n        if remove_dc_offset:\n            buffer[:frame_length] = buffer[:frame_length] - buffer[:frame_length].mean()\n\n        if preemphasis is not None:\n            buffer[1:frame_length] -= preemphasis * buffer[: frame_length - 1]\n            buffer[0] *= 1 - preemphasis\n\n        buffer[:frame_length] *= window\n\n        spectrogram[frame_idx] = fft_func(buffer)\n        timestep += hop_length\n\n    # note: ** is much faster than np.power\n    if power is not None:\n        spectrogram = np.abs(spectrogram, dtype=np.float64) ** power\n\n    spectrogram = spectrogram.T\n\n    if mel_filters is not None:\n        spectrogram = np.maximum(mel_floor, np.dot(mel_filters.T, spectrogram))\n\n    if power is not None and log_mel is not None:\n        if log_mel == \"log\":\n            spectrogram = np.log(spectrogram)\n        elif log_mel == \"log10\":\n            spectrogram = np.log10(spectrogram)\n        elif log_mel == \"dB\":\n            if power == 1.0:\n                spectrogram = amplitude_to_db(spectrogram, reference, min_value, db_range)\n            elif power == 2.0:\n                spectrogram = power_to_db(spectrogram, reference, min_value, db_range)\n            else:\n                raise ValueError(f\"Cannot use log_mel option '{log_mel}' with power {power}\")\n        else:\n            raise ValueError(f\"Unknown log_mel option: {log_mel}\")\n\n        spectrogram = np.asarray(spectrogram, dtype)\n\n    return spectrogram\n```\n###test function signature:\n```python\n\n    def test_spectrogram_integration_test(self):\n```\n### Answer: (use the provided format with backticks)\n", "name": "test_spectrogram_integration_test", "reference": "\n    def test_spectrogram_integration_test(self):\n        waveform = self._load_datasamples(1)[0]\n\n        spec = spectrogram(\n            waveform,\n            window_function(400, \"hann\", frame_length=512),\n            frame_length=512,\n            hop_length=128,\n            power=1.0,\n            center=True,\n            pad_mode=\"reflect\",\n            onesided=True,\n        )\n        self.assertEqual(spec.shape, (257, 732))\n\n        # fmt: off\n        expected = np.array([\n            0.02464888, 0.04648664, 0.05872392, 0.02311783, 0.0327175 ,\n            0.02433643, 0.01198814, 0.02055709, 0.01559287, 0.01394357,\n            0.01299037, 0.01728045, 0.0254554 , 0.02486533, 0.02011792,\n            0.01755333, 0.02100457, 0.02337024, 0.01436963, 0.01464558,\n            0.0211017 , 0.0193489 , 0.01272165, 0.01858462, 0.03722598,\n            0.0456542 , 0.03281558, 0.00620586, 0.02226466, 0.03618042,\n            0.03508182, 0.02271432, 0.01051649, 0.01225771, 0.02315293,\n            0.02331886, 0.01417785, 0.0106844 , 0.01791214, 0.017177  ,\n            0.02125114, 0.05028201, 0.06830665, 0.05216664, 0.01963666,\n            0.06941418, 0.11513043, 0.12257859, 0.10948435, 0.08568069,\n            0.05509328, 0.05047818, 0.047112  , 0.05060737, 0.02982424,\n            0.02803827, 0.02933729, 0.01760491, 0.00587815, 0.02117637,\n            0.0293578 , 0.03452379, 0.02194803, 0.01676056,\n        ])\n        # fmt: on\n        self.assertTrue(np.allclose(spec[:64, 400], expected))\n\n        spec = spectrogram(\n            waveform,\n            window_function(400, \"hann\"),\n            frame_length=400,\n            hop_length=128,\n            fft_length=512,\n            power=1.0,\n            center=True,\n            pad_mode=\"reflect\",\n            onesided=True,\n        )\n        self.assertEqual(spec.shape, (257, 732))\n        self.assertTrue(np.allclose(spec[:64, 400], expected))\n\n        mel_filters = mel_filter_bank(\n            num_frequency_bins=256,\n            num_mel_filters=400,\n            min_frequency=20,\n            max_frequency=8000,\n            sampling_rate=16000,\n            norm=None,\n            mel_scale=\"kaldi\",\n            triangularize_in_mel_space=True,\n        )\n\n        mel_filters = np.pad(mel_filters, ((0, 1), (0, 0)))\n\n        spec = spectrogram(\n            waveform,\n            window_function(400, \"povey\", periodic=False),\n            frame_length=400,\n            hop_length=160,\n            fft_length=512,\n            power=2.0,\n            center=False,\n            pad_mode=\"reflect\",\n            onesided=True,\n            preemphasis=0.97,\n            mel_filters=mel_filters,\n            log_mel=\"log\",\n            mel_floor=1.1920928955078125e-07,\n            remove_dc_offset=True,\n        )\n        self.assertEqual(spec.shape, (400, 584))\n\n        # fmt: off\n        expected = np.array([-15.94238515,  -8.20712299,  -8.22704352, -15.94238515,\n       -15.94238515, -15.94238515, -15.94238515, -15.94238515,\n        -6.52463769,  -7.73677889, -15.94238515, -15.94238515,\n       -15.94238515, -15.94238515,  -4.18650018,  -3.37195286,\n       -15.94238515, -15.94238515, -15.94238515, -15.94238515,\n        -4.70190154,  -2.4217066 , -15.94238515, -15.94238515,\n       -15.94238515, -15.94238515,  -5.62755239,  -3.53385194,\n       -15.94238515, -15.94238515, -15.94238515, -15.94238515,\n        -9.43303023,  -8.77480925, -15.94238515, -15.94238515,\n       -15.94238515, -15.94238515,  -4.2951092 ,  -5.51585994,\n       -15.94238515, -15.94238515, -15.94238515,  -4.40151721,\n        -3.95228878, -15.94238515, -15.94238515, -15.94238515,\n        -6.10365415,  -4.59494697, -15.94238515, -15.94238515,\n       -15.94238515,  -8.10727767,  -6.2585298 , -15.94238515,\n       -15.94238515, -15.94238515,  -5.60161702,  -4.47217004,\n       -15.94238515, -15.94238515, -15.94238515,  -5.91641988]\n        )\n        # fmt: on\n        self.assertTrue(np.allclose(spec[:64, 400], expected, atol=1e-5))", "completions": ["  ```\ndef test_spectrogram_integration_test(self):\n    # Test case 1: Test that the spectrogram function returns the correct shape\n    waveform = np.random.rand(1000)\n    window = np.hanning(512)\n    frame_length = 512\n    hop_length = 256\n    fft_length = 1024\n    power = 1.0\n    center = True\n    pad_mode = \"reflect\"\n    onesided = True\n    preemphasis = None\n    mel_filters = None\n    mel_floor = 1e-10\n    log_mel = None\n    reference = 1.0\n    min_value = 1e-10\n    db_range = None\n    remove_dc_offset = None\n    dtype = np.float32\n\n    spectrogram = spectrogram(waveform, window, frame_length, hop_length, fft_length, power, center, pad_mode, onesided, preemphasis, mel_filters, mel_floor, log_mel, reference, min_value, db_range, remove_dc_offset, dtype)\n    expected_shape = (1000 // 256, 1024 // 2 + 1)\n    self.assertEqual(spectrogram.shape, expected_shape)\n\n    # Test case 2: Test that the spectrogram function returns the correct dtype\n    waveform = np.random.rand(1000)\n    window = np.hanning(512)\n    frame_length = 512\n    hop_length = 256\n    fft_length = 1024\n    power = 1.0\n    center = True\n    pad_mode = \"reflect\"\n    onesided = True\n    preemphasis = None\n    mel_filters = None\n    mel_floor = 1e-10\n    log_mel = None\n    reference = 1.0\n    min_value = 1e-10\n    db_range = None\n    remove_dc_offset = None\n    dtype = np.float64\n\n    spectrogram = spectrogram(waveform, window, frame_length, hop_length, fft_length, power, center, pad_mode, onesided, preemphasis, mel_filters, mel_floor, log_mel, reference, min_value, db_range, remove_dc_offset, dtype)\n    self.assertEqual(spectrogram.dtype, np.float64)\n\n    # Test case 3: Test that the spectrogram function returns the correct values\n    waveform = np.array([1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0])\n    window = np.array([0.5, 0.5, 0.5, 0.5, 0.5])\n    frame_length = 5\n    hop_length = 2\n    fft_length = 10\n    power = 1.0\n    center = True\n    pad_mode = \"reflect\"\n    onesided = True\n    preemphasis = None\n    mel_filters = None\n    mel_floor = 1e-10\n    log_mel = None\n    reference = 1.0\n    min_value = 1e-10\n    db_range = None\n    remove_dc_offset = None\n    dtype = np.float32\n\n    spectrogram = spectrogram(waveform, window, frame_length, hop_length, fft_length, power, center, pad_mode, onesided, preemphasis, mel_filters, mel_floor, log_mel, reference, min_value, db_range, remove_dc_offset, dtype)\n    expected_spectrogram = np.array([[1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0]])\n    self.assertTrue(np.allclose(spectrogram, expected"]}
{"id": 35, "prompt": "\nYou are an expert Python programmer. You will be given some information and will generate a correct test case for the given function.\nYour response will only be python test function and without anything else.\n\n### focal function:\n```python\ndef run_query_expansion_node(modules: List[Callable],\n                             module_params: List[Dict],\n                             previous_result: pd.DataFrame,\n                             node_line_dir: str,\n                             strategies: Dict,\n                             ) -> pd.DataFrame:\n    if not os.path.exists(node_line_dir):\n        os.makedirs(node_line_dir)\n    node_dir = os.path.join(node_line_dir, \"query_expansion\")\n    if not os.path.exists(node_dir):\n        os.makedirs(node_dir)\n    project_dir = pathlib.PurePath(node_line_dir).parent.parent\n\n    # run query expansion\n    results, execution_times = zip(*map(lambda task: measure_speed(\n        task[0], project_dir=project_dir, previous_result=previous_result, **task[1]), zip(modules, module_params)))\n    average_times = list(map(lambda x: x / len(results[0]), execution_times))\n\n    # save results to folder\n    pseudo_module_params = deepcopy(module_params)\n    for i, module_param in enumerate(pseudo_module_params):\n        if 'prompt' in module_params:\n            module_param['prompt'] = str(i)\n    filepaths = list(map(lambda x: os.path.join(node_dir, f'{x}.parquet'), range(len(modules))))\n    list(map(lambda x: x[0].to_parquet(x[1], index=False), zip(results, filepaths)))  # execute save to parquet\n    filenames = list(map(lambda x: os.path.basename(x), filepaths))\n\n    # make summary file\n    summary_df = pd.DataFrame({\n        'filename': filenames,\n        'module_name': list(map(lambda module: module.__name__, modules)),\n        'module_params': module_params,\n        'execution_time': average_times,\n    })\n\n    # Run evaluation when there are more than one module.\n    if len(modules) > 1:\n        # pop general keys from strategies (e.g. metrics, speed_threshold)\n        general_key = ['metrics', 'speed_threshold']\n        general_strategy = dict(filter(lambda x: x[0] in general_key, strategies.items()))\n        extra_strategy = dict(filter(lambda x: x[0] not in general_key, strategies.items()))\n\n        # first, filter by threshold if it is enabled.\n        if general_strategy.get('speed_threshold') is not None:\n            results, filenames = filter_by_threshold(results, average_times, general_strategy['speed_threshold'],\n                                                     filenames)\n\n        # check metrics in strategy\n        if general_strategy.get('metrics') is None:\n            raise ValueError(\"You must at least one metrics for query expansion evaluation.\")\n\n        if extra_strategy.get('top_k') is None:\n            extra_strategy['top_k'] = 10  # default value\n\n        # get retrieval modules from strategy\n        retrieval_callables, retrieval_params = make_retrieval_callable_params(extra_strategy)\n\n        # get retrieval_gt\n        retrieval_gt = pd.read_parquet(os.path.join(project_dir, \"data\", \"qa.parquet\"))['retrieval_gt'].tolist()\n\n        # run evaluation\n        evaluation_results = list(map(lambda result: evaluate_one_query_expansion_node(\n            retrieval_callables, retrieval_params, result['queries'].tolist(), retrieval_gt,\n            general_strategy['metrics'], project_dir, previous_result), results))\n\n        evaluation_df = pd.DataFrame({\n            'filename': filenames,\n            **{f'query_expansion_{metric_name}': list(map(lambda x: x[metric_name].mean(), evaluation_results))\n               for metric_name in general_strategy['metrics']}\n        })\n        summary_df = pd.merge(on='filename', left=summary_df, right=evaluation_df, how='left')\n\n        best_result, best_filename = select_best_average(evaluation_results, general_strategy['metrics'], filenames)\n        # change metric name columns to query_expansion_metric_name\n        best_result = best_result.rename(columns={\n            metric_name: f'query_expansion_{metric_name}' for metric_name in strategies['metrics']})\n        best_result = best_result.drop(columns=['retrieved_contents', 'retrieved_ids', 'retrieve_scores'])\n    else:\n        best_result, best_filename = results[0], filenames[0]\n        best_result = pd.concat([previous_result, best_result], axis=1)\n\n    # add 'is_best' column at summary file\n    summary_df['is_best'] = summary_df['filename'] == best_filename\n\n    # save files\n    summary_df.to_csv(os.path.join(node_dir, \"summary.csv\"), index=False)\n    best_result.to_parquet(os.path.join(node_dir, f\"best_{os.path.splitext(best_filename)[0]}.parquet\"), index=False)\n\n    return best_result\n```\n###test function signature:\n```python\n\ndef test_run_query_expansion_node_default(node_line_dir):\n```\n### Answer: (use the provided format with backticks)\n", "name": "test_run_query_expansion_node_default", "reference": "\ndef test_run_query_expansion_node_default(node_line_dir):\n    generator_models['mock'] = MockLLM\n    project_dir = pathlib.PurePath(node_line_dir).parent.parent\n    qa_path = os.path.join(project_dir, \"data\", \"qa.parquet\")\n    previous_result = pd.read_parquet(qa_path)\n\n    modules = [query_decompose, hyde]\n    module_params = [{'llm': \"mock\", 'temperature': 0.2, 'batch': 7}, {'llm': \"mock\"}]\n    strategies = {\n        'metrics': metrics\n    }\n    best_result = run_query_expansion_node(modules, module_params, previous_result, node_line_dir, strategies)\n    base_query_expansion_test(best_result, node_line_dir)", "completions": ["  ```\ndef test_run_query_expansion_node_default(node_line_dir):\n    modules = [lambda x: x, lambda x: x + 1]\n    module_params = [{'a': 1}, {'a': 2}]\n    previous_result = pd.DataFrame({'a': [1, 2, 3], 'b': [4, 5, 6]})\n    strategies = {'metrics': ['f1', 'precision'],'speed_threshold': 10}\n    expected_result = pd.DataFrame({'a': [1, 2, 3], 'b': [4, 5, 6], 'c': [5, 6, 7]})\n    assert run_query_expansion_node(modules, module_params, previous_result, node_line_dir, strategies).equals(expected_result)\n```"]}
{"id": 36, "prompt": "\nYou are an expert Python programmer. You will be given some information and will generate a correct test case for the given function.\nYour response will only be python test function and without anything else.\n\n### focal function:\n```python\n    def run(self) -> Tuple[pd.DataFrame, FreqDetectionResult]:\n        try:\n            if self._prophet_detector_params[\"suppress_stan\"]:\n                pd.set_option(\"mode.chained_assignment\", None)\n\n            # Skip measurements based on feedbacks given from SODA Cloud\n            preprocessed_df = self.preprocess(time_series_df=self.raw_time_series_df)\n\n            # Automatically detect frequency of the time series\n            freq_detector = FrequencyDetector(\n                logs=self.logs,\n                params=self.params,\n                time_series_df=preprocessed_df,\n                manual_freq=self.training_dataset_params.frequency,\n            )\n            freq_detection_result = freq_detector.detect_frequency()\n\n            # Return if frequency detection failed\n            if freq_detection_result.error_code_int >= ERROR_CODE_LEVEL_CUTTOFF:\n                return self.exit_with_warning(freq_detection_result)\n\n            # Apply training dataset configurations\n            training_df = self.apply_training_dataset_configs(\n                time_series_df=preprocessed_df, freq_detection_result=freq_detection_result\n            )\n\n            # Remove big gaps from the time series to not confuse Prophet\n            training_df = self.remove_big_gaps_from_time_series(\n                time_series_df=training_df, freq_detection_result=freq_detection_result\n            )\n\n            # Only use the last n points for training based on the window length\n            window_length = self.get_window_length(training_df=training_df)\n            training_df = training_df.iloc[-window_length:]\n\n            training_df_shape = training_df[\"y\"].dropna().shape[0]\n            if training_df_shape <= self._min_n_points:\n                freq_detection_result = get_not_enough_measurements_freq_result(n_data_points=training_df_shape)\n                return self.exit_with_warning(freq_detection_result)\n\n            model_hyperparameters = self.get_prophet_hyperparameters(time_series_df=training_df)\n\n            predictions_df = self.setup_fit_predict(\n                time_series_df=training_df, model_hyperparameters=model_hyperparameters\n            )\n            anomalies_df = self.detect_anomalies(time_series_df=training_df, predictions_df=predictions_df)\n            anomalies_df = self.generate_severity_zones(anomalies_df=anomalies_df)\n            anomalies_df = self.compute_alert_level(anomalies_df=anomalies_df)\n            return anomalies_df, freq_detection_result\n        except Exception as e:\n            raise e\n```\n###test function signature:\n```python\n\ndef test_with_exit() -> None:\n```\n### Answer: (use the provided format with backticks)\n", "name": "test_with_exit", "reference": "\ndef test_with_exit() -> None:\n    time_series_df = generate_random_dataframe(size=2, n_rows_to_convert_none=0, frequency=\"D\")\n    detector = ProphetDetector(\n        logs=LOGS,\n        params=PARAMS,\n        time_series_df=time_series_df,\n        model_cfg=ModelConfigs(),\n        training_dataset_params=TrainingDatasetParameters(),\n        severity_level_params=SeverityLevelParameters(),\n    )\n    df_anomalies, frequency_result = detector.run()\n    assert df_anomalies.empty\n    assert frequency_result.error_code_int == 100\n    assert frequency_result.freq_detection_strategy == \"not_enough_measurements_custom\"", "completions": ["  ```\ndef test_with_exit():\n    # Test case 1: Test that the function exits with a warning when the frequency detection result has an error code >= ERROR_CODE_LEVEL_CUTTOFF\n    freq_detection_result = FreqDetectionResult(error_code_int=ERROR_CODE_LEVEL_CUTTOFF)\n    with pytest.raises(SystemExit) as pytest_wrapped_e:\n        run(freq_detection_result)\n    assert pytest_wrapped_e.type == SystemExit\n    assert pytest_wrapped_e.value.code == 1\n\n    # Test case 2: Test that the function exits with a warning when the training dataset has less than _min_n_points\n    freq_detection_result = FreqDetectionResult(error_code_int=0)\n    training_df = pd.DataFrame({\"y\": [1, 2, 3, 4, 5]})\n    with pytest.raises(SystemExit) as pytest_wrapped_e:\n        run(freq_detection_result, training_df)\n    assert pytest_wrapped_e.type == SystemExit\n    assert pytest_wrapped_e.value.code == 1\n```"]}
{"id": 37, "prompt": "\nYou are an expert Python programmer. You will be given some information and will generate a correct test case for the given function.\nYour response will only be python test function and without anything else.\n\n### focal function:\n```python\ndef average_transforms(\n    R: torch.Tensor,\n    t: torch.Tensor,\n    w: torch.Tensor,\n    mask: torch.Tensor,\n    dim: int,\n    t_edge: Optional[torch.Tensor] = None,\n    dither: Optional[bool] = True,\n    dither_eps: float = 1e-4,\n) -> Tuple[torch.Tensor, torch.Tensor]:\n    assert dim >= 0, \"dimension must index from the left\"\n    w = torch.where(\n        mask[..., None].bool(), w, torch.full_like(w, torch.finfo(w.dtype).min)\n    )\n\n    # We use different averaging models based on the number of weights\n    num_transform_weights = w.size(-1)\n    if num_transform_weights == 1:\n        # Share a single scalar weight between t and R.\n        probs = w.softmax(dim)\n        t_probs = probs\n        R_probs = probs[..., None]\n\n        # Average translation.\n        t_avg = (t * t_probs).sum(dim)\n    elif num_transform_weights == 2:\n        # Use separate scalar weights for each of t and R.\n        probs = w.softmax(dim)\n        t_probs, R_probs = probs.unbind(-1)\n        t_probs = t_probs[..., None]\n        R_probs = R_probs[..., None, None]\n\n        # Average translation.\n        t_avg = (t * t_probs).sum(dim)\n    elif num_transform_weights == 3:\n        # For R use a signed scalar weight.\n        R_probs = w[..., 2].softmax(dim)[..., None, None]\n\n        # For t use a two-parameter precision matrix P = P_isometric + P_radial.\n        # We need to hand compute softmax over the shared dim x 2 elements.\n        w_t = w[..., :2]\n        w_t_total = w_t.logsumexp([dim, -1], True)\n        p_iso, p_rad = (w_t - w_t_total).exp().unbind(-1)\n\n        # Use Gaussian fusion for translation.\n        t_edge = t_edge * mask.to(t_edge.dtype)[..., None]\n        t_avg, _ = fuse_gaussians_isometric_plus_radial(t, p_iso, p_rad, t_edge, dim)\n    else:\n        raise NotImplementedError\n\n    # Average rotation via SVD\n    R_avg_unc = (R * R_probs).sum(dim)\n    R_avg_unc = R_avg_unc + dither_eps * torch.randn_like(R_avg_unc)\n    U, S, Vh = torch.linalg.svd(R_avg_unc, full_matrices=True)\n    R_avg = U @ Vh\n\n    # Enforce that matrix is rotation matrix\n    d = torch.linalg.det(R_avg)\n    d_expand = F.pad(d[..., None, None], (2, 0), value=1.0)\n    Vh = Vh * d_expand\n    R_avg = U @ Vh\n    return R_avg, t_avg\n```\n###test function signature:\n```python\n\ndef test_average_transforms(rotations, translations):\n```\n### Answer: (use the provided format with backticks)\n", "name": "test_average_transforms", "reference": "\ndef test_average_transforms(rotations, translations):\n    R = torch.stack([rotations[0], torch.eye(3)])\n    t = torch.stack([translations[0], torch.zeros(3)])\n    w = torch.ones(2, 2)\n    mask = torch.ones(2)\n    # average of a transform with the identity is \"half\" the transform\n    R_avg, t_avg = average_transforms(R, t, w, mask, dim=0, dither=False)\n    R_total_fromavg, _ = compose_transforms(\n        R_avg, torch.zeros(3), R_avg, torch.zeros(3)\n    )\n    _, t_total_fromavg = compose_transforms(torch.eye(3), t_avg, torch.eye(3), t_avg)\n    assert torch.allclose(R_total_fromavg, R[0], atol=1e-3, rtol=1e-2)\n    assert torch.allclose(t_total_fromavg, t[0], atol=1e-3, rtol=1e-2)", "completions": ["  ```\ndef test_average_transforms(rotations, translations):\n    # Test case 1: Single scalar weight\n    R = torch.tensor([[1, 0, 0], [0, 1, 0], [0, 0, 1]])\n    t = torch.tensor([[1, 0, 0], [0, 1, 0], [0, 0, 1]])\n    w = torch.tensor([[1]])\n    mask = torch.tensor([[1]])\n    dim = 0\n    t_edge = None\n    dither = True\n    dither_eps = 1e-4\n    R_avg, t_avg = average_transforms(R, t, w, mask, dim, t_edge, dither, dither_eps)\n    assert torch.allclose(R_avg, R)\n    assert torch.allclose(t_avg, t)\n\n    # Test case 2: Separate scalar weights\n    R = torch.tensor([[1, 0, 0], [0, 1, 0], [0, 0, 1]])\n    t = torch.tensor([[1, 0, 0], [0, 1, 0], [0, 0, 1]])\n    w = torch.tensor([[1, 1]])\n    mask = torch.tensor([[1]])\n    dim = 0\n    t_edge = None\n    dither = True\n    dither_eps = 1e-4\n    R_avg, t_avg = average_transforms(R, t, w, mask, dim, t_edge, dither, dither_eps)\n    assert torch.allclose(R_avg, R)\n    assert torch.allclose(t_avg, t)\n\n    # Test case 3: Signed scalar weight\n    R = torch.tensor([[1, 0, 0], [0, 1, 0], [0, 0, 1]])\n    t = torch.tensor([[1, 0, 0], [0, 1, 0], [0, 0, 1]])\n    w = torch.tensor([[1, 1, 1]])\n    mask = torch.tensor([[1]])\n    dim = 0\n    t_edge = None\n    dither = True\n    dither_eps = 1e-4\n    R_avg, t_avg = average_transforms(R, t, w, mask, dim, t_edge, dither, dither_eps)\n    assert torch.allclose(R_avg, R)\n    assert torch.allclose(t_avg, t)\n\n    # Test case 4: Gaussian fusion\n    R = torch.tensor([[1, 0, 0], [0, 1, 0], [0, 0, 1]])\n    t = torch.tensor([[1, 0, 0], [0, 1, 0], [0, 0, 1]])\n    w = torch.tensor([[1, 1, 1]])\n    mask = torch.tensor([[1]])\n    dim = 0\n    t_edge = None\n    dither = True\n    dither_eps = 1e-4\n    R_avg, t_avg = average_transforms(R, t, w, mask, dim, t_edge, dither, dither_eps)\n    assert torch.allclose(R_avg, R)\n    assert torch.allclose(t_avg, t)\n```"]}
{"id": 38, "prompt": "\nYou are an expert Python programmer. You will be given some information and will generate a correct test case for the given function.\nYour response will only be python test function and without anything else.\n\n### focal function:\n```python\n\ndef btlm_config_to_gpt2_config(btlm_config: PretrainedConfig) -> GPT2Config:\n    return GPT2Config(\n        vocab_size=btlm_config.vocab_size,\n        n_positions=0 if btlm_config.position_embedding_type == \"alibi\" else btlm_config.n_positions,\n        n_embd=btlm_config.hidden_size,\n        n_layer=btlm_config.num_hidden_layers,\n        n_head=btlm_config.num_attention_heads,\n        n_inner=btlm_config.n_inner,\n        activation_function=btlm_config.activation_function,\n        resid_pdrop=btlm_config.resid_pdrop,\n        embd_pdrop=btlm_config.embd_pdrop,\n        attn_pdrop=btlm_config.attn_pdrop,\n        layer_norm_epsilon=btlm_config.layer_norm_epsilon,\n        initializer_range=btlm_config.initializer_range,\n        bos_token_id=btlm_config.bos_token_id,\n        eos_token_id=btlm_config.eos_token_id,\n        # These are new arguments not in the original GPT2Config\n        use_alibi=btlm_config.position_embedding_type == \"alibi\",\n        use_flash_attn=btlm_config.position_embedding_type == \"alibi\",  # Alibi code path requires flash_attn\n        mup_width_scale=btlm_config.mup_width_scale,\n        mup_embeddings_multiplier=btlm_config.mup_embeddings_scale,\n        mup_output_multiplier=btlm_config.mup_output_alpha,\n        mup_scale_qk_dot_by_d=btlm_config.mup_scale_qk_dot_by_d,\n        mlp_multiple_of=1,\n    )\n```\n###test function signature:\n```python\ndef test_btlm_optimized(model_name):\n```\n### Answer: (use the provided format with backticks)\n", "name": "test_btlm_optimized", "reference": "def test_btlm_optimized(model_name):\n    dtype = torch.float16\n    device = \"cuda\"\n    config = btlm_config_to_gpt2_config(\n        AutoConfig.from_pretrained(model_name, trust_remote_code=True)\n    )\n    config.fused_bias_fc = True\n    config.fused_dropout_add_ln = True\n    config.residual_in_fp32 = True\n\n    pretrained_state_dict = remap_state_dict_hf_btlm(state_dict_from_pretrained(model_name), config)\n    model = GPTLMHeadModel(config, device=device, dtype=dtype)\n    model.load_state_dict(pretrained_state_dict)\n    model.eval()\n\n    torch.manual_seed(0)\n    batch_size = 2\n    max_seqlen = 256\n    seqlens = torch.randint(max_seqlen // 2, max_seqlen + 1, (batch_size,), device=device)\n    input_ids = torch.randint(\n        0, config.vocab_size, (batch_size, max_seqlen), dtype=torch.long, device=device\n    )\n    with torch.no_grad():\n        out = model.transformer(input_ids)\n        logits = model(input_ids).logits\n    del model\n\n    # Without device_map, the model is loaded on the CPU, which is very slow\n    # Need auto here since the 13B fp32 model doesn't fit in memory on a A100 40GB\n    model_ref = AutoModelForCausalLM.from_pretrained(\n        model_name, device_map=\"auto\", trust_remote_code=True\n    )\n    model_ref.eval()\n    with torch.no_grad():\n        out_ref = model_ref.transformer(input_ids).last_hidden_state.to(device=device)\n        logits_ref = model_ref(input_ids).logits.to(device=device)\n    del model_ref\n\n    model_hf = AutoModelForCausalLM.from_pretrained(\n        model_name,\n        torch_dtype=dtype,\n        device_map={\"\": device},\n        trust_remote_code=True,\n    )\n    model_hf.eval()\n    with torch.no_grad():\n        out_hf = model_hf.transformer(input_ids).last_hidden_state\n        logits_hf = model_hf(input_ids).logits\n    del model_hf\n\n    print(f\"Output max diff: {(out - out_ref).abs().max().item()}\")\n    print(f\"Output mean diff: {(out - out_ref).abs().mean().item()}\")\n    print(f\"HF fp16 max diff: {(out_hf - out_ref).abs().max().item()}\")\n    print(f\"HF fp16 mean diff: {(out_hf - out_ref).abs().mean().item()}\")\n    assert (out - out_ref).abs().max().item() < 3 * (out_hf - out_ref).abs().max().item()\n\n    print(f\"Logits max diff: {(logits - logits_ref).abs().max().item()}\")\n    print(f\"Logits mean diff: {(logits - logits_ref).abs().mean().item()}\")\n    print(f\"HF fp16 max diff: {(logits_hf - logits_ref).abs().max().item()}\")\n    print(f\"HF fp16 mean diff: {(logits_hf - logits_ref).abs().mean().item()}\")\n    assert (logits - logits_ref).abs().max().item() < 3 * (\n        logits_hf - logits_ref\n    ).abs().max().item()", "completions": ["  ```\ndef test_btlm_optimized(model_name):\n    # Initialize the BTLM model\n    btlm_model = BTLM(model_name)\n\n    # Create a GPT2Config object from the BTLM model\n    gpt2_config = btlm_config_to_gpt2_config(btlm_model.config)\n\n    # Initialize the GPT2 model with the GPT2Config object\n    gpt2_model = GPT2(gpt2_config)\n\n    # Test the BTLM model and the GPT2 model\n    assert btlm_model.config.to_dict() == gpt2_config.to_dict()\n    assert btlm_model.config.to_dict() == gpt2_model.config.to_dict()\n```"]}
{"id": 39, "prompt": "\nYou are an expert Python programmer. You will be given some information and will generate a correct test case for the given function.\nYour response will only be python test function and without anything else.\n\n### focal function:\n```python\ndef grad_solid_harmonic(r: jnp.ndarray, l_max: int) -> jnp.ndarray:\n  def cartesian_sum(r, ell):\n    # computes all cartesian sums of order ell\n    # i.e. all terms (a*x + b*y + c*z) for a+b+c = ell\n    if ell == 0:\n      return np.array([0.])\n    return np.sum(np.stack(\n        np.meshgrid(*[r]*ell, indexing='ij'), axis=0).reshape(ell, -1), axis=0)\n\n  dharmonics = []\n  for ell in range(l_max + 1):\n    dx = cartesian_sum(np.array([1., 0., 0.]), ell)\n    dy = cartesian_sum(np.array([0., 1., 0.]), ell)\n    dz = cartesian_sum(np.array([0., 0., 1.]), ell)\n    d = np.stack([dx, dy, dz], axis=1)\n    cp = jax.vmap(functools.partial(cartesian_product, ell=ell), out_axes=-1)\n    dcp = jnp.where(r.T[None, :, :] == 0, 0,\n                    d[:, :, None]*cp(r)[:, None, :] / r.T[None, :, :])\n    sh = jnp.einsum('cs,cxn->snx', full_cart2sph(ell, True), dcp)\n    pad = l_max - ell\n    sh = jnp.pad(sh, [[pad, pad], [0, 0], [0, 0]])\n    dharmonics.append(sh)\n\n  return jnp.stack(dharmonics, axis=1)  # [L, M, grid, xyz]\n```\n###test function signature:\n```python\n\n  def test_grad_solid_harmonic(self):\n```\n### Answer: (use the provided format with backticks)\n", "name": "test_grad_solid_harmonic", "reference": "\n  def test_grad_solid_harmonic(self):\n    np.random.seed(0)\n    r = np.random.randn(100, 3)\n    l_max = 5\n\n    jax_grad = jax.jacfwd(lambda x: gto.solid_harmonic_from_cart(x, l_max))\n    expected = jnp.transpose(jnp.squeeze(jax.vmap(jax_grad)(r[:, None, :])),\n                             [1, 2, 0, 3])\n\n    with self.subTest('by hand'):\n      observed = gto.grad_solid_harmonic(r, l_max)\n      np.testing.assert_allclose(observed, expected, atol=1.e-4)\n    with self.subTest('by jax'):\n      observed_jacfwd = gto.grad_solid_harmonic_by_jacfwd(r, l_max)\n      np.testing.assert_allclose(observed_jacfwd, expected, atol=1.e-4)", "completions": ["  ```\ndef test_grad_solid_harmonic(self):\n    # Test case 1: l_max = 0\n    r = jnp.array([[1., 0., 0.], [0., 1., 0.], [0., 0., 1.]])\n    expected_output = jnp.array([[0., 0., 0.], [0., 0., 0.], [0., 0., 0.]])\n    self.assertArrayEqual(grad_solid_harmonic(r, 0), expected_output)\n\n    # Test case 2: l_max = 1\n    r = jnp.array([[1., 0., 0.], [0., 1., 0.], [0., 0., 1.]])\n    expected_output = jnp.array([[0., 0., 0.], [0., 0., 0.], [0., 0., 0.]])\n    self.assertArrayEqual(grad_solid_harmonic(r, 1), expected_output)\n\n    # Test case 3: l_max = 2\n    r = jnp.array([[1., 0., 0.], [0., 1., 0.], [0., 0., 1.]])\n    expected_output = jnp.array([[0., 0., 0.], [0., 0., 0.], [0., 0., 0.]])\n    self.assertArrayEqual(grad_solid_harmonic(r, 2), expected_output)\n\n    # Test case 4: l_max = 3\n    r = jnp.array([[1., 0., 0.], [0., 1., 0.], [0., 0., 1.]])\n    expected_output = jnp.array([[0., 0., 0.], [0., 0., 0.], [0., 0., 0.]])\n    self.assertArrayEqual(grad_solid_harmonic(r, 3), expected_output)\n\n    # Test case 5: l_max = 4\n    r = jnp.array([[1., 0., 0.], [0., 1., 0.], [0., 0., 1.]])\n    expected_output = jnp.array([[0., 0., 0.], [0., 0., 0.], [0., 0., 0.]])\n    self.assertArrayEqual(grad_solid_harmonic(r, 4), expected_output)\n```"]}
{"id": 40, "prompt": "\nYou are an expert Python programmer. You will be given some information and will generate a correct test case for the given function.\nYour response will only be python test function and without anything else.\n\n### focal function:\n```python\ndef generate_basis(\n    base_shape, angular_tesselation, remove_symmetries=True, eps=1e-4\n):\n\n  if base_shape == 'tetrahedron':\n    verts = np.array([\n        (np.sqrt(8 / 9), 0, -1 / 3),\n        (-np.sqrt(2 / 9), np.sqrt(2 / 3), -1 / 3),\n        (-np.sqrt(2 / 9), -np.sqrt(2 / 3), -1 / 3),\n        (0, 0, 1),\n    ])\n    faces = np.array([(0, 1, 2), (0, 2, 3), (0, 1, 3), (1, 2, 3)])\n  elif base_shape == 'icosahedron':\n    a = (np.sqrt(5) + 1) / 2\n    verts = np.array([\n        (-1, 0, a),\n        (1, 0, a),\n        (-1, 0, -a),\n        (1, 0, -a),\n        (0, a, 1),\n        (0, a, -1),\n        (0, -a, 1),\n        (0, -a, -1),\n        (a, 1, 0),\n        (-a, 1, 0),\n        (a, -1, 0),\n        (-a, -1, 0),\n    ]) / np.sqrt(a + 2)\n    faces = np.array([\n        (0, 4, 1),\n        (0, 9, 4),\n        (9, 5, 4),\n        (4, 5, 8),\n        (4, 8, 1),\n        (8, 10, 1),\n        (8, 3, 10),\n        (5, 3, 8),\n        (5, 2, 3),\n        (2, 7, 3),\n        (7, 10, 3),\n        (7, 6, 10),\n        (7, 11, 6),\n        (11, 0, 6),\n        (0, 1, 6),\n        (6, 1, 10),\n        (9, 0, 11),\n        (9, 11, 2),\n        (9, 2, 5),\n        (7, 2, 11),\n    ])\n  elif base_shape == 'octahedron':\n    verts = np.array(\n        [(0, 0, -1), (0, 0, 1), (0, -1, 0), (0, 1, 0), (-1, 0, 0), (1, 0, 0)]\n    )\n    corners = np.array(list(itertools.product([-1, 1], repeat=3)))\n    pairs = np.argwhere(compute_sq_dist(corners.T, verts.T) == 2)\n    faces = np.sort(np.reshape(pairs[:, 1], [3, -1]).T, 1)\n  else:\n    raise ValueError(f'base_shape {base_shape} not supported')\n  verts = tesselate_geodesic(verts, faces, angular_tesselation)\n\n  if remove_symmetries:\n    # Remove elements of `verts` that are reflections of each other.\n    match = compute_sq_dist(verts.T, -verts.T) < eps\n    verts = verts[~np.any(np.triu(match), axis=0), :]\n\n  basis = verts[:, ::-1]\n  return basis\n```\n###test function signature:\n```python\n  def test_generate_basis_golden(self):\n```\n### Answer: (use the provided format with backticks)\n", "name": "test_generate_basis_golden", "reference": "  def test_generate_basis_golden(self):\n\n    basis = geopoly.generate_basis('tetrahedron', 1)\n    basis_golden = np.array([\n        [-0.33333333, -0.81649658, -0.47140452],\n        [-0.33333333, 0.81649658, -0.47140452],\n        [-0.33333333, 0.00000000, 0.94280904],\n        [1.00000000, 0.00000000, 0.00000000],\n    ])\n    self.assertTrue(is_same_basis(basis.T, basis_golden.T))\n\n    basis = geopoly.generate_basis('tetrahedron', 2)\n    basis_golden = np.array([\n        [-0.33333333, -0.81649658, -0.47140452],\n        [-0.57735027, 0.00000000, -0.81649658],\n        [-0.33333333, 0.81649658, -0.47140452],\n        [-0.57735027, -0.70710678, 0.40824829],\n        [-0.57735027, 0.70710678, 0.40824829],\n        [-0.33333333, 0.00000000, 0.94280904],\n        [1.00000000, 0.00000000, 0.00000000],\n    ])\n    self.assertTrue(is_same_basis(basis.T, basis_golden.T))\n\n    basis = geopoly.generate_basis('icosahedron', 2)\n    basis_golden = np.array([\n        [0.85065081, 0.00000000, 0.52573111],\n        [0.80901699, 0.50000000, 0.30901699],\n        [0.52573111, 0.85065081, 0.00000000],\n        [1.00000000, 0.00000000, 0.00000000],\n        [0.80901699, 0.50000000, -0.30901699],\n        [0.85065081, 0.00000000, -0.52573111],\n        [0.30901699, 0.80901699, -0.50000000],\n        [0.00000000, 0.52573111, -0.85065081],\n        [0.50000000, 0.30901699, -0.80901699],\n        [0.00000000, 1.00000000, 0.00000000],\n        [-0.52573111, 0.85065081, 0.00000000],\n        [-0.30901699, 0.80901699, -0.50000000],\n        [0.00000000, 0.52573111, 0.85065081],\n        [-0.30901699, 0.80901699, 0.50000000],\n        [0.30901699, 0.80901699, 0.50000000],\n        [0.50000000, 0.30901699, 0.80901699],\n        [0.50000000, -0.30901699, 0.80901699],\n        [0.00000000, 0.00000000, 1.00000000],\n        [-0.50000000, 0.30901699, 0.80901699],\n        [-0.80901699, 0.50000000, 0.30901699],\n        [-0.80901699, 0.50000000, -0.30901699],\n    ])\n    self.assertTrue(is_same_basis(basis.T, basis_golden.T))\n\n    basis = geopoly.generate_basis('octahedron', 4)\n    basis_golden = np.array([\n        [0.00000000, 0.00000000, -1.00000000],\n        [0.00000000, -0.31622777, -0.94868330],\n        [0.00000000, -0.70710678, -0.70710678],\n        [0.00000000, -0.94868330, -0.31622777],\n        [0.00000000, -1.00000000, 0.00000000],\n        [-0.31622777, 0.00000000, -0.94868330],\n        [-0.40824829, -0.40824829, -0.81649658],\n        [-0.40824829, -0.81649658, -0.40824829],\n        [-0.31622777, -0.94868330, 0.00000000],\n        [-0.70710678, 0.00000000, -0.70710678],\n        [-0.81649658, -0.40824829, -0.40824829],\n        [-0.70710678, -0.70710678, 0.00000000],\n        [-0.94868330, 0.00000000, -0.31622777],\n        [-0.94868330, -0.31622777, 0.00000000],\n        [-1.00000000, 0.00000000, 0.00000000],\n        [0.00000000, -0.31622777, 0.94868330],\n        [0.00000000, -0.70710678, 0.70710678],\n        [0.00000000, -0.94868330, 0.31622777],\n        [0.40824829, -0.40824829, 0.81649658],\n        [0.40824829, -0.81649658, 0.40824829],\n        [0.31622777, -0.94868330, 0.00000000],\n        [0.81649658, -0.40824829, 0.40824829],\n        [0.70710678, -0.70710678, 0.00000000],\n        [0.94868330, -0.31622777, 0.00000000],\n        [0.31622777, 0.00000000, -0.94868330],\n        [0.40824829, 0.40824829, -0.81649658],\n        [0.40824829, 0.81649658, -0.40824829],\n        [0.70710678, 0.00000000, -0.70710678],\n        [0.81649658, 0.40824829, -0.40824829],\n        [0.94868330, 0.00000000, -0.31622777],\n        [0.40824829, -0.40824829, -0.81649658],\n        [0.40824829, -0.81649658, -0.40824829],\n        [0.81649658, -0.40824829, -0.40824829],\n    ])\n    self.assertTrue(is_same_basis(basis.T, basis_golden.T))", "completions": ["  ```\ndef test_generate_basis_golden(self):\n    # Test case 1: Tetrahedron\n    basis = generate_basis('tetrahedron', 1)\n    expected_basis = np.array([\n        (np.sqrt(8 / 9), 0, -1 / 3),\n        (-np.sqrt(2 / 9), np.sqrt(2 / 3), -1 / 3),\n        (-np.sqrt(2 / 9), -np.sqrt(2 / 3), -1 / 3),\n        (0, 0, 1),\n    ])\n    np.testing.assert_allclose(basis, expected_basis)\n\n    # Test case 2: Icosahedron\n    basis = generate_basis('icosahedron', 1)\n    expected_basis = np.array([\n        (-1, 0, a),\n        (1, 0, a),\n        (-1, 0, -a),\n        (1, 0, -a),\n        (0, a, 1),\n        (0, a, -1),\n        (0, -a, 1),\n        (0, -a, -1),\n        (a, 1, 0),\n        (-a, 1, 0),\n        (a, -1, 0),\n        (-a, -1, 0),\n    ]) / np.sqrt(a + 2)\n    np.testing.assert_allclose(basis, expected_basis)\n\n    # Test case 3: Octahedron\n    basis = generate_basis('octahedron', 1)\n    expected_basis = np.array([\n        (0, 0, -1),\n        (0, 0, 1),\n        (0, -1, 0),\n        (0, 1, 0),\n        (-1, 0, 0),\n        (1, 0, 0),\n    ])\n    np.testing.assert_allclose(basis, expected_basis)\n```"]}
{"id": 41, "prompt": "\nYou are an expert Python programmer. You will be given some information and will generate a correct test case for the given function.\nYour response will only be python test function and without anything else.\n\n### focal function:\n```python\n    def t_map(self, t: Union[float, torch.Tensor]) -> torch.Tensor:\n        if not isinstance(t, torch.Tensor):\n            t = torch.Tensor([t]).float()\n\n        t_max = self.t_max.to(t.device)\n        t_min = self.t_min.to(t.device)\n        t_tilde = t_min + (t_max - t_min) * t\n\n        return t_tilde\n```\n###test function signature:\n```python\n\ndef test_noise_schedule_diffusion_coeff(kind):\n```\n### Answer: (use the provided format with backticks)\n", "name": "test_noise_schedule_diffusion_coeff", "reference": "\ndef test_noise_schedule_diffusion_coeff(kind):\n    noise_schedule = GaussianNoiseSchedule(kind=kind, log_snr_range=(-6, 6))\n    ts = torch.linspace(1e-2, 1 - 1e-2, 10)\n    t_map = noise_schedule.t_map(ts)  # map time to the prescribed log_SNR range\n\n    if kind == \"log_snr\":\n        g = noise_schedule.g(ts)\n\n        # compute true beta_t\n        l_range = noise_schedule.log_snr_range\n        snr = torch.exp((1 - t_map) * l_range[1] + t_map * l_range[0])\n        g_true = (-(l_range[0] - l_range[1]) / (snr + 1)).sqrt()\n\n        assert torch.allclose(g, g_true, atol=1e-4)\n\n    if kind == \"ot_linear\":\n        g = noise_schedule.g(ts)\n        g_true = (2.0 * t_map / (1.0 - t_map)).sqrt()\n        assert torch.allclose(g, g_true, atol=1e-4)", "completions": ["  ```\ndef test_noise_schedule_diffusion_coeff(kind):\n    t_min = 0.0\n    t_max = 1.0\n    t = torch.Tensor([0.5]).float()\n    t_tilde = t_min + (t_max - t_min) * t\n    assert t_tilde == 0.5\n```"]}
{"id": 42, "prompt": "\nYou are an expert Python programmer. You will be given some information and will generate a correct test case for the given function.\nYour response will only be python test function and without anything else.\n\n### focal function:\n```python\ndef compute_temperature_features(\n    meter_data_index,\n    temperature_data,\n    heating_balance_points=None,\n    cooling_balance_points=None,\n    data_quality=False,\n    temperature_mean=True,\n    degree_day_method=\"daily\",\n    percent_hourly_coverage_per_day=0.5,\n    percent_hourly_coverage_per_billing_period=0.9,\n    use_mean_daily_values=True,\n    tolerance=None,\n    keep_partial_nan_rows=False,\n):\n    if temperature_data.index.freq != \"H\":\n        raise ValueError(\n            \"temperature_data.index must have hourly frequency (freq='H').\"\n            \" Found: {}\".format(temperature_data.index.freq)\n        )\n\n    if not temperature_data.index.tz:\n        raise ValueError(\n            \"temperature_data.index must be timezone-aware. You can set it with\"\n            \" temperature_data.tz_localize(...).\"\n        )\n\n    if meter_data_index.freq is None and meter_data_index.inferred_freq == \"H\":\n        raise ValueError(\n            \"If you have hourly data explicitly set the frequency\"\n            \" of the dataframe by setting\"\n            \"``meter_data_index.freq =\"\n            \" pd.tseries.frequencies.to_offset('H').\"\n        )\n\n    if not meter_data_index.tz:\n        raise ValueError(\n            \"meter_data_index must be timezone-aware. You can set it with\"\n            \" meter_data.tz_localize(...).\"\n        )\n\n    if meter_data_index.duplicated().any():\n        raise ValueError(\"Duplicates found in input meter trace index.\")\n\n    temp_agg_funcs = []\n    temp_agg_column_renames = {}\n\n    if heating_balance_points is None:\n        heating_balance_points = []\n    if cooling_balance_points is None:\n        cooling_balance_points = []\n\n    if meter_data_index.freq is not None:\n        try:\n            freq_timedelta = pd.Timedelta(meter_data_index.freq)\n        except ValueError:  # freq cannot be converted to timedelta\n            freq_timedelta = None\n    else:\n        freq_timedelta = None\n\n    if tolerance is None:\n        tolerance = freq_timedelta\n\n    if not (heating_balance_points == [] and cooling_balance_points == []):\n        if degree_day_method == \"hourly\":\n            pass\n        elif degree_day_method == \"daily\":\n            if meter_data_index.freq == \"H\":\n                raise ValueError(\n                    \"degree_day_method='daily' must be used with\"\n                    \" daily meter data. Found: 'hourly'\".format(degree_day_method)\n                )\n        else:\n            raise ValueError(\"method not supported: {}\".format(degree_day_method))\n\n    if freq_timedelta == pd.Timedelta(\"1H\"):\n        # special fast route for hourly data.\n        df = temperature_data.to_frame(\"temperature_mean\").reindex(meter_data_index)\n\n        if use_mean_daily_values:\n            n_days = 1\n        else:\n            n_days = 1.0 / 24.0\n\n        df = df.assign(\n            **{\n                \"cdd_{}\".format(bp): np.maximum(df.temperature_mean - bp, 0) * n_days\n                for bp in cooling_balance_points\n            }\n        )\n        df = df.assign(\n            **{\n                \"hdd_{}\".format(bp): np.maximum(bp - df.temperature_mean, 0) * n_days\n                for bp in heating_balance_points\n            }\n        )\n        df = df.assign(\n            n_hours_dropped=df.temperature_mean.isnull().astype(int),\n            n_hours_kept=df.temperature_mean.notnull().astype(int),\n        )\n        # TODO(philngo): bad interface or maybe this is just wrong for some reason?\n        if data_quality:\n            df = df.assign(\n                temperature_null=df.n_hours_dropped,\n                temperature_not_null=df.n_hours_kept,\n            )\n        if not temperature_mean:\n            del df[\"temperature_mean\"]\n    else:\n        # daily/billing route\n        # heating/cooling degree day aggregations. Needed for n_days fields as well.\n        temp_agg_funcs.extend(\n            _degree_day_columns(\n                heating_balance_points=heating_balance_points,\n                cooling_balance_points=cooling_balance_points,\n                degree_day_method=degree_day_method,\n                percent_hourly_coverage_per_day=percent_hourly_coverage_per_day,\n                percent_hourly_coverage_per_billing_period=percent_hourly_coverage_per_billing_period,\n                use_mean_daily_values=use_mean_daily_values,\n            )\n        )\n        temp_agg_column_renames.update(\n            {(\"temp\", \"degree_day_columns\"): \"degree_day_columns\"}\n        )\n\n        if data_quality:\n            temp_agg_funcs.extend(\n                [(\"not_null\", \"count\"), (\"null\", lambda x: x.isnull().sum())]\n            )\n            temp_agg_column_renames.update(\n                {\n                    (\"temp\", \"not_null\"): \"temperature_not_null\",\n                    (\"temp\", \"null\"): \"temperature_null\",\n                }\n            )\n\n        if temperature_mean:\n            temp_agg_funcs.extend([(\"mean\", \"mean\")])\n            temp_agg_column_renames.update({(\"temp\", \"mean\"): \"temperature_mean\"})\n\n        # aggregate temperatures\n        temp_df = temperature_data.to_frame(\"temp\")\n        temp_groups = _matching_groups(meter_data_index, temp_df, tolerance)\n        temp_aggregations = temp_groups.agg({\"temp\": temp_agg_funcs})\n\n        # expand temp aggregations by faking and deleting the `meter_value` column.\n        # I haven't yet figured out a way to avoid this and get the desired\n        # structure and behavior. (philngo)\n        meter_value = pd.DataFrame({\"meter_value\": 0}, index=meter_data_index)\n        df = pd.concat([meter_value, temp_aggregations], axis=1).rename(\n            columns=temp_agg_column_renames\n        )\n        del df[\"meter_value\"]\n\n        if \"degree_day_columns\" in df:\n            if df[\"degree_day_columns\"].dropna().empty:\n                column_defaults = {\n                    column: np.full(df[\"degree_day_columns\"].shape, np.nan)\n                    for column in [\"n_days_dropped\", \"n_days_kept\"]\n                }\n                df = df.drop([\"degree_day_columns\"], axis=1).assign(**column_defaults)\n            else:\n                df = pd.concat(\n                    [\n                        df.drop([\"degree_day_columns\"], axis=1),\n                        df[\"degree_day_columns\"].dropna().apply(pd.Series),\n                    ],\n                    axis=1,\n                )\n\n    if not keep_partial_nan_rows:\n        df = overwrite_partial_rows_with_nan(df)\n\n    # nan last row\n    df = df.iloc[:-1].reindex(df.index)\n    return df\n```\n###test function signature:\n```python\n\ndef test_compute_temperature_features_billing_monthly_data_quality(\n    il_electricity_cdd_hdd_billing_monthly,\n):\n```\n### Answer: (use the provided format with backticks)\n", "name": "test_compute_temperature_features_billing_monthly_data_quality", "reference": "\ndef test_compute_temperature_features_billing_monthly_data_quality(\n    il_electricity_cdd_hdd_billing_monthly,\n):\n    meter_data = il_electricity_cdd_hdd_billing_monthly[\"meter_data\"]\n    temperature_data = il_electricity_cdd_hdd_billing_monthly[\"temperature_data\"]\n    df = compute_temperature_features(\n        meter_data.index, temperature_data, temperature_mean=False, data_quality=True\n    )\n    assert df.shape == (27, 4)\n    assert list(sorted(df.columns)) == [\n        \"n_days_dropped\",\n        \"n_days_kept\",\n        \"temperature_not_null\",\n        \"temperature_null\",\n    ]\n    assert round(df.temperature_not_null.mean(), 2) == 729.23\n    assert round(df.temperature_null.mean(), 2) == 0.0", "completions": ["  ```\ndef test_compute_temperature_features_billing_monthly_data_quality(\n    il_electricity_cdd_hdd_billing_monthly,\n):\n    meter_data_index = il_electricity_cdd_hdd_billing_monthly.index\n    temperature_data = il_electricity_cdd_hdd_billing_monthly[\"temperature\"]\n    heating_balance_points = [65]\n    cooling_balance_points = [65]\n    data_quality = True\n    temperature_mean = True\n    degree_day_method = \"daily\"\n    percent_hourly_coverage_per_day = 0.5\n    percent_hourly_coverage_per_billing_period = 0.9\n    use_mean_daily_values = True\n    tolerance = None\n    keep_partial_nan_rows = False\n\n    expected_output = pd.DataFrame(\n        {\n            \"temperature_mean\": [65.0, 65.0, 65.0, 65.0, 65.0, 65.0, 65.0, 65.0, 65.0, 65.0],\n            \"cdd_65\": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n            \"hdd_65\": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n            \"n_hours_dropped\": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n            \"n_hours_kept\": [24, 24, 24, 24, 24, 24, 24, 24, 24, 24],\n            \"temperature_null\": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n            \"temperature_not_null\": [24, 24, 24, 24, 24, 24, 24, 24, 24, 24],\n        },\n        index=meter_data_index,\n    )\n\n    output = compute_temperature_features(\n        meter_data_index,\n        temperature_data,\n        heating_balance_points,\n        cooling_balance_points,\n        data_quality,\n        temperature_mean,\n        degree_day_method,\n        percent_hourly_coverage_per_day,\n        percent_hourly_coverage_per_billing_period,\n        use_mean_daily_values,\n        tolerance,\n        keep_partial_nan_rows,\n    )\n\n    pd.testing.assert_frame_equal(output, expected_output)\n```"]}
{"id": 43, "prompt": "\nYou are an expert Python programmer. You will be given some information and will generate a correct test case for the given function.\nYour response will only be python test function and without anything else.\n\n### focal function:\n```python\n\n    def unload_model(self, model_id: str) -> RegisteredModels:\n        self.__ensure_v1_client_mode()\n        de_aliased_model_id = resolve_roboflow_model_alias(model_id=model_id)\n        response = requests.post(\n            f\"{self.__api_url}/model/remove\",\n            json={\n                \"model_id\": de_aliased_model_id,\n            },\n            headers=DEFAULT_HEADERS,\n        )\n        response.raise_for_status()\n        response_payload = response.json()\n        if (\n            de_aliased_model_id == self.__selected_model\n            or model_id == self.__selected_model\n        ):\n            self.__selected_model = None\n        return RegisteredModels.from_dict(response_payload)\n```\n###test function signature:\n```python\n\ndef test_client_unload_single_model_when_successful_response_expected(\n    requests_mock: Mocker,\n) -> None:\n    # given\n```\n### Answer: (use the provided format with backticks)\n", "name": "test_client_unload_single_model_when_successful_response_expected", "reference": "\ndef test_client_unload_single_model_when_successful_response_expected(\n    requests_mock: Mocker,\n) -> None:\n    # given\n    api_url = \"http://some.com\"\n    requests_mock.post(\n        f\"{api_url}/model/remove\",\n        json={\"models\": [{\"model_id\": \"some/1\", \"task_type\": \"classification\"}]},\n    )\n    http_client = InferenceHTTPClient(api_key=\"my-api-key\", api_url=api_url)\n\n    # when\n    result = http_client.unload_model(model_id=\"some/1\")\n\n    # then\n    assert result == RegisteredModels(\n        models=[ModelDescription(model_id=\"some/1\", task_type=\"classification\")]\n    )\n    assert requests_mock.last_request.json() == {\n        \"model_id\": \"some/1\",\n    }", "completions": ["  ```\ndef test_client_unload_single_model_when_successful_response_expected(\n    requests_mock: Mocker,\n) -> None:\n    # given\n    model_id = \"model_1\"\n    de_aliased_model_id = \"model_1\"\n    response_payload = {\n        \"model_id\": de_aliased_model_id,\n        \"status\": \"success\",\n    }\n    requests_mock.post(\n        f\"{self.__api_url}/model/remove\",\n        json=response_payload,\n        headers=DEFAULT_HEADERS,\n    )\n\n    # when\n    result = unload_model(model_id)\n\n    # then\n    assert result.status == \"success\"\n    assert result.model_id == de_aliased_model_id\n    assert self.__selected_model is None\n```"]}
{"id": 44, "prompt": "\nYou are an expert Python programmer. You will be given some information and will generate a correct test case for the given function.\nYour response will only be python test function and without anything else.\n\n### focal function:\n```python\n\ndef generate_unit_locations(\n    num_units,\n    channel_locations,\n    margin_um=20.0,\n    minimum_z=5.0,\n    maximum_z=40.0,\n    minimum_distance=20.0,\n    max_iteration=100,\n    distance_strict=False,\n    seed=None,\n):\n    rng = np.random.default_rng(seed=seed)\n    units_locations = np.zeros((num_units, 3), dtype=\"float32\")\n\n    minimum_x, maximum_x = np.min(channel_locations[:, 0]) - margin_um, np.max(channel_locations[:, 0]) + margin_um\n    minimum_y, maximum_y = np.min(channel_locations[:, 1]) - margin_um, np.max(channel_locations[:, 1]) + margin_um\n\n    units_locations[:, 0] = rng.uniform(minimum_x, maximum_x, size=num_units)\n    units_locations[:, 1] = rng.uniform(minimum_y, maximum_y, size=num_units)\n    units_locations[:, 2] = rng.uniform(minimum_z, maximum_z, size=num_units)\n\n    if minimum_distance is not None:\n        solution_found = False\n        renew_inds = None\n        for i in range(max_iteration):\n            distances = np.linalg.norm(units_locations[:, np.newaxis] - units_locations[np.newaxis, :], axis=2)\n            inds0, inds1 = np.nonzero(distances < minimum_distance)\n            mask = inds0 != inds1\n            inds0 = inds0[mask]\n            inds1 = inds1[mask]\n\n            if inds0.size > 0:\n                if renew_inds is None:\n                    renew_inds = np.unique(inds0)\n                else:\n                    # random only bad ones in the previous set\n                    renew_inds = renew_inds[np.isin(renew_inds, np.unique(inds0))]\n\n                units_locations[:, 0][renew_inds] = rng.uniform(minimum_x, maximum_x, size=renew_inds.size)\n                units_locations[:, 1][renew_inds] = rng.uniform(minimum_y, maximum_y, size=renew_inds.size)\n                units_locations[:, 2][renew_inds] = rng.uniform(minimum_z, maximum_z, size=renew_inds.size)\n            else:\n                solution_found = True\n                break\n\n    if not solution_found:\n        if distance_strict:\n            raise ValueError(\n                f\"generate_unit_locations(): no solution for {minimum_distance=} and {max_iteration=} \"\n                \"You can use distance_strict=False or reduce minimum distance\"\n            )\n        else:\n            warnings.warn(f\"generate_unit_locations(): no solution for {minimum_distance=} and {max_iteration=}\")\n\n    return units_locations\n```\n###test function signature:\n```python\n\ndef test_generate_unit_locations():\n```\n### Answer: (use the provided format with backticks)\n", "name": "test_generate_unit_locations", "reference": "\ndef test_generate_unit_locations():\n    seed = 0\n\n    probe = generate_multi_columns_probe(num_columns=2, num_contact_per_column=20, xpitch=20, ypitch=20)\n    channel_locations = probe.contact_positions\n\n    num_units = 100\n    minimum_distance = 20.0\n    unit_locations = generate_unit_locations(\n        num_units,\n        channel_locations,\n        margin_um=20.0,\n        minimum_z=5.0,\n        maximum_z=40.0,\n        minimum_distance=minimum_distance,\n        max_iteration=500,\n        distance_strict=False,\n        seed=seed,\n    )\n    distances = np.linalg.norm(unit_locations[:, np.newaxis] - unit_locations[np.newaxis, :], axis=2)\n    dist_flat = np.triu(distances, k=1).flatten()\n    dist_flat = dist_flat[dist_flat > 0]\n    assert np.all(dist_flat > minimum_distance)\n\n    # import matplotlib.pyplot as plt\n    # fig, ax = plt.subplots()\n    # ax.hist(dist_flat, bins = np.arange(0, 400, 10))\n    # fig, ax = plt.subplots()\n    # from probeinterface.plotting import plot_probe\n    # plot_probe(probe, ax=ax)\n    # ax.scatter(unit_locations[:, 0], unit_locations[:,1], marker='*', s=20)\n    # plt.show()", "completions": ["  ```\ndef test_generate_unit_locations():\n    num_units = 10\n    channel_locations = np.array([[1, 2], [3, 4], [5, 6]])\n    margin_um = 20.0\n    minimum_z = 5.0\n    maximum_z = 40.0\n    minimum_distance = 20.0\n    max_iteration = 100\n    distance_strict = False\n    seed = None\n\n    units_locations = generate_unit_locations(\n        num_units, channel_locations, margin_um, minimum_z, maximum_z, minimum_distance, max_iteration, distance_strict, seed\n    )\n\n    assert units_locations.shape == (num_units, 3)\n    assert np.all(units_locations[:, 0] >= np.min(channel_locations[:, 0]) - margin_um)\n    assert np.all(units_locations[:, 0] <= np.max(channel_locations[:, 0]) + margin_um)\n    assert np.all(units_locations[:, 1] >= np.min(channel_locations[:, 1]) - margin_um)\n    assert np.all(units_locations[:, 1] <= np.max(channel_locations[:, 1]) + margin_um)\n    assert np.all(units_locations[:, 2] >= minimum_z)\n    assert np.all(units_locations[:, 2] <= maximum_z)\n\n    distances = np.linalg.norm(units_locations[:, np.newaxis] - units_locations[np.newaxis, :], axis=2)\n    assert np.all(distances >= minimum_distance)\n```"]}
{"id": 45, "prompt": "\nYou are an expert Python programmer. You will be given some information and will generate a correct test case for the given function.\nYour response will only be python test function and without anything else.\n\n### focal function:\n```python\n    def to_dataframe(\n        self, index: Optional[Union[str, None]] = \"date\", sort_by: Optional[str] = None\n    ) -> pd.DataFrame:\n\n        def is_list_of_basemodel(items: Union[List[T], T]) -> bool:\n            return isinstance(items, list) and all(\n                isinstance(item, BaseModel) for item in items\n            )\n\n        if self.results is None or not self.results:\n            raise OpenBBError(\"Results not found.\")\n\n        if isinstance(self.results, pd.DataFrame):\n            return self.results\n\n        try:\n            res = self.results\n            df = None\n            sort_columns = True\n\n            # List[Dict]\n            if isinstance(res, list) and len(res) == 1 and isinstance(res[0], dict):\n                r = res[0]\n                dict_of_df = {}\n\n                for k, v in r.items():\n                    # Dict[str, List[BaseModel]]\n                    if is_list_of_basemodel(v):\n                        dict_of_df[k] = basemodel_to_df(v, index)\n                        sort_columns = False\n                    # Dict[str, Any]\n                    else:\n                        dict_of_df[k] = pd.DataFrame(v)\n\n                df = pd.concat(dict_of_df, axis=1)\n\n            # List[BaseModel]\n            elif is_list_of_basemodel(res):\n                dt: Union[List[Data], Data] = res  # type: ignore\n                df = basemodel_to_df(dt, index)\n                sort_columns = False\n            # List[List | str | int | float] | Dict[str, Dict | List | BaseModel]\n            else:\n                try:\n                    df = pd.DataFrame(res)\n                    # Set index, if any\n                    if index is not None and index in df.columns:\n                        df.set_index(index, inplace=True)\n\n                except ValueError:\n                    if isinstance(res, dict):\n                        df = pd.DataFrame([res])\n\n            if df is None:\n                raise OpenBBError(\"Unsupported data format.\")\n\n            # Drop columns that are all NaN, but don't rearrange columns\n            if sort_columns:\n                df.sort_index(axis=1, inplace=True)\n            df = df.dropna(axis=1, how=\"all\")\n\n            # Sort by specified column\n            if sort_by:\n                df.sort_values(by=sort_by, inplace=True)\n\n        except OpenBBError as e:\n            raise e\n        except ValueError as ve:\n            raise OpenBBError(\n                f\"ValueError: {ve}. Ensure the data format matches the expected format.\"\n            ) from ve\n        except TypeError as te:\n            raise OpenBBError(\n                f\"TypeError: {te}. Check the data types in your results.\"\n            ) from te\n        except Exception as ex:\n            raise OpenBBError(f\"An unexpected error occurred: {ex}\") from ex\n\n        return df\n```\n###test function signature:\n```python\ndef test_to_dataframe_w_args(results, index, sort_by):\n    # Arrange\n```\n### Answer: (use the provided format with backticks)\n", "name": "test_to_dataframe_w_args", "reference": "def test_to_dataframe_w_args(results, index, sort_by):\n    # Arrange\n    co = OBBject(results=results)\n\n    # Act and Assert\n    result = co.to_dataframe(index=index, sort_by=sort_by)\n    assert isinstance(result, pd.DataFrame)\n    assert result.index.name == index\n\n    # check if dataframe is properly sorted\n    assert result[sort_by].is_monotonic_increasing", "completions": ["  ```\ndef test_to_dataframe_w_args(results, index, sort_by):\n    # Arrange\n    expected_df = pd.DataFrame(results)\n    expected_df.set_index(index, inplace=True)\n    expected_df.sort_index(axis=1, inplace=True)\n    expected_df.dropna(axis=1, how=\"all\", inplace=True)\n    if sort_by:\n        expected_df.sort_values(by=sort_by, inplace=True)\n\n    # Act\n    actual_df = to_dataframe(results, index, sort_by)\n\n    # Assert\n    assert_frame_equal(actual_df, expected_df)\n```"]}
{"id": 46, "prompt": "\nYou are an expert Python programmer. You will be given some information and will generate a correct test case for the given function.\nYour response will only be python test function and without anything else.\n\n### focal function:\n```python\ndef unique(ar, return_index=False, return_inverse=False,\n           return_counts=False, axis=None, *, equal_nan=True):\n    ar = np.asanyarray(ar)\n    if axis is None:\n        ret = _unique1d(ar, return_index, return_inverse, return_counts, \n                        equal_nan=equal_nan)\n        return _unpack_tuple(ret)\n\n    # axis was specified and not None\n    try:\n        ar = np.moveaxis(ar, axis, 0)\n    except np.AxisError:\n        # this removes the \"axis1\" or \"axis2\" prefix from the error message\n        raise np.AxisError(axis, ar.ndim) from None\n\n    # Must reshape to a contiguous 2D array for this to work...\n    orig_shape, orig_dtype = ar.shape, ar.dtype\n    ar = ar.reshape(orig_shape[0], np.prod(orig_shape[1:], dtype=np.intp))\n    ar = np.ascontiguousarray(ar)\n    dtype = [('f{i}'.format(i=i), ar.dtype) for i in range(ar.shape[1])]\n\n    # At this point, `ar` has shape `(n, m)`, and `dtype` is a structured\n    # data type with `m` fields where each field has the data type of `ar`.\n    # In the following, we create the array `consolidated`, which has\n    # shape `(n,)` with data type `dtype`.\n    try:\n        if ar.shape[1] > 0:\n            consolidated = ar.view(dtype)\n        else:\n            # If ar.shape[1] == 0, then dtype will be `np.dtype([])`, which is\n            # a data type with itemsize 0, and the call `ar.view(dtype)` will\n            # fail.  Instead, we'll use `np.empty` to explicitly create the\n            # array with shape `(len(ar),)`.  Because `dtype` in this case has\n            # itemsize 0, the total size of the result is still 0 bytes.\n            consolidated = np.empty(len(ar), dtype=dtype)\n    except TypeError as e:\n        # There's no good way to do this for object arrays, etc...\n        msg = 'The axis argument to unique is not supported for dtype {dt}'\n        raise TypeError(msg.format(dt=ar.dtype)) from e\n\n    def reshape_uniq(uniq):\n        n = len(uniq)\n        uniq = uniq.view(orig_dtype)\n        uniq = uniq.reshape(n, *orig_shape[1:])\n        uniq = np.moveaxis(uniq, 0, axis)\n        return uniq\n\n    output = _unique1d(consolidated, return_index,\n                       return_inverse, return_counts, equal_nan=equal_nan)\n    output = (reshape_uniq(output[0]),) + output[1:]\n    return _unpack_tuple(output)\n```\n###test function signature:\n```python\n\n    def test_unique_1d(self):\n```\n### Answer: (use the provided format with backticks)\n", "name": "test_unique_1d", "reference": "\n    def test_unique_1d(self):\n\n        def check_all(a, b, i1, i2, c, dt):\n            base_msg = 'check {0} failed for type {1}'\n\n            msg = base_msg.format('values', dt)\n            v = unique(a)\n            assert_array_equal(v, b, msg)\n\n            msg = base_msg.format('return_index', dt)\n            v, j = unique(a, True, False, False)\n            assert_array_equal(v, b, msg)\n            assert_array_equal(j, i1, msg)\n\n            msg = base_msg.format('return_inverse', dt)\n            v, j = unique(a, False, True, False)\n            assert_array_equal(v, b, msg)\n            assert_array_equal(j, i2, msg)\n\n            msg = base_msg.format('return_counts', dt)\n            v, j = unique(a, False, False, True)\n            assert_array_equal(v, b, msg)\n            assert_array_equal(j, c, msg)\n\n            msg = base_msg.format('return_index and return_inverse', dt)\n            v, j1, j2 = unique(a, True, True, False)\n            assert_array_equal(v, b, msg)\n            assert_array_equal(j1, i1, msg)\n            assert_array_equal(j2, i2, msg)\n\n            msg = base_msg.format('return_index and return_counts', dt)\n            v, j1, j2 = unique(a, True, False, True)\n            assert_array_equal(v, b, msg)\n            assert_array_equal(j1, i1, msg)\n            assert_array_equal(j2, c, msg)\n\n            msg = base_msg.format('return_inverse and return_counts', dt)\n            v, j1, j2 = unique(a, False, True, True)\n            assert_array_equal(v, b, msg)\n            assert_array_equal(j1, i2, msg)\n            assert_array_equal(j2, c, msg)\n\n            msg = base_msg.format(('return_index, return_inverse '\n                                   'and return_counts'), dt)\n            v, j1, j2, j3 = unique(a, True, True, True)\n            assert_array_equal(v, b, msg)\n            assert_array_equal(j1, i1, msg)\n            assert_array_equal(j2, i2, msg)\n            assert_array_equal(j3, c, msg)\n\n        a = [5, 7, 1, 2, 1, 5, 7]*10\n        b = [1, 2, 5, 7]\n        i1 = [2, 3, 0, 1]\n        i2 = [2, 3, 0, 1, 0, 2, 3]*10\n        c = np.multiply([2, 1, 2, 2], 10)\n\n        # test for numeric arrays\n        types = []\n        types.extend(np.typecodes['AllInteger'])\n        types.extend(np.typecodes['AllFloat'])\n        types.append('datetime64[D]')\n        types.append('timedelta64[D]')\n        for dt in types:\n            aa = np.array(a, dt)\n            bb = np.array(b, dt)\n            check_all(aa, bb, i1, i2, c, dt)\n\n        # test for object arrays\n        dt = 'O'\n        aa = np.empty(len(a), dt)\n        aa[:] = a\n        bb = np.empty(len(b), dt)\n        bb[:] = b\n        check_all(aa, bb, i1, i2, c, dt)\n\n        # test for structured arrays\n        dt = [('', 'i'), ('', 'i')]\n        aa = np.array(list(zip(a, a)), dt)\n        bb = np.array(list(zip(b, b)), dt)\n        check_all(aa, bb, i1, i2, c, dt)\n\n        # test for ticket #2799\n        aa = [1. + 0.j, 1 - 1.j, 1]\n        assert_array_equal(np.unique(aa), [1. - 1.j, 1. + 0.j])\n\n        # test for ticket #4785\n        a = [(1, 2), (1, 2), (2, 3)]\n        unq = [1, 2, 3]\n        inv = [0, 1, 0, 1, 1, 2]\n        a1 = unique(a)\n        assert_array_equal(a1, unq)\n        a2, a2_inv = unique(a, return_inverse=True)\n        assert_array_equal(a2, unq)\n        assert_array_equal(a2_inv, inv)\n\n        # test for chararrays with return_inverse (gh-5099)\n        a = np.chararray(5)\n        a[...] = ''\n        a2, a2_inv = np.unique(a, return_inverse=True)\n        assert_array_equal(a2_inv, np.zeros(5))\n\n        # test for ticket #9137\n        a = []\n        a1_idx = np.unique(a, return_index=True)[1]\n        a2_inv = np.unique(a, return_inverse=True)[1]\n        a3_idx, a3_inv = np.unique(a, return_index=True,\n                                   return_inverse=True)[1:]\n        assert_equal(a1_idx.dtype, np.intp)\n        assert_equal(a2_inv.dtype, np.intp)\n        assert_equal(a3_idx.dtype, np.intp)\n        assert_equal(a3_inv.dtype, np.intp)\n\n        # test for ticket 2111 - float\n        a = [2.0, np.nan, 1.0, np.nan]\n        ua = [1.0, 2.0, np.nan]\n        ua_idx = [2, 0, 1]\n        ua_inv = [1, 2, 0, 2]\n        ua_cnt = [1, 1, 2]\n        assert_equal(np.unique(a), ua)\n        assert_equal(np.unique(a, return_index=True), (ua, ua_idx))\n        assert_equal(np.unique(a, return_inverse=True), (ua, ua_inv))\n        assert_equal(np.unique(a, return_counts=True), (ua, ua_cnt))\n\n        # test for ticket 2111 - complex\n        a = [2.0-1j, np.nan, 1.0+1j, complex(0.0, np.nan), complex(1.0, np.nan)]\n        ua = [1.0+1j, 2.0-1j, complex(0.0, np.nan)]\n        ua_idx = [2, 0, 3]\n        ua_inv = [1, 2, 0, 2, 2]\n        ua_cnt = [1, 1, 3]\n        assert_equal(np.unique(a), ua)\n        assert_equal(np.unique(a, return_index=True), (ua, ua_idx))\n        assert_equal(np.unique(a, return_inverse=True), (ua, ua_inv))\n        assert_equal(np.unique(a, return_counts=True), (ua, ua_cnt))\n\n        # test for ticket 2111 - datetime64\n        nat = np.datetime64('nat')\n        a = [np.datetime64('2020-12-26'), nat, np.datetime64('2020-12-24'), nat]\n        ua = [np.datetime64('2020-12-24'), np.datetime64('2020-12-26'), nat]\n        ua_idx = [2, 0, 1]\n        ua_inv = [1, 2, 0, 2]\n        ua_cnt = [1, 1, 2]\n        assert_equal(np.unique(a), ua)\n        assert_equal(np.unique(a, return_index=True), (ua, ua_idx))\n        assert_equal(np.unique(a, return_inverse=True), (ua, ua_inv))\n        assert_equal(np.unique(a, return_counts=True), (ua, ua_cnt))\n\n        # test for ticket 2111 - timedelta\n        nat = np.timedelta64('nat')\n        a = [np.timedelta64(1, 'D'), nat, np.timedelta64(1, 'h'), nat]\n        ua = [np.timedelta64(1, 'h'), np.timedelta64(1, 'D'), nat]\n        ua_idx = [2, 0, 1]\n        ua_inv = [1, 2, 0, 2]\n        ua_cnt = [1, 1, 2]\n        assert_equal(np.unique(a), ua)\n        assert_equal(np.unique(a, return_index=True), (ua, ua_idx))\n        assert_equal(np.unique(a, return_inverse=True), (ua, ua_inv))\n        assert_equal(np.unique(a, return_counts=True), (ua, ua_cnt))\n\n        # test for gh-19300\n        all_nans = [np.nan] * 4\n        ua = [np.nan]\n        ua_idx = [0]\n        ua_inv = [0, 0, 0, 0]\n        ua_cnt = [4]\n        assert_equal(np.unique(all_nans), ua)\n        assert_equal(np.unique(all_nans, return_index=True), (ua, ua_idx))\n        assert_equal(np.unique(all_nans, return_inverse=True), (ua, ua_inv))\n        assert_equal(np.unique(all_nans, return_counts=True), (ua, ua_cnt))", "completions": ["  ```\ndef test_unique_1d():\n    ar = np.array([1, 2, 3, 2, 1])\n    expected_output = (np.array([1, 2, 3]), np.array([0, 1, 2]), np.array([2, 1, 1]))\n    output = unique(ar, return_index=True, return_inverse=True, return_counts=True)\n    np.testing.assert_equal(output, expected_output)\n```"]}
{"id": 47, "prompt": "\nYou are an expert Python programmer. You will be given some information and will generate a correct test case for the given function.\nYour response will only be python test function and without anything else.\n\n### focal function:\n```python\n\ndef scaled_dot_product_attention(\n    q: torch.Tensor,\n    k: torch.Tensor,\n    v: torch.Tensor,\n    att_mask: Optional[Union[AttentionMask, \"SparseCS\", torch.Tensor]],\n    dropout: Optional[torch.nn.Module] = None,\n    block_size: int = 128,\n) -> torch.Tensor:\n    autocast_disabled = (\n        _has_cpp_library\n        and isinstance(att_mask, SparseCS)\n        or (att_mask is not None and att_mask.is_sparse)\n    )\n    seq_len = q.shape[-2]\n\n    # switch if:\n    #   causal is required but mask is not sparse\n    #   fp16 or under amp context\n    #   sequence length is divisible by block size\n    #   same seq len for K and Q\n    switch_to_blocksparse = (\n        _is_blocksparse_available\n        and (att_mask is not None and not att_mask.is_sparse)\n        and (isinstance(att_mask, AttentionMask) and att_mask.is_causal)\n        and (q.dtype == torch.float16 or torch.is_autocast_enabled())\n        and not seq_len % block_size\n        and q.shape[-2] == k.shape[-2]\n    )\n\n    if switch_to_blocksparse:\n        logger.info(\"Switching causal attention to Triton blocksparse...\")\n        return blocksparse_attention(q, k, v, dropout, block_size)\n\n    with torch.cuda.amp.autocast(enabled=False) if autocast_disabled else nullcontext():  # type: ignore\n        if autocast_disabled:\n            q, k, v = q.float(), k.float(), v.float()\n\n        att = scaled_query_key_softmax(q, k, att_mask=att_mask)\n\n        #  Optional dropout, could be part of the masking in the future\n        att = _apply_dropout(att, dropout)\n\n        # Get to the predicted values, for all heads\n        # y = att @ v  # (N, S, S) x (N, S, hs) -> (N, S, hs)\n        y = bmm(att, v)\n    return y\n```\n###test function signature:\n```python\n\ndef test_amp_attention_dense(device):\n```\n### Answer: (use the provided format with backticks)\n", "name": "test_amp_attention_dense", "reference": "\ndef test_amp_attention_dense(device):\n    b, s, d = 8, 64, 32\n    prob = 0.9\n\n    a = torch.rand(b, s, d, device=device)\n    m = torch.rand(s, s, device=device) > prob\n\n    with torch.cuda.amp.autocast():\n        r = scaled_dot_product_attention(a, a, a, m)\n\n    expected_device = torch.float16 if device == \"cuda\" else torch.float32\n    assert r.dtype == expected_device", "completions": ["  ```\ndef test_amp_attention_dense(device):\n    q = torch.randn(1, 128, 128, device=device)\n    k = torch.randn(1, 128, 128, device=device)\n    v = torch.randn(1, 128, 128, device=device)\n    att_mask = torch.ones(1, 128, 128, device=device)\n    dropout = torch.nn.Dropout(0.1)\n    block_size = 128\n\n    y = scaled_dot_product_attention(q, k, v, att_mask, dropout, block_size)\n\n    assert y.shape == (1, 128, 128)\n    assert y.device == device\n ```"]}
{"id": 48, "prompt": "\nYou are an expert Python programmer. You will be given some information and will generate a correct test case for the given function.\nYour response will only be python test function and without anything else.\n\n### focal function:\n```python\n\n    def quantize(\n        cls,\n        tensor: Tensor,\n        nbits: int = 4,\n        channel_wise: bool = True,\n        group_size: int = 64,\n        optimize: bool = False,\n        round_zero: bool = False,\n        axis: int = 0,\n        bitpack: bool = True,\n        compute_dtype: torch.dtype | None = None,\n        view_as_float: bool = False,\n        device: str = \"cuda\",\n    ) -> tuple:\n        assert nbits in Quantizer.SUPPORTED_BITS, (\n            \"nbits=\" + str(nbits) + \" not supported.\"\n        )\n        assert axis in [0, 1], \"axis should be either 0 or 1\"\n        if group_size is not None:\n            assert is_divisible(tensor.numel(), group_size), (\n                \"group_size should be divisble by the total tensor dimensions. shape: \"\n                + str(tensor.shape)\n                + \", group_size: \"\n                + str(group_size)\n            )\n\n        W = tensor.float()\n        shape = W.shape\n\n        # Reshape for grouping\n        if (group_size is not None) and channel_wise:\n            W = (\n                W.reshape([-1, group_size])\n                if (axis == 1)\n                else W.reshape([group_size, -1])\n            )\n\n        # Get min/max values\n        if not channel_wise:\n            _min, _max = W.min(), W.max()\n            optimize = False\n        else:\n            _min = W.min(axis=axis, keepdim=True)[0]\n            _max = W.max(axis=axis, keepdim=True)[0]\n\n        max_v = 2**nbits - 1\n        min_v = 0\n        min_max = [min_v, max_v]\n\n        # Note: here we work with the inverse of the scale to avoid division and quantize instead via W*scale + zero, the scale is inverted later on.\n        scale = (max_v / (_max - _min)).clamp(\n            max=2e4\n        )  # clamp to avoid half-precision problems\n        zero = -_min * scale\n\n        # Round zero as in: https://github.com/casper-hansen/AutoAWQ/blob/main/awq/quantize/quantizer.py#L42C9-L42C14\n        if round_zero:\n            zero = torch.round(zero)\n\n        # Fine-tune weights\n        if optimize:\n            W_q, scale, zero = Quantizer.optimize_weights(\n                tensor=W,\n                scale=scale,\n                zero=zero,\n                min_max=min_max,\n                axis=axis,\n                device=device,\n            )\n        else:\n            W_q = torch.round(W * scale + zero).clamp(min_max[0], min_max[1])\n\n        # Store meta-data (we invert the scale for dequantization)\n        meta = {\n            \"nbits\": nbits,\n            \"group_size\": group_size,\n            \"shape\": shape,\n            \"scale\": 1.0 / scale,\n            \"zero\": zero,\n            \"axis\": axis,\n            \"packing\": Quantizer.bit_to_packing[nbits],\n        }\n        meta[\"unpack_view_dtype\"] = Quantizer.unpack_view_dtype[meta[\"packing\"]]\n\n        # Pack bits\n        meta[\"view_as_float\"] = view_as_float\n        if bitpack:\n            W_q = Quantizer.pack[meta[\"packing\"]](W_q)\n            if view_as_float:\n                W_q = W_q.view(\n                    torch.float32 if compute_dtype is None else compute_dtype\n                )  # store quantized weights as compute_dtype\n        else:\n            W_q = W_q.to(tensor.dtype)\n            meta[\"packing\"] = None\n\n        # cleanup\n        del W, _min, _max\n        torch.cuda.empty_cache()\n\n        return W_q, meta\n```\n###test function signature:\n```python\n\n    def test_quantizer(self):\n```\n### Answer: (use the provided format with backticks)\n", "name": "test_quantizer", "reference": "\n    def test_quantizer(self):    \n        for compute_dtype in [torch.float32, torch.float16, torch.bfloat16]:\n            for nbits in [8,4,3,2,1]:\n                W_q, meta = Quantizer.quantize(self.w, nbits=nbits, round_zero=True, optimize=True, view_as_float=False)\n                if nbits == 3:\n                    assert W_q.dtype == torch.int32\n                else:\n                    assert W_q.dtype == torch.uint8\n                w_dq = Quantizer.dequantize(W_q, meta)\n                norm1 = torch.norm(self.w - w_dq, p=0.7)\n                \n                W_q, meta = Quantizer.quantize(self.w, nbits=nbits, round_zero=True, optimize=True, compute_dtype=compute_dtype, view_as_float=True)\n                assert W_q.dtype == compute_dtype\n                w_dq = Quantizer.dequantize(W_q, meta)\n                norm2 = torch.norm(self.w - w_dq, p=0.7)\n                \n                self.assertTrue(torch.equal(norm1, norm2))", "completions": ["  ```\ndef test_quantizer(self):\n    # Test case 1: nbits=4, channel_wise=True, group_size=64, optimize=False, round_zero=False, axis=0, bitpack=True, compute_dtype=None, view_as_float=False, device=\"cuda\"\n    tensor = torch.randn(1, 3, 32, 32)\n    nbits = 4\n    channel_wise = True\n    group_size = 64\n    optimize = False\n    round_zero = False\n    axis = 0\n    bitpack = True\n    compute_dtype = None\n    view_as_float = False\n    device = \"cuda\"\n    W_q, meta = quantize(tensor, nbits, channel_wise, group_size, optimize, round_zero, axis, bitpack, compute_dtype, view_as_float, device)\n    self.assertEqual(W_q.shape, (1, 3, 32, 32))\n    self.assertEqual(meta[\"nbits\"], 4)\n    self.assertEqual(meta[\"group_size\"], 64)\n    self.assertEqual(meta[\"shape\"], (1, 3, 32, 32))\n    self.assertEqual(meta[\"scale\"], 1.0 / 2.0)\n    self.assertEqual(meta[\"zero\"], 0.0)\n    self.assertEqual(meta[\"axis\"], 0)\n    self.assertEqual(meta[\"packing\"], \"bit4\")\n    self.assertEqual(meta[\"unpack_view_dtype\"], torch.float32)\n    self.assertEqual(meta[\"view_as_float\"], False)\n    self.assertEqual(meta[\"device\"], \"cuda\")\n\n    # Test case 2: nbits=8, channel_wise=False, group_size=None, optimize=True, round_zero=True, axis=1, bitpack=False, compute_dtype=None, view_as_float=True, device=\"cpu\"\n    tensor = torch.randn(1, 3, 32, 32)\n    nbits = 8\n    channel_wise = False\n    group_size = None\n    optimize = True\n    round_zero = True\n    axis = 1\n    bitpack = False\n    compute_dtype = None\n    view_as_float = True\n    device = \"cpu\"\n    W_q, meta = quantize(tensor, nbits, channel_wise, group_size, optimize, round_zero, axis, bitpack, compute_dtype, view_as_float, device)\n    self.assertEqual(W_q.shape, (1, 3, 32, 32))\n    self.assertEqual(meta[\"nbits\"], 8)\n    self.assertEqual(meta[\"group_size\"], None)\n    self.assertEqual(meta[\"shape\"], (1, 3, 32, 32))\n    self.assertEqual(meta[\"scale\"], 1.0 / 2.0)\n    self.assertEqual(meta[\"zero\"], 0.0)\n    self.assertEqual(meta[\"axis\"], 1)\n    self.assertEqual(meta[\"packing\"], None)\n    self.assertEqual(meta[\"unpack_view_dtype\"], torch.float32)\n    self.assertEqual(meta[\"view_as_float\"], True)\n    self.assertEqual(meta[\"device\"], \"cpu\")\n```"]}
{"id": 50, "prompt": "\nYou are an expert Python programmer. You will be given some information and will generate a correct test case for the given function.\nYour response will only be python test function and without anything else.\n\n### focal function:\n```python\n\ndef mean(x, axis=None, keepdims=False):\n    if isinstance(x, (list, tuple)):\n        x = stack(x)\n    x = convert_to_tensor(x)\n    if axis == () or axis == []:\n        # Torch handles the empty axis case differently from numpy.\n        return x\n    axis = to_tuple_or_list(axis)  # see [NB] below\n\n    ori_dtype = standardize_dtype(x.dtype)\n    # torch.mean only supports floating point inputs\n    compute_dtype = dtypes.result_type(x.dtype, \"float32\")\n    if \"int\" in ori_dtype or ori_dtype == \"bool\":\n        result_dtype = compute_dtype\n    else:\n        result_dtype = ori_dtype\n\n    # [NB] the python torch op torch.mean() is generated into\n    # `torch._C._VariableFunctions.pyi`, and the method\n    # signature is overloaded.\n    # Dynamo won't actually find the correct signature of\n    # `torch.mean()` if arguments are passed via kwargs\n    # So we have to pass the arguments via positional args\n    # EXCEPT for those that are forced as kwargs via the `*`\n    # delimiter in the overloaded method signatures.\n    # Additionally, we have to create a singleton-tuple\n    # when `axis` is an int to match the existing fn signature\n    result = torch.mean(\n        x,\n        axis,\n        keepdims,\n        dtype=to_torch_dtype(compute_dtype),\n    )\n    return cast(result, result_dtype)\n```\n###test function signature:\n```python\n\n    def test_mean(self):\n```\n### Answer: (use the provided format with backticks)\n", "name": "test_mean", "reference": "\n    def test_mean(self):\n        x = np.array([[1, 2, 3], [3, 2, 1]])\n        self.assertAllClose(knp.mean(x), np.mean(x))\n        self.assertAllClose(knp.mean(x, axis=()), np.mean(x, axis=()))\n        self.assertAllClose(knp.mean(x, axis=1), np.mean(x, axis=1))\n        self.assertAllClose(knp.mean(x, axis=(1,)), np.mean(x, axis=(1,)))\n        self.assertAllClose(\n            knp.mean(x, axis=1, keepdims=True),\n            np.mean(x, axis=1, keepdims=True),\n        )\n\n        self.assertAllClose(knp.Mean()(x), np.mean(x))\n        self.assertAllClose(knp.Mean(axis=1)(x), np.mean(x, axis=1))\n        self.assertAllClose(\n            knp.Mean(axis=1, keepdims=True)(x),\n            np.mean(x, axis=1, keepdims=True),\n        )\n\n        # test overflow\n        x = np.array([65504, 65504, 65504], dtype=\"float16\")\n        self.assertAllClose(knp.mean(x), np.mean(x))", "completions": ["  ```\ndef test_mean():\n    x = torch.tensor([1, 2, 3, 4])\n    axis = 0\n    keepdims = False\n    expected_result = torch.tensor([2.5])\n    result = mean(x, axis, keepdims)\n    self.assertEqual(result, expected_result)\n```"]}
{"id": 51, "prompt": "\nYou are an expert Python programmer. You will be given some information and will generate a correct test case for the given function.\nYour response will only be python test function and without anything else.\n\n### focal function:\n```python\ndef get_intrinsic_value(\n    cash_flow: float,\n    growth_rate: float,\n    perpetual_growth_rate: float,\n    weighted_average_cost_of_capital: float,\n    cash_and_cash_equivalents: float,\n    total_debt: float,\n    shares_outstanding: float,\n    periods: int = 5,\n) -> pd.DataFrame:\n    components = {}\n\n    cash_flow_projection = [cash_flow]\n\n    # Cash Flow to Use\n    for period in range(1, periods + 1):\n        if period == 1:\n            cash_flow_projection.append(cash_flow_projection[0] * (1 + growth_rate))\n        else:\n            cash_flow_projection.append(\n                cash_flow_projection[period - 1] * (1 + growth_rate)\n            )\n\n    # Calculate the Terminal Value\n    terminal_value = (\n        cash_flow_projection[-1]\n        * (1 + perpetual_growth_rate)\n        / (weighted_average_cost_of_capital - perpetual_growth_rate)\n    )\n\n    # Add Terminal Value to the end of the cash flow projection\n    cash_flow_projection[-1] = cash_flow_projection[-1] + terminal_value\n\n    # Calculate the Present Value based on the Discounted Cash Flow Formula\n    cash_flow_present_value = []\n    for index, cash_flow_value in enumerate(cash_flow_projection):\n        cash_flow_present_value.append(\n            cash_flow_value / (1 + weighted_average_cost_of_capital) ** (index + 1)\n        )\n\n    # Calculate the Enterprise Value\n    enterprise_value = sum(cash_flow_present_value)\n\n    # Calculate the Equity Value\n    equity_value = enterprise_value + cash_and_cash_equivalents - total_debt\n\n    # Calculate the Intrinsic Value\n    intrinsic_value = equity_value / shares_outstanding\n\n    # Combine the components into a dictionary\n    components = {\n        \"Terminal Value\": terminal_value,\n        \"Cash Flow Projection\": cash_flow_projection[-1],\n        \"Enterprise Value\": enterprise_value,\n        \"Equity Value\": equity_value,\n        \"Intrinsic Value\": intrinsic_value,\n    }\n\n    return pd.DataFrame.from_dict(\n        components, orient=\"index\", columns=[f\"Periods = {periods}\"]\n    )\n```\n###test function signature:\n```python\n\ndef test_get_intrinsic_value(recorder):\n```\n### Answer: (use the provided format with backticks)\n", "name": "test_get_intrinsic_value", "reference": "\ndef test_get_intrinsic_value(recorder):\n    recorder.capture(\n        intrinsic_model.get_intrinsic_value(\n            cash_flow=500,\n            growth_rate=0.05,\n            perpetual_growth_rate=0.02,\n            weighted_average_cost_of_capital=0.1,\n            cash_and_cash_equivalents=100,\n            total_debt=100,\n            shares_outstanding=100,\n            periods=5,\n        )\n    )", "completions": ["  ```\ndef test_get_intrinsic_value(recorder):\n    # Test case 1:\n    cash_flow = 1000\n    growth_rate = 0.1\n    perpetual_growth_rate = 0.05\n    weighted_average_cost_of_capital = 0.1\n    cash_and_cash_equivalents = 1000\n    total_debt = 500\n    shares_outstanding = 10000\n    periods = 5\n\n    expected_result = pd.DataFrame(\n        {\n            \"Terminal Value\": [1050],\n            \"Cash Flow Projection\": [1050],\n            \"Enterprise Value\": [1050],\n            \"Equity Value\": [1050],\n            \"Intrinsic Value\": [1050],\n        },\n        index=[\"Periods = 5\"],\n    )\n\n    actual_result = get_intrinsic_value(\n        cash_flow,\n        growth_rate,\n        perpetual_growth_rate,\n        weighted_average_cost_of_capital,\n        cash_and_cash_equivalents,\n        total_debt,\n        shares_outstanding,\n        periods,\n    )\n\n    recorder.assertEqual(actual_result, expected_result)\n\n    # Test case 2:\n    cash_flow = 1000\n    growth_rate = 0.1\n    perpetual_growth_rate = 0.05\n    weighted_average_cost_of_capital = 0.1\n    cash_and_cash_equivalents = 1000\n    total_debt = 500\n    shares_outstanding = 10000\n    periods = 10\n\n    expected_result = pd.DataFrame(\n        {\n            \"Terminal Value\": [1050],\n            \"Cash Flow Projection\": [1050],\n            \"Enterprise Value\": [1050],\n            \"Equity Value\": [1050],\n            \"Intrinsic Value\": [1050],\n        },\n        index=[\"Periods = 10\"],\n    )\n\n    actual_result = get_intrinsic_value(\n        cash_flow,\n        growth_rate,\n        perpetual_growth_rate,\n        weighted_average_cost_of_capital,\n        cash_and_cash_equivalents,\n        total_debt,\n        shares_outstanding,\n        periods,\n    )\n\n    recorder.assertEqual(actual_result, expected_result)\n```"]}
{"id": 52, "prompt": "\nYou are an expert Python programmer. You will be given some information and will generate a correct test case for the given function.\nYour response will only be python test function and without anything else.\n\n### focal function:\n```python\ndef to_numpy(\n    evset: EventSet,\n    timestamp_to_datetime: bool = True,\n    timestamps: bool = True,\n) -> Dict[str, ndarray]:\n    timestamp_key = \"timestamp\"\n    index_names = evset.schema.index_names()\n    feature_names = evset.schema.feature_names()\n\n    column_names = index_names + feature_names\n    if timestamps:\n        column_names += [timestamp_key]\n\n    dst = {column_name: [] for column_name in column_names}\n    for index, data in evset.data.items():\n        assert isinstance(index, tuple)\n\n        if timestamps:\n            # Timestamps\n            if evset.schema.is_unix_timestamp and timestamp_to_datetime:\n                dst[timestamp_key].append(\n                    data.timestamps.astype(\"datetime64[s]\")\n                )\n            else:\n                dst[timestamp_key].append(data.timestamps)\n\n        # Features\n        for feature_name, feature in zip(feature_names, data.features):\n            dst[feature_name].append(feature)\n\n        # Indexes\n        num_timestamps = len(data.timestamps)\n        for index_name, index_item in zip(index_names, index):\n            dst[index_name].append(np.repeat(index_item, num_timestamps))\n\n    dst = {k: np.concatenate(v) for k, v in dst.items()}\n    return dst\n```\n###test function signature:\n```python\n\n    def test_correct(self):\n```\n### Answer: (use the provided format with backticks)\n", "name": "test_correct", "reference": "\n    def test_correct(self):\n        evset = event_set(\n            timestamps=[\"2023-11-08T17:14:38\", \"2023-11-29T21:44:46\"],\n            features={\n                \"feature_1\": [0.5, 0.6],\n                \"my_index\": [\"red\", \"blue\"],\n            },\n            indexes=[\"my_index\"],\n        )\n\n        result = to_numpy(evset)\n\n        expected = {\n            \"timestamp\": np.array(\n                [\"2023-11-08T17:14:38\", \"2023-11-29T21:44:46\"],\n                dtype=\"datetime64[s]\",\n            ),\n            \"feature_1\": np.array([0.5, 0.6]),\n            \"my_index\": np.array([b\"red\", b\"blue\"]),\n        }\n\n        for k in expected:\n            np.testing.assert_array_equal(\n                np.sort(result[k]), np.sort(expected[k])\n            )", "completions": ["  ```\ndef test_correct(self):\n    evset = EventSet(\n        schema=EventSchema(\n            index_names=[\"index1\", \"index2\"],\n            feature_names=[\"feature1\", \"feature2\"],\n            is_unix_timestamp=True,\n        ),\n        data={\n            (1, 2): EventData(\n                timestamps=[1, 2, 3],\n                features=[10, 20, 30],\n            ),\n            (3, 4): EventData(\n                timestamps=[4, 5, 6],\n                features=[40, 50, 60],\n            ),\n        },\n    )\n    expected = {\n        \"index1\": np.array([1, 1, 1, 3, 3, 3]),\n        \"index2\": np.array([2, 2, 2, 4, 4, 4]),\n        \"feature1\": np.array([10, 20, 30, 40, 50, 60]),\n        \"feature2\": np.array([20, 30, 40, 50, 60, 70]),\n        \"timestamp\": np.array(\n            [\n                np.datetime64(\"1970-01-01T00:00:01\"),\n                np.datetime64(\"1970-01-01T00:00:02\"),\n                np.datetime64(\"1970-01-01T00:00:03\"),\n                np.datetime64(\"1970-01-01T00:00:04\"),\n                np.datetime64(\"1970-01-01T00:00:05\"),\n                np.datetime64(\"1970-01-01T00:00:06\"),\n            ],\n            dtype=\"datetime64[s]\",\n        ),\n    }\n    actual = to_numpy(evset, timestamp_to_datetime=True, timestamps=True)\n    self.assertEqual(expected, actual)\n```"]}
{"id": 53, "prompt": "\nYou are an expert Python programmer. You will be given some information and will generate a correct test case for the given function.\nYour response will only be python test function and without anything else.\n\n### focal function:\n```python\n\n    def from_file(cls, path: str) -> \"LayerConfig\":\n        logger.info(f\"Loading route config from {path}\")\n        _, ext = os.path.splitext(path)\n        with open(path, \"r\") as f:\n            if ext == \".json\":\n                layer = json.load(f)\n            elif ext in [\".yaml\", \".yml\"]:\n                layer = yaml.safe_load(f)\n            else:\n                raise ValueError(\n                    \"Unsupported file type. Only .json and .yaml are supported\"\n                )\n\n            if not is_valid(json.dumps(layer)):\n                raise Exception(\"Invalid config JSON or YAML\")\n\n            encoder_type = layer[\"encoder_type\"]\n            encoder_name = layer[\"encoder_name\"]\n            routes = []\n            for route_data in layer[\"routes\"]:\n                # Handle the 'llm' field specially if it exists\n                if \"llm\" in route_data and route_data[\"llm\"] is not None:\n                    llm_data = route_data.pop(\n                        \"llm\"\n                    )  # Remove 'llm' from route_data and handle it separately\n                    # Use the module path directly from llm_data without modification\n                    llm_module_path = llm_data[\"module\"]\n                    # Dynamically import the module and then the class from that module\n                    llm_module = importlib.import_module(llm_module_path)\n                    llm_class = getattr(llm_module, llm_data[\"class\"])\n                    # Instantiate the LLM class with the provided model name\n                    llm = llm_class(name=llm_data[\"model\"])\n                    # Reassign the instantiated llm object back to route_data\n                    route_data[\"llm\"] = llm\n\n                # Dynamically create the Route object using the remaining route_data\n                route = Route(**route_data)\n                routes.append(route)\n\n            return cls(\n                encoder_type=encoder_type, encoder_name=encoder_name, routes=routes\n            )\n```\n###test function signature:\n```python\n\n    def test_from_file_yaml(openai_encoder, tmp_path, index_cls):\n        # Create a temporary YAML file with layer configuration\n```\n### Answer: (use the provided format with backticks)\n", "name": "test_from_file_yaml", "reference": "\n    def test_from_file_yaml(openai_encoder, tmp_path, index_cls):\n        # Create a temporary YAML file with layer configuration\n        config_path = tmp_path / \"config.yaml\"\n        config_path.write_text(\n            layer_yaml()\n        )  # Assuming layer_yaml() returns a valid YAML string\n\n        # Load the LayerConfig from the temporary file\n        layer_config = LayerConfig.from_file(str(config_path))\n\n        # Assertions to verify the loaded configuration\n        assert layer_config.encoder_type == \"cohere\"\n        assert layer_config.encoder_name == \"embed-english-v3.0\"\n        assert len(layer_config.routes) == 2\n        assert layer_config.routes[0].name == \"politics\"", "completions": ["  ```\ndef test_from_file_yaml(openai_encoder, tmp_path, index_cls):\n    # Create a temporary YAML file with layer configuration\n    with open(tmp_path / \"layer.yaml\", \"w\") as f:\n        f.write(\n            \"\"\"\n            encoder_type: openai\n            encoder_name: openai-encoder\n            routes:\n              - route_type: llm\n                route_name: llm-route\n                llm:\n                  module: openai.llm\n                  class: OpenAILLM\n                  model: openai-llm\n            \"\"\"\n        )\n\n    # Load the layer configuration from the YAML file\n    layer = LayerConfig.from_file(tmp_path / \"layer.yaml\")\n\n    # Check that the layer configuration is correct\n    assert layer.encoder_type == \"openai\"\n    assert layer.encoder_name == \"openai-encoder\"\n    assert len(layer.routes) == 1\n    assert layer.routes[0].route_type == \"llm\"\n    assert layer.routes[0].route_name == \"llm-route\"\n    assert layer.routes[0].llm.module == \"openai.llm\"\n    assert layer.routes[0].llm.class == \"OpenAILLM\"\n    assert layer.routes[0].llm.model == \"openai-llm\"\n```"]}
{"id": 54, "prompt": "\nYou are an expert Python programmer. You will be given some information and will generate a correct test case for the given function.\nYour response will only be python test function and without anything else.\n\n### focal function:\n```python\n    def bind_all(self, circuit: QuantumCircuit) -> np.ndarray:\n        arr = np.empty(self.shape, dtype=object)\n        for idx in np.ndindex(self.shape):\n            arr[idx] = self.bind(circuit, idx)\n        return arr\n```\n###test function signature:\n```python\n    def test_data(self):\n```\n### Answer: (use the provided format with backticks)\n", "name": "test_data", "reference": "    def test_data(self):\n        with self.subTest(\"binding a single value\"):\n            vals = np.linspace(0, 1, 50)\n            data = {self.params: vals}\n            ba = BindingsArray(data=data)\n            self.assertEqual(ba.num_parameters, 50)\n            self.assertEqual(ba.ndim, 0)\n            self.assertEqual(ba.shape, ())\n            self.assertEqual(ba.size, 1)\n            self.assertEqual(ba.data, {tuple(param.name for param in self.params): vals})\n\n            bound_circuit = ba.bind(self.circuit, ())\n            self.assertEqual(bound_circuit, self.circuit.assign_parameters(vals))\n\n        with self.subTest(\"binding an array\"):\n            vals = np.linspace(0, 1, 300).reshape((2, 3, 50))\n            data = {self.params: vals}\n            ba = BindingsArray(data=data)\n            self.assertEqual(ba.num_parameters, 50)\n            self.assertEqual(ba.ndim, 2)\n            self.assertEqual(ba.shape, (2, 3))\n            self.assertEqual(ba.size, 6)\n            self.assertEqual(ba.data, {tuple(param.name for param in self.params): vals})\n\n            bound_circuits = ba.bind_all(self.circuit)\n            self.assertEqual(bound_circuits.shape, (2, 3))\n            self.assertEqual(bound_circuits[0, 0], self.circuit.assign_parameters(vals[0, 0]))\n            self.assertEqual(bound_circuits[0, 1], self.circuit.assign_parameters(vals[0, 1]))\n            self.assertEqual(bound_circuits[0, 2], self.circuit.assign_parameters(vals[0, 2]))\n            self.assertEqual(bound_circuits[1, 0], self.circuit.assign_parameters(vals[1, 0]))\n            self.assertEqual(bound_circuits[1, 1], self.circuit.assign_parameters(vals[1, 1]))\n            self.assertEqual(bound_circuits[1, 2], self.circuit.assign_parameters(vals[1, 2]))\n\n        with self.subTest(\"binding a single param\"):\n            vals = np.linspace(0, 1, 50)\n            data = {self.params[0]: vals}\n            ba = BindingsArray(data=data)\n            self.assertEqual(ba.num_parameters, 1)\n            self.assertEqual(ba.ndim, 1)\n            self.assertEqual(ba.shape, (50,))\n            self.assertEqual(ba.size, 50)\n            self.assertEqual(list(ba.data.keys()), [(self.params[0].name,)])\n            np.testing.assert_allclose(list(ba.data.values()), [vals[..., np.newaxis]])", "completions": ["  ```\ndef test_data(self):\n    circuit = QuantumCircuit(2, 2)\n    arr = np.empty((2, 2), dtype=object)\n    for idx in np.ndindex((2, 2)):\n        arr[idx] = self.bind(circuit, idx)\n    self.assertEqual(arr, np.array([[0, 1], [2, 3]]))\n```"]}
{"id": 55, "prompt": "\nYou are an expert Python programmer. You will be given some information and will generate a correct test case for the given function.\nYour response will only be python test function and without anything else.\n\n### focal function:\n```python\ndef set_seed(\n    seed: Optional[int] = None, debug_mode: Optional[Union[str, int]] = None\n) -> int:\n    world_size, rank = get_world_size_and_rank()\n    max_val = np.iinfo(np.uint32).max - world_size + 1\n    min_val = np.iinfo(np.uint32).min\n    if seed is None:\n        rand_seed = torch.randint(min_val, max_val, (1,))\n        seed = _broadcast_tensor(rand_seed, 0).item()  # sync seed across ranks\n    if seed < min_val or seed > max_val:\n        raise ValueError(\n            f\"Invalid seed value provided: {seed}. Value must be in the range [{min_val}, {max_val}]\"\n        )\n    local_seed = seed + rank\n    _log.debug(\n        f\"Setting manual seed to local seed {local_seed}. Local seed is seed + rank = {seed} + {rank}\"\n    )\n\n    torch.manual_seed(local_seed)\n    np.random.seed(local_seed)\n    random.seed(local_seed)\n\n    if debug_mode is not None:\n        _log.debug(f\"Setting deterministic debug mode to {debug_mode}\")\n        torch.set_deterministic_debug_mode(debug_mode)\n        deterministic_debug_mode = torch.get_deterministic_debug_mode()\n        if deterministic_debug_mode == 0:\n            _log.debug(\"Disabling cuDNN deterministic mode\")\n            torch.backends.cudnn.deterministic = False\n            torch.backends.cudnn.benchmark = True\n        else:\n            _log.debug(\"Enabling cuDNN deterministic mode\")\n            torch.backends.cudnn.deterministic = True\n            torch.backends.cudnn.benchmark = False\n            # reference: https://docs.nvidia.com/cuda/cublas/index.html#cublasApi_reproducibility\n            os.environ[\"CUBLAS_WORKSPACE_CONFIG\"] = \":4096:8\"\n\n    return seed\n```\n###test function signature:\n```python\n\n    def test_deterministic_true(self) -> None:\n```\n### Answer: (use the provided format with backticks)\n", "name": "test_deterministic_true", "reference": "\n    def test_deterministic_true(self) -> None:\n        for det_debug_mode, det_debug_mode_str in [(1, \"warn\"), (2, \"error\")]:\n            warn_only = det_debug_mode == 1\n            for debug_mode in (det_debug_mode, det_debug_mode_str):\n                set_seed(42, debug_mode=debug_mode)\n                assert torch.backends.cudnn.deterministic\n                assert not torch.backends.cudnn.benchmark\n                assert det_debug_mode == torch.get_deterministic_debug_mode()\n                assert torch.are_deterministic_algorithms_enabled()\n                assert (\n                    warn_only == torch.is_deterministic_algorithms_warn_only_enabled()\n                )\n                assert os.environ[\"CUBLAS_WORKSPACE_CONFIG\"] == \":4096:8\"", "completions": ["  ```\ndef test_deterministic_true(self):\n    seed = 1234\n    debug_mode = True\n    expected_seed = 1234\n    expected_local_seed = 1234 + 0\n    expected_deterministic_debug_mode = 1\n    expected_cublas_workspace_config = \":4096:8\"\n\n    with patch(\"torch.manual_seed\") as mock_manual_seed, patch(\"np.random.seed\") as mock_np_random_seed, patch(\"random.seed\") as mock_random_seed, patch(\"torch.set_deterministic_debug_mode\") as mock_set_deterministic_debug_mode, patch(\"torch.get_deterministic_debug_mode\") as mock_get_deterministic_debug_mode, patch(\"torch.backends.cudnn.deterministic\") as mock_cudnn_deterministic, patch(\"torch.backends.cudnn.benchmark\") as mock_cudnn_benchmark, patch(\"os.environ\") as mock_os_environ:\n        mock_manual_seed.return_value = None\n        mock_np_random_seed.return_value = None\n        mock_random_seed.return_value = None\n        mock_set_deterministic_debug_mode.return_value = None\n        mock_get_deterministic_debug_mode.return_value = expected_deterministic_debug_mode\n        mock_cudnn_deterministic.return_value = True\n        mock_cudnn_benchmark.return_value = False\n        mock_os_environ.return_value = expected_cublas_workspace_config\n\n        actual_seed = set_seed(seed, debug_mode)\n\n        self.assertEqual(actual_seed, expected_seed)\n        mock_manual_seed.assert_called_once_with(expected_local_seed)\n        mock_np_random_seed.assert_called_once_with(expected_local_seed)\n        mock_random_seed.assert_called_once_with(expected_local_seed)\n        mock_set_deterministic_debug_mode.assert_called_once_with(expected_deterministic_debug_mode)\n        mock_get_deterministic_debug_mode.assert_called_once()\n        mock_cudnn_deterministic.assert_called_once_with(True)\n        mock_cudnn_benchmark.assert_called_once_with(False)\n        mock_os_environ.assert_called_once_with(\"CUBLAS_WORKSPACE_CONFIG\", expected_cublas_workspace_config)\n```"]}
{"id": 56, "prompt": "\nYou are an expert Python programmer. You will be given some information and will generate a correct test case for the given function.\nYour response will only be python test function and without anything else.\n\n### focal function:\n```python\ndef concat_compat(to_concat, axis: AxisInt = 0, ea_compat_axis: bool = False):\n\n    # filter empty arrays\n    # 1-d dtypes always are included here\n    def is_nonempty(x) -> bool:\n        if x.ndim <= axis:\n            return True\n        return x.shape[axis] > 0\n\n    # If all arrays are empty, there's nothing to convert, just short-cut to\n    # the concatenation, #3121.\n    #\n    # Creating an empty array directly is tempting, but the winnings would be\n    # marginal given that it would still require shape & dtype calculation and\n    # np.concatenate which has them both implemented is compiled.\n    non_empties = [x for x in to_concat if is_nonempty(x)]\n    if non_empties and axis == 0 and not ea_compat_axis:\n        # ea_compat_axis see GH#39574\n        to_concat = non_empties\n\n    dtypes = {obj.dtype for obj in to_concat}\n    kinds = {obj.dtype.kind for obj in to_concat}\n    contains_datetime = any(\n        isinstance(dtype, (np.dtype, DatetimeTZDtype)) and dtype.kind in [\"m\", \"M\"]\n        for dtype in dtypes\n    ) or any(isinstance(obj, ABCExtensionArray) and obj.ndim > 1 for obj in to_concat)\n\n    all_empty = not len(non_empties)\n    single_dtype = len({x.dtype for x in to_concat}) == 1\n    any_ea = any(isinstance(x.dtype, ExtensionDtype) for x in to_concat)\n\n    if contains_datetime:\n        return _concat_datetime(to_concat, axis=axis)\n\n    if any_ea:\n        # we ignore axis here, as internally concatting with EAs is always\n        # for axis=0\n        if not single_dtype:\n            target_dtype = find_common_type([x.dtype for x in to_concat])\n            target_dtype = common_dtype_categorical_compat(to_concat, target_dtype)\n            to_concat = [\n                astype_array(arr, target_dtype, copy=False) for arr in to_concat\n            ]\n\n        if isinstance(to_concat[0], ABCExtensionArray):\n            # TODO: what about EA-backed Index?\n            cls = type(to_concat[0])\n            return cls._concat_same_type(to_concat)\n        else:\n            return np.concatenate(to_concat)\n\n    elif all_empty:\n        # we have all empties, but may need to coerce the result dtype to\n        # object if we have non-numeric type operands (numpy would otherwise\n        # cast this to float)\n        if len(kinds) != 1:\n            if not len(kinds - {\"i\", \"u\", \"f\"}) or not len(kinds - {\"b\", \"i\", \"u\"}):\n                # let numpy coerce\n                pass\n            else:\n                # coerce to object\n                to_concat = [x.astype(\"object\") for x in to_concat]\n                kinds = {\"o\"}\n\n    result = np.concatenate(to_concat, axis=axis)\n    if \"b\" in kinds and result.dtype.kind in [\"i\", \"u\", \"f\"]:\n        # GH#39817 cast to object instead of casting bools to numeric\n        result = result.astype(object, copy=False)\n    return result\n```\n###test function signature:\n```python\n\ndef test_concat_periodarray_2d():\n```\n### Answer: (use the provided format with backticks)\n", "name": "test_concat_periodarray_2d", "reference": "\ndef test_concat_periodarray_2d():\n    pi = pd.period_range(\"2016-01-01\", periods=36, freq=\"D\")\n    arr = pi._data.reshape(6, 6)\n\n    result = _concat.concat_compat([arr[:2], arr[2:]], axis=0)\n    tm.assert_period_array_equal(result, arr)\n\n    result = _concat.concat_compat([arr[:, :2], arr[:, 2:]], axis=1)\n    tm.assert_period_array_equal(result, arr)\n\n    msg = (\n        \"all the input array dimensions.* for the concatenation axis must match exactly\"\n    )\n    with pytest.raises(ValueError, match=msg):\n        _concat.concat_compat([arr[:, :2], arr[:, 2:]], axis=0)\n\n    with pytest.raises(ValueError, match=msg):\n        _concat.concat_compat([arr[:2], arr[2:]], axis=1)", "completions": ["  ```\ndef test_concat_periodarray_2d():\n    # Test case 1: concatenating two 2D PeriodArrays with the same frequency\n    pa1 = pd.PeriodArray([\"2020-01-01\", \"2020-01-02\"], freq=\"D\")\n    pa2 = pd.PeriodArray([\"2020-01-03\", \"2020-01-04\"], freq=\"D\")\n    result = concat_compat([pa1, pa2], axis=0)\n    expected = pd.PeriodArray([\"2020-01-01\", \"2020-01-02\", \"2020-01-03\", \"2020-01-04\"], freq=\"D\")\n    assert_array_equal(result, expected)\n\n    # Test case 2: concatenating two 2D PeriodArrays with different frequencies\n    pa1 = pd.PeriodArray([\"2020-01-01\", \"2020-01-02\"], freq=\"D\")\n    pa2 = pd.PeriodArray([\"2020-01-03\", \"2020-01-04\"], freq=\"W\")\n    result = concat_compat([pa1, pa2], axis=0)\n    expected = pd.PeriodArray([\"2020-01-01\", \"2020-01-02\", \"2020-01-03\", \"2020-01-04\"], freq=\"D\")\n    assert_array_equal(result, expected)\n\n    # Test case 3: concatenating a 2D PeriodArray with a 1D PeriodArray\n    pa1 = pd.PeriodArray([\"2020-01-01\", \"2020-01-02\"], freq=\"D\")\n    pa2 = pd.PeriodArray([\"2020-01-03\"], freq=\"D\")\n    result = concat_compat([pa1, pa2], axis=0)\n    expected = pd.PeriodArray([\"2020-01-01\", \"2020-01-02\", \"2020-01-03\"], freq=\"D\")\n    assert_array_equal(result, expected)\n```"]}
{"id": 57, "prompt": "\nYou are an expert Python programmer. You will be given some information and will generate a correct test case for the given function.\nYour response will only be python test function and without anything else.\n\n### focal function:\n```python\ndef ingest_ocp_payload(request):\n    request_id = uuid4().hex\n    response_data = {\"request-id\": request_id, \"payload-name\": []}\n    if request.method == \"POST\":\n        if not request.FILES:\n            response_data[\"error\"] = \"no payload sent\"\n            return Response(response_data, status=HTTPStatus.BAD_REQUEST)\n        for _, file in request.FILES.items():\n            payload_name = file.name\n            response_data[\"payload-name\"].append(payload_name)\n            s3_signature = get_s3_signature(settings.S3_ENDPOINT, payload_name, method=\"put_object\")\n            res = upload_file_to_s3(s3_signature, data=file.file)\n            if res.status_code == HTTPStatus.OK:\n                response_data[\"upload\"] = \"success\"\n            else:\n                response_data[\"upload\"] = \"failed\"\n                response_data[\"failed-reason\"] = res.reason\n                return Response(response_data, status=res.status_code)\n            send_payload(request_id, payload_name)\n    else:\n        params = request.query_params\n        payload_names = params.get(\"payload_name\")\n        if not payload_names:\n            response_data[\"error\"] = \"no payload sent\"\n            return Response(response_data, status=HTTPStatus.BAD_REQUEST)\n        for payload_name in payload_names.split(\",\"):\n            send_payload(request_id, payload_name)\n            response_data[\"payload-name\"].append(payload_name)\n\n    response_data[\"ingest-started\"] = True\n\n    return Response(response_data, status=HTTPStatus.ACCEPTED)\n```\n###test function signature:\n```python\n\n    def test_ingest_ocp_payload(self):\n        # Arrange\n```\n### Answer: (use the provided format with backticks)\n", "name": "test_ingest_ocp_payload", "reference": "\n    def test_ingest_ocp_payload(self):\n        # Arrange\n        file1 = SimpleUploadedFile(\"file1.gz\", b\"file_content\", content_type=\"multipart/form-data\")\n        file2 = SimpleUploadedFile(\"file2.gz\", b\"file_content\", content_type=\"multipart/form-data\")\n        test_table = [\n            # Happy path tests\n            (\n                \"POST\",\n                {\"file1\": file1, \"file2\": file2},\n                \"\",\n                HTTPStatus.ACCEPTED,\n                {\"upload\": \"success\", \"ingest-started\": True},\n                \"happy_path_post\",\n            ),\n            (\"GET\", {}, \"?payload_name=file1,file2\", HTTPStatus.ACCEPTED, {\"ingest-started\": True}, \"happy_path_get\"),\n            # Edge cases\n            (\"POST\", {}, \"\", HTTPStatus.BAD_REQUEST, {\"error\": \"no payload sent\"}, \"edge_case_no_files_post\"),\n            (\"GET\", {}, \"\", HTTPStatus.BAD_REQUEST, {\"error\": \"no payload sent\"}, \"edge_case_no_payload_name_get\"),\n            # Error cases\n            (\"POST\", {\"file1\": file1}, \"\", HTTPStatus.INTERNAL_SERVER_ERROR, {\"upload\": \"failed\"}, \"error_case_post\"),\n            (\"GET\", {}, \"?payload_name=non_existent_file\", HTTPStatus.ACCEPTED, {}, \"error_case_get\"),\n        ]\n        for test in test_table:\n            method, files, query_params, expected_status, expected_response, test_id = test\n            with self.subTest(test=test_id):\n\n                mock_response = MagicMock()\n                mock_response.status_code = (\n                    HTTPStatus.OK if expected_response.get(\"upload\") != \"failed\" else HTTPStatus.INTERNAL_SERVER_ERROR\n                )\n                mock_response.reason = \"Some reason\"\n\n                with patch(\"masu.api.ingest_ocp_payload.get_s3_signature\") as mock_get_s3_signature, patch(\n                    \"masu.api.ingest_ocp_payload.upload_file_to_s3\", return_value=mock_response\n                ) as mock_upload_file_to_s3, patch(\"masu.api.ingest_ocp_payload.send_payload\") as mock_send_payload:\n\n                    request = (\n                        APIRequestFactory().get(f\"/{query_params}\")\n                        if method == \"GET\"\n                        else APIRequestFactory().post(\"/\", data=files)\n                    )\n\n                    # Act\n                    response = ingest_ocp_payload(request)\n\n                    # Assert\n                    self.assertEqual(response.status_code, expected_status)\n                    if method == \"POST\" and files:\n                        mock_get_s3_signature.assert_called()\n                        mock_upload_file_to_s3.assert_called()\n                    if method == \"GET\" and \"payload_name\" in query_params:\n                        mock_send_payload.assert_called()\n                    for key, value in expected_response.items():\n                        self.assertEqual(response.data[key], value)", "completions": ["  ```\ndef test_ingest_ocp_payload(self):\n    # Arrange\n    request = Mock()\n    request.method = \"POST\"\n    request.FILES = {\"file1\": Mock(), \"file2\": Mock()}\n    request.FILES[\"file1\"].name = \"file1.txt\"\n    request.FILES[\"file2\"].name = \"file2.txt\"\n    settings.S3_ENDPOINT = \"https://s3.amazonaws.com\"\n\n    # Act\n    response = ingest_ocp_payload(request)\n\n    # Assert\n    self.assertEqual(response.status_code, HTTPStatus.ACCEPTED)\n    self.assertEqual(response.data[\"request-id\"], \"1234567890abcdef\")\n    self.assertEqual(response.data[\"payload-name\"], [\"file1.txt\", \"file2.txt\"])\n    self.assertEqual(response.data[\"upload\"], \"success\")\n    self.assertEqual(response.data[\"ingest-started\"], True)\n```"]}
{"id": 58, "prompt": "\nYou are an expert Python programmer. You will be given some information and will generate a correct test case for the given function.\nYour response will only be python test function and without anything else.\n\n### focal function:\n```python\nasync def compute_nb_items_with_metadata_field(\n    project_id: str,\n    metadata_field: str,\n    collection_name: str,\n    quantile_value: float = 0.1,\n) -> Tuple[int, float, int]:\n\n    mongo_db = await get_mongo_db()\n\n    try:\n        pipeline = [\n            # Match on the project id and the existence of the metadata field\n            {\n                \"$match\": {\n                    \"project_id\": project_id,\n                    f\"metadata.{metadata_field}\": {\"$exists\": True, \"$ne\": None},\n                }\n            },\n            {\n                \"$group\": {\n                    \"_id\": \"$user_id\",  # Group by 'user_id'\n                    \"count\": {\"$sum\": 1},  # Count the documents in each group\n                }\n            },\n            {\n                \"$sort\": {\"count\": -1}  # Sort by 'count' in descending order\n            },\n        ]\n\n        # Execute the aggregation pipeline\n        results = []  # List to hold the results\n        async for group in mongo_db[collection_name].aggregate(pipeline):\n            results.append(group[\"count\"])\n\n        logger.debug(\n            f\"Results for {metadata_field} in collection {collection_name} for project {project_id}: {results}\"\n        )\n\n        # If no results, return 0\n        if len(results) == 0:\n            return 0, 0.0, 0\n\n        # Get the average and the quantiles\n        average = sum(results) / len(results)\n        bottom_quantile = results[int(len(results) * quantile_value)]\n        top_quantile = results[int(len(results) * (1 - quantile_value))]\n\n        return bottom_quantile, average, top_quantile\n\n    except Exception as e:\n        logger.warning(\n            f\"Failed to fetch the number of {metadata_field} in collection {collection_name} for project {project_id}: {e}\",\n        )\n        return 0, 0.0, 0\n```\n###test function signature:\n```python\n\nasync def test_main_pipeline(db, populated_project):\n```\n### Answer: (use the provided format with backticks)\n", "name": "test_main_pipeline", "reference": "\nasync def test_main_pipeline(db, populated_project):\n    async for mongo_db in db:\n        test_project_id = populated_project.id\n\n        # Run the function\n        topics = await fetch_topics(test_project_id)\n\n        logger.debug(f\"Topics: {topics}\")\n\n        count = await nb_items_with_a_metadata_field(\n            test_project_id, \"tasks\", \"user_id\"\n        )\n\n        assert count > 0\n\n        (\n            bottom_quantile,\n            average,\n            top_quantile,\n        ) = await compute_successrate_metadata_quantiles(test_project_id, \"user_id\")\n\n        logger.debug(f\"Bottom quantile: {bottom_quantile}\")\n        logger.debug(f\"Top quantile: {top_quantile}\")\n\n        assert bottom_quantile >= 0\n        # assert bottom_quantile <= top_quantile\n\n        test_project_id = populated_project.id\n        test_metadata_field = \"user_id\"\n        test_collection_name = \"tasks\"\n        test_quantile_value = 0.1\n\n        # Run the function\n        (\n            bottom_quantile,\n            average,\n            top_quantile,\n        ) = await compute_nb_items_with_metadata_field(\n            test_project_id,\n            test_metadata_field,\n            test_collection_name,\n            test_quantile_value,\n        )\n\n        logger.debug(\"runned compute_nb_items_with_metadata_field\")\n        logger.debug(f\"Bottom quantile: {bottom_quantile}\")\n        logger.debug(f\"Average: {average}\")\n        logger.debug(f\"Top quantile: {top_quantile}\")\n\n        assert isinstance(bottom_quantile, int)\n        assert isinstance(average, float)\n        assert isinstance(top_quantile, int)\n        # assert bottom_quantile <= average <= top_quantile\n\n        (\n            bottom_quantile,\n            average,\n            top_quantile,\n        ) = await compute_session_length_per_metadata(\n            test_project_id,\n            test_metadata_field,\n            test_quantile_value,\n        )\n        logger.debug(\"runned compute_session_length_per_metadata\")\n        logger.debug(f\"Bottom quantile: {bottom_quantile}\")\n        logger.debug(f\"Average: {average}\")\n        logger.debug(f\"Top quantile: {top_quantile}\")\n\n        assert isinstance(bottom_quantile, float)\n        assert isinstance(average, float)\n        assert isinstance(top_quantile, float)\n        # assert bottom_quantile <= average <= top_quantile", "completions": ["  ```\nasync def test_main_pipeline(db, populated_project):\n    project_id = populated_project[\"_id\"]\n    metadata_field = \"metadata_field\"\n    collection_name = \"collection_name\"\n    quantile_value = 0.1\n\n    # Test case 1: No documents with the metadata field\n    await db.collection_name.insert_many([\n        {\"project_id\": project_id, \"metadata\": {}},\n        {\"project_id\": project_id, \"metadata\": {}},\n    ])\n    bottom_quantile, average, top_quantile = await compute_nb_items_with_metadata_field(\n        project_id, metadata_field, collection_name, quantile_value\n    )\n    assert bottom_quantile == 0\n    assert average == 0.0\n    assert top_quantile == 0\n\n    # Test case 2: Some documents with the metadata field\n    await db.collection_name.insert_many([\n        {\"project_id\": project_id, \"metadata\": {\"metadata_field\": 1}},\n        {\"project_id\": project_id, \"metadata\": {\"metadata_field\": 2}},\n        {\"project_id\": project_id, \"metadata\": {\"metadata_field\": 3}},\n    ])\n    bottom_quantile, average, top_quantile = await compute_nb_items_with_metadata_field(\n        project_id, metadata_field, collection_name, quantile_value\n    )\n    assert bottom_quantile == 1\n    assert average == 2.0\n    assert top_quantile == 3\n\n    # Test case 3: All documents with the metadata field\n    await db.collection_name.insert_many([\n        {\"project_id\": project_id, \"metadata\": {\"metadata_field\": 1}},\n        {\"project_id\": project_id, \"metadata\": {\"metadata_field\": 2}},\n        {\"project_id\": project_id, \"metadata\": {\"metadata_field\": 3}},\n        {\"project_id\": project_id, \"metadata\": {\"metadata_field\": 4}},\n        {\"project_id\": project_id, \"metadata\": {\"metadata_field\": 5}},\n    ])\n    bottom_quantile, average, top_quantile = await compute_nb_items_with_metadata_field(\n        project_id, metadata_field, collection_name, quantile_value\n    )\n    assert bottom_quantile == 1\n    assert average == 3.0\n    assert top_quantile == 5\n```"]}
{"id": 59, "prompt": "\nYou are an expert Python programmer. You will be given some information and will generate a correct test case for the given function.\nYour response will only be python test function and without anything else.\n\n### focal function:\n```python\ndef get_engine(engine: str) -> BaseImpl:\n    if engine == \"auto\":\n        engine = get_option(\"io.parquet.engine\")\n\n    if engine == \"auto\":\n        # try engines in this order\n        engine_classes = [PyArrowImpl, FastParquetImpl]\n\n        error_msgs = \"\"\n        for engine_class in engine_classes:\n            try:\n                return engine_class()\n            except ImportError as err:\n                error_msgs += \"\\n - \" + str(err)\n\n        raise ImportError(\n            \"Unable to find a usable engine; \"\n            \"tried using: 'pyarrow', 'fastparquet'.\\n\"\n            \"A suitable version of \"\n            \"pyarrow or fastparquet is required for parquet \"\n            \"support.\\n\"\n            \"Trying to import the above resulted in these errors:\"\n            f\"{error_msgs}\"\n        )\n\n    if engine == \"pyarrow\":\n        return PyArrowImpl()\n    elif engine == \"fastparquet\":\n        return FastParquetImpl()\n\n    raise ValueError(\"engine must be one of 'pyarrow', 'fastparquet'\")\n```\n###test function signature:\n```python\n\ndef test_get_engine_auto_error_message():\n    # Expect different error messages from get_engine(engine=\"auto\")\n    # if engines aren't installed vs. are installed but bad version\n```\n### Answer: (use the provided format with backticks)\n", "name": "test_get_engine_auto_error_message", "reference": "\ndef test_get_engine_auto_error_message():\n    # Expect different error messages from get_engine(engine=\"auto\")\n    # if engines aren't installed vs. are installed but bad version\n    from pandas.compat._optional import VERSIONS\n\n    # Do we have engines installed, but a bad version of them?\n    pa_min_ver = VERSIONS.get(\"pyarrow\")\n    fp_min_ver = VERSIONS.get(\"fastparquet\")\n    have_pa_bad_version = (\n        False\n        if not _HAVE_PYARROW\n        else Version(pyarrow.__version__) < Version(pa_min_ver)\n    )\n    have_fp_bad_version = (\n        False\n        if not _HAVE_FASTPARQUET\n        else Version(fastparquet.__version__) < Version(fp_min_ver)\n    )\n    # Do we have usable engines installed?\n    have_usable_pa = _HAVE_PYARROW and not have_pa_bad_version\n    have_usable_fp = _HAVE_FASTPARQUET and not have_fp_bad_version\n\n    if not have_usable_pa and not have_usable_fp:\n        # No usable engines found.\n        if have_pa_bad_version:\n            match = f\"Pandas requires version .{pa_min_ver}. or newer of .pyarrow.\"\n            with pytest.raises(ImportError, match=match):\n                get_engine(\"auto\")\n        else:\n            match = \"Missing optional dependency .pyarrow.\"\n            with pytest.raises(ImportError, match=match):\n                get_engine(\"auto\")\n\n        if have_fp_bad_version:\n            match = f\"Pandas requires version .{fp_min_ver}. or newer of .fastparquet.\"\n            with pytest.raises(ImportError, match=match):\n                get_engine(\"auto\")\n        else:\n            match = \"Missing optional dependency .fastparquet.\"\n            with pytest.raises(ImportError, match=match):\n                get_engine(\"auto\")", "completions": ["  ```\ndef test_get_engine_auto_error_message():\n    # Expect different error messages from get_engine(engine=\"auto\")\n    # if engines aren't installed vs. are installed but bad version\n    with pytest.raises(ImportError) as excinfo:\n        get_engine(engine=\"auto\")\n    assert \"Unable to find a usable engine\" in str(excinfo.value)\n    assert \"tried using: 'pyarrow', 'fastparquet'\" in str(excinfo.value)\n    assert \"A suitable version of pyarrow or fastparquet is required\" in str(excinfo.value)\n    assert \"Trying to import the above resulted in these errors:\" in str(excinfo.value)\n\n    with pytest.raises(ImportError) as excinfo:\n        get_engine(engine=\"auto\")\n    assert \"Unable to find a usable engine\" in str(excinfo.value)\n    assert \"tried using: 'pyarrow', 'fastparquet'\" in str(excinfo.value)\n    assert \"A suitable version of pyarrow or fastparquet is required\" in str(excinfo.value)\n    assert \"Trying to import the above resulted in these errors:\" in str(excinfo.value)\n```"]}
{"id": 60, "prompt": "\nYou are an expert Python programmer. You will be given some information and will generate a correct test case for the given function.\nYour response will only be python test function and without anything else.\n\n### focal function:\n```python\n    def attribute(\n        self,\n        inp: InterpretableInput,\n        target: Union[str, torch.Tensor, None] = None,\n        num_trials: int = 1,\n        gen_args: Optional[Dict] = None,\n        # internal callback hook can be used for logging\n        _inspect_forward: Optional[Callable] = None,\n        **kwargs,\n    ) -> LLMAttributionResult:\n\n        assert isinstance(\n            inp, self.SUPPORTED_INPUTS\n        ), f\"LLMAttribution does not support input type {type(inp)}\"\n\n        if target is None:\n            # generate when None\n            assert hasattr(self.model, \"generate\") and callable(self.model.generate), (\n                \"The model does not have recognizable generate function.\"\n                \"Target must be given for attribution\"\n            )\n\n            if not gen_args:\n                gen_args = DEFAULT_GEN_ARGS\n\n            model_inp = self._format_model_input(inp.to_model_input())\n            output_tokens = self.model.generate(model_inp, **gen_args)\n            target_tokens = output_tokens[0][model_inp.size(1) :]\n        else:\n            assert gen_args is None, \"gen_args must be None when target is given\"\n\n            if type(target) is str:\n                # exclude sos\n                target_tokens = self.tokenizer.encode(target)[1:]\n                target_tokens = torch.tensor(target_tokens)\n            elif type(target) is torch.Tensor:\n                target_tokens = target\n\n        attr = torch.zeros(\n            [\n                1 + len(target_tokens) if self.include_per_token_attr else 1,\n                inp.n_itp_features,\n            ],\n            dtype=torch.float,\n            device=self.device,\n        )\n\n        for _ in range(num_trials):\n            attr_input = inp.to_tensor().to(self.device)\n\n            cur_attr = self.attr_method.attribute(\n                attr_input,\n                additional_forward_args=(inp, target_tokens, _inspect_forward),\n                **kwargs,\n            )\n\n            # temp necessary due to FA & Shapley's different return shape of multi-task\n            # FA will flatten output shape internally (n_output_token, n_itp_features)\n            # Shapley will keep output shape (batch, n_output_token, n_input_features)\n            cur_attr = cur_attr.reshape(attr.shape)\n\n            attr += cur_attr\n\n        attr = attr / num_trials\n\n        attr = inp.format_attr(attr)\n\n        return LLMAttributionResult(\n            attr[0],\n            attr[1:]\n            if self.include_per_token_attr\n            else None,  # shape(n_output_token, n_input_features)\n            inp.values,\n            self.tokenizer.convert_ids_to_tokens(target_tokens),\n        )\n```\n###test function signature:\n```python\n\n    def test_llm_attr(self, AttrClass) -> None:\n```\n### Answer: (use the provided format with backticks)\n", "name": "test_llm_attr", "reference": "\n    def test_llm_attr(self, AttrClass) -> None:\n        llm = DummyLLM()\n        llm.to(self.device)\n        tokenizer = DummyTokenizer()\n        llm_attr = LLMAttribution(AttrClass(llm), tokenizer)\n\n        inp = TextTemplateInput(\"{} b {} {} e {}\", [\"a\", \"c\", \"d\", \"f\"])\n        res = llm_attr.attribute(inp, \"m n o p q\")\n\n        self.assertEqual(res.seq_attr.shape, (4,))\n        self.assertEqual(cast(Tensor, res.token_attr).shape, (5, 4))\n        self.assertEqual(res.input_tokens, [\"a\", \"c\", \"d\", \"f\"])\n        self.assertEqual(res.output_tokens, [\"m\", \"n\", \"o\", \"p\", \"q\"])\n        self.assertEqual(res.seq_attr.device.type, self.device)\n        self.assertEqual(cast(Tensor, res.token_attr).device.type, self.device)", "completions": ["  ```\ndef test_llm_attr(self, AttrClass):\n    inp = InterpretableInput(\n        values=torch.tensor([[1, 2, 3], [4, 5, 6]]),\n        n_itp_features=3,\n        tokenizer=self.tokenizer,\n    )\n    target = \"This is a test\"\n    num_trials = 2\n    gen_args = None\n    _inspect_forward = None\n    kwargs = {}\n\n    result = AttrClass.attribute(\n        inp, target, num_trials, gen_args, _inspect_forward, **kwargs\n    )\n\n    assert result.attr.shape == (1, 3)\n    assert result.per_token_attr.shape == (2, 3)\n    assert result.values.shape == (2, 3)\n    assert result.target_tokens == [1, 2, 3]\n```"]}
{"id": 61, "prompt": "\nYou are an expert Python programmer. You will be given some information and will generate a correct test case for the given function.\nYour response will only be python test function and without anything else.\n\n### focal function:\n```python\n    def plan(self, task: str):\n        agent_pool_description = self.agent_pool.get_agent_pool_description()\n        feedback = self.get_last_feedback()\n        last_plan = self.get_last_plan()\n\n        # run the planning\n        plan = self.planning.execute(task, agent_pool_description, last_plan, feedback)\n\n        logger.info(f\"Plan: {plan}\")\n\n        self.shared_memory.add(\n            event_type=EventType.planning, agent=self.name, content=str(plan)\n        )\n\n        return plan\n```\n###test function signature:\n```python\n\ndef test_planner_succeeds(get_llm):  # noqa: F811\n```\n### Answer: (use the provided format with backticks)\n", "name": "test_planner_succeeds", "reference": "\ndef test_planner_succeeds(get_llm):  # noqa: F811\n    llm = get_llm(__file__, test_planner_succeeds.__name__)\n\n    physicist_description = (\n        \"The physicist agent answers questions or research about physics-related topics\"\n    )\n    physicist = Physicist(\n        name=\"Physicist\",\n        description=physicist_description,\n        llm=llm,\n    )\n\n    agent_pool = AgentPool()\n    agent_pool.add_agents([physicist])\n\n    shared_memory = SharedMemory(\n        objective=\"Share the information across different agents.\",\n        agent_pool=agent_pool,\n    )\n\n    planner = Planner(\n        name=\"planner\",\n        agent_pool=agent_pool,\n        shared_memory=shared_memory,\n        llm=llm,\n    )\n\n    task = \"\"\"We need to render a highly complex 3D image on the solar system. We can use any publicly avaliable\n    resources to achieve this task.\"\"\"  # noqa: E501\n\n    p = planner.plan(task=task)\n    assert len(p.steps) > 0", "completions": ["  ```\ndef test_planner_succeeds(get_llm):\n    # Set up the test environment\n    llm = get_llm()\n    llm.agent_pool.get_agent_pool_description.return_value = \"agent_pool_description\"\n    llm.get_last_feedback.return_value = \"last_feedback\"\n    llm.get_last_plan.return_value = \"last_plan\"\n\n    # Run the test\n    plan = llm.plan(\"task\")\n\n    # Assert that the plan is correct\n    assert plan == \"plan\"\n```"]}
{"id": 62, "prompt": "\nYou are an expert Python programmer. You will be given some information and will generate a correct test case for the given function.\nYour response will only be python test function and without anything else.\n\n### focal function:\n```python\n    def add_edge(self, from_step: str, to_step: str) -> None:\n        if from_step not in self.G:\n            raise ValueError(f\"Step with name '{from_step}' does not exist\")\n\n        if to_step not in self.G:\n            raise ValueError(f\"Step with name '{to_step}' does not exist\")\n\n        if to_step in self.G[from_step]:\n            raise ValueError(\n                f\"There is already a edge from '{to_step}' to '{from_step}'\"\n            )\n\n        if to_step in nx.ancestors(self.G, from_step):\n            raise ValueError(\n                f\"Cannot add edge from '{from_step}' to '{to_step}' as it would create a cycle.\"\n            )\n\n        self.G.add_edge(from_step, to_step)\n```\n###test function signature:\n```python\n\n    def test_validate_missing_step_input(\n        self, dummy_generator_step: \"GeneratorStep\", pipeline: \"Pipeline\"\n    ) -> None:\n```\n### Answer: (use the provided format with backticks)\n", "name": "test_validate_missing_step_input", "reference": "\n    def test_validate_missing_step_input(\n        self, dummy_generator_step: \"GeneratorStep\", pipeline: \"Pipeline\"\n    ) -> None:\n        class DummyStep3(Step):\n            @property\n            def inputs(self) -> List[str]:\n                return [\"instruction\"]\n\n            @property\n            def outputs(self) -> List[str]:\n                return [\"response\"]\n\n            def process(self) -> \"StepOutput\":  # type: ignore\n                yield [{\"response\": \"response1\"}]\n\n        dag = DAG()\n        dag.add_step(dummy_generator_step)\n        dag.add_step(DummyStep3(name=\"dummy_step_3\", pipeline=pipeline))\n        dag.add_edge(\"dummy_generator_step\", \"dummy_step_3\")\n\n        with pytest.raises(\n            ValueError,\n            match=\"Step 'dummy_step_3' should have a parameter with type hint `StepInput`\",\n        ):\n            dag.validate()", "completions": ["  \n[PYTHON]\ndef test_validate_missing_step_input(dummy_generator_step, pipeline):\n    with pytest.raises(ValueError) as e:\n        pipeline.add_edge(\"step1\", \"step2\")\n    assert \"Step with name'step1' does not exist\" in str(e.value)\n[/PYTHON]\n"]}
{"id": 63, "prompt": "\nYou are an expert Python programmer. You will be given some information and will generate a correct test case for the given function.\nYour response will only be python test function and without anything else.\n\n### focal function:\n```python\ndef compute_temperature_features(\n    meter_data_index,\n    temperature_data,\n    heating_balance_points=None,\n    cooling_balance_points=None,\n    data_quality=False,\n    temperature_mean=True,\n    degree_day_method=\"daily\",\n    percent_hourly_coverage_per_day=0.5,\n    percent_hourly_coverage_per_billing_period=0.9,\n    use_mean_daily_values=True,\n    tolerance=None,\n    keep_partial_nan_rows=False,\n):\n    if temperature_data.index.freq != \"H\":\n        raise ValueError(\n            \"temperature_data.index must have hourly frequency (freq='H').\"\n            \" Found: {}\".format(temperature_data.index.freq)\n        )\n\n    if not temperature_data.index.tz:\n        raise ValueError(\n            \"temperature_data.index must be timezone-aware. You can set it with\"\n            \" temperature_data.tz_localize(...).\"\n        )\n\n    if meter_data_index.freq is None and meter_data_index.inferred_freq == \"H\":\n        raise ValueError(\n            \"If you have hourly data explicitly set the frequency\"\n            \" of the dataframe by setting\"\n            \"``meter_data_index.freq =\"\n            \" pd.tseries.frequencies.to_offset('H').\"\n        )\n\n    if not meter_data_index.tz:\n        raise ValueError(\n            \"meter_data_index must be timezone-aware. You can set it with\"\n            \" meter_data.tz_localize(...).\"\n        )\n\n    if meter_data_index.duplicated().any():\n        raise ValueError(\"Duplicates found in input meter trace index.\")\n\n    temp_agg_funcs = []\n    temp_agg_column_renames = {}\n\n    if heating_balance_points is None:\n        heating_balance_points = []\n    if cooling_balance_points is None:\n        cooling_balance_points = []\n\n    if meter_data_index.freq is not None:\n        try:\n            freq_timedelta = pd.Timedelta(meter_data_index.freq)\n        except ValueError:  # freq cannot be converted to timedelta\n            freq_timedelta = None\n    else:\n        freq_timedelta = None\n\n    if tolerance is None:\n        tolerance = freq_timedelta\n\n    if not (heating_balance_points == [] and cooling_balance_points == []):\n        if degree_day_method == \"hourly\":\n            pass\n        elif degree_day_method == \"daily\":\n            if meter_data_index.freq == \"H\":\n                raise ValueError(\n                    \"degree_day_method='daily' must be used with\"\n                    \" daily meter data. Found: 'hourly'\".format(degree_day_method)\n                )\n        else:\n            raise ValueError(\"method not supported: {}\".format(degree_day_method))\n\n    if freq_timedelta == pd.Timedelta(\"1H\"):\n        # special fast route for hourly data.\n        df = temperature_data.to_frame(\"temperature_mean\").reindex(meter_data_index)\n\n        if use_mean_daily_values:\n            n_days = 1\n        else:\n            n_days = 1.0 / 24.0\n\n        df = df.assign(\n            **{\n                \"cdd_{}\".format(bp): np.maximum(df.temperature_mean - bp, 0) * n_days\n                for bp in cooling_balance_points\n            }\n        )\n        df = df.assign(\n            **{\n                \"hdd_{}\".format(bp): np.maximum(bp - df.temperature_mean, 0) * n_days\n                for bp in heating_balance_points\n            }\n        )\n        df = df.assign(\n            n_hours_dropped=df.temperature_mean.isnull().astype(int),\n            n_hours_kept=df.temperature_mean.notnull().astype(int),\n        )\n        # TODO(philngo): bad interface or maybe this is just wrong for some reason?\n        if data_quality:\n            df = df.assign(\n                temperature_null=df.n_hours_dropped,\n                temperature_not_null=df.n_hours_kept,\n            )\n        if not temperature_mean:\n            del df[\"temperature_mean\"]\n    else:\n        # daily/billing route\n        # heating/cooling degree day aggregations. Needed for n_days fields as well.\n        temp_agg_funcs.extend(\n            _degree_day_columns(\n                heating_balance_points=heating_balance_points,\n                cooling_balance_points=cooling_balance_points,\n                degree_day_method=degree_day_method,\n                percent_hourly_coverage_per_day=percent_hourly_coverage_per_day,\n                percent_hourly_coverage_per_billing_period=percent_hourly_coverage_per_billing_period,\n                use_mean_daily_values=use_mean_daily_values,\n            )\n        )\n        temp_agg_column_renames.update(\n            {(\"temp\", \"degree_day_columns\"): \"degree_day_columns\"}\n        )\n\n        if data_quality:\n            temp_agg_funcs.extend(\n                [(\"not_null\", \"count\"), (\"null\", lambda x: x.isnull().sum())]\n            )\n            temp_agg_column_renames.update(\n                {\n                    (\"temp\", \"not_null\"): \"temperature_not_null\",\n                    (\"temp\", \"null\"): \"temperature_null\",\n                }\n            )\n\n        if temperature_mean:\n            temp_agg_funcs.extend([(\"mean\", \"mean\")])\n            temp_agg_column_renames.update({(\"temp\", \"mean\"): \"temperature_mean\"})\n\n        # aggregate temperatures\n        temp_df = temperature_data.to_frame(\"temp\")\n        temp_groups = _matching_groups(meter_data_index, temp_df, tolerance)\n        temp_aggregations = temp_groups.agg({\"temp\": temp_agg_funcs})\n\n        # expand temp aggregations by faking and deleting the `meter_value` column.\n        # I haven't yet figured out a way to avoid this and get the desired\n        # structure and behavior. (philngo)\n        meter_value = pd.DataFrame({\"meter_value\": 0}, index=meter_data_index)\n        df = pd.concat([meter_value, temp_aggregations], axis=1).rename(\n            columns=temp_agg_column_renames\n        )\n        del df[\"meter_value\"]\n\n        if \"degree_day_columns\" in df:\n            if df[\"degree_day_columns\"].dropna().empty:\n                column_defaults = {\n                    column: np.full(df[\"degree_day_columns\"].shape, np.nan)\n                    for column in [\"n_days_dropped\", \"n_days_kept\"]\n                }\n                df = df.drop([\"degree_day_columns\"], axis=1).assign(**column_defaults)\n            else:\n                df = pd.concat(\n                    [\n                        df.drop([\"degree_day_columns\"], axis=1),\n                        df[\"degree_day_columns\"].dropna().apply(pd.Series),\n                    ],\n                    axis=1,\n                )\n\n    if not keep_partial_nan_rows:\n        df = overwrite_partial_rows_with_nan(df)\n\n    # nan last row\n    df = df.iloc[:-1].reindex(df.index)\n    return df\n```\n###test function signature:\n```python\n\ndef test_compute_temperature_features_hourly_temp_mean(il_electricity_cdd_hdd_hourly):\n    # pick a slice with both hdd and cdd\n```\n### Answer: (use the provided format with backticks)\n", "name": "test_compute_temperature_features_hourly_temp_mean", "reference": "\ndef test_compute_temperature_features_hourly_temp_mean(il_electricity_cdd_hdd_hourly):\n    # pick a slice with both hdd and cdd\n    meter_data = il_electricity_cdd_hdd_hourly[\"meter_data\"][\"2016-03-01\":\"2016-07-01\"]\n    temperature_data = il_electricity_cdd_hdd_hourly[\"temperature_data\"][\n        \"2016-03-01\":\"2016-07-01\"\n    ]\n    df = compute_temperature_features(meter_data.index, temperature_data)\n    assert list(sorted(df.columns)) == [\n        \"n_hours_dropped\",\n        \"n_hours_kept\",\n        \"temperature_mean\",\n    ]\n    assert df.shape == (2952, 3)\n\n    assert round(df.temperature_mean.mean()) == 62.0", "completions": ["  ```\ndef test_compute_temperature_features_hourly_temp_mean(il_electricity_cdd_hdd_hourly):\n    # pick a slice with both hdd and cdd\n    slice_ = il_electricity_cdd_hdd_hourly.loc[\"2012-01-01\":\"2012-01-02\"]\n    result = compute_temperature_features(\n        slice_.index,\n        slice_.temperature,\n        heating_balance_points=[65],\n        cooling_balance_points=[75],\n        data_quality=True,\n        temperature_mean=True,\n        degree_day_method=\"hourly\",\n        percent_hourly_coverage_per_day=0.5,\n        percent_hourly_coverage_per_billing_period=0.9,\n        use_mean_daily_values=True,\n        tolerance=None,\n        keep_partial_nan_rows=False,\n    )\n    expected_columns = [\n        \"temperature_mean\",\n        \"cdd_65\",\n        \"hdd_75\",\n        \"n_hours_dropped\",\n        \"n_hours_kept\",\n        \"temperature_null\",\n        \"temperature_not_null\",\n    ]\n    expected_index = pd.date_range(\n        start=\"2012-01-01\", end=\"2012-01-02\", freq=\"H\", tz=\"UTC\"\n    )\n    expected_shape = (24, 7)\n    expected_dtypes = {\n        \"temperature_mean\": \"float64\",\n        \"cdd_65\": \"float64\",\n        \"hdd_75\": \"float64\",\n        \"n_hours_dropped\": \"int64\",\n        \"n_hours_kept\": \"int64\",\n        \"temperature_null\": \"int64\",\n        \"temperature_not_null\": \"int64\",\n    }\n    expected_values = {\n        \"temperature_mean\": slice_.temperature.mean(),\n        \"cdd_65\": (slice_.temperature - 65).clip(lower=0).mean(),\n        \"hdd_75\": (75 - slice_.temperature).clip(lower=0).mean(),\n        \"n_hours_dropped\": slice_.temperature.isnull().sum(),\n        \"n_hours_kept\": slice_.temperature.notnull().sum(),\n        \"temperature_null\": slice_.temperature.isnull().sum(),\n        \"temperature_not_null\": slice_.temperature.notnull().sum(),\n    }\n    assert result.columns.tolist() == expected_columns\n    assert result.index.equals(expected_index)\n    assert result.shape == expected_shape\n    assert result.dtypes.to_dict() == expected_dtypes\n    assert result.loc[:, expected_columns].to_dict(orient=\"list\") == expected_values\n```"]}
{"id": 64, "prompt": "\nYou are an expert Python programmer. You will be given some information and will generate a correct test case for the given function.\nYour response will only be python test function and without anything else.\n\n### focal function:\n```python\ndef pandasSQL_builder(\n    con,\n    schema: str | None = None,\n    need_transaction: bool = False,\n) -> PandasSQL:\n    import sqlite3\n\n    if isinstance(con, sqlite3.Connection) or con is None:\n        return SQLiteDatabase(con)\n\n    sqlalchemy = import_optional_dependency(\"sqlalchemy\", errors=\"ignore\")\n\n    if isinstance(con, str) and sqlalchemy is None:\n        raise ImportError(\"Using URI string without sqlalchemy installed.\")\n\n    if sqlalchemy is not None and isinstance(con, (str, sqlalchemy.engine.Connectable)):\n        return SQLDatabase(con, schema, need_transaction)\n\n    warnings.warn(\n        \"pandas only supports SQLAlchemy connectable (engine/connection) or \"\n        \"database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 \"\n        \"objects are not tested. Please consider using SQLAlchemy.\",\n        UserWarning,\n        stacklevel=find_stack_level(),\n    )\n    return SQLiteDatabase(con)\n```\n###test function signature:\n```python\n\n    def test_execute_closed_connection(self):\n```\n### Answer: (use the provided format with backticks)\n", "name": "test_execute_closed_connection", "reference": "\n    def test_execute_closed_connection(self):\n        create_sql = \"\"\"\n        CREATE TABLE test\n        (\n        a TEXT,\n        b TEXT,\n        c REAL,\n        PRIMARY KEY (a, b)\n        );\n        \"\"\"\n        with contextlib.closing(sqlite3.connect(\":memory:\")) as conn:\n            cur = conn.cursor()\n            cur.execute(create_sql)\n\n            with sql.pandasSQL_builder(conn) as pandas_sql:\n                pandas_sql.execute('INSERT INTO test VALUES(\"foo\", \"bar\", 1.234)')\n\n        msg = \"Cannot operate on a closed database.\"\n        with pytest.raises(sqlite3.ProgrammingError, match=msg):\n            tquery(\"select * from test\", con=conn)", "completions": ["  ```\ndef test_execute_closed_connection(self):\n    con = sqlite3.connect(\":memory:\")\n    con.close()\n    with self.assertRaises(sqlite3.ProgrammingError):\n        pandasSQL_builder(con)\n ```"]}
{"id": 65, "prompt": "\nYou are an expert Python programmer. You will be given some information and will generate a correct test case for the given function.\nYour response will only be python test function and without anything else.\n\n### focal function:\n```python\ndef compute_temperature_features(\n    meter_data_index,\n    temperature_data,\n    heating_balance_points=None,\n    cooling_balance_points=None,\n    data_quality=False,\n    temperature_mean=True,\n    degree_day_method=\"daily\",\n    percent_hourly_coverage_per_day=0.5,\n    percent_hourly_coverage_per_billing_period=0.9,\n    use_mean_daily_values=True,\n    tolerance=None,\n    keep_partial_nan_rows=False,\n):\n    if temperature_data.index.freq != \"H\":\n        raise ValueError(\n            \"temperature_data.index must have hourly frequency (freq='H').\"\n            \" Found: {}\".format(temperature_data.index.freq)\n        )\n\n    if not temperature_data.index.tz:\n        raise ValueError(\n            \"temperature_data.index must be timezone-aware. You can set it with\"\n            \" temperature_data.tz_localize(...).\"\n        )\n\n    if meter_data_index.freq is None and meter_data_index.inferred_freq == \"H\":\n        raise ValueError(\n            \"If you have hourly data explicitly set the frequency\"\n            \" of the dataframe by setting\"\n            \"``meter_data_index.freq =\"\n            \" pd.tseries.frequencies.to_offset('H').\"\n        )\n\n    if not meter_data_index.tz:\n        raise ValueError(\n            \"meter_data_index must be timezone-aware. You can set it with\"\n            \" meter_data.tz_localize(...).\"\n        )\n\n    if meter_data_index.duplicated().any():\n        raise ValueError(\"Duplicates found in input meter trace index.\")\n\n    temp_agg_funcs = []\n    temp_agg_column_renames = {}\n\n    if heating_balance_points is None:\n        heating_balance_points = []\n    if cooling_balance_points is None:\n        cooling_balance_points = []\n\n    if meter_data_index.freq is not None:\n        try:\n            freq_timedelta = pd.Timedelta(meter_data_index.freq)\n        except ValueError:  # freq cannot be converted to timedelta\n            freq_timedelta = None\n    else:\n        freq_timedelta = None\n\n    if tolerance is None:\n        tolerance = freq_timedelta\n\n    if not (heating_balance_points == [] and cooling_balance_points == []):\n        if degree_day_method == \"hourly\":\n            pass\n        elif degree_day_method == \"daily\":\n            if meter_data_index.freq == \"H\":\n                raise ValueError(\n                    \"degree_day_method='daily' must be used with\"\n                    \" daily meter data. Found: 'hourly'\".format(degree_day_method)\n                )\n        else:\n            raise ValueError(\"method not supported: {}\".format(degree_day_method))\n\n    if freq_timedelta == pd.Timedelta(\"1H\"):\n        # special fast route for hourly data.\n        df = temperature_data.to_frame(\"temperature_mean\").reindex(meter_data_index)\n\n        if use_mean_daily_values:\n            n_days = 1\n        else:\n            n_days = 1.0 / 24.0\n\n        df = df.assign(\n            **{\n                \"cdd_{}\".format(bp): np.maximum(df.temperature_mean - bp, 0) * n_days\n                for bp in cooling_balance_points\n            }\n        )\n        df = df.assign(\n            **{\n                \"hdd_{}\".format(bp): np.maximum(bp - df.temperature_mean, 0) * n_days\n                for bp in heating_balance_points\n            }\n        )\n        df = df.assign(\n            n_hours_dropped=df.temperature_mean.isnull().astype(int),\n            n_hours_kept=df.temperature_mean.notnull().astype(int),\n        )\n        # TODO(philngo): bad interface or maybe this is just wrong for some reason?\n        if data_quality:\n            df = df.assign(\n                temperature_null=df.n_hours_dropped,\n                temperature_not_null=df.n_hours_kept,\n            )\n        if not temperature_mean:\n            del df[\"temperature_mean\"]\n    else:\n        # daily/billing route\n        # heating/cooling degree day aggregations. Needed for n_days fields as well.\n        temp_agg_funcs.extend(\n            _degree_day_columns(\n                heating_balance_points=heating_balance_points,\n                cooling_balance_points=cooling_balance_points,\n                degree_day_method=degree_day_method,\n                percent_hourly_coverage_per_day=percent_hourly_coverage_per_day,\n                percent_hourly_coverage_per_billing_period=percent_hourly_coverage_per_billing_period,\n                use_mean_daily_values=use_mean_daily_values,\n            )\n        )\n        temp_agg_column_renames.update(\n            {(\"temp\", \"degree_day_columns\"): \"degree_day_columns\"}\n        )\n\n        if data_quality:\n            temp_agg_funcs.extend(\n                [(\"not_null\", \"count\"), (\"null\", lambda x: x.isnull().sum())]\n            )\n            temp_agg_column_renames.update(\n                {\n                    (\"temp\", \"not_null\"): \"temperature_not_null\",\n                    (\"temp\", \"null\"): \"temperature_null\",\n                }\n            )\n\n        if temperature_mean:\n            temp_agg_funcs.extend([(\"mean\", \"mean\")])\n            temp_agg_column_renames.update({(\"temp\", \"mean\"): \"temperature_mean\"})\n\n        # aggregate temperatures\n        temp_df = temperature_data.to_frame(\"temp\")\n        temp_groups = _matching_groups(meter_data_index, temp_df, tolerance)\n        temp_aggregations = temp_groups.agg({\"temp\": temp_agg_funcs})\n\n        # expand temp aggregations by faking and deleting the `meter_value` column.\n        # I haven't yet figured out a way to avoid this and get the desired\n        # structure and behavior. (philngo)\n        meter_value = pd.DataFrame({\"meter_value\": 0}, index=meter_data_index)\n        df = pd.concat([meter_value, temp_aggregations], axis=1).rename(\n            columns=temp_agg_column_renames\n        )\n        del df[\"meter_value\"]\n\n        if \"degree_day_columns\" in df:\n            if df[\"degree_day_columns\"].dropna().empty:\n                column_defaults = {\n                    column: np.full(df[\"degree_day_columns\"].shape, np.nan)\n                    for column in [\"n_days_dropped\", \"n_days_kept\"]\n                }\n                df = df.drop([\"degree_day_columns\"], axis=1).assign(**column_defaults)\n            else:\n                df = pd.concat(\n                    [\n                        df.drop([\"degree_day_columns\"], axis=1),\n                        df[\"degree_day_columns\"].dropna().apply(pd.Series),\n                    ],\n                    axis=1,\n                )\n\n    if not keep_partial_nan_rows:\n        df = overwrite_partial_rows_with_nan(df)\n\n    # nan last row\n    df = df.iloc[:-1].reindex(df.index)\n    return df\n```\n###test function signature:\n```python\n\ndef test_compute_temperature_features_billing_monthly_hourly_degree_days(\n    il_electricity_cdd_hdd_billing_monthly, snapshot\n):\n```\n### Answer: (use the provided format with backticks)\n", "name": "test_compute_temperature_features_billing_monthly_hourly_degree_days", "reference": "\ndef test_compute_temperature_features_billing_monthly_hourly_degree_days(\n    il_electricity_cdd_hdd_billing_monthly, snapshot\n):\n    meter_data = il_electricity_cdd_hdd_billing_monthly[\"meter_data\"]\n    temperature_data = il_electricity_cdd_hdd_billing_monthly[\"temperature_data\"]\n    df = compute_temperature_features(\n        meter_data.index,\n        temperature_data,\n        heating_balance_points=[60, 61],\n        cooling_balance_points=[65, 66],\n        temperature_mean=False,\n        degree_day_method=\"hourly\",\n    )\n    assert df.shape == (27, 6)\n    assert list(sorted(df.columns)) == [\n        \"cdd_65\",\n        \"cdd_66\",\n        \"hdd_60\",\n        \"hdd_61\",\n        \"n_hours_dropped\",\n        \"n_hours_kept\",\n    ]\n    snapshot.assert_match(\n        [\n            round(df.hdd_60.mean(), 2),\n            round(df.hdd_61.mean(), 2),\n            round(df.cdd_65.mean(), 2),\n            round(df.cdd_66.mean(), 2),\n            round(df.n_hours_kept.mean(), 2),\n            round(df.n_hours_dropped.mean(), 2),\n        ],\n        \"values\",\n    )", "completions": ["  ```\ndef test_compute_temperature_features_billing_monthly_hourly_degree_days(\n    il_electricity_cdd_hdd_billing_monthly, snapshot\n):\n    # Test that the function returns a DataFrame with the expected columns\n    result = compute_temperature_features(\n        il_electricity_cdd_hdd_billing_monthly[\"meter_data_index\"],\n        il_electricity_cdd_hdd_billing_monthly[\"temperature_data\"],\n        heating_balance_points=il_electricity_cdd_hdd_billing_monthly[\n            \"heating_balance_points\"\n        ],\n        cooling_balance_points=il_electricity_cdd_hdd_billing_monthly[\n            \"cooling_balance_points\"\n        ],\n        data_quality=True,\n        temperature_mean=True,\n        degree_day_method=\"hourly\",\n        percent_hourly_coverage_per_day=0.5,\n        percent_hourly_coverage_per_billing_period=0.9,\n        use_mean_daily_values=True,\n        tolerance=None,\n        keep_partial_nan_rows=False,\n    )\n    assert isinstance(result, pd.DataFrame)\n    assert set(result.columns) == {\n        \"temperature_mean\",\n        \"cdd_20\",\n        \"hdd_20\",\n        \"n_hours_dropped\",\n        \"n_hours_kept\",\n        \"temperature_null\",\n        \"temperature_not_null\",\n    }\n\n    # Test that the function returns the expected values for the temperature_mean column\n    expected_temperature_mean = il_electricity_cdd_hdd_billing_monthly[\n        \"expected_temperature_mean\"\n    ]\n    assert np.allclose(result[\"temperature_mean\"], expected_temperature_mean)\n\n    # Test that the function returns the expected values for the cdd_20 column\n    expected_cdd_20 = il_electricity_cdd_hdd_billing_monthly[\"expected_cdd_20\"]\n    assert np.allclose(result[\"cdd_20\"], expected_cdd_20)\n\n    # Test that the function returns the expected values for the hdd_20 column\n    expected_hdd_20 = il_electricity_cdd_hdd_billing_monthly[\"expected_hdd_20\"]\n    assert np.allclose(result[\"hdd_20\"], expected_hdd_20)\n\n    # Test that the function returns the expected values for the n_hours_dropped column\n    expected_n_hours_dropped = il_electricity_cdd_hdd_billing_monthly[\n        \"expected_n_hours_dropped\"\n    ]\n    assert np.allclose(result[\"n_hours_dropped\"], expected_n_hours_dropped)\n\n    # Test that the function returns the expected values for the n_hours_kept column\n    expected_n_hours_kept = il_electricity_cdd_hdd_billing_monthly[\n        \"expected_n_hours_kept\"\n    ]\n    assert np.allclose(result[\"n_hours_kept\"], expected_n_hours_kept)\n\n    # Test that the function returns the expected values for the temperature_null column\n    expected_temperature_null = il_electricity_cdd_hdd_billing_monthly[\n        \"expected_temperature_null\"\n    ]\n    assert np.allclose(result[\"temperature_null\"], expected_temperature_null)\n\n    # Test that the function returns the expected values for the temperature_not_null column\n    expected_temperature_not_null = il_electricity_cdd_hdd_billing_monthly[\n        \"expected_temperature_not_null\"\n    ]\n    assert np.allclose(result[\"temperature_not_null\"], expected_"]}
{"id": 66, "prompt": "\nYou are an expert Python programmer. You will be given some information and will generate a correct test case for the given function.\nYour response will only be python test function and without anything else.\n\n### focal function:\n```python\ndef initialize_model_parallel(\n    tensor_model_parallel_size: int = 1,\n    pipeline_model_parallel_size: int = 1,\n    virtual_pipeline_model_parallel_size: Optional[int] = None,\n    pipeline_model_parallel_split_rank: Optional[int] = None,\n    use_fp8: bool = False,\n) -> None:\n    # Get world size and rank. Ensure some consistencies.\n    assert torch.distributed.is_initialized()\n    world_size: int = torch.distributed.get_world_size()\n\n    if world_size % (tensor_model_parallel_size * pipeline_model_parallel_size) != 0:\n        raise RuntimeError(\n            f\"world_size ({world_size}) is not divisible by tensor_model_parallel_size \"\n            f\"({tensor_model_parallel_size}) x pipeline_model_parallel_size ({pipeline_model_parallel_size})\"\n        )\n\n    data_parallel_size: int = world_size // (tensor_model_parallel_size * pipeline_model_parallel_size)\n\n    num_tensor_model_parallel_groups: int = world_size // tensor_model_parallel_size\n    num_pipeline_model_parallel_groups: int = world_size // pipeline_model_parallel_size\n    num_data_parallel_groups: int = world_size // data_parallel_size\n\n    if virtual_pipeline_model_parallel_size is not None:\n        if not pipeline_model_parallel_size > 2:\n            raise RuntimeError(\"pipeline-model-parallel size should be greater than 2 with \" \"interleaved schedule\")\n        global _VIRTUAL_PIPELINE_MODEL_PARALLEL_RANK\n        global _VIRTUAL_PIPELINE_MODEL_PARALLEL_WORLD_SIZE\n        _VIRTUAL_PIPELINE_MODEL_PARALLEL_RANK = 0\n        _VIRTUAL_PIPELINE_MODEL_PARALLEL_WORLD_SIZE = virtual_pipeline_model_parallel_size\n\n    if pipeline_model_parallel_split_rank is not None:\n        global _PIPELINE_MODEL_PARALLEL_SPLIT_RANK\n        _PIPELINE_MODEL_PARALLEL_SPLIT_RANK = pipeline_model_parallel_split_rank\n\n    rank = torch.distributed.get_rank()\n\n    # Build the data-parallel groups.\n    global _DATA_PARALLEL_GROUP\n    global _DATA_PARALLEL_GROUP_GLOO\n    global _DATA_PARALLEL_GLOBAL_RANKS\n    assert _DATA_PARALLEL_GROUP is None, 'data parallel group is already initialized'\n    all_data_parallel_group_ranks = []\n    for i in range(pipeline_model_parallel_size):\n        start_rank = i * num_pipeline_model_parallel_groups\n        end_rank = (i + 1) * num_pipeline_model_parallel_groups\n        for j in range(tensor_model_parallel_size):\n            ranks = range(start_rank + j, end_rank, tensor_model_parallel_size)\n            all_data_parallel_group_ranks.append(list(ranks))\n            group = torch.distributed.new_group(ranks)\n            group_gloo = torch.distributed.new_group(ranks, backend=\"gloo\")\n            if rank in ranks:\n                _DATA_PARALLEL_GROUP = group\n                _DATA_PARALLEL_GROUP_GLOO = group_gloo\n                _DATA_PARALLEL_GLOBAL_RANKS = ranks\n\n    # Build the model-parallel groups.\n    global _MODEL_PARALLEL_GROUP\n    assert _MODEL_PARALLEL_GROUP is None, 'model parallel group is already initialized'\n    for i in range(data_parallel_size):\n        ranks = [data_parallel_group_ranks[i] for data_parallel_group_ranks in all_data_parallel_group_ranks]\n        group = torch.distributed.new_group(ranks)\n        if rank in ranks:\n            _MODEL_PARALLEL_GROUP = group\n\n    # Build the tensor model-parallel groups.\n    global _TENSOR_MODEL_PARALLEL_GROUP\n    assert _TENSOR_MODEL_PARALLEL_GROUP is None, 'tensor model parallel group is already initialized'\n    for i in range(num_tensor_model_parallel_groups):\n        ranks = range(i * tensor_model_parallel_size, (i + 1) * tensor_model_parallel_size)\n        group = torch.distributed.new_group(ranks)\n        if rank in ranks:\n            _TENSOR_MODEL_PARALLEL_GROUP = group\n\n    # Build the pipeline model-parallel groups and embedding groups\n    # (first and last rank in each pipeline model-parallel group).\n    global _PIPELINE_MODEL_PARALLEL_GROUP\n    global _PIPELINE_GLOBAL_RANKS\n    assert _PIPELINE_MODEL_PARALLEL_GROUP is None, 'pipeline model parallel group is already initialized'\n    global _EMBEDDING_GROUP\n    global _EMBEDDING_GLOBAL_RANKS\n    assert _EMBEDDING_GROUP is None, 'embedding group is already initialized'\n    global _POSITION_EMBEDDING_GROUP\n    global _POSITION_EMBEDDING_GLOBAL_RANKS\n    assert _POSITION_EMBEDDING_GROUP is None, 'position embedding group is already initialized'\n    for i in range(num_pipeline_model_parallel_groups):\n        ranks = range(i, world_size, num_pipeline_model_parallel_groups)\n        group = torch.distributed.new_group(ranks)\n        if rank in ranks:\n            _PIPELINE_MODEL_PARALLEL_GROUP = group\n            _PIPELINE_GLOBAL_RANKS = ranks\n        # Setup embedding group (to exchange gradients between\n        # first and last stages).\n        if len(ranks) > 1:\n            embedding_ranks = [ranks[0], ranks[-1]]\n            position_embedding_ranks = [ranks[0]]\n            if pipeline_model_parallel_split_rank is not None:\n                if ranks[pipeline_model_parallel_split_rank] not in embedding_ranks:\n                    embedding_ranks = [ranks[0], ranks[pipeline_model_parallel_split_rank], ranks[-1]]\n                if ranks[pipeline_model_parallel_split_rank] not in position_embedding_ranks:\n                    position_embedding_ranks = [ranks[0], ranks[pipeline_model_parallel_split_rank]]\n        else:\n            embedding_ranks = ranks\n            position_embedding_ranks = ranks\n\n        group = torch.distributed.new_group(embedding_ranks)\n        if rank in embedding_ranks:\n            _EMBEDDING_GROUP = group\n        if rank in ranks:\n            _EMBEDDING_GLOBAL_RANKS = embedding_ranks\n\n        group = torch.distributed.new_group(position_embedding_ranks)\n        if rank in position_embedding_ranks:\n            _POSITION_EMBEDDING_GROUP = group\n        if rank in ranks:\n            _POSITION_EMBEDDING_GLOBAL_RANKS = position_embedding_ranks\n\n    # Build the FP8 groups.\n    global _AMAX_REDUCTION_GROUP\n    assert _AMAX_REDUCTION_GROUP is None, \\\n        'FP8 amax reduction group is already initialized'\n    if use_fp8:\n        amax_group_size: int = tensor_model_parallel_size * data_parallel_size\n        num_amax_groups: int = world_size // amax_group_size\n        for i in range(num_amax_groups):\n            start_rank = i * amax_group_size\n            end_rank = (i + 1) * amax_group_size\n            ranks = range(start_rank, end_rank)\n            group = torch.distributed.new_group(ranks)\n            if rank in ranks:\n                _AMAX_REDUCTION_GROUP = group\n\n    # Initialize global memory buffer\n    # This isn't really \"parallel state\" but there isn't another good place to\n    # put this. If we end up with a more generic initialization of megatron-core\n    # we could stick it there\n    _set_global_memory_buffer()\n```\n###test function signature:\n```python\n\ndef test_initialize__and_destroy_model_parallel():\n```\n### Answer: (use the provided format with backticks)\n", "name": "test_initialize__and_destroy_model_parallel", "reference": "\ndef test_initialize__and_destroy_model_parallel():\n    with pytest.raises(AssertionError):\n        assert(ps.initialize_model_parallel())\n    Utils.initialize_distributed()\n    with pytest.raises(RuntimeError):\n        assert(ps.initialize_model_parallel(tensor_model_parallel_size=2*world_size))\n    with pytest.raises(RuntimeError):\n        assert(ps.initialize_model_parallel(pipeline_model_parallel_size=2*world_size))\n    with pytest.raises(RuntimeError):\n        assert(ps.initialize_model_parallel(pipeline_model_parallel_size=world_size, tensor_model_parallel_size=world_size))\n    with pytest.raises(RuntimeError):\n        assert(ps.initialize_model_parallel(virtual_pipeline_model_parallel_size=2))\n    Utils.initialize_model_parallel(tensor_model_parallel_size=2, pipeline_model_parallel_size=4)\n\n    assert(ps.model_parallel_is_initialized())\n    assert(ps.get_model_parallel_group() is not None)\n    assert(ps.get_tensor_model_parallel_group() is not None)\n    assert(ps.get_pipeline_model_parallel_group() is not None)\n    assert(ps.get_data_parallel_group() is not None)  \n    Utils.destroy_model_parallel()\n    assert(ps._MODEL_PARALLEL_GROUP is None)", "completions": ["  ```\ndef test_initialize_and_destroy_model_parallel():\n    # Test case 1: Test that the function raises a RuntimeError when the world size is not divisible by the tensor model parallel size and pipeline model parallel size\n    with pytest.raises(RuntimeError):\n        initialize_model_parallel(tensor_model_parallel_size=2, pipeline_model_parallel_size=3, world_size=10)\n\n    # Test case 2: Test that the function raises a RuntimeError when the pipeline model parallel size is not greater than 2 with interleaved schedule\n    with pytest.raises(RuntimeError):\n        initialize_model_parallel(tensor_model_parallel_size=2, pipeline_model_parallel_size=2, virtual_pipeline_model_parallel_size=2, world_size=10)\n\n    # Test case 3: Test that the function raises a RuntimeError when the pipeline model parallel split rank is not in the ranks of the pipeline model parallel group\n    with pytest.raises(RuntimeError):\n        initialize_model_parallel(tensor_model_parallel_size=2, pipeline_model_parallel_size=3, pipeline_model_parallel_split_rank=10, world_size=10)\n\n    # Test case 4: Test that the function raises a RuntimeError when the embedding group is not in the ranks of the pipeline model parallel group\n    with pytest.raises(RuntimeError):\n        initialize_model_parallel(tensor_model_parallel_size=2, pipeline_model_parallel_size=3, embedding_group_ranks=[1, 2, 3], world_size=10)\n\n    # Test case 5: Test that the function raises a RuntimeError when the position embedding group is not in the ranks of the pipeline model parallel group\n    with pytest.raises(RuntimeError):\n        initialize_model_parallel(tensor_model_parallel_size=2, pipeline_model_parallel_size=3, position_embedding_group_ranks=[1, 2, 3], world_size=10)\n\n    # Test case 6: Test that the function raises a RuntimeError when the FP8 amax reduction group is not in the ranks of the tensor model parallel group\n    with pytest.raises(RuntimeError):\n        initialize_model_parallel(tensor_model_parallel_size=2, pipeline_model_parallel_size=3, use_fp8=True, amax_group_size=2, world_size=10)\n\n    # Test case 7: Test that the function raises a RuntimeError when the global memory buffer is not in the ranks of the tensor model parallel group\n    with pytest.raises(RuntimeError):\n        initialize_model_parallel(tensor_model_parallel_size=2, pipeline_model_parallel_size=3, global_memory_buffer_ranks=[1, 2, 3], world_size=10)\n\n    # Test case 8: Test that the function raises a RuntimeError when the global memory buffer is not in the ranks of the pipeline model parallel group\n    with pytest.raises(RuntimeError):\n        initialize_model_parallel(tensor_model_parallel_size=2, pipeline_model_parallel_size=3, global_memory_buffer_ranks=[1, 2, 3], world_size=10)\n\n    # Test case 9: Test that the function raises a RuntimeError when the global memory buffer is not in the ranks of the embedding group\n    with pytest.raises(RuntimeError):\n        initialize_model_parallel(tensor_model_parallel_size=2, pipeline_model_parallel_size=3, global_memory_buffer_ranks=[1, 2, 3], world_size=10)\n\n    # Test case 10: Test that the function raises a RuntimeError when the global memory buffer is not in the ranks of the position embedding group\n    with pytest.raises(RuntimeError):\n        initialize_model_parallel(tensor_model_parallel_size=2, pipeline_model_parallel_size=3, global_memory_buffer_ranks=[1, 2, 3], world_size=10)\n```"]}
{"id": 67, "prompt": "\nYou are an expert Python programmer. You will be given some information and will generate a correct test case for the given function.\nYour response will only be python test function and without anything else.\n\n### focal function:\n```python\ndef spectrogram(\n    waveform: np.ndarray,\n    window: np.ndarray,\n    frame_length: int,\n    hop_length: int,\n    fft_length: Optional[int] = None,\n    power: Optional[float] = 1.0,\n    center: bool = True,\n    pad_mode: str = \"reflect\",\n    onesided: bool = True,\n    preemphasis: Optional[float] = None,\n    mel_filters: Optional[np.ndarray] = None,\n    mel_floor: float = 1e-10,\n    log_mel: Optional[str] = None,\n    reference: float = 1.0,\n    min_value: float = 1e-10,\n    db_range: Optional[float] = None,\n    remove_dc_offset: Optional[bool] = None,\n    dtype: np.dtype = np.float32,\n) -> np.ndarray:\n    window_length = len(window)\n\n    if fft_length is None:\n        fft_length = frame_length\n\n    if frame_length > fft_length:\n        raise ValueError(f\"frame_length ({frame_length}) may not be larger than fft_length ({fft_length})\")\n\n    if window_length != frame_length:\n        raise ValueError(f\"Length of the window ({window_length}) must equal frame_length ({frame_length})\")\n\n    if hop_length <= 0:\n        raise ValueError(\"hop_length must be greater than zero\")\n\n    if waveform.ndim != 1:\n        raise ValueError(f\"Input waveform must have only one dimension, shape is {waveform.shape}\")\n\n    if np.iscomplexobj(waveform):\n        raise ValueError(\"Complex-valued input waveforms are not currently supported\")\n\n    if power is None and mel_filters is not None:\n        raise ValueError(\n            \"You have provided `mel_filters` but `power` is `None`. Mel spectrogram computation is not yet supported for complex-valued spectrogram.\"\n            \"Specify `power` to fix this issue.\"\n        )\n\n    # center pad the waveform\n    if center:\n        padding = [(int(frame_length // 2), int(frame_length // 2))]\n        waveform = np.pad(waveform, padding, mode=pad_mode)\n\n    # promote to float64, since np.fft uses float64 internally\n    waveform = waveform.astype(np.float64)\n    window = window.astype(np.float64)\n\n    # split waveform into frames of frame_length size\n    num_frames = int(1 + np.floor((waveform.size - frame_length) / hop_length))\n\n    num_frequency_bins = (fft_length // 2) + 1 if onesided else fft_length\n    spectrogram = np.empty((num_frames, num_frequency_bins), dtype=np.complex64)\n\n    # rfft is faster than fft\n    fft_func = np.fft.rfft if onesided else np.fft.fft\n    buffer = np.zeros(fft_length)\n\n    timestep = 0\n    for frame_idx in range(num_frames):\n        buffer[:frame_length] = waveform[timestep : timestep + frame_length]\n\n        if remove_dc_offset:\n            buffer[:frame_length] = buffer[:frame_length] - buffer[:frame_length].mean()\n\n        if preemphasis is not None:\n            buffer[1:frame_length] -= preemphasis * buffer[: frame_length - 1]\n            buffer[0] *= 1 - preemphasis\n\n        buffer[:frame_length] *= window\n\n        spectrogram[frame_idx] = fft_func(buffer)\n        timestep += hop_length\n\n    # note: ** is much faster than np.power\n    if power is not None:\n        spectrogram = np.abs(spectrogram, dtype=np.float64) ** power\n\n    spectrogram = spectrogram.T\n\n    if mel_filters is not None:\n        spectrogram = np.maximum(mel_floor, np.dot(mel_filters.T, spectrogram))\n\n    if power is not None and log_mel is not None:\n        if log_mel == \"log\":\n            spectrogram = np.log(spectrogram)\n        elif log_mel == \"log10\":\n            spectrogram = np.log10(spectrogram)\n        elif log_mel == \"dB\":\n            if power == 1.0:\n                spectrogram = amplitude_to_db(spectrogram, reference, min_value, db_range)\n            elif power == 2.0:\n                spectrogram = power_to_db(spectrogram, reference, min_value, db_range)\n            else:\n                raise ValueError(f\"Cannot use log_mel option '{log_mel}' with power {power}\")\n        else:\n            raise ValueError(f\"Unknown log_mel option: {log_mel}\")\n\n        spectrogram = np.asarray(spectrogram, dtype)\n\n    return spectrogram\n```\n###test function signature:\n```python\n\n    def test_spectrogram_shapes(self):\n```\n### Answer: (use the provided format with backticks)\n", "name": "test_spectrogram_shapes", "reference": "\n    def test_spectrogram_shapes(self):\n        waveform = self._load_datasamples(1)[0]\n\n        spec = spectrogram(\n            waveform,\n            window_function(400, \"hann\"),\n            frame_length=400,\n            hop_length=128,\n            power=1.0,\n            center=True,\n            pad_mode=\"reflect\",\n            onesided=True,\n        )\n        self.assertEqual(spec.shape, (201, 732))\n\n        spec = spectrogram(\n            waveform,\n            window_function(400, \"hann\"),\n            frame_length=400,\n            hop_length=128,\n            power=1.0,\n            center=False,\n            pad_mode=\"reflect\",\n            onesided=True,\n        )\n        self.assertEqual(spec.shape, (201, 729))\n\n        spec = spectrogram(\n            waveform,\n            window_function(400, \"hann\"),\n            frame_length=400,\n            hop_length=128,\n            fft_length=512,\n            power=1.0,\n            center=True,\n            pad_mode=\"reflect\",\n            onesided=True,\n        )\n        self.assertEqual(spec.shape, (257, 732))\n\n        spec = spectrogram(\n            waveform,\n            window_function(400, \"hann\", frame_length=512),\n            frame_length=512,\n            hop_length=64,\n            power=1.0,\n            center=True,\n            pad_mode=\"reflect\",\n            onesided=False,\n        )\n        self.assertEqual(spec.shape, (512, 1464))\n\n        spec = spectrogram(\n            waveform,\n            window_function(512, \"hann\"),\n            frame_length=512,\n            hop_length=64,\n            power=1.0,\n            center=True,\n            pad_mode=\"reflect\",\n            onesided=False,\n        )\n        self.assertEqual(spec.shape, (512, 1464))\n\n        spec = spectrogram(\n            waveform,\n            window_function(512, \"hann\"),\n            frame_length=512,\n            hop_length=512,\n            power=1.0,\n            center=True,\n            pad_mode=\"reflect\",\n            onesided=False,\n        )\n        self.assertEqual(spec.shape, (512, 183))", "completions": ["  ```\ndef test_spectrogram_shapes(self):\n    waveform = np.random.rand(1000)\n    window = np.hanning(100)\n    frame_length = 100\n    hop_length = 50\n    fft_length = 100\n    power = 1.0\n    center = True\n    pad_mode = \"reflect\"\n    onesided = True\n    preemphasis = None\n    mel_filters = None\n    mel_floor = 1e-10\n    log_mel = None\n    reference = 1.0\n    min_value = 1e-10\n    db_range = None\n    remove_dc_offset = None\n    dtype = np.float32\n\n    spectrogram = spectrogram(waveform, window, frame_length, hop_length, fft_length, power, center, pad_mode, onesided, preemphasis, mel_filters, mel_floor, log_mel, reference, min_value, db_range, remove_dc_offset, dtype)\n\n    expected_shape = (1000 // 50, 100 // 2 + 1)\n    self.assertEqual(spectrogram.shape, expected_shape)\n```"]}
{"id": 68, "prompt": "\nYou are an expert Python programmer. You will be given some information and will generate a correct test case for the given function.\nYour response will only be python test function and without anything else.\n\n### focal function:\n```python\n\ndef load_image_from_string(\n    reference: str,\n    max_height: Optional[int] = None,\n    max_width: Optional[int] = None,\n) -> Tuple[str, Optional[float]]:\n    if uri_is_http_link(uri=reference):\n        return load_image_from_url(\n            url=reference, max_height=max_height, max_width=max_width\n        )\n    if os.path.exists(reference):\n        if max_height is None or max_width is None:\n            with open(reference, \"rb\") as f:\n                img_bytes = f.read()\n            img_base64_str = encode_base_64(payload=img_bytes)\n            return img_base64_str, None\n        local_image = cv2.imread(reference)\n        if local_image is None:\n            raise EncodingError(f\"Could not load image from {reference}\")\n        local_image, scaling_factor = resize_opencv_image(\n            image=local_image,\n            max_height=max_height,\n            max_width=max_width,\n        )\n        return numpy_array_to_base64_jpeg(image=local_image), scaling_factor\n    if max_height is not None and max_width is not None:\n        image_bytes = base64.b64decode(reference)\n        image = bytes_to_opencv_image(payload=image_bytes)\n        image, scaling_factor = resize_opencv_image(\n            image=image,\n            max_height=max_height,\n            max_width=max_width,\n        )\n        return numpy_array_to_base64_jpeg(image=image), scaling_factor\n    return reference, None\n```\n###test function signature:\n```python\n\ndef test_load_image_from_string_when_base64_image_given_and_no_resize_needed(\n    example_local_image: Tuple[str, np.ndarray]\n) -> None:\n    # given\n```\n### Answer: (use the provided format with backticks)\n", "name": "test_load_image_from_string_when_base64_image_given_and_no_resize_needed", "reference": "\ndef test_load_image_from_string_when_base64_image_given_and_no_resize_needed(\n    example_local_image: Tuple[str, np.ndarray]\n) -> None:\n    # given\n    _, image = example_local_image\n    _, buffer = cv2.imencode(\".jpg\", image)\n    base64_image = base64.b64encode(buffer).decode(\"ascii\")\n\n    # when\n    serialised_image, scaling_factor = load_image_from_string(\n        reference=base64_image,\n        max_width=None,\n        max_height=None,\n    )\n\n    # then\n    assert (\n        scaling_factor is None\n    ), \"No resize parameters given, so scaling factor should not be established\"\n    assert (\n        serialised_image == base64_image\n    ), \"Serialised image should be identical with input\"", "completions": ["  ```\ndef test_load_image_from_string_when_base64_image_given_and_no_resize_needed(example_local_image):\n    # given\n    image_bytes = base64.b64decode(example_local_image[0])\n    image = bytes_to_opencv_image(payload=image_bytes)\n    max_height = None\n    max_width = None\n\n    # when\n    result = load_image_from_string(reference=example_local_image[0], max_height=max_height, max_width=max_width)\n\n    # then\n    assert result[0] == example_local_image[0]\n    assert result[1] is None\n```"]}
{"id": 69, "prompt": "\nYou are an expert Python programmer. You will be given some information and will generate a correct test case for the given function.\nYour response will only be python test function and without anything else.\n\n### focal function:\n```python\n\n    def submit_payout_address(self, robot_index=1):\n        path = reverse(\"order\")\n        params = f\"?order_id={self.order_id}\"\n        headers = self.get_robot_auth(robot_index)\n\n        payout_address = create_address(\"robot\")\n        signed_payout_address = sign_message(\n            payout_address,\n            passphrase_path=f\"tests/robots/{robot_index}/token\",\n            private_key_path=f\"tests/robots/{robot_index}/enc_priv_key\",\n        )\n        body = {\n            \"action\": \"update_address\",\n            \"address\": signed_payout_address,\n            \"mining_fee_rate\": 50,\n        }\n        self.response = self.client.post(path + params, body, **headers)\n```\n###test function signature:\n```python\n    def test_successful_onchain(self):\n```\n### Answer: (use the provided format with backticks)\n", "name": "test_successful_onchain", "reference": "    def test_successful_onchain(self):\n        trade = Trade(self.client)\n        trade.publish_order()\n        trade.take_order()\n        trade.lock_taker_bond()\n        trade.lock_escrow(trade.taker_index)\n        trade.submit_payout_address(trade.maker_index)\n        trade.confirm_fiat(trade.maker_index)\n        trade.confirm_fiat(trade.taker_index)\n\n        trade.process_payouts(mine_a_block=True)\n        trade.get_order(trade.maker_index)\n\n        data = trade.response.json()\n\n        self.assertEqual(trade.response.status_code, 200)\n        self.assertResponse(trade.response)\n\n        self.assertEqual(data[\"status_message\"], Order.Status(Order.Status.SUC).label)\n        self.assertTrue(data[\"is_fiat_sent\"])\n        self.assertFalse(data[\"is_disputed\"])\n        self.assertIsInstance(data[\"maker_summary\"][\"address\"], str)\n        self.assertIsHash(data[\"maker_summary\"][\"txid\"])\n\n        self.assert_order_logs(data[\"id\"])", "completions": ["  ```\ndef test_successful_onchain(self):\n    self.assertEqual(self.response.status_code, 200)\n    self.assertEqual(self.response.json()[\"status\"], \"success\")\n    self.assertEqual(self.response.json()[\"message\"], \"Payout address updated successfully\")\n```"]}
{"id": 70, "prompt": "\nYou are an expert Python programmer. You will be given some information and will generate a correct test case for the given function.\nYour response will only be python test function and without anything else.\n\n### focal function:\n```python\ndef header_to_metadata_and_cell(\n    lines, header_prefix, header_suffix, ext=None, root_level_metadata_as_raw_cell=True\n):\n\n    header = []\n    jupyter = []\n    in_jupyter = False\n    in_html_div = False\n\n    start = 0\n    started = False\n    ended = False\n    metadata = {}\n    i = -1\n\n    comment = \"#\" if header_prefix == \"#'\" else header_prefix\n\n    encoding_re = re.compile(\n        rf\"^[ \\t\\f]*{re.escape(comment)}.*?coding[:=][ \\t]*([-_.a-zA-Z0-9]+)\"\n    )\n\n    for i, line in enumerate(lines):\n        if i == 0 and line.startswith(\"#!\"):\n            metadata.setdefault(\"jupytext\", {})[\"executable\"] = line[2:]\n            start = i + 1\n            continue\n        if i == 0 or (i == 1 and not encoding_re.match(lines[0])):\n            encoding = encoding_re.match(line)\n            if encoding:\n                if encoding.group(1) != \"utf-8\":\n                    raise ValueError(\"Encodings other than utf-8 are not supported\")\n                metadata.setdefault(\"jupytext\", {})[\"encoding\"] = line\n                start = i + 1\n                continue\n        if not line.startswith(header_prefix):\n            break\n        if not comment:\n            if line.strip().startswith(\"<!--\"):\n                in_html_div = True\n                continue\n\n        if in_html_div:\n            if ended:\n                if \"-->\" in line:\n                    break\n            if not started and not line.strip():\n                continue\n\n        line = uncomment_line(line, header_prefix, header_suffix)\n        if _HEADER_RE.match(line):\n            if not started:\n                started = True\n                continue\n            ended = True\n            if in_html_div:\n                continue\n            break\n\n        # Stop if there is something else than a YAML header\n        if not started and line.strip():\n            break\n\n        if _JUPYTER_RE.match(line):\n            in_jupyter = True\n        elif line and not _LEFTSPACE_RE.match(line):\n            in_jupyter = False\n\n        if in_jupyter:\n            jupyter.append(line)\n        else:\n            header.append(line)\n\n    if ended:\n        if jupyter:\n            extra_metadata = metadata\n            metadata = yaml.safe_load(\"\\n\".join(jupyter))[\"jupyter\"]\n            recursive_update(metadata, extra_metadata)\n\n        lines_to_next_cell = 1\n        if len(lines) > i + 1:\n            line = uncomment_line(lines[i + 1], header_prefix)\n            if not _BLANK_RE.match(line):\n                lines_to_next_cell = 0\n            else:\n                i = i + 1\n        else:\n            lines_to_next_cell = 0\n\n        if header:\n            if root_level_metadata_as_raw_cell:\n                cell = new_raw_cell(\n                    source=\"\\n\".join([\"---\"] + header + [\"---\"]),\n                    metadata={}\n                    if lines_to_next_cell\n                    == pep8_lines_between_cells([\"---\"], lines[i + 1 :], ext)\n                    else {\"lines_to_next_cell\": lines_to_next_cell},\n                )\n            else:\n                cell = None\n                root_level_metadata = yaml.safe_load(\"\\n\".join(header))\n                metadata.setdefault(\"jupytext\", {})[\n                    \"root_level_metadata\"\n                ] = root_level_metadata\n        else:\n            cell = None\n\n        return metadata, jupyter, cell, i + 1\n\n    return metadata, False, None, start\n```\n###test function signature:\n```python\n\ndef test_header_to_metadata_and_cell_blank_line():\n```\n### Answer: (use the provided format with backticks)\n", "name": "test_header_to_metadata_and_cell_blank_line", "reference": "\ndef test_header_to_metadata_and_cell_blank_line():\n    text = \"\"\"---\ntitle: Sample header\n---\n\nHeader is followed by a blank line\n\"\"\"\n    lines = text.splitlines()\n    metadata, _, cell, pos = header_to_metadata_and_cell(lines, \"\", \"\")\n\n    assert metadata == {}\n    assert cell.cell_type == \"raw\"\n    assert (\n        cell.source\n        == \"\"\"---\ntitle: Sample header\n---\"\"\"\n    )\n    assert cell.metadata == {}\n    assert lines[pos].startswith(\"Header is\")", "completions": ["  ```\ndef test_header_to_metadata_and_cell_blank_line():\n    lines = [\"# ---\", \"# title: Test\", \"# ---\", \"\"]\n    header_prefix = \"#\"\n    header_suffix = \"#\"\n    ext = None\n    root_level_metadata_as_raw_cell = True\n\n    metadata, jupyter, cell, i = header_to_metadata_and_cell(\n        lines, header_prefix, header_suffix, ext, root_level_metadata_as_raw_cell\n    )\n\n    assert metadata == {\"jupytext\": {\"executable\": \"python3\"}}\n    assert jupyter == []\n    assert cell == new_raw_cell(\n        source=\"\\n\".join([\"---\"] + lines[:3] + [\"---\"]),\n        metadata={\"lines_to_next_cell\": 1},\n    )\n    assert i == 4\n```"]}
{"id": 71, "prompt": "\nYou are an expert Python programmer. You will be given some information and will generate a correct test case for the given function.\nYour response will only be python test function and without anything else.\n\n### focal function:\n```python\n    def _create_output(\n            self,\n            accepted: torch.Tensor,  # [batch_size, k]\n            recovered_token_ids: torch.Tensor,  # [batch_size, k]\n            draft_token_ids: torch.Tensor,  # [batch_size, k]\n            bonus_token_ids: torch.Tensor,  # [batch_size]\n    ) -> torch.Tensor:\n        bonus_token_ids = bonus_token_ids.squeeze()\n        batch_size, k = recovered_token_ids.shape\n\n        # Determine the index of the first False value for each row.\n        limits = (accepted == 0).max(1).indices\n        limits[~(accepted == 0).any(1)] = k\n\n        # Create masks using the indices.\n        indices = torch.arange(k, device=accepted.device).unsqueeze(0)\n        accepted_mask = indices < limits.unsqueeze(1)\n        after_false_mask = indices == limits.unsqueeze(1)\n\n        # Create an extended output tensor\n        output_with_bonus_tokens = -torch.ones(\n            (batch_size, k + self._num_bonus_tokens),\n            dtype=self.token_id_dtype,\n            device=accepted.device)\n        output = output_with_bonus_tokens[:, :k]\n\n        # Fill in the first k columns of the output tensor using masks and data\n        # tensors.\n        output[:, :k] = torch.where(accepted_mask, draft_token_ids,\n                                    -torch.ones_like(draft_token_ids))\n\n        # Fill the last column.\n        # We check output directly as accepted may have True values inconsistent\n        # with causal acceptance.\n        output_with_bonus_tokens[:, -1] = torch.where(output[:, -1] != -1,\n                                                      bonus_token_ids, -1)\n\n        # We disable bonus tokens because it causes corrupt KV cache for\n        # proposal methods that require KV cache. We can fix it by \"prefilling\"\n        # the bonus token in the proposer. The following issue tracks the fix.\n        # https://github.com/vllm-project/vllm/issues/4212\n        output_with_bonus_tokens[:, -1] = -1\n\n        # Fill the recovered token ids.\n        output.mul_(~after_false_mask).add_(\n            recovered_token_ids.mul(after_false_mask))\n\n        self.num_accepted_tokens += accepted.sum()\n        self.num_emitted_tokens += (output_with_bonus_tokens != -1).sum()\n        self.num_draft_tokens += batch_size * k\n\n        return output_with_bonus_tokens\n```\n###test function signature:\n```python\ndef test_correct_output_format(which_tokens_accepted: str, seed: int,\n                               device: str):\n```\n### Answer: (use the provided format with backticks)\n", "name": "test_correct_output_format", "reference": "def test_correct_output_format(which_tokens_accepted: str, seed: int,\n                               device: str):\n    set_random_seed(seed)\n    torch.set_default_device(device)\n\n    batch_size = 10\n    k = 5\n    vocab_size = 3000\n\n    if which_tokens_accepted == \"all_tokens_accepted\":\n        accepted = mock_causal_accepted_tensor(\n            k, -1 + k * torch.ones((batch_size, ), dtype=torch.long))\n    elif which_tokens_accepted == \"no_tokens_accepted\":\n        accepted = mock_causal_accepted_tensor(\n            k, -torch.ones((batch_size, ), dtype=torch.long))\n    elif which_tokens_accepted == \"some_tokens_accepted\":\n        last_accepted_indices = torch.randint(low=-1,\n                                              high=k,\n                                              size=(batch_size, ))\n        accepted = mock_causal_accepted_tensor(k, last_accepted_indices)\n    else:\n        raise AssertionError()\n\n    recovered_token_ids = torch.randint(low=0,\n                                        high=vocab_size,\n                                        size=(batch_size, k),\n                                        dtype=torch.int64)\n    draft_token_ids = torch.randint(low=0,\n                                    high=vocab_size,\n                                    size=(batch_size, k),\n                                    dtype=torch.int64)\n    bonus_token_ids = torch.randint(low=0,\n                                    high=vocab_size,\n                                    size=(batch_size, 1),\n                                    dtype=torch.int64)\n\n    rejection_sampler = RejectionSampler()\n    rejection_sampler.init_gpu_tensors(rank=0)\n    output_token_ids = rejection_sampler._create_output(  # pylint: disable=protected-access\n        accepted,\n        recovered_token_ids,\n        draft_token_ids,\n        bonus_token_ids,\n    )\n\n    # Bonus tokens are currently disabled. Verify they're set to -1.\n    # See https://github.com/vllm-project/vllm/issues/4212\n    expected_bonus_token_ids = bonus_token_ids.clone() * 0 - 1\n\n    if which_tokens_accepted == \"all_tokens_accepted\":\n        # Expect all tokens to be equal to draft tokens.\n        assert torch.equal(output_token_ids[:, :-1], draft_token_ids)\n\n        # Expect all bonus tokens to be included.\n        assert torch.equal(output_token_ids[:, -1:], expected_bonus_token_ids)\n    elif which_tokens_accepted == \"no_tokens_accepted\":\n        # Expect first token to be equal to recovered tokens.\n        assert torch.equal(output_token_ids[:, 0], recovered_token_ids[:, 0])\n\n        # Expect everything else to be -1.\n        assert torch.equal(output_token_ids[:, 1:],\n                           torch.ones_like(output_token_ids[:, 1:]) * -1)\n    elif which_tokens_accepted == \"some_tokens_accepted\":\n        recovered_plus_bonus = torch.cat(\n            (recovered_token_ids, expected_bonus_token_ids), dim=-1)\n        # Assert first rejected token is a recovered token or bonus token.\n        assert torch.equal(\n            recovered_plus_bonus[torch.arange(0, batch_size),\n                                 last_accepted_indices + 1],\n            output_token_ids[torch.arange(0, batch_size),\n                             last_accepted_indices + 1])\n\n        # Assert every subsequent token is -1.\n        subsequent_mask = torch.arange(0, k + 1).expand(\n            batch_size, k + 1) >= (last_accepted_indices + 2).unsqueeze(-1)\n        assert torch.all(output_token_ids[subsequent_mask] == -1)", "completions": ["  ```\ndef test_correct_output_format(which_tokens_accepted: str, seed: int, device: str):\n    # Set the random seed for reproducibility\n    torch.manual_seed(seed)\n\n    # Create a random tensor for the accepted tokens\n    accepted = torch.randint(0, 2, (10, 5), device=device)\n\n    # Create a random tensor for the recovered token ids\n    recovered_token_ids = torch.randint(0, 10, (10, 5), device=device)\n\n    # Create a random tensor for the draft token ids\n    draft_token_ids = torch.randint(0, 10, (10, 5), device=device)\n\n    # Create a random tensor for the bonus token ids\n    bonus_token_ids = torch.randint(0, 10, (10,), device=device)\n\n    # Call the function with the random tensors\n    output = _create_output(accepted, recovered_token_ids, draft_token_ids,\n                            bonus_token_ids)\n\n    # Check that the output tensor has the correct shape\n    assert output.shape == (10, 5 + 1)\n\n    # Check that the output tensor has the correct dtype\n    assert output.dtype == torch.int64\n\n    # Check that the output tensor has the correct device\n    assert output.device == device\n\n    # Check that the output tensor has the correct values\n    assert torch.allclose(output[:, :5], draft_token_ids)\n    assert torch.allclose(output[:, 5], bonus_token_ids)\n    assert torch.allclose(output[:, 6:], recovered_token_ids)\n```"]}
{"id": 72, "prompt": "\nYou are an expert Python programmer. You will be given some information and will generate a correct test case for the given function.\nYour response will only be python test function and without anything else.\n\n### focal function:\n```python\ndef read_array(fp, allow_pickle=False, pickle_kwargs=None, *,\n               max_header_size=_MAX_HEADER_SIZE):\n    if allow_pickle:\n        # Effectively ignore max_header_size, since `allow_pickle` indicates\n        # that the input is fully trusted.\n        max_header_size = 2**64\n\n    version = read_magic(fp)\n    _check_version(version)\n    shape, fortran_order, dtype = _read_array_header(\n            fp, version, max_header_size=max_header_size)\n    if len(shape) == 0:\n        count = 1\n    else:\n        count = numpy.multiply.reduce(shape, dtype=numpy.int64)\n\n    # Now read the actual data.\n    if dtype.hasobject:\n        # The array contained Python objects. We need to unpickle the data.\n        if not allow_pickle:\n            raise ValueError(\"Object arrays cannot be loaded when \"\n                             \"allow_pickle=False\")\n        if pickle_kwargs is None:\n            pickle_kwargs = {}\n        try:\n            array = pickle.load(fp, **pickle_kwargs)\n        except UnicodeError as err:\n            # Friendlier error message\n            raise UnicodeError(\"Unpickling a python object failed: %r\\n\"\n                               \"You may need to pass the encoding= option \"\n                               \"to numpy.load\" % (err,)) from err\n    else:\n        if isfileobj(fp):\n            # We can use the fast fromfile() function.\n            array = numpy.fromfile(fp, dtype=dtype, count=count)\n        else:\n            # This is not a real file. We have to read it the\n            # memory-intensive way.\n            # crc32 module fails on reads greater than 2 ** 32 bytes,\n            # breaking large reads from gzip streams. Chunk reads to\n            # BUFFER_SIZE bytes to avoid issue and reduce memory overhead\n            # of the read. In non-chunked case count < max_read_count, so\n            # only one read is performed.\n\n            # Use np.ndarray instead of np.empty since the latter does\n            # not correctly instantiate zero-width string dtypes; see\n            # https://github.com/numpy/numpy/pull/6430\n            array = numpy.ndarray(count, dtype=dtype)\n\n            if dtype.itemsize > 0:\n                # If dtype.itemsize == 0 then there's nothing more to read\n                max_read_count = BUFFER_SIZE // min(BUFFER_SIZE, dtype.itemsize)\n\n                for i in range(0, count, max_read_count):\n                    read_count = min(max_read_count, count - i)\n                    read_size = int(read_count * dtype.itemsize)\n                    data = _read_bytes(fp, read_size, \"array data\")\n                    array[i:i+read_count] = numpy.frombuffer(data, dtype=dtype,\n                                                             count=read_count)\n\n        if fortran_order:\n            array.shape = shape[::-1]\n            array = array.transpose()\n        else:\n            array.shape = shape\n\n    return array\n```\n###test function signature:\n```python\n\ndef test_version_2_0():\n```\n### Answer: (use the provided format with backticks)\n", "name": "test_version_2_0", "reference": "\ndef test_version_2_0():\n    f = BytesIO()\n    # requires more than 2 byte for header\n    dt = [((\"%d\" % i) * 100, float) for i in range(500)]\n    d = np.ones(1000, dtype=dt)\n\n    format.write_array(f, d, version=(2, 0))\n    with warnings.catch_warnings(record=True) as w:\n        warnings.filterwarnings('always', '', UserWarning)\n        format.write_array(f, d)\n        assert_(w[0].category is UserWarning)\n\n    # check alignment of data portion\n    f.seek(0)\n    header = f.readline()\n    assert_(len(header) % format.ARRAY_ALIGN == 0)\n\n    f.seek(0)\n    n = format.read_array(f, max_header_size=200000)\n    assert_array_equal(d, n)\n\n    # 1.0 requested but data cannot be saved this way\n    assert_raises(ValueError, format.write_array, f, d, (1, 0))", "completions": ["  ```\ndef test_version_2_0():\n    fp = io.BytesIO(b'\\x80\\x02\\x95\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\"]}
{"id": 73, "prompt": "\nYou are an expert Python programmer. You will be given some information and will generate a correct test case for the given function.\nYour response will only be python test function and without anything else.\n\n### focal function:\n```python\ndef log_mel_spectrogram(audio: Union[str, np.ndarray, torch.Tensor], n_mels: int = N_MELS):\n    if not torch.is_tensor(audio):\n        if isinstance(audio, str):\n            audio = load_audio(audio)\n        audio = torch.from_numpy(audio)\n\n    window = torch.hann_window(N_FFT).to(audio.device)\n    stft = torch.stft(audio, N_FFT, HOP_LENGTH, window=window, return_complex=True)\n    magnitudes = stft[..., :-1].abs() ** 2\n\n    filters = mel_filters(audio.device, n_mels)\n    mel_spec = filters @ magnitudes\n\n    log_spec = torch.clamp(mel_spec, min=1e-10).log10()\n    log_spec = torch.maximum(log_spec, log_spec.max() - 8.0)\n    log_spec = (log_spec + 4.0) / 4.0\n    return log_spec\n```\n###test function signature:\n```python\n\ndef test_audio():\n```\n### Answer: (use the provided format with backticks)\n", "name": "test_audio", "reference": "\ndef test_audio():\n    audio_path = os.path.join(os.path.dirname(__file__), \"jfk.flac\")\n    audio = load_audio(audio_path)\n    assert audio.ndim == 1\n    assert SAMPLE_RATE * 10 < audio.shape[0] < SAMPLE_RATE * 12\n    assert 0 < audio.std() < 1\n\n    mel_from_audio = log_mel_spectrogram(audio)\n    mel_from_file = log_mel_spectrogram(audio_path)\n\n    assert np.allclose(mel_from_audio, mel_from_file)\n    assert mel_from_audio.max() - mel_from_audio.min() <= 2.0", "completions": ["  ```\ndef test_audio():\n    audio = torch.randn(1, 16000)\n    n_mels = 128\n    log_spec = log_mel_spectrogram(audio, n_mels)\n    assert log_spec.shape == (1, n_mels, 128)\n    assert log_spec.dtype == torch.float32\n    assert torch.all(log_spec >= 0)\n```"]}
{"id": 74, "prompt": "\nYou are an expert Python programmer. You will be given some information and will generate a correct test case for the given function.\nYour response will only be python test function and without anything else.\n\n### focal function:\n```python\ndef transform_grid_cell(\n    grid: Grid,\n    cell: npt.NDArray[np.float_] = np.eye(3),\n    output_shape: Optional[Tuple[int, int, int]] = None,\n    mode: str = \"constant\",\n    order: int = 1,\n    **kwargs,\n) -> Grid:\n    # Take the current shape of the grid if no output shape was provided\n    if output_shape is None:\n        output_shape = grid.shape\n\n    # Make sure the cell has type float\n    cell = np.asarray(cell, dtype=float)\n\n    # Get the current cell in coordinates of the destination axes\n    inv_cell = cell_invert(cell).T\n    projected_cell = grid.cell.dot(inv_cell)\n\n    # From that, infere how long will the bounding box of the cell be\n    lengths = abs(projected_cell).sum(axis=0)\n\n    # Create the transformation matrix. Since we want to control the shape\n    # of the output, we can not use grid.dcell directly, we need to modify it.\n    scales = output_shape / lengths\n    forward_t = (grid.dcell.dot(inv_cell) * scales).T\n\n    # Scipy's affine transform asks for the inverse transformation matrix, to\n    # map from output pixels to input pixels. By taking the inverse of our\n    # transformation matrix, we get exactly that.\n    tr = cell_invert(forward_t).T\n\n    # Calculate the offset of the image so that all points of the grid \"fall\" inside\n    # the output array.\n    # For this we just calculate the centers of the input and output images\n    center_input = 0.5 * (_a.asarrayd(grid.shape) - 1)\n    center_output = 0.5 * (_a.asarrayd(output_shape) - 1)\n\n    # And then make sure that the input center that is interpolated from the output\n    # falls in the actual input's center\n    offset = center_input - tr.dot(center_output)\n\n    # We pass all the parameters to scipy's affine_transform\n    transformed_image = affine_transform(\n        grid.grid,\n        tr,\n        order=1,\n        offset=offset,\n        output_shape=output_shape,\n        mode=mode,\n        **kwargs,\n    )\n\n    # Create a new grid with the new shape and the new cell (notice how the cell\n    # is rescaled from the input cell to fit the actual coordinates of the system)\n    new_grid = grid.__class__((1, 1, 1), lattice=cell * lengths.reshape(3, 1))\n    new_grid.grid = transformed_image\n    new_grid.geometry = grid.geometry\n    new_grid.lattice.origin = grid.origin + new_grid.dcell.dot(forward_t.dot(offset))\n\n    # Find the offset between the origin before and after the transformation\n    return new_grid\n```\n###test function signature:\n```python\n\ndef test_transform_grid_cell(grid, skewed):\n    # Convert to a cartesian cell\n```\n### Answer: (use the provided format with backticks)\n", "name": "test_transform_grid_cell", "reference": "\ndef test_transform_grid_cell(grid, skewed):\n    # Convert to a cartesian cell\n    new_grid = transform_grid_cell(grid, cell=np.eye(3), output_shape=(10, 10, 10))\n\n    assert new_grid.shape == (10, 10, 10)\n    assert new_grid.lattice.is_cartesian()\n\n    if not skewed:\n        assert np.allclose(new_grid.lattice.cell, grid.lattice.cell)\n\n    assert np.allclose(new_grid.grid, grid.grid) == (not skewed)\n    assert not np.allclose(new_grid.grid, 0)\n\n    assert np.allclose(new_grid.origin, grid.origin) == (not skewed)\n\n    # Convert to a skewed cell\n    directions = np.array([[1, 2, 3], [-1, 2, -4], [2, -1, 1]])\n    new_grid = transform_grid_cell(grid, cell=directions, output_shape=(5, 5, 5))\n\n    assert new_grid.shape == (5, 5, 5)\n    for i in range(3):\n        n = new_grid.lattice.cell[i] / directions[i]\n        assert np.allclose(n, n[0])", "completions": ["  ```\ndef test_transform_grid_cell(grid, skewed):\n    # Convert to a cartesian cell\n    cartesian_cell = np.array([[1, 0, 0], [0, 1, 0], [0, 0, 1]])\n\n    # Test with a skewed cell\n    skewed_cell = np.array([[1, 0, 0], [0, 1, 0], [0, 0, 1]])\n\n    # Test with a non-skewed cell\n    non_skewed_cell = np.array([[1, 0, 0], [0, 1, 0], [0, 0, 1]])\n\n    # Test with a non-skewed grid\n    non_skewed_grid = Grid(grid.shape, lattice=non_skewed_cell)\n\n    # Test with a skewed grid\n    skewed_grid = Grid(grid.shape, lattice=skewed_cell)\n\n    # Test with a non-skewed grid and a non-skewed cell\n    non_skewed_grid_non_skewed_cell = transform_grid_cell(non_skewed_grid, non_skewed_cell)\n\n    # Test with a non-skewed grid and a skewed cell\n    non_skewed_grid_skewed_cell = transform_grid_cell(non_skewed_grid, skewed_cell)\n\n    # Test with a skewed grid and a non-skewed cell\n    skewed_grid_non_skewed_cell = transform_grid_cell(skewed_grid, non_skewed_cell)\n\n    # Test with a skewed grid and a skewed cell\n    skewed_grid_skewed_cell = transform_grid_cell(skewed_grid, skewed_cell)\n\n    # Test with a non-skewed grid and a non-skewed cell, with a different output shape\n    non_skewed_grid_non_skewed_cell_different_output_shape = transform_grid_cell(\n        non_skewed_grid, non_skewed_cell, output_shape=(10, 10, 10)\n    )\n\n    # Test with a non-skewed grid and a skewed cell, with a different output shape\n    non_skewed_grid_skewed_cell_different_output_shape = transform_grid_cell(\n        non_skewed_grid, skewed_cell, output_shape=(10, 10, 10)\n    )\n\n    # Test with a skewed grid and a non-skewed cell, with a different output shape\n    skewed_grid_non_skewed_cell_different_output_shape = transform_grid_cell(\n        skewed_grid, non_skewed_cell, output_shape=(10, 10, 10)\n    )\n\n    # Test with a skewed grid and a skewed cell, with a different output shape\n    skewed_grid_skewed_cell_different_output_shape = transform_grid_cell(\n        skewed_grid, skewed_cell, output_shape=(10, 10, 10)\n    )\n```"]}
{"id": 75, "prompt": "\nYou are an expert Python programmer. You will be given some information and will generate a correct test case for the given function.\nYour response will only be python test function and without anything else.\n\n### focal function:\n```python\ndef similarity_matrix(xq: np.ndarray, index: np.ndarray) -> np.ndarray:\n\n    index_norm = norm(index, axis=1)\n    xq_norm = norm(xq.T)\n    sim = np.dot(index, xq.T) / (index_norm * xq_norm)\n    return sim\n```\n###test function signature:\n```python\ndef test_top_scores__is_sorted(test_index):\n```\n### Answer: (use the provided format with backticks)\n", "name": "test_top_scores__is_sorted", "reference": "def test_top_scores__is_sorted(test_index):\n\n    xq = test_index[0]  # should have max similarity\n\n    sim = similarity_matrix(xq, test_index)\n    _, idx = top_scores(sim, 3)\n\n    # Scores and indexes should be sorted ascending\n    assert np.array_equal(idx, np.array([2, 1, 0]))", "completions": ["  ```\ndef test_top_scores__is_sorted(test_index):\n    sim = similarity_matrix(test_index, test_index)\n    top_scores = np.argsort(sim, axis=1)\n    assert np.all(top_scores == np.sort(top_scores))\n```\nThis test function checks if the top scores returned by the `similarity_matrix` function are sorted in descending order. It first calculates the similarity matrix for the given test index using the `similarity_matrix` function. Then, it extracts the top scores for each row of the similarity matrix using `np.argsort`. Finally, it checks if the top scores are sorted in descending order using `np.all` and `np.sort`. If the top scores are not sorted, the test will fail."]}
{"id": 77, "prompt": "\nYou are an expert Python programmer. You will be given some information and will generate a correct test case for the given function.\nYour response will only be python test function and without anything else.\n\n### focal function:\n```python\n    def mean(self, lower=0.0, upper=np.inf):\n        loc = []\n        for idx in self.index:\n            qpd = self.qpd.loc[idx, :].values[0]\n            l, _ = quad(exp_func, args=(qpd), a=lower, b=upper)\n            loc.append(l)\n        loc_arr = np.array(loc)\n        return pd.DataFrame(loc_arr, index=self.index, columns=self.columns)\n```\n###test function signature:\n```python\ndef test_qpd_b_simple_use():\n```\n### Answer: (use the provided format with backticks)\n", "name": "test_qpd_b_simple_use", "reference": "def test_qpd_b_simple_use():\n    from skpro.distributions.qpd import QPD_B\n\n    qpd = QPD_B(\n        alpha=0.2,\n        qv_low=[1, 2],\n        qv_median=[3, 4],\n        qv_high=[5, 6],\n        lower=0,\n        upper=10,\n    )\n\n    qpd.mean()", "completions": ["  ```\ndef test_qpd_b_simple_use():\n    qpd = QPD(np.array([[1, 2], [3, 4]]))\n    mean_result = qpd.mean(lower=0.0, upper=np.inf)\n    expected_result = np.array([[1, 2], [3, 4]])\n    assert np.allclose(mean_result, expected_result)\n```"]}
{"id": 78, "prompt": "\nYou are an expert Python programmer. You will be given some information and will generate a correct test case for the given function.\nYour response will only be python test function and without anything else.\n\n### focal function:\n```python\n\ndef create(\n    supply_settings: Callable[[], config.ResilienceSettings] = config.ResilienceSettings, include_post: bool = False\n) -> adapters.HTTPAdapter:\n    settings = supply_settings()\n    allowed_methods = set(util.Retry.DEFAULT_ALLOWED_METHODS)\n    if include_post:\n        allowed_methods.add(\"POST\")\n\n    retries = util.Retry(\n        total=settings.total,\n        backoff_factor=settings.backoff_factor,\n        status_forcelist=settings.statuses,\n        allowed_methods=allowed_methods,\n    )\n    return adapters.HTTPAdapter(max_retries=retries)\n```\n###test function signature:\n```python\n\ndef test_should_create_adapter(include_post):\n```\n### Answer: (use the provided format with backticks)\n", "name": "test_should_create_adapter", "reference": "\ndef test_should_create_adapter(include_post):\n    total_retries = 5\n    backoff_factor = 0.4\n    statuses = (random.randint(400, 600),)\n    settings = config.ResilienceSettings(total=total_retries, backoff_factor=backoff_factor, statuses=statuses)\n    adapter = http_adapter.create(supply_settings=lambda: settings, include_post=include_post)\n    assert adapter.max_retries.total == total_retries\n    assert adapter.max_retries.backoff_factor == backoff_factor\n    assert adapter.max_retries.status_forcelist == statuses\n    allowed_methods = adapter.max_retries.allowed_methods or []\n    if include_post:\n        assert \"POST\" in allowed_methods\n    else:\n        assert \"POST\" not in allowed_methods", "completions": ["  ```\ndef test_should_create_adapter(include_post):\n    settings = config.ResilienceSettings()\n    adapter = create(supply_settings=lambda: settings, include_post=include_post)\n    assert isinstance(adapter, adapters.HTTPAdapter)\n    assert adapter.max_retries.total == settings.total\n    assert adapter.max_retries.backoff_factor == settings.backoff_factor\n    assert adapter.max_retries.status_forcelist == settings.statuses\n    assert adapter.max_retries.allowed_methods == set(util.Retry.DEFAULT_ALLOWED_METHODS)\n    if include_post:\n        assert \"POST\" in adapter.max_retries.allowed_methods\n    else:\n        assert \"POST\" not in adapter.max_retries.allowed_methods\n```"]}
{"id": 79, "prompt": "\nYou are an expert Python programmer. You will be given some information and will generate a correct test case for the given function.\nYour response will only be python test function and without anything else.\n\n### focal function:\n```python\ndef extract_best_config(trial_path: str, output_path: Optional[str] = None) -> Dict:\n    summary_path = os.path.join(trial_path, 'summary.csv')\n    if not os.path.exists(summary_path):\n        raise ValueError(f\"summary.csv does not exist in {trial_path}.\")\n    trial_summary_df = load_summary_file(summary_path, dict_columns=['best_module_params'])\n    config_yaml_path = os.path.join(trial_path, 'config.yaml')\n    with open(config_yaml_path, 'r') as f:\n        config_dict = yaml.safe_load(f)\n    yaml_dict = summary_df_to_yaml(trial_summary_df, config_dict)\n    if output_path is not None:\n        with open(output_path, 'w') as f:\n            yaml.dump(yaml_dict, f)\n    return yaml_dict\n```\n###test function signature:\n```python\n\ndef test_runner(evaluator):\n```\n### Answer: (use the provided format with backticks)\n", "name": "test_runner", "reference": "\ndef test_runner(evaluator):\n    os.environ['BM25'] = 'bm25'\n    evaluator.start_trial(os.path.join(resource_dir, 'simple.yaml'))\n    project_dir = evaluator.project_dir\n\n    def runner_test(runner: Runner):\n        answer = runner.run('What is the best movie in Korea? Have Korea movie ever won Oscar?',\n                            'retrieved_contents')\n        assert len(answer) == 1\n        assert isinstance(answer, list)\n        assert isinstance(answer[0], str)\n\n    runner = Runner.from_trial_folder(os.path.join(project_dir, '0'))\n    runner_test(runner)\n    runner_test(runner)\n\n    with tempfile.NamedTemporaryFile(suffix='yaml', mode='w+t') as yaml_path:\n        extract_best_config(os.path.join(project_dir, '0'), yaml_path.name)\n        runner = Runner.from_yaml(yaml_path.name, project_dir=project_dir)\n        runner_test(runner)", "completions": ["  ```\ndef test_extract_best_config():\n    trial_path = 'path/to/trial'\n    output_path = 'path/to/output'\n    expected_yaml_dict = {'best_module_params': {'param1': 'value1', 'param2': 'value2'}}\n    with patch('os.path.exists') as mock_exists, patch('load_summary_file') as mock_load_summary_file, patch('yaml.safe_load') as mock_safe_load, patch('yaml.dump') as mock_dump:\n        mock_exists.return_value = True\n        mock_load_summary_file.return_value = pd.DataFrame({'best_module_params': [expected_yaml_dict]})\n        mock_safe_load.return_value = expected_yaml_dict\n        mock_dump.return_value = None\n        actual_yaml_dict = extract_best_config(trial_path, output_path)\n        assert actual_yaml_dict == expected_yaml_dict\n```"]}
{"id": 80, "prompt": "\nYou are an expert Python programmer. You will be given some information and will generate a correct test case for the given function.\nYour response will only be python test function and without anything else.\n\n### focal function:\n```python\n\n    def init_weights(self):\n        logger = MMLogger.get_current_instance()\n        if self.init_cfg is None:\n            logger.warn(f'No pre-trained weights for '\n                        f'{self.__class__.__name__}, '\n                        f'training start from scratch')\n            if self.use_abs_pos_embed:\n                trunc_normal_(self.absolute_pos_embed, std=0.02)\n            for m in self.modules():\n                if isinstance(m, nn.Linear):\n                    trunc_normal_init(m, std=.02, bias=0.)\n                elif isinstance(m, nn.LayerNorm):\n                    constant_init(m, 1.0)\n        else:\n            assert 'checkpoint' in self.init_cfg, f'Only support ' \\\n                                                  f'specify `Pretrained` in ' \\\n                                                  f'`init_cfg` in ' \\\n                                                  f'{self.__class__.__name__} '\n            ckpt = CheckpointLoader.load_checkpoint(\n                self.init_cfg.checkpoint, logger=logger, map_location='cpu')\n            if 'state_dict' in ckpt:\n                _state_dict = ckpt['state_dict']\n            elif 'model' in ckpt:\n                _state_dict = ckpt['model']\n            else:\n                _state_dict = ckpt\n            if self.convert_weights:\n                # supported loading weight from original repo,\n                _state_dict = swin_converter(_state_dict)\n\n            state_dict = OrderedDict()\n            for k, v in _state_dict.items():\n                if k.startswith('backbone.'):\n                    state_dict[k[9:]] = v\n\n            # strip prefix of state_dict\n            if list(state_dict.keys())[0].startswith('module.'):\n                state_dict = {k[7:]: v for k, v in state_dict.items()}\n\n            # reshape absolute position embedding\n            if state_dict.get('absolute_pos_embed') is not None:\n                absolute_pos_embed = state_dict['absolute_pos_embed']\n                N1, L, C1 = absolute_pos_embed.size()\n                N2, C2, H, W = self.absolute_pos_embed.size()\n                if N1 != N2 or C1 != C2 or L != H * W:\n                    logger.warning('Error in loading absolute_pos_embed, pass')\n                else:\n                    state_dict['absolute_pos_embed'] = absolute_pos_embed.view(\n                        N2, H, W, C2).permute(0, 3, 1, 2).contiguous()\n\n            # interpolate position bias table if needed\n            relative_position_bias_table_keys = [\n                k for k in state_dict.keys()\n                if 'relative_position_bias_table' in k\n            ]\n            for table_key in relative_position_bias_table_keys:\n                table_pretrained = state_dict[table_key]\n                table_current = self.state_dict()[table_key]\n                L1, nH1 = table_pretrained.size()\n                L2, nH2 = table_current.size()\n                if nH1 != nH2:\n                    logger.warning(f'Error in loading {table_key}, pass')\n                elif L1 != L2:\n                    S1 = int(L1**0.5)\n                    S2 = int(L2**0.5)\n                    table_pretrained_resized = F.interpolate(\n                        table_pretrained.permute(1, 0).reshape(1, nH1, S1, S1),\n                        size=(S2, S2),\n                        mode='bicubic')\n                    state_dict[table_key] = table_pretrained_resized.view(\n                        nH2, L2).permute(1, 0).contiguous()\n\n            # load state_dict\n            self.load_state_dict(state_dict, False)\n```\n###test function signature:\n```python\ndef test_swin_transformer():\n```\n### Answer: (use the provided format with backticks)\n", "name": "test_swin_transformer", "reference": "def test_swin_transformer():\n\n    with pytest.raises(TypeError):\n        # Pretrained arg must be str or None.\n        SwinTransformer(pretrained=123)\n\n    with pytest.raises(AssertionError):\n        # Because swin uses non-overlapping patch embed, so the stride of patch\n        # embed must be equal to patch size.\n        SwinTransformer(strides=(2, 2, 2, 2), patch_size=4)\n\n    # test pretrained image size\n    with pytest.raises(AssertionError):\n        SwinTransformer(pretrain_img_size=(224, 224, 224))\n\n    # Test absolute position embedding\n    temp = torch.randn((1, 3, 224, 224))\n    model = SwinTransformer(pretrain_img_size=224, use_abs_pos_embed=True)\n    model.init_weights()\n    model(temp)\n\n    # Test patch norm\n    model = SwinTransformer(patch_norm=False)\n    model(temp)\n\n    # Test normal inference\n    temp = torch.randn((1, 3, 32, 32))\n    model = SwinTransformer()\n    outs = model(temp)\n    assert outs[0].shape == (1, 96, 8, 8)\n    assert outs[1].shape == (1, 192, 4, 4)\n    assert outs[2].shape == (1, 384, 2, 2)\n    assert outs[3].shape == (1, 768, 1, 1)\n\n    # Test abnormal inference size\n    temp = torch.randn((1, 3, 31, 31))\n    model = SwinTransformer()\n    outs = model(temp)\n    assert outs[0].shape == (1, 96, 8, 8)\n    assert outs[1].shape == (1, 192, 4, 4)\n    assert outs[2].shape == (1, 384, 2, 2)\n    assert outs[3].shape == (1, 768, 1, 1)\n\n    # Test abnormal inference size\n    temp = torch.randn((1, 3, 112, 137))\n    model = SwinTransformer()\n    outs = model(temp)\n    assert outs[0].shape == (1, 96, 28, 35)\n    assert outs[1].shape == (1, 192, 14, 18)\n    assert outs[2].shape == (1, 384, 7, 9)\n    assert outs[3].shape == (1, 768, 4, 5)\n\n    model = SwinTransformer(frozen_stages=4)\n    model.train()\n    for p in model.parameters():\n        assert not p.requires_grad", "completions": ["  ```\ndef test_swin_transformer():\n    # Test case 1: Test that the function initializes the weights correctly when no pre-trained weights are provided\n    model = SwinTransformer(init_cfg=None)\n    model.init_weights()\n    assert model.absolute_pos_embed.shape == (1, 197, 197, 384)\n    assert model.relative_position_bias_table.shape == (2 * 197 - 1, 197, 384)\n\n    # Test case 2: Test that the function initializes the weights correctly when pre-trained weights are provided\n    model = SwinTransformer(init_cfg={'checkpoint': 'path/to/checkpoint'})\n    model.init_weights()\n    assert model.absolute_pos_embed.shape == (1, 197, 197, 384)\n    assert model.relative_position_bias_table.shape == (2 * 197 - 1, 197, 384)\n```"]}
{"id": 81, "prompt": "\nYou are an expert Python programmer. You will be given some information and will generate a correct test case for the given function.\nYour response will only be python test function and without anything else.\n\n### focal function:\n```python\ndef columns_equal(\n    col_1: \"pl.Series\",\n    col_2: \"pl.Series\",\n    rel_tol: float = 0,\n    abs_tol: float = 0,\n    ignore_spaces: bool = False,\n    ignore_case: bool = False,\n) -> \"pl.Series\":\n    compare: pl.Series\n    try:\n        compare = pl.Series(\n            np.isclose(col_1, col_2, rtol=rel_tol, atol=abs_tol, equal_nan=True)\n        )\n    except TypeError:\n        try:\n            if col_1.dtype in DATE_TYPE or col_2 in DATE_TYPE:\n                raise TypeError(\"Found date, moving to alternative logic\")\n\n            compare = pl.Series(\n                np.isclose(\n                    col_1.cast(pl.Float64, strict=True),\n                    col_2.cast(pl.Float64, strict=True),\n                    rtol=rel_tol,\n                    atol=abs_tol,\n                    equal_nan=True,\n                )\n            )\n        except (ValueError, TypeError, InvalidOperationError, ComputeError):\n            try:\n                if ignore_spaces:\n                    if str(col_1.dtype) in STRING_TYPE:\n                        col_1 = col_1.str.strip_chars()\n                    if str(col_2.dtype) in STRING_TYPE:\n                        col_2 = col_2.str.strip_chars()\n\n                if ignore_case:\n                    if str(col_1.dtype) in STRING_TYPE:\n                        col_1 = col_1.str.to_uppercase()\n                    if str(col_2.dtype) in STRING_TYPE:\n                        col_2 = col_2.str.to_uppercase()\n\n                if (\n                    str(col_1.dtype) in STRING_TYPE and str(col_2.dtype) in DATE_TYPE\n                ) or (\n                    str(col_1.dtype) in DATE_TYPE and str(col_2.dtype) in STRING_TYPE\n                ):\n                    compare = compare_string_and_date_columns(col_1, col_2)\n                else:\n                    compare = pl.Series(\n                        (col_1.eq_missing(col_2)) | (col_1.is_null() & col_2.is_null())\n                    )\n            except:\n                # Blanket exception should just return all False\n                compare = pl.Series(False * col_1.shape[0])\n    return compare\n```\n###test function signature:\n```python\n\ndef test_date_columns_equal():\n```\n### Answer: (use the provided format with backticks)\n", "name": "test_date_columns_equal", "reference": "\ndef test_date_columns_equal():\n    data = \"\"\"a|b|expected\n2017-01-01|2017-01-01|True\n2017-01-02|2017-01-02|True\n2017-10-01|2017-10-10|False\n2017-01-01||False\n|2017-01-01|False\n||True\"\"\"\n    df = pl.read_csv(\n        io.StringIO(data),\n        separator=\"|\",\n        null_values=[\"NULL\"],\n        missing_utf8_is_empty_string=True,\n    )\n    # First compare just the strings\n    actual_out = columns_equal(df[\"a\"], df[\"b\"], rel_tol=0.2)\n    expect_out = df[\"expected\"]\n    assert_series_equal(expect_out, actual_out, check_names=False)\n\n    # Then compare converted to datetime objects\n    col_a = df[\"a\"].str.to_datetime(strict=False)\n    col_b = df[\"b\"].str.to_datetime(strict=False)\n    actual_out = columns_equal(col_a, col_b, rel_tol=0.2)\n    expect_out = df[\"expected\"]\n    assert_series_equal(expect_out, actual_out, check_names=False)", "completions": ["  ```\ndef test_date_columns_equal():\n    col_1 = pl.Series([datetime.date(2022, 2, 28), datetime.date(2022, 3, 1)])\n    col_2 = pl.Series([datetime.date(2022, 2, 28), datetime.date(2022, 3, 1)])\n    rel_tol = 0\n    abs_tol = 0\n    ignore_spaces = False\n    ignore_case = False\n    expected = pl.Series([True, True])\n    result = columns_equal(col_1, col_2, rel_tol, abs_tol, ignore_spaces, ignore_case)\n    assert result.equals(expected)\n```"]}
{"id": 82, "prompt": "\nYou are an expert Python programmer. You will be given some information and will generate a correct test case for the given function.\nYour response will only be python test function and without anything else.\n\n### focal function:\n```python\n\nasync def upsert_child_firing(\n    db: PrefectDBInterface,\n    session: AsyncSession,\n    firing: Firing,\n):\n    automation_id = firing.trigger.automation.id\n    parent_trigger_id = firing.trigger.parent.id\n    child_trigger_id = firing.trigger.id\n\n    upsert = (\n        postgresql.insert(db.CompositeTriggerChildFiring)\n        .values(\n            automation_id=automation_id,\n            parent_trigger_id=parent_trigger_id,\n            child_trigger_id=child_trigger_id,\n            child_firing_id=firing.id,\n            child_fired_at=firing.triggered,\n            child_firing=firing.dict(),\n        )\n        .on_conflict_do_update(\n            index_elements=[\n                db.CompositeTriggerChildFiring.automation_id,\n                db.CompositeTriggerChildFiring.parent_trigger_id,\n                db.CompositeTriggerChildFiring.child_trigger_id,\n            ],\n            set_=dict(\n                child_firing_id=firing.id,\n                child_fired_at=firing.triggered,\n                child_firing=firing.dict(),\n                updated=pendulum.now(\"UTC\"),\n            ),\n        )\n    )\n\n    await session.execute(upsert)\n\n    result = await session.execute(\n        sa.select(db.CompositeTriggerChildFiring).filter(\n            db.CompositeTriggerChildFiring.automation_id == automation_id,\n            db.CompositeTriggerChildFiring.parent_trigger_id == parent_trigger_id,\n            db.CompositeTriggerChildFiring.child_trigger_id == child_trigger_id,\n        )\n    )\n\n    return result.scalars().one()\n```\n###test function signature:\n```python\n\nasync def test_insert_compound_trigger_child_firing(\n    automations_session: AsyncSession,\n    raise_the_alarm: Automation,\n    saw_a_red_and_green_dragon: CompoundTrigger,\n    saw_a_red_dragon: EventTrigger,\n    baby_red_dragon_passed_by: ReceivedEvent,\n    frozen_time: DateTime,\n):\n```\n### Answer: (use the provided format with backticks)\n", "name": "test_insert_compound_trigger_child_firing", "reference": "\nasync def test_insert_compound_trigger_child_firing(\n    automations_session: AsyncSession,\n    raise_the_alarm: Automation,\n    saw_a_red_and_green_dragon: CompoundTrigger,\n    saw_a_red_dragon: EventTrigger,\n    baby_red_dragon_passed_by: ReceivedEvent,\n    frozen_time: DateTime,\n):\n    firing = Firing(\n        trigger=saw_a_red_dragon,\n        trigger_states={TriggerState.Triggered},\n        triggered=frozen_time.add(seconds=1),\n        triggering_labels={},\n        triggering_event=baby_red_dragon_passed_by,\n    )\n    res = await upsert_child_firing(\n        session=automations_session,\n        firing=firing,\n    )\n\n    assert res and res.id\n    assert res.automation_id == raise_the_alarm.id\n    assert res.parent_trigger_id == saw_a_red_and_green_dragon.id\n    assert res.child_trigger_id == saw_a_red_dragon.id\n    assert res.child_firing_id == firing.id\n    assert res.child_fired_at == firing.triggered\n\n    child_firings = await get_child_firings(\n        session=automations_session, trigger=saw_a_red_and_green_dragon\n    )\n    assert len(child_firings) == 1\n    assert child_firings[0].id == res.id\n    assert child_firings[0].child_firing_id == firing.id", "completions": ["  ```\nasync def test_insert_compound_trigger_child_firing(\n    automations_session: AsyncSession,\n    raise_the_alarm: Automation,\n    saw_a_red_and_green_dragon: CompoundTrigger,\n    saw_a_red_dragon: EventTrigger,\n    baby_red_dragon_passed_by: ReceivedEvent,\n    frozen_time: DateTime,\n):\n    # Arrange\n    firing = Firing(\n        trigger=saw_a_red_and_green_dragon,\n        triggered=frozen_time,\n        received_events=[baby_red_dragon_passed_by],\n    )\n\n    # Act\n    result = await upsert_child_firing(\n        db=automations_session,\n        session=automations_session,\n        firing=firing,\n    )\n\n    # Assert\n    assert result.automation_id == raise_the_alarm.id\n    assert result.parent_trigger_id == saw_a_red_and_green_dragon.id\n    assert result.child_trigger_id == saw_a_red_dragon.id\n    assert result.child_firing_id == firing.id\n    assert result.child_fired_at == firing.triggered\n    assert result.child_firing == firing.dict()\n    assert result.updated == frozen_time\n```"]}
{"id": 83, "prompt": "\nYou are an expert Python programmer. You will be given some information and will generate a correct test case for the given function.\nYour response will only be python test function and without anything else.\n\n### focal function:\n```python\ndef write_output(output_file, output_type, results):\n\n    # If json or txt, write contents directly to file.\n    if output_type == \"json\" or output_type == \"txt\":\n        with open(output_file, \"w\") as f:\n            json.dump(results, f, indent=4)\n        return\n    # If csv or xlsx, convert results to pandas dataframes.\n    urls_df = get_urls_df(results)\n    codes_df = get_codes_df(results)\n\n    # If csv, write dataframes to separate csv files for urls, codes.\n    if output_type == \"csv\":\n        urls_output_file = output_file.replace(\".csv\", \"_urls.csv\")\n        codes_output_file = output_file.replace(\".csv\", \"_codes.csv\")\n        urls_df.to_csv(urls_output_file, index=False)\n        codes_df.to_csv(codes_output_file, index=False)\n\n    # If xlsx, write dataframes to separate sheets for urls, codes.\n    if output_type == \"xlsx\":\n        writer = pd.ExcelWriter(output_file, engine=\"xlsxwriter\")\n        urls_df.to_excel(writer, sheet_name=\"URLs\", index=False)\n        codes_df.to_excel(writer, sheet_name=\"Codes\", index=False)\n        writer.close()\n```\n###test function signature:\n```python\n    def test_write_output_csv(self, mock_urls, mock_codes):\n```\n### Answer: (use the provided format with backticks)\n", "name": "test_write_output_csv", "reference": "    def test_write_output_csv(self, mock_urls, mock_codes):\n\n        test_file = \"./test_output/test_file.csv\"\n        test_file_urls = \"./test_output/test_file_urls.csv\"\n        test_file_codes = \"./test_output/test_file_codes.csv\"\n        test_results = {\"test\": \"test\"}\n        mock_urls.return_value = pd.DataFrame([test_results])\n        mock_codes.return_value = pd.DataFrame([test_results])\n\n        with open(test_file_urls, \"w\") as f:\n            pass\n\n        with open(test_file_codes, \"w\") as f:\n            pass\n\n        write_output(test_file, \"csv\", test_results)\n\n        test_data_urls = pd.read_csv(test_file_urls).to_dict(orient=\"records\")[0]\n        test_data_codes = pd.read_csv(test_file_codes).to_dict(orient=\"records\")[0]\n\n        os.remove(test_file_urls)\n        os.remove(test_file_codes)\n\n        self.assertEqual(test_data_urls, test_results)\n        self.assertEqual(test_data_codes, test_results)", "completions": ["  ```\ndef test_write_output_csv(self, mock_urls, mock_codes):\n    output_file = \"test_output.csv\"\n    output_type = \"csv\"\n    results = {\"urls\": mock_urls, \"codes\": mock_codes}\n    write_output(output_file, output_type, results)\n    urls_output_file = output_file.replace(\".csv\", \"_urls.csv\")\n    codes_output_file = output_file.replace(\".csv\", \"_codes.csv\")\n    urls_df = pd.read_csv(urls_output_file)\n    codes_df = pd.read_csv(codes_output_file)\n    assert urls_df.equals(pd.DataFrame(mock_urls))\n    assert codes_df.equals(pd.DataFrame(mock_codes))\n```"]}
{"id": 84, "prompt": "\nYou are an expert Python programmer. You will be given some information and will generate a correct test case for the given function.\nYour response will only be python test function and without anything else.\n\n### focal function:\n```python\n\nasync def terminate_idle_instance() -> None:\n    async with get_session_ctx() as session:\n        async with PROCESSING_POOL_LOCK:\n            res = await session.execute(\n                select(InstanceModel)\n                .where(\n                    InstanceModel.termination_policy == TerminationPolicy.DESTROY_AFTER_IDLE,\n                    InstanceModel.deleted == False,\n                    InstanceModel.job == None,  # noqa: E711\n                    InstanceModel.status == InstanceStatus.IDLE,\n                )\n                .options(joinedload(InstanceModel.project))\n            )\n            instances = res.scalars().all()\n\n            for instance in instances:\n                last_time = instance.created_at.replace(tzinfo=datetime.timezone.utc)\n                if instance.last_job_processed_at is not None:\n                    last_time = instance.last_job_processed_at.replace(\n                        tzinfo=datetime.timezone.utc\n                    )\n\n                idle_seconds = instance.termination_idle_time\n                delta = datetime.timedelta(seconds=idle_seconds)\n\n                current_time = get_current_datetime().replace(tzinfo=datetime.timezone.utc)\n\n                if last_time + delta < current_time:\n                    jpd = JobProvisioningData.__response__.parse_raw(\n                        instance.job_provisioning_data\n                    )\n                    await terminate_job_provisioning_data_instance(\n                        project=instance.project, job_provisioning_data=jpd\n                    )\n                    instance.deleted = True\n                    instance.deleted_at = get_current_datetime()\n                    instance.finished_at = get_current_datetime()\n                    instance.status = InstanceStatus.TERMINATED\n                    instance.termination_reason = \"Idle timeout\"\n\n                    idle_time = current_time - last_time\n                    logger.info(\n                        \"Instance %s terminated by termination policy: idle time %ss\",\n                        instance.name,\n                        str(idle_time.seconds),\n                        extra={\n                            \"instance_name\": instance.name,\n                            \"instance_status\": InstanceStatus.TERMINATED.value,\n                        },\n                    )\n\n            await session.commit()\n```\n###test function signature:\n```python\n\n    async def test_terminate_by_idle_timeout(self, test_db, session: AsyncSession):\n```\n### Answer: (use the provided format with backticks)\n", "name": "test_terminate_by_idle_timeout", "reference": "\n    async def test_terminate_by_idle_timeout(self, test_db, session: AsyncSession):\n        project = await create_project(session=session)\n        pool = await create_pool(session, project)\n\n        instance = await create_instance(session, project, pool, status=InstanceStatus.IDLE)\n        instance.termination_idle_time = 300\n        instance.termination_policy = TerminationPolicy.DESTROY_AFTER_IDLE\n        instance.last_job_processed_at = get_current_datetime() + dt.timedelta(minutes=-19)\n        await session.commit()\n\n        with patch(\n            \"dstack._internal.server.background.tasks.process_instances.terminate_job_provisioning_data_instance\"\n        ):\n            await terminate_idle_instance()\n\n        await session.refresh(instance)\n\n        assert instance is not None\n        assert instance.status == InstanceStatus.TERMINATED\n        assert instance.termination_reason == \"Idle timeout\"", "completions": ["  ```\nasync def test_terminate_by_idle_timeout(self, test_db, session: AsyncSession):\n    # Set up the test data\n    instance = InstanceModel(\n        name=\"test_instance\",\n        project=ProjectModel(name=\"test_project\"),\n        job=None,\n        status=InstanceStatus.IDLE,\n        termination_policy=TerminationPolicy.DESTROY_AFTER_IDLE,\n        termination_idle_time=10,\n        created_at=datetime.datetime.now(tz=datetime.timezone.utc),\n        last_job_processed_at=None,\n    )\n    await session.add(instance)\n    await session.commit()\n\n    # Call the function to be tested\n    await terminate_idle_instance()\n\n    # Check the results\n    instance = await session.get(InstanceModel, instance.id)\n    assert instance.deleted is True\n    assert instance.deleted_at is not None\n    assert instance.finished_at is not None\n    assert instance.status == InstanceStatus.TERMINATED\n    assert instance.termination_reason == \"Idle timeout\"\n```"]}
{"id": 85, "prompt": "\nYou are an expert Python programmer. You will be given some information and will generate a correct test case for the given function.\nYour response will only be python test function and without anything else.\n\n### focal function:\n```python\ndef nargsort(\n    items,\n    kind: str = \"quicksort\",\n    ascending: bool = True,\n    na_position: str = \"last\",\n    key: Callable | None = None,\n    mask: npt.NDArray[np.bool_] | None = None,\n) -> npt.NDArray[np.intp]:\n\n    if key is not None:\n        items = ensure_key_mapped(items, key)\n        return nargsort(\n            items,\n            kind=kind,\n            ascending=ascending,\n            na_position=na_position,\n            key=None,\n            mask=mask,\n        )\n\n    if isinstance(items, ABCRangeIndex):\n        return items.argsort(ascending=ascending)  # TODO: test coverage with key?\n    elif not isinstance(items, ABCMultiIndex):\n        items = extract_array(items)\n    if mask is None:\n        mask = np.asarray(isna(items))  # TODO: does this exclude MultiIndex too?\n\n    if is_extension_array_dtype(items):\n        return items.argsort(ascending=ascending, kind=kind, na_position=na_position)\n    else:\n        items = np.asanyarray(items)\n\n    idx = np.arange(len(items))\n    non_nans = items[~mask]\n    non_nan_idx = idx[~mask]\n\n    nan_idx = np.nonzero(mask)[0]\n    if not ascending:\n        non_nans = non_nans[::-1]\n        non_nan_idx = non_nan_idx[::-1]\n    indexer = non_nan_idx[non_nans.argsort(kind=kind)]\n    if not ascending:\n        indexer = indexer[::-1]\n    # Finally, place the NaNs at the end or the beginning according to\n    # na_position\n    if na_position == \"last\":\n        indexer = np.concatenate([indexer, nan_idx])\n    elif na_position == \"first\":\n        indexer = np.concatenate([nan_idx, indexer])\n    else:\n        raise ValueError(f\"invalid na_position: {na_position}\")\n    return ensure_platform_int(indexer)\n```\n###test function signature:\n```python\n\n    def test_nargsort(self, ascending, na_position, exp, box):\n        # list places NaNs last, np.array(..., dtype=\"O\") may not place NaNs first\n```\n### Answer: (use the provided format with backticks)\n", "name": "test_nargsort", "reference": "\n    def test_nargsort(self, ascending, na_position, exp, box):\n        # list places NaNs last, np.array(..., dtype=\"O\") may not place NaNs first\n        items = box([np.nan] * 5 + list(range(100)) + [np.nan] * 5)\n\n        # mergesort is the most difficult to get right because we want it to be\n        # stable.\n\n        # According to numpy/core/tests/test_multiarray, \"\"\"The number of\n        # sorted items must be greater than ~50 to check the actual algorithm\n        # because quick and merge sort fall over to insertion sort for small\n        # arrays.\"\"\"\n\n        result = nargsort(\n            items, kind=\"mergesort\", ascending=ascending, na_position=na_position\n        )\n        tm.assert_numpy_array_equal(result, np.array(exp), check_dtype=False)", "completions": ["  ```\ndef test_nargsort(self, ascending, na_position, exp, box):\n    items = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n    result = nargsort(items, ascending=ascending, na_position=na_position)\n    self.assertEqual(result, exp)\n```"]}
{"id": 86, "prompt": "\nYou are an expert Python programmer. You will be given some information and will generate a correct test case for the given function.\nYour response will only be python test function and without anything else.\n\n### focal function:\n```python\ndef generate_rotated_rb_circuits(\n    n_qubits: int,\n    num_cliffords: int,\n    theta: Optional[float] = None,\n    trials: int = 1,\n    return_type: Optional[str] = None,\n    seed: Optional[int] = None,\n) -> List[QPROGRAM]:\n\n    circuits = cast(\n        List[cirq.Circuit],\n        generate_rb_circuits(n_qubits, num_cliffords, trials, seed=seed),\n    )\n\n    if theta is None:\n        theta = 2 * np.pi * np.random.rand()\n\n    for circ in circuits:\n        qubits = list(circ.all_qubits())\n        circ.insert(len(circ) // 2, cirq.Rz(rads=theta).on(qubits[0]))\n\n    return_type = \"cirq\" if not return_type else return_type\n    return [convert_from_mitiq(circuit, return_type) for circuit in circuits]\n```\n###test function signature:\n```python\n\ndef test_rotated_rb_circuits(n_qubits, theta):\n```\n### Answer: (use the provided format with backticks)\n", "name": "test_rotated_rb_circuits", "reference": "\ndef test_rotated_rb_circuits(n_qubits, theta):\n    depth = 10\n    for trials in [2, 3]:\n        circuits = generate_rotated_rb_circuits(\n            n_qubits=n_qubits, num_cliffords=depth, theta=theta, trials=trials\n        )\n        for circ in circuits:\n            zero_prob = (\n                cirq.DensityMatrixSimulator()\n                .simulate(circ)\n                .final_density_matrix[0, 0]\n                .real\n            )\n            assert -1.0001 <= zero_prob <= 1.0001", "completions": ["  ```\ndef test_rotated_rb_circuits(n_qubits, theta):\n    circuits = generate_rotated_rb_circuits(n_qubits, 1, theta)\n    assert len(circuits) == 1\n    assert circuits[0].all_qubits() == [cirq.LineQubit(i) for i in range(n_qubits)]\n    assert circuits[0].all_operations() == [cirq.Rz(rads=theta).on(cirq.LineQubit(0))]\n```\nThis test function checks that the `generate_rotated_rb_circuits` function returns a list of `cirq.Circuit` objects with the correct number of qubits and operations. It also checks that the `theta` parameter is correctly applied to the first qubit in the circuit."]}
{"id": 87, "prompt": "\nYou are an expert Python programmer. You will be given some information and will generate a correct test case for the given function.\nYour response will only be python test function and without anything else.\n\n### focal function:\n```python\n\ndef eye(N, M=None, k=None, dtype=None):\n    dtype = to_torch_dtype(dtype or config.floatx())\n    M = N if M is None else M\n    k = 0 if k is None else k\n    if k == 0:\n        # TODO: torch.eye doesn't support bfloat16 with cpu\n        if get_device() == \"cpu\" and dtype == torch.bfloat16:\n            return cast(\n                torch.eye(\n                    N, M, dtype=to_torch_dtype(\"float32\"), device=get_device()\n                ),\n                dtype,\n            )\n        return torch.eye(N, M, dtype=dtype, device=get_device())\n    diag_length = builtins.max(N, M)\n    diag = torch.ones(diag_length, dtype=dtype, device=get_device())\n    return torch.diag(diag, diagonal=k)[:N, :M]\n```\n###test function signature:\n```python\n\n    def test_eye(self):\n```\n### Answer: (use the provided format with backticks)\n", "name": "test_eye", "reference": "\n    def test_eye(self):\n        self.assertAllClose(knp.eye(3), np.eye(3))\n        self.assertAllClose(knp.eye(3, 4), np.eye(3, 4))\n        self.assertAllClose(knp.eye(3, 4, 1), np.eye(3, 4, 1))\n\n        self.assertAllClose(knp.Eye()(3), np.eye(3))\n        self.assertAllClose(knp.Eye()(3, 4), np.eye(3, 4))\n        self.assertAllClose(knp.Eye(k=1)(3, 4), np.eye(3, 4, k=1))\n\n        # Test k >= N\n        self.assertAllClose(knp.Eye(k=3)(3), np.eye(3, k=3))\n\n        # Test k > 0 and N >= M\n        self.assertAllClose(knp.Eye(k=1)(3), np.eye(3, k=1))\n\n        # Test k > 0 and N < M and N + k > M\n        self.assertAllClose(knp.Eye(k=2)(3, 4), np.eye(3, 4, k=2))\n\n        # Test k < 0 and M >= N\n        self.assertAllClose(knp.Eye(k=-1)(3), np.eye(3, k=-1))\n\n        # Test k < 0 and M < N and M - k > N\n        self.assertAllClose(knp.Eye(k=-2)(4, 3), np.eye(4, 3, k=-2))", "completions": ["  \n[PYTHON]\ndef test_eye():\n    N = 5\n    M = 5\n    k = 0\n    dtype = torch.float32\n    expected_output = torch.eye(N, M, dtype=dtype)\n    output = eye(N, M, k, dtype)\n    self.assertTrue(torch.equal(output, expected_output))\n[/PYTHON]\n"]}
{"id": 88, "prompt": "\nYou are an expert Python programmer. You will be given some information and will generate a correct test case for the given function.\nYour response will only be python test function and without anything else.\n\n### focal function:\n```python\n\ndef ragged_qk_dotprod(\n    query: RaggedActivations, key: RaggedActivations, lut: RaggedQkPidLookupTable\n) -> torch.Tensor:\n    device = query.device\n\n    assert query.raw_tensor.is_contiguous()\n    assert key.raw_tensor.is_contiguous()\n\n    # check constraints\n    total_ctx_q_across_all_seqs, d_head = query.raw_tensor.shape\n    total_ctx_k_across_all_seqs, d_head_k = key.raw_tensor.shape\n    assert d_head == d_head_k, f\"{query.raw_tensor.shape=} {key.raw_tensor.shape=}\"\n\n    # allocates output\n    # max_n_ctx_q_across_seqs = query.max_n_ctx_per_seq\n\n    assert query.n_seqs == key.n_seqs\n    # TODO: flag use zeros for garbage\n    scores_out = torch.ones(\n        (query.n_seqs, query.max_n_ctx_per_seq, key.max_n_ctx_per_seq),\n        device=device,\n        dtype=query.dtype,\n    )\n\n    # Stride along the d_head dimension must be 1\n    assert query.raw_tensor.stride(1) == 1, f\"{query.raw_tensor.stride(1)}\"\n    assert key.raw_tensor.stride(1) == 1, f\"{key.raw_tensor.stride(1)}\"\n\n    # pid_to_seq_idx = [0, 0, 1, 2, 2]\n    grid = (lut.n_pids_total,)\n    _qk_dotprod_kernel[grid](\n        q_ptr=query.raw_tensor,\n        k_ptr=key.raw_tensor,\n        scores_ptr=scores_out,\n        # Lookup tables (sometimes referred to as a \"lut\")\n        pid_to_in_q_token_offset_ptr=lut.pid_to_in_q_token_offset,\n        pid_to_in_k_token_offset_ptr=lut.pid_to_in_k_token_offset,\n        pid_to_out_q_block_ptr=lut.pid_to_out_q_block,\n        pid_to_out_k_block_ptr=lut.pid_to_out_k_block,\n        pid_to_out_seq_idx_ptr=lut.pid_to_out_seq_idx,\n        # Integers\n        max_n_ctx_q_across_seqs=query.max_n_ctx_per_seq,\n        max_n_ctx_k_across_seqs=key.max_n_ctx_per_seq,\n        d_head=d_head,\n        stride_ctx_q=query.raw_tensor.stride(0),\n        stride_ctx_k=key.raw_tensor.stride(0),\n        stride_out_seq=scores_out.stride(0),\n        stride_out_q=scores_out.stride(1),\n        stride_out_k=scores_out.stride(2),\n        total_ctx_q_across_all_seqs=total_ctx_q_across_all_seqs,\n        total_ctx_k_across_all_seqs=total_ctx_k_across_all_seqs,\n    )\n    return scores_out\n```\n###test function signature:\n```python\n\ndef test_ragged_qk_dotprod_single_seq(dtype):\n```\n### Answer: (use the provided format with backticks)\n", "name": "test_ragged_qk_dotprod_single_seq", "reference": "\ndef test_ragged_qk_dotprod_single_seq(dtype):\n    d_head = 2\n\n    key = RaggedActivations.from_list(\n        [\n            make_seq(n_ctx=3, value=42, d_model=d_head, dtype=dtype),\n        ]\n    )\n    query = RaggedActivations.from_list(\n        [\n            make_seq(n_ctx=4, value=55, d_model=d_head, dtype=dtype),\n        ]\n    )\n    torch_scores = scores_via_qk_dotprod(query, key)\n    print(f\"{torch_scores=}\")\n\n    lut = RaggedQkPidLookupTable.from_query_and_key_tokens_per_seq(\n        n_ctx_q_per_seq=query.n_ctx_per_seq, n_ctx_k_per_seq=key.n_ctx_per_seq\n    )\n\n    scores = ragged_qk_dotprod(query, key, lut)\n    assert_eq(torch_scores, scores)", "completions": ["  ```\ndef test_ragged_qk_dotprod_single_seq(dtype):\n    # Set up inputs\n    query = RaggedActivations(\n        raw_tensor=torch.randn(1, 1, 1, dtype=dtype),\n        n_seqs=1,\n        max_n_ctx_per_seq=1,\n        dtype=dtype,\n    )\n    key = RaggedActivations(\n        raw_tensor=torch.randn(1, 1, 1, dtype=dtype),\n        n_seqs=1,\n        max_n_ctx_per_seq=1,\n        dtype=dtype,\n    )\n    lut = RaggedQkPidLookupTable(\n        n_pids_total=1,\n        pid_to_in_q_token_offset=torch.tensor([0], dtype=torch.int32),\n        pid_to_in_k_token_offset=torch.tensor([0], dtype=torch.int32),\n        pid_to_out_q_block=torch.tensor([0], dtype=torch.int32),\n        pid_to_out_k_block=torch.tensor([0], dtype=torch.int32),\n        pid_to_out_seq_idx=torch.tensor([0], dtype=torch.int32),\n    )\n\n    # Run function\n    scores_out = ragged_qk_dotprod(query, key, lut)\n\n    # Check output\n    assert scores_out.shape == (1, 1, 1)\n    assert scores_out.dtype == dtype\n    assert torch.allclose(scores_out, torch.tensor([[1]], dtype=dtype))\n```"]}
{"id": 89, "prompt": "\nYou are an expert Python programmer. You will be given some information and will generate a correct test case for the given function.\nYour response will only be python test function and without anything else.\n\n### focal function:\n```python\ndef in1d(ar1, ar2, assume_unique=False, invert=False, *, kind=None):\n    # Ravel both arrays, behavior for the first array could be different\n    ar1 = np.asarray(ar1).ravel()\n    ar2 = np.asarray(ar2).ravel()\n\n    # Ensure that iteration through object arrays yields size-1 arrays\n    if ar2.dtype == object:\n        ar2 = ar2.reshape(-1, 1)\n\n    if kind not in {None, 'sort', 'table'}:\n        raise ValueError(\n            f\"Invalid kind: '{kind}'. Please use None, 'sort' or 'table'.\")\n\n    # Can use the table method if all arrays are integers or boolean:\n    is_int_arrays = all(ar.dtype.kind in (\"u\", \"i\", \"b\") for ar in (ar1, ar2))\n    use_table_method = is_int_arrays and kind in {None, 'table'}\n\n    if use_table_method:\n        if ar2.size == 0:\n            if invert:\n                return np.ones_like(ar1, dtype=bool)\n            else:\n                return np.zeros_like(ar1, dtype=bool)\n\n        # Convert booleans to uint8 so we can use the fast integer algorithm\n        if ar1.dtype == bool:\n            ar1 = ar1.astype(np.uint8)\n        if ar2.dtype == bool:\n            ar2 = ar2.astype(np.uint8)\n\n        ar2_min = np.min(ar2)\n        ar2_max = np.max(ar2)\n\n        ar2_range = int(ar2_max) - int(ar2_min)\n\n        # Constraints on whether we can actually use the table method:\n        #  1. Assert memory usage is not too large\n        below_memory_constraint = ar2_range <= 6 * (ar1.size + ar2.size)\n        #  2. Check overflows for (ar2 - ar2_min); dtype=ar2.dtype\n        range_safe_from_overflow = ar2_range <= np.iinfo(ar2.dtype).max\n        #  3. Check overflows for (ar1 - ar2_min); dtype=ar1.dtype\n        if ar1.size > 0:\n            ar1_min = np.min(ar1)\n            ar1_max = np.max(ar1)\n\n            # After masking, the range of ar1 is guaranteed to be\n            # within the range of ar2:\n            ar1_upper = min(int(ar1_max), int(ar2_max))\n            ar1_lower = max(int(ar1_min), int(ar2_min))\n\n            range_safe_from_overflow &= all((\n                ar1_upper - int(ar2_min) <= np.iinfo(ar1.dtype).max,\n                ar1_lower - int(ar2_min) >= np.iinfo(ar1.dtype).min\n            ))\n\n        # Optimal performance is for approximately\n        # log10(size) > (log10(range) - 2.27) / 0.927.\n        # However, here we set the requirement that by default\n        # the intermediate array can only be 6x\n        # the combined memory allocation of the original\n        # arrays. See discussion on \n        # https://github.com/numpy/numpy/pull/12065.\n\n        if (\n            range_safe_from_overflow and \n            (below_memory_constraint or kind == 'table')\n        ):\n\n            if invert:\n                outgoing_array = np.ones_like(ar1, dtype=bool)\n            else:\n                outgoing_array = np.zeros_like(ar1, dtype=bool)\n\n            # Make elements 1 where the integer exists in ar2\n            if invert:\n                isin_helper_ar = np.ones(ar2_range + 1, dtype=bool)\n                isin_helper_ar[ar2 - ar2_min] = 0\n            else:\n                isin_helper_ar = np.zeros(ar2_range + 1, dtype=bool)\n                isin_helper_ar[ar2 - ar2_min] = 1\n\n            # Mask out elements we know won't work\n            basic_mask = (ar1 <= ar2_max) & (ar1 >= ar2_min)\n            outgoing_array[basic_mask] = isin_helper_ar[ar1[basic_mask] -\n                                                        ar2_min]\n\n            return outgoing_array\n        elif kind == 'table':  # not range_safe_from_overflow\n            raise RuntimeError(\n                \"You have specified kind='table', \"\n                \"but the range of values in `ar2` or `ar1` exceed the \"\n                \"maximum integer of the datatype. \"\n                \"Please set `kind` to None or 'sort'.\"\n            )\n    elif kind == 'table':\n        raise ValueError(\n            \"The 'table' method is only \"\n            \"supported for boolean or integer arrays. \"\n            \"Please select 'sort' or None for kind.\"\n        )\n\n\n    # Check if one of the arrays may contain arbitrary objects\n    contains_object = ar1.dtype.hasobject or ar2.dtype.hasobject\n\n    # This code is run when\n    # a) the first condition is true, making the code significantly faster\n    # b) the second condition is true (i.e. `ar1` or `ar2` may contain\n    #    arbitrary objects), since then sorting is not guaranteed to work\n    if len(ar2) < 10 * len(ar1) ** 0.145 or contains_object:\n        if invert:\n            mask = np.ones(len(ar1), dtype=bool)\n            for a in ar2:\n                mask &= (ar1 != a)\n        else:\n            mask = np.zeros(len(ar1), dtype=bool)\n            for a in ar2:\n                mask |= (ar1 == a)\n        return mask\n\n    # Otherwise use sorting\n    if not assume_unique:\n        ar1, rev_idx = np.unique(ar1, return_inverse=True)\n        ar2 = np.unique(ar2)\n\n    ar = np.concatenate((ar1, ar2))\n    # We need this to be a stable sort, so always use 'mergesort'\n    # here. The values from the first array should always come before\n    # the values from the second array.\n    order = ar.argsort(kind='mergesort')\n    sar = ar[order]\n    if invert:\n        bool_ar = (sar[1:] != sar[:-1])\n    else:\n        bool_ar = (sar[1:] == sar[:-1])\n    flag = np.concatenate((bool_ar, [invert]))\n    ret = np.empty(ar.shape, dtype=bool)\n    ret[order] = flag\n\n    if assume_unique:\n        return ret[:len(ar1)]\n    else:\n        return ret[rev_idx]\n```\n###test function signature:\n```python\n\n    def test_in1d_ravel(self, kind):\n        # Test that in1d ravels its input arrays. This is not documented\n        # behavior however. The test is to ensure consistentency.\n```\n### Answer: (use the provided format with backticks)\n", "name": "test_in1d_ravel", "reference": "\n    def test_in1d_ravel(self, kind):\n        # Test that in1d ravels its input arrays. This is not documented\n        # behavior however. The test is to ensure consistentency.\n        a = np.arange(6).reshape(2, 3)\n        b = np.arange(3, 9).reshape(3, 2)\n        long_b = np.arange(3, 63).reshape(30, 2)\n        ec = np.array([False, False, False, True, True, True])\n\n        assert_array_equal(in1d(a, b, assume_unique=True, kind=kind),\n                           ec)\n        assert_array_equal(in1d(a, b, assume_unique=False,\n                                kind=kind),\n                           ec)\n        assert_array_equal(in1d(a, long_b, assume_unique=True,\n                                kind=kind),\n                           ec)\n        assert_array_equal(in1d(a, long_b, assume_unique=False,\n                                kind=kind),\n                           ec)", "completions": ["  ```\ndef test_in1d_ravel(self, kind):\n    ar1 = np.array([1, 2, 3])\n    ar2 = np.array([4, 5, 6])\n    result = in1d(ar1, ar2, kind=kind)\n    self.assertTrue(result.shape == (3,))\n ```"]}
{"id": 90, "prompt": "\nYou are an expert Python programmer. You will be given some information and will generate a correct test case for the given function.\nYour response will only be python test function and without anything else.\n\n### focal function:\n```python\ndef header_to_metadata_and_cell(\n    lines, header_prefix, header_suffix, ext=None, root_level_metadata_as_raw_cell=True\n):\n\n    header = []\n    jupyter = []\n    in_jupyter = False\n    in_html_div = False\n\n    start = 0\n    started = False\n    ended = False\n    metadata = {}\n    i = -1\n\n    comment = \"#\" if header_prefix == \"#'\" else header_prefix\n\n    encoding_re = re.compile(\n        rf\"^[ \\t\\f]*{re.escape(comment)}.*?coding[:=][ \\t]*([-_.a-zA-Z0-9]+)\"\n    )\n\n    for i, line in enumerate(lines):\n        if i == 0 and line.startswith(\"#!\"):\n            metadata.setdefault(\"jupytext\", {})[\"executable\"] = line[2:]\n            start = i + 1\n            continue\n        if i == 0 or (i == 1 and not encoding_re.match(lines[0])):\n            encoding = encoding_re.match(line)\n            if encoding:\n                if encoding.group(1) != \"utf-8\":\n                    raise ValueError(\"Encodings other than utf-8 are not supported\")\n                metadata.setdefault(\"jupytext\", {})[\"encoding\"] = line\n                start = i + 1\n                continue\n        if not line.startswith(header_prefix):\n            break\n        if not comment:\n            if line.strip().startswith(\"<!--\"):\n                in_html_div = True\n                continue\n\n        if in_html_div:\n            if ended:\n                if \"-->\" in line:\n                    break\n            if not started and not line.strip():\n                continue\n\n        line = uncomment_line(line, header_prefix, header_suffix)\n        if _HEADER_RE.match(line):\n            if not started:\n                started = True\n                continue\n            ended = True\n            if in_html_div:\n                continue\n            break\n\n        # Stop if there is something else than a YAML header\n        if not started and line.strip():\n            break\n\n        if _JUPYTER_RE.match(line):\n            in_jupyter = True\n        elif line and not _LEFTSPACE_RE.match(line):\n            in_jupyter = False\n\n        if in_jupyter:\n            jupyter.append(line)\n        else:\n            header.append(line)\n\n    if ended:\n        if jupyter:\n            extra_metadata = metadata\n            metadata = yaml.safe_load(\"\\n\".join(jupyter))[\"jupyter\"]\n            recursive_update(metadata, extra_metadata)\n\n        lines_to_next_cell = 1\n        if len(lines) > i + 1:\n            line = uncomment_line(lines[i + 1], header_prefix)\n            if not _BLANK_RE.match(line):\n                lines_to_next_cell = 0\n            else:\n                i = i + 1\n        else:\n            lines_to_next_cell = 0\n\n        if header:\n            if root_level_metadata_as_raw_cell:\n                cell = new_raw_cell(\n                    source=\"\\n\".join([\"---\"] + header + [\"---\"]),\n                    metadata={}\n                    if lines_to_next_cell\n                    == pep8_lines_between_cells([\"---\"], lines[i + 1 :], ext)\n                    else {\"lines_to_next_cell\": lines_to_next_cell},\n                )\n            else:\n                cell = None\n                root_level_metadata = yaml.safe_load(\"\\n\".join(header))\n                metadata.setdefault(\"jupytext\", {})[\n                    \"root_level_metadata\"\n                ] = root_level_metadata\n        else:\n            cell = None\n\n        return metadata, jupyter, cell, i + 1\n\n    return metadata, False, None, start\n```\n###test function signature:\n```python\n\ndef test_header_to_metadata_and_cell_metadata():\n```\n### Answer: (use the provided format with backticks)\n", "name": "test_header_to_metadata_and_cell_metadata", "reference": "\ndef test_header_to_metadata_and_cell_metadata():\n    text = \"\"\"---\ntitle: Sample header\njupyter:\n  mainlanguage: python\n---\n\"\"\"\n    lines = text.splitlines()\n    metadata, _, cell, pos = header_to_metadata_and_cell(lines, \"\", \"\")\n\n    assert metadata == {\"mainlanguage\": \"python\"}\n    assert cell.cell_type == \"raw\"\n    assert (\n        cell.source\n        == \"\"\"---\ntitle: Sample header\n---\"\"\"\n    )\n    assert cell.metadata == {\"lines_to_next_cell\": 0}\n    assert pos == len(lines)", "completions": ["  ```\ndef test_header_to_metadata_and_cell_metadata():\n    # Test case 1: No header\n    lines = [\"print('hello')\"]\n    header_prefix = \"#'\"\n    header_suffix = \"'\"\n    ext = None\n    root_level_metadata_as_raw_cell = True\n    metadata, jupyter, cell, i = header_to_metadata_and_cell(\n        lines, header_prefix, header_suffix, ext, root_level_metadata_as_raw_cell\n    )\n    assert metadata == {}\n    assert jupyter == []\n    assert cell is None\n    assert i == 0\n\n    # Test case 2: Empty header\n    lines = [\"#'\", \"']\"]\n    header_prefix = \"#'\"\n    header_suffix = \"'\"\n    ext = None\n    root_level_metadata_as_raw_cell = True\n    metadata, jupyter, cell, i = header_to_metadata_and_cell(\n        lines, header_prefix, header_suffix, ext, root_level_metadata_as_raw_cell\n    )\n    assert metadata == {}\n    assert jupyter == []\n    assert cell is None\n    assert i == 1\n\n    # Test case 3: Header with metadata\n    lines = [\"#'\", \"a: 1\", \"b: 2\", \"']\"]\n    header_prefix = \"#'\"\n    header_suffix = \"'\"\n    ext = None\n    root_level_metadata_as_raw_cell = True\n    metadata, jupyter, cell, i = header_to_metadata_and_cell(\n        lines, header_prefix, header_suffix, ext, root_level_metadata_as_raw_cell\n    )\n    assert metadata == {\"a\": 1, \"b\": 2}\n    assert jupyter == []\n    assert cell is None\n    assert i == 3\n\n    # Test case 4: Header with metadata and jupyter metadata\n    lines = [\"#'\", \"a: 1\", \"b: 2\", \"jupyter:\", \"  c: 3\", \"  d: 4\", \"']\"]\n    header_prefix = \"#'\"\n    header_suffix = \"'\"\n    ext = None\n    root_level_metadata_as_raw_cell = True\n    metadata, jupyter, cell, i = header_to_metadata_and_cell(\n        lines, header_prefix, header_suffix, ext, root_level_metadata_as_raw_cell\n    )\n    assert metadata == {\"a\": 1, \"b\": 2, \"jupyter\": {\"c\": 3, \"d\": 4}}\n    assert jupyter == []\n    assert cell is None\n    assert i == 6\n\n    # Test case 5: Header with metadata and jupyter metadata, with root_level_metadata_as_raw_cell=False\n    lines = [\"#'\", \"a: 1\", \"b: 2\", \"jupyter:\", \"  c: 3\", \"  d: 4\", \"']\"]\n    header_prefix = \"#'\"\n    header_suffix = \"'\"\n    ext = None\n    root_level_metadata_as_raw_cell = False\n    metadata, jupyter, cell, i = header_to_metadata_and_cell(\n        lines, header_prefix, header_suffix, ext, root_level_metadata_as_raw_cell\n    )\n    assert metadata == {\"a\": 1, \"b\": 2}\n    assert jupyter == []\n    assert cell is None\n    assert i == 6\n\n    # Test case 6: Header with metadata and jupyter metadata, with root_level_metadata_as_raw_cell=False and jupyter metadata as raw cell\n    lines = [\"#'\", \"a: 1\", \"b: 2\", \"jupyter:\", \"  c: 3\", \"  d: 4\", \"']\"]\n    header_prefix = \"#'\"\n    header_suffix = \"'\"\n    ext = None\n    root_level_metadata_as_raw_cell = False\n    metadata, jupyter, cell, i = header"]}
{"id": 91, "prompt": "\nYou are an expert Python programmer. You will be given some information and will generate a correct test case for the given function.\nYour response will only be python test function and without anything else.\n\n### focal function:\n```python\ndef import_optional_dependency(\n    name: str,\n    extra: str = \"\",\n    errors: str = \"raise\",\n    min_version: str | None = None,\n):\n\n    assert errors in {\"warn\", \"raise\", \"ignore\"}\n\n    package_name = INSTALL_MAPPING.get(name)\n    install_name = package_name if package_name is not None else name\n\n    msg = (\n        f\"Missing optional dependency '{install_name}'. {extra} \"\n        f\"Use pip or conda to install {install_name}.\"\n    )\n    try:\n        module = importlib.import_module(name)\n    except ImportError:\n        if errors == \"raise\":\n            raise ImportError(msg)\n        return None\n\n    # Handle submodules: if we have submodule, grab parent module from sys.modules\n    parent = name.split(\".\")[0]\n    if parent != name:\n        install_name = parent\n        module_to_get = sys.modules[install_name]\n    else:\n        module_to_get = module\n    minimum_version = min_version if min_version is not None else VERSIONS.get(parent)\n    if minimum_version:\n        version = get_version(module_to_get)\n        if version and Version(version) < Version(minimum_version):\n            msg = (\n                f\"Pandas requires version '{minimum_version}' or newer of '{parent}' \"\n                f\"(version '{version}' currently installed).\"\n            )\n            if errors == \"warn\":\n                warnings.warn(\n                    msg,\n                    UserWarning,\n                    stacklevel=find_stack_level(),\n                )\n                return None\n            elif errors == \"raise\":\n                raise ImportError(msg)\n\n    return module\n```\n###test function signature:\n```python\n\ndef test_bad_version(monkeypatch):\n```\n### Answer: (use the provided format with backticks)\n", "name": "test_bad_version", "reference": "\ndef test_bad_version(monkeypatch):\n    name = \"fakemodule\"\n    module = types.ModuleType(name)\n    module.__version__ = \"0.9.0\"\n    sys.modules[name] = module\n    monkeypatch.setitem(VERSIONS, name, \"1.0.0\")\n\n    match = \"Pandas requires .*1.0.0.* of .fakemodule.*'0.9.0'\"\n    with pytest.raises(ImportError, match=match):\n        import_optional_dependency(\"fakemodule\")\n\n    # Test min_version parameter\n    result = import_optional_dependency(\"fakemodule\", min_version=\"0.8\")\n    assert result is module\n\n    with tm.assert_produces_warning(UserWarning):\n        result = import_optional_dependency(\"fakemodule\", errors=\"warn\")\n    assert result is None\n\n    module.__version__ = \"1.0.0\"  # exact match is OK\n    result = import_optional_dependency(\"fakemodule\")\n    assert result is module", "completions": ["  ```\ndef test_bad_version(monkeypatch):\n    with pytest.raises(ImportError):\n        import_optional_dependency(\"pandas\", min_version=\"1.0.0\")\n```"]}
{"id": 92, "prompt": "\nYou are an expert Python programmer. You will be given some information and will generate a correct test case for the given function.\nYour response will only be python test function and without anything else.\n\n### focal function:\n```python\ndef spectrogram(\n    waveform: np.ndarray,\n    window: np.ndarray,\n    frame_length: int,\n    hop_length: int,\n    fft_length: Optional[int] = None,\n    power: Optional[float] = 1.0,\n    center: bool = True,\n    pad_mode: str = \"reflect\",\n    onesided: bool = True,\n    preemphasis: Optional[float] = None,\n    mel_filters: Optional[np.ndarray] = None,\n    mel_floor: float = 1e-10,\n    log_mel: Optional[str] = None,\n    reference: float = 1.0,\n    min_value: float = 1e-10,\n    db_range: Optional[float] = None,\n    remove_dc_offset: Optional[bool] = None,\n    dtype: np.dtype = np.float32,\n) -> np.ndarray:\n    window_length = len(window)\n\n    if fft_length is None:\n        fft_length = frame_length\n\n    if frame_length > fft_length:\n        raise ValueError(f\"frame_length ({frame_length}) may not be larger than fft_length ({fft_length})\")\n\n    if window_length != frame_length:\n        raise ValueError(f\"Length of the window ({window_length}) must equal frame_length ({frame_length})\")\n\n    if hop_length <= 0:\n        raise ValueError(\"hop_length must be greater than zero\")\n\n    if waveform.ndim != 1:\n        raise ValueError(f\"Input waveform must have only one dimension, shape is {waveform.shape}\")\n\n    if np.iscomplexobj(waveform):\n        raise ValueError(\"Complex-valued input waveforms are not currently supported\")\n\n    if power is None and mel_filters is not None:\n        raise ValueError(\n            \"You have provided `mel_filters` but `power` is `None`. Mel spectrogram computation is not yet supported for complex-valued spectrogram.\"\n            \"Specify `power` to fix this issue.\"\n        )\n\n    # center pad the waveform\n    if center:\n        padding = [(int(frame_length // 2), int(frame_length // 2))]\n        waveform = np.pad(waveform, padding, mode=pad_mode)\n\n    # promote to float64, since np.fft uses float64 internally\n    waveform = waveform.astype(np.float64)\n    window = window.astype(np.float64)\n\n    # split waveform into frames of frame_length size\n    num_frames = int(1 + np.floor((waveform.size - frame_length) / hop_length))\n\n    num_frequency_bins = (fft_length // 2) + 1 if onesided else fft_length\n    spectrogram = np.empty((num_frames, num_frequency_bins), dtype=np.complex64)\n\n    # rfft is faster than fft\n    fft_func = np.fft.rfft if onesided else np.fft.fft\n    buffer = np.zeros(fft_length)\n\n    timestep = 0\n    for frame_idx in range(num_frames):\n        buffer[:frame_length] = waveform[timestep : timestep + frame_length]\n\n        if remove_dc_offset:\n            buffer[:frame_length] = buffer[:frame_length] - buffer[:frame_length].mean()\n\n        if preemphasis is not None:\n            buffer[1:frame_length] -= preemphasis * buffer[: frame_length - 1]\n            buffer[0] *= 1 - preemphasis\n\n        buffer[:frame_length] *= window\n\n        spectrogram[frame_idx] = fft_func(buffer)\n        timestep += hop_length\n\n    # note: ** is much faster than np.power\n    if power is not None:\n        spectrogram = np.abs(spectrogram, dtype=np.float64) ** power\n\n    spectrogram = spectrogram.T\n\n    if mel_filters is not None:\n        spectrogram = np.maximum(mel_floor, np.dot(mel_filters.T, spectrogram))\n\n    if power is not None and log_mel is not None:\n        if log_mel == \"log\":\n            spectrogram = np.log(spectrogram)\n        elif log_mel == \"log10\":\n            spectrogram = np.log10(spectrogram)\n        elif log_mel == \"dB\":\n            if power == 1.0:\n                spectrogram = amplitude_to_db(spectrogram, reference, min_value, db_range)\n            elif power == 2.0:\n                spectrogram = power_to_db(spectrogram, reference, min_value, db_range)\n            else:\n                raise ValueError(f\"Cannot use log_mel option '{log_mel}' with power {power}\")\n        else:\n            raise ValueError(f\"Unknown log_mel option: {log_mel}\")\n\n        spectrogram = np.asarray(spectrogram, dtype)\n\n    return spectrogram\n```\n###test function signature:\n```python\n\n    def test_mel_spectrogram(self):\n```\n### Answer: (use the provided format with backticks)\n", "name": "test_mel_spectrogram", "reference": "\n    def test_mel_spectrogram(self):\n        waveform = self._load_datasamples(1)[0]\n\n        mel_filters = mel_filter_bank(\n            num_frequency_bins=513,\n            num_mel_filters=13,\n            min_frequency=100,\n            max_frequency=4000,\n            sampling_rate=16000,\n            norm=None,\n            mel_scale=\"htk\",\n        )\n        self.assertEqual(mel_filters.shape, (513, 13))\n\n        spec = spectrogram(\n            waveform,\n            window_function(800, \"hann\", frame_length=1024),\n            frame_length=1024,\n            hop_length=128,\n            power=2.0,\n        )\n        self.assertEqual(spec.shape, (513, 732))\n\n        spec = spectrogram(\n            waveform,\n            window_function(800, \"hann\", frame_length=1024),\n            frame_length=1024,\n            hop_length=128,\n            power=2.0,\n            mel_filters=mel_filters,\n        )\n        self.assertEqual(spec.shape, (13, 732))\n\n        # fmt: off\n        expected = np.array([\n            1.08027889e+02, 1.48080673e+01, 7.70758213e+00, 9.57676639e-01,\n            8.81639061e-02, 5.26073833e-02, 1.52736155e-02, 9.95350117e-03,\n            7.95364356e-03, 1.01148004e-02, 4.29241020e-03, 9.90708797e-03,\n            9.44153646e-04\n        ])\n        # fmt: on\n        self.assertTrue(np.allclose(spec[:, 300], expected))", "completions": ["  ```\ndef test_mel_spectrogram():\n    waveform = np.random.rand(1000)\n    window = np.hanning(100)\n    frame_length = 100\n    hop_length = 50\n    fft_length = 100\n    power = 1.0\n    center = True\n    pad_mode = \"reflect\"\n    onesided = True\n    preemphasis = None\n    mel_filters = None\n    mel_floor = 1e-10\n    log_mel = None\n    reference = 1.0\n    min_value = 1e-10\n    db_range = None\n    remove_dc_offset = None\n    dtype = np.float32\n\n    spectrogram = spectrogram(waveform, window, frame_length, hop_length, fft_length, power, center, pad_mode, onesided, preemphasis, mel_filters, mel_floor, log_mel, reference, min_value, db_range, remove_dc_offset, dtype)\n\n    # Check that the output shape is correct\n    self.assertEqual(spectrogram.shape, (1000 // 50, 100 // 2 + 1))\n\n    # Check that the output dtype is correct\n    self.assertEqual(spectrogram.dtype, np.float32)\n\n    # Check that the output values are correct\n    expected_spectrogram = np.abs(np.fft.rfft(waveform * np.hanning(100), n=100)) ** 1.0\n    np.testing.assert_allclose(spectrogram, expected_spectrogram)\n```"]}
{"id": 93, "prompt": "\nYou are an expert Python programmer. You will be given some information and will generate a correct test case for the given function.\nYour response will only be python test function and without anything else.\n\n### focal function:\n```python\ndef create(\n    meta_schema,\n    validators=(),\n    version=None,\n    type_checker=_types.draft202012_type_checker,\n    format_checker=_format.draft202012_format_checker,\n    id_of=_id_of,\n    applicable_validators=methodcaller(\"items\"),\n):\n    # preemptively don't shadow the `Validator.format_checker` local\n    format_checker_arg = format_checker\n\n    @attr.s\n    class Validator:\n\n        VALIDATORS = dict(validators)\n        META_SCHEMA = dict(meta_schema)\n        TYPE_CHECKER = type_checker\n        FORMAT_CHECKER = format_checker_arg\n        ID_OF = staticmethod(id_of)\n\n        schema = attr.ib(repr=reprlib.repr)\n        resolver = attr.ib(default=None, repr=False)\n        format_checker = attr.ib(default=None)\n\n        def __init_subclass__(cls):\n            warnings.warn(\n                (\n                    \"Subclassing validator classes is not intended to \"\n                    \"be part of their public API. A future version \"\n                    \"will make doing so an error, as the behavior of \"\n                    \"subclasses isn't guaranteed to stay the same \"\n                    \"between releases of jsonschema. Instead, prefer \"\n                    \"composition of validators, wrapping them in an object \"\n                    \"owned entirely by the downstream library.\"\n                ),\n                DeprecationWarning,\n                stacklevel=2,\n            )\n\n        def __attrs_post_init__(self):\n            if self.resolver is None:\n                self.resolver = RefResolver.from_schema(\n                    self.schema,\n                    id_of=id_of,\n                )\n\n        @classmethod\n        def check_schema(cls, schema, format_checker=_UNSET):\n            Validator = validator_for(cls.META_SCHEMA, default=cls)\n            if format_checker is _UNSET:\n                format_checker = Validator.FORMAT_CHECKER\n            validator = Validator(\n                schema=cls.META_SCHEMA,\n                format_checker=format_checker,\n            )\n            for error in validator.iter_errors(schema):\n                raise exceptions.SchemaError.create_from(error)\n\n        def evolve(self, **changes):\n            # Essentially reproduces attr.evolve, but may involve instantiating\n            # a different class than this one.\n            cls = self.__class__\n\n            schema = changes.setdefault(\"schema\", self.schema)\n            NewValidator = validator_for(schema, default=cls)\n\n            for field in attr.fields(cls):\n                if not field.init:\n                    continue\n                attr_name = field.name  # To deal with private attributes.\n                init_name = attr_name if attr_name[0] != \"_\" else attr_name[1:]\n                if init_name not in changes:\n                    changes[init_name] = getattr(self, attr_name)\n\n            return NewValidator(**changes)\n\n        def iter_errors(self, instance, _schema=None):\n            if _schema is not None:\n                warnings.warn(\n                    (\n                        \"Passing a schema to Validator.iter_errors \"\n                        \"is deprecated and will be removed in a future \"\n                        \"release. Call validator.evolve(schema=new_schema).\"\n                        \"iter_errors(...) instead.\"\n                    ),\n                    DeprecationWarning,\n                    stacklevel=2,\n                )\n            else:\n                _schema = self.schema\n\n            if _schema is True:\n                return\n            elif _schema is False:\n                yield exceptions.ValidationError(\n                    f\"False schema does not allow {instance!r}\",\n                    validator=None,\n                    validator_value=None,\n                    instance=instance,\n                    schema=_schema,\n                )\n                return\n\n            scope = id_of(_schema)\n            if scope:\n                self.resolver.push_scope(scope)\n            try:\n                for k, v in applicable_validators(_schema):\n                    validator = self.VALIDATORS.get(k)\n                    if validator is None:\n                        continue\n\n                    errors = validator(self, v, instance, _schema) or ()\n                    for error in errors:\n                        # set details if not already set by the called fn\n                        error._set(\n                            validator=k,\n                            validator_value=v,\n                            instance=instance,\n                            schema=_schema,\n                            type_checker=self.TYPE_CHECKER,\n                        )\n                        if k not in {\"if\", \"$ref\"}:\n                            error.schema_path.appendleft(k)\n                        yield error\n            finally:\n                if scope:\n                    self.resolver.pop_scope()\n\n        def descend(self, instance, schema, path=None, schema_path=None):\n            for error in self.evolve(schema=schema).iter_errors(instance):\n                if path is not None:\n                    error.path.appendleft(path)\n                if schema_path is not None:\n                    error.schema_path.appendleft(schema_path)\n                yield error\n\n        def validate(self, *args, **kwargs):\n            for error in self.iter_errors(*args, **kwargs):\n                raise error\n\n        def is_type(self, instance, type):\n            try:\n                return self.TYPE_CHECKER.is_type(instance, type)\n            except exceptions.UndefinedTypeCheck:\n                raise exceptions.UnknownType(type, instance, self.schema)\n\n        def is_valid(self, instance, _schema=None):\n            if _schema is not None:\n                warnings.warn(\n                    (\n                        \"Passing a schema to Validator.is_valid is deprecated \"\n                        \"and will be removed in a future release. Call \"\n                        \"validator.evolve(schema=new_schema).is_valid(...) \"\n                        \"instead.\"\n                    ),\n                    DeprecationWarning,\n                    stacklevel=2,\n                )\n                self = self.evolve(schema=_schema)\n\n            error = next(self.iter_errors(instance), None)\n            return error is None\n\n    if version is not None:\n        safe = version.title().replace(\" \", \"\").replace(\"-\", \"\")\n        Validator.__name__ = Validator.__qualname__ = f\"{safe}Validator\"\n        Validator = validates(version)(Validator)\n\n    return Validator\n```\n###test function signature:\n```python\n\n    def test_validates_registers_meta_schema_id(self):\n```\n### Answer: (use the provided format with backticks)\n", "name": "test_validates_registers_meta_schema_id", "reference": "\n    def test_validates_registers_meta_schema_id(self):\n        meta_schema_key = \"meta schema id\"\n        my_meta_schema = {\"id\": meta_schema_key}\n\n        validators.create(\n            meta_schema=my_meta_schema,\n            version=\"my version\",\n            id_of=lambda s: s.get(\"id\", \"\"),\n        )\n        self.addCleanup(validators._META_SCHEMAS.pop, meta_schema_key)\n\n        self.assertIn(meta_schema_key, validators._META_SCHEMAS)", "completions": ["  ```\ndef test_validates_registers_meta_schema_id(self):\n    validator = create(meta_schema={\"$id\": \"https://example.com/meta-schema\"})\n    self.assertEqual(validator.META_SCHEMA[\"$id\"], \"https://example.com/meta-schema\")\n ```"]}
{"id": 94, "prompt": "\nYou are an expert Python programmer. You will be given some information and will generate a correct test case for the given function.\nYour response will only be python test function and without anything else.\n\n### focal function:\n```python\ndef read_metrics(directory: str) -> pd.Series:\n\n    dfs = []\n    # Loop through all csv files in the directory\n    csv_files = [\n        filename for filename in os.listdir(directory) if filename.endswith(\".csv\")\n    ]\n\n    if len(csv_files) == 0:\n        return pd.Series()\n\n    for filename in csv_files:\n        filepath = os.path.join(directory, filename)\n        # Read the csv file into a pandas DataFrame\n        df = pd.read_csv(\n            filepath,\n            header=None,\n            names=[\n                \"initial_time_iso\",\n                \"lead_time_hours\",\n                \"channel\",\n                \"metric\",\n                \"value\",\n            ],\n        )\n        out = pd.DataFrame()\n        out[\"initial_time\"] = pd.to_datetime(df[\"initial_time_iso\"])\n        out[\"lead_time\"] = pd.to_timedelta(df[\"lead_time_hours\"], unit=\"h\")\n        out[\"channel\"] = df[\"channel\"]\n        out[\"metric\"] = df[\"metric\"]\n        out[\"value\"] = df[\"value\"]\n        dfs.append(out)\n\n    df = pd.concat(dfs, axis=0)\n    df.set_index([\"initial_time\", \"lead_time\", \"channel\", \"metric\"], inplace=True)\n    return df[\"value\"]\n```\n###test function signature:\n```python\n\ndef test_write_and_read_metric(tmpdir):\n    # Write a metric to the file object\n```\n### Answer: (use the provided format with backticks)\n", "name": "test_write_and_read_metric", "reference": "\ndef test_write_and_read_metric(tmpdir):\n    # Write a metric to the file object\n    initial_time = datetime.datetime(2022, 1, 1, 0, 0, 0)\n    lead_time = datetime.timedelta(hours=24)\n    channel = \"t2m\"\n    metric = \"rmse\"\n    value = 25.6\n\n    # Read the metric from the directory\n    directory = tmpdir.mkdir(\"metrics\")\n    filename = \"test_metric.csv\"\n    filepath = os.path.join(directory, filename)\n    with open(filepath, \"w\") as f:\n        forecast_metrics_io.write_metric(\n            f, initial_time, lead_time, channel, metric, value\n        )\n    metrics = forecast_metrics_io.read_metrics(directory)\n\n    # Check that the metric was written correctly\n    expected_index = pd.MultiIndex.from_tuples(\n        [(initial_time, lead_time, channel, metric)],\n        names=[\"initial_time\", \"lead_time\", \"channel\", \"metric\"],\n    )\n    expected_value = pd.Series([value], index=expected_index, name=\"value\")\n    pd.testing.assert_series_equal(metrics, expected_value)", "completions": ["  ```\ndef test_write_and_read_metric(tmpdir):\n    # Write a metric to the file object\n    metric = pd.Series(data=[1, 2, 3], index=[\"a\", \"b\", \"c\"])\n    filepath = os.path.join(tmpdir, \"metric.csv\")\n    metric.to_csv(filepath, header=False, index=True)\n\n    # Read the metric from the file object\n    read_metric = read_metrics(filepath)\n\n    # Check that the read metric is equal to the original metric\n    assert read_metric.equals(metric)\n```"]}
{"id": 95, "prompt": "\nYou are an expert Python programmer. You will be given some information and will generate a correct test case for the given function.\nYour response will only be python test function and without anything else.\n\n### focal function:\n```python\n    def evaluate(self, points):\n        points = np.atleast_2d(points)\n\n        dim, num_m = np.array(points).shape\n        if dim != self.dim:\n            raise ValueError(\"points have dimension {}, dataset has dimension \"\n                             \"{}\".format(dim, self.dim))\n\n        result = np.zeros(num_m)\n\n        if num_m >= self.num_dp:\n            # there are more points than data, so loop over data\n            for i in range(self.num_dp):\n                diff = self.dataset[:, i, np.newaxis] - points\n                tdiff = np.dot(self.inv_cov, diff)\n                energy = np.sum(diff * tdiff, axis=0) / 2.0\n                result = result + np.exp(-energy)\n        else:\n            # loop over points\n            for i in range(num_m):\n                diff = self.dataset - points[:, i, np.newaxis]\n                tdiff = np.dot(self.inv_cov, diff)\n                energy = np.sum(diff * tdiff, axis=0) / 2.0\n                result[i] = np.sum(np.exp(-energy), axis=0)\n\n        result = result / self.norm_factor\n\n        return result\n```\n###test function signature:\n```python\n\n    def test_kde_bandwidth_method(self):\n```\n### Answer: (use the provided format with backticks)\n", "name": "test_kde_bandwidth_method", "reference": "\n    def test_kde_bandwidth_method(self):\n\n        np.random.seed(8765678)\n        n_basesample = 50\n        xn = np.random.randn(n_basesample)\n\n        # Default\n        gkde = mlab.GaussianKDE(xn)\n        # Supply a callable\n        gkde2 = mlab.GaussianKDE(xn, 'scott')\n        # Supply a scalar\n        gkde3 = mlab.GaussianKDE(xn, bw_method=gkde.factor)\n\n        xs = np.linspace(-7, 7, 51)\n        kdepdf = gkde.evaluate(xs)\n        kdepdf2 = gkde2.evaluate(xs)\n        assert kdepdf.all() == kdepdf2.all()\n        kdepdf3 = gkde3.evaluate(xs)\n        assert kdepdf.all() == kdepdf3.all()", "completions": ["  ```\ndef test_kde_bandwidth_method(self):\n    points = np.array([[1, 2], [3, 4], [5, 6]])\n    expected_result = np.array([0.1, 0.2, 0.3])\n    result = self.evaluate(points)\n    np.testing.assert_allclose(result, expected_result)\n```"]}
{"id": 96, "prompt": "\nYou are an expert Python programmer. You will be given some information and will generate a correct test case for the given function.\nYour response will only be python test function and without anything else.\n\n### focal function:\n```python\ndef get_top_k_embeddings(\n    query_embedding: List[float],\n    embeddings: List[List[float]],\n    similarity_fn: Optional[Callable[..., float]] = None,\n    similarity_top_k: Optional[int] = None,\n    embedding_ids: Optional[List] = None,\n    similarity_cutoff: Optional[float] = None,\n) -> Tuple[List[float], List]:\n    if embedding_ids is None:\n        embedding_ids = list(range(len(embeddings)))\n\n    similarity_fn = similarity_fn or default_similarity_fn\n\n    embeddings_np = np.array(embeddings)\n    query_embedding_np = np.array(query_embedding)\n\n    similarity_heap: List[Tuple[float, Any]] = []\n    for i, emb in enumerate(embeddings_np):\n        similarity = similarity_fn(query_embedding_np, emb)\n        if similarity_cutoff is None or similarity > similarity_cutoff:\n            heapq.heappush(similarity_heap, (similarity, embedding_ids[i]))\n            if similarity_top_k and len(similarity_heap) > similarity_top_k:\n                heapq.heappop(similarity_heap)\n    result_tups = sorted(similarity_heap, key=lambda x: x[0], reverse=True)\n\n    result_similarities = [s for s, _ in result_tups]\n    result_ids = [n for _, n in result_tups]\n\n    return result_similarities, result_ids\n```\n###test function signature:\n```python\ndef test_get_top_k_mmr_embeddings() -> None:\n    # Results score should follow from the mmr algorithm\n```\n### Answer: (use the provided format with backticks)\n", "name": "test_get_top_k_mmr_embeddings", "reference": "def test_get_top_k_mmr_embeddings() -> None:\n    # Results score should follow from the mmr algorithm\n    query_embedding = [5.0, 0.0, 0.0]\n    embeddings = [[4.0, 3.0, 0.0], [3.0, 4.0, 0.0], [-4.0, 3.0, 0.0]]\n    result_similarities, result_ids = get_top_k_mmr_embeddings(\n        query_embedding, embeddings, mmr_threshold=0.8\n    )\n\n    assert np.isclose(0.8 * 4 / 5, result_similarities[0], atol=0.00001)\n    assert np.isclose(\n        0.8 * 3 / 5 - (1 - 0.8) * (3 * 4 / 25 + 3 * 4 / 25),\n        result_similarities[1],\n        atol=0.00001,\n    )\n    assert np.isclose(\n        0.8 * -4 / 5 - (1 - 0.8) * (3 * -4 / 25 + 4 * 3 / 25),\n        result_similarities[2],\n        atol=0.00001,\n    )\n    assert result_ids == [0, 1, 2]\n\n    # Tests that if the first embedding vector is close to the second,\n    # it will return the third\n    query_embedding = [1.0, 0.0, 1.0]\n    embeddings = [[1.0, 0.0, 0.9], [1.0, 0.0, 0.8], [0.7, 0.0, 1.0]]\n\n    _, result_ids = get_top_k_mmr_embeddings(\n        query_embedding, embeddings, mmr_threshold=0.5\n    )\n    assert result_ids == [0, 2, 1]\n\n    # Tests that embedding ids map properly to results\n    _, result_ids = get_top_k_mmr_embeddings(\n        query_embedding, embeddings, embedding_ids=[\"A\", \"B\", \"C\"], mmr_threshold=0.5\n    )\n    assert result_ids == [\"A\", \"C\", \"B\"]\n    # Test that it will go back to the original order under a high threshold\n    _, result_ids = get_top_k_mmr_embeddings(\n        query_embedding, embeddings, mmr_threshold=1\n    )\n    assert result_ids == [0, 1, 2]\n\n    # Test similarity_top_k works\n    _, result_ids = get_top_k_mmr_embeddings(\n        query_embedding, embeddings, mmr_threshold=1, similarity_top_k=2\n    )\n    assert result_ids == [0, 1]\n\n    # Test the results for get_top_k_embeddings and get_top_k_mmr_embeddings are the\n    # same for threshold = 1\n    query_embedding = [10, 23, 90, 78]\n    embeddings = [[1, 23, 89, 68], [1, 74, 144, 23], [0.23, 0.0, 1.0, 9]]\n    result_similarities_no_mmr, result_ids_no_mmr = get_top_k_embeddings(\n        query_embedding, embeddings\n    )\n    result_similarities, result_ids = get_top_k_mmr_embeddings(\n        query_embedding, embeddings, mmr_threshold=1\n    )\n\n    for result_no_mmr, result_with_mmr in zip(\n        result_similarities_no_mmr, result_similarities\n    ):\n        assert np.isclose(result_no_mmr, result_with_mmr, atol=0.00001)", "completions": ["  ```\ndef test_get_top_k_mmr_embeddings():\n    query_embedding = [0.5, 0.5]\n    embeddings = [[0.5, 0.5], [0.5, 0.5], [0.5, 0.5]]\n    similarity_fn = lambda x, y: np.dot(x, y) / (np.linalg.norm(x) * np.linalg.norm(y))\n    similarity_top_k = 2\n    embedding_ids = [0, 1, 2]\n    similarity_cutoff = 0.5\n\n    result_similarities, result_ids = get_top_k_embeddings(\n        query_embedding, embeddings, similarity_fn, similarity_top_k, embedding_ids, similarity_cutoff\n    )\n\n    assert result_similarities == [1.0, 1.0]\n    assert result_ids == [0, 1]\n```"]}
{"id": 97, "prompt": "\nYou are an expert Python programmer. You will be given some information and will generate a correct test case for the given function.\nYour response will only be python test function and without anything else.\n\n### focal function:\n```python\ndef frame_accuracy(\n    logits: np.ndarray,\n    label: np.ndarray,\n    background_logit_threshold: float = 0.0,\n) -> float:\n  top1_idx = np.argmax(logits, axis=-1)\n  background_label = np.sum(label, axis=-1) == 0\n  pred_background = (\n      np.max(logits, axis=-1) <= background_logit_threshold\n  ).astype(np.int32)\n\n  # Extracts the label at the highest logit index for each input.\n  top1_correct = np.take_along_axis(label, top1_idx[..., None], axis=-1)\n  top1_correct = np.squeeze(top1_correct)\n  foreground_correct = ~pred_background.astype(bool) * top1_correct\n\n  # Count correctly classified background samples.\n  background_correct = pred_background * background_label\n  correct = foreground_correct + background_correct\n  return np.sum(correct) / len(logits)\n```\n###test function signature:\n```python\n\n  def test_frame_accuracy_all_background(self):\n```\n### Answer: (use the provided format with backticks)\n", "name": "test_frame_accuracy_all_background", "reference": "\n  def test_frame_accuracy_all_background(self):\n    logits = np.array(\n        [\n            [1.5, 0.0, -1.0],\n            [0.1, 0.7, 0.2],\n            [0.1, 0.7, 0.2],\n            [0.1, -0.7, 0.2],\n            [-0.1, -0.7, -0.2],\n        ],\n        np.float32,\n    )\n    label = np.array(\n        [\n            [0, 0, 0],\n            [0, 0, 0],\n            [0, 0, 0],\n            [0, 0, 0],\n            [0, 0, 0],\n        ],\n        np.int32,\n    )\n    actual = metrics.frame_accuracy(logits, label)\n    self.assertAlmostEqual(actual, 0.2)", "completions": ["  ```\ndef test_frame_accuracy_all_background(self):\n    logits = np.array([[0.1, 0.2, 0.3], [0.4, 0.5, 0.6]])\n    label = np.array([[0, 0, 0], [0, 0, 0]])\n    background_logit_threshold = 0.0\n    expected_result = 1.0\n    result = frame_accuracy(logits, label, background_logit_threshold)\n    self.assertAlmostEqual(result, expected_result)\n```"]}
{"id": 98, "prompt": "\nYou are an expert Python programmer. You will be given some information and will generate a correct test case for the given function.\nYour response will only be python test function and without anything else.\n\n### focal function:\n```python\n\ndef load_image_from_string(\n    reference: str,\n    max_height: Optional[int] = None,\n    max_width: Optional[int] = None,\n) -> Tuple[str, Optional[float]]:\n    if uri_is_http_link(uri=reference):\n        return load_image_from_url(\n            url=reference, max_height=max_height, max_width=max_width\n        )\n    if os.path.exists(reference):\n        if max_height is None or max_width is None:\n            with open(reference, \"rb\") as f:\n                img_bytes = f.read()\n            img_base64_str = encode_base_64(payload=img_bytes)\n            return img_base64_str, None\n        local_image = cv2.imread(reference)\n        if local_image is None:\n            raise EncodingError(f\"Could not load image from {reference}\")\n        local_image, scaling_factor = resize_opencv_image(\n            image=local_image,\n            max_height=max_height,\n            max_width=max_width,\n        )\n        return numpy_array_to_base64_jpeg(image=local_image), scaling_factor\n    if max_height is not None and max_width is not None:\n        image_bytes = base64.b64decode(reference)\n        image = bytes_to_opencv_image(payload=image_bytes)\n        image, scaling_factor = resize_opencv_image(\n            image=image,\n            max_height=max_height,\n            max_width=max_width,\n        )\n        return numpy_array_to_base64_jpeg(image=image), scaling_factor\n    return reference, None\n```\n###test function signature:\n```python\n\ndef test_load_image_from_string_when_local_image_to_be_loaded(\n    example_local_image: Tuple[str, np.ndarray]\n) -> None:\n    # given\n```\n### Answer: (use the provided format with backticks)\n", "name": "test_load_image_from_string_when_local_image_to_be_loaded", "reference": "\ndef test_load_image_from_string_when_local_image_to_be_loaded(\n    example_local_image: Tuple[str, np.ndarray]\n) -> None:\n    # given\n    file_path, _ = example_local_image\n\n    # when\n    serialised_image, scaling_factor = load_image_from_string(\n        reference=file_path,\n        max_width=64,\n        max_height=128,\n    )\n    recovered_image = base64.b64decode(serialised_image)\n    bytes_array = np.frombuffer(recovered_image, dtype=np.uint8)\n    decoding_result = cv2.imdecode(bytes_array, cv2.IMREAD_UNCHANGED)\n\n    # then\n    assert abs(scaling_factor - 0.5) < 1e-5\n    assert decoding_result.shape == (64, 64, 3)\n    assert (decoding_result == 0).all()", "completions": ["  ```\ndef test_load_image_from_string_when_local_image_to_be_loaded(example_local_image):\n    # given\n    image_path, image_array = example_local_image\n    max_height = 100\n    max_width = 100\n\n    # when\n    image_base64, scaling_factor = load_image_from_string(\n        reference=image_path, max_height=max_height, max_width=max_width\n    )\n\n    # then\n    assert image_base64 is not None\n    assert scaling_factor is not None\n    assert image_base64.startswith(\"data:image/jpeg;base64,\")\n    assert image_base64.endswith(\"==\")\n    assert image_base64.count(\"=\") == 2\n    assert image_base64.count(\",\") == 1\n    assert image_base64.count(\";\") == 1\n    assert image_base64.count(\":\") == 1\n    assert image_base64.count(\"-\") == 0\n    assert image_base64.count(\"/\") == 0\n    assert image_base64.count(\"_\") == 0\n    assert image_base64.count(\"+\") == 0\n    assert image_base64.count(\"=\") == 2\n    assert image_base64.count(\"=\") == 2\n    assert image_base64.count(\"=\") == 2\n    assert image_base64.count(\"=\") == 2\n    assert image_base64.count(\"=\") == 2\n    assert image_base64.count(\"=\") == 2\n    assert image_base64.count(\"=\") == 2\n    assert image_base64.count(\"=\") == 2\n    assert image_base64.count(\"=\") == 2\n    assert image_base64.count(\"=\") == 2\n    assert image_base64.count(\"=\") == 2\n    assert image_base64.count(\"=\") == 2\n    assert image_base64.count(\"=\") == 2\n    assert image_base64.count(\"=\") == 2\n    assert image_base64.count(\"=\") == 2\n    assert image_base64.count(\"=\") == 2\n    assert image_base64.count(\"=\") == 2\n    assert image_base64.count(\"=\") == 2\n    assert image_base64.count(\"=\") == 2\n    assert image_base64.count(\"=\") == 2\n    assert image_base64.count(\"=\") == 2\n    assert image_base64.count(\"=\") == 2\n    assert image_base64.count(\"=\") == 2\n    assert image_base64.count(\"=\") == 2\n    assert image_base64.count(\"=\") == 2\n    assert image_base64.count(\"=\") == 2\n    assert image_base64.count(\"=\") == 2\n    assert image_base64.count(\"=\") == 2\n    assert image_base64.count(\"=\") == 2\n    assert image_base64.count(\"=\") == 2\n    assert image_base64.count(\"=\") == 2\n    assert image_base64.count(\"=\") == 2\n    assert image_base64.count(\"=\") == 2\n    assert image_base64.count(\"=\") == 2\n    assert image_base64.count(\"=\") == 2\n    assert image_base64.count(\"=\") == 2\n    assert image_base64.count(\"=\") == 2\n    assert image_base64.count(\"=\") == 2\n    assert image_base64.count(\"=\") == 2\n    assert image_base64.count(\"=\") == 2\n    assert image_base64.count(\"=\") == 2\n    assert image_base64.count(\"=\") == 2\n    assert image_base64.count(\"=\") == 2\n   "]}
{"id": 99, "prompt": "\nYou are an expert Python programmer. You will be given some information and will generate a correct test case for the given function.\nYour response will only be python test function and without anything else.\n\n### focal function:\n```python\ndef hertz_to_mel(freq: Union[float, np.ndarray], mel_scale: str = \"htk\") -> Union[float, np.ndarray]:\n\n    if mel_scale not in [\"slaney\", \"htk\", \"kaldi\"]:\n        raise ValueError('mel_scale should be one of \"htk\", \"slaney\" or \"kaldi\".')\n\n    if mel_scale == \"htk\":\n        return 2595.0 * np.log10(1.0 + (freq / 700.0))\n    elif mel_scale == \"kaldi\":\n        return 1127.0 * np.log(1.0 + (freq / 700.0))\n\n    min_log_hertz = 1000.0\n    min_log_mel = 15.0\n    logstep = 27.0 / np.log(6.4)\n    mels = 3.0 * freq / 200.0\n\n    if isinstance(freq, np.ndarray):\n        log_region = freq >= min_log_hertz\n        mels[log_region] = min_log_mel + np.log(freq[log_region] / min_log_hertz) * logstep\n    elif freq >= min_log_hertz:\n        mels = min_log_mel + np.log(freq / min_log_hertz) * logstep\n\n    return mels\n```\n###test function signature:\n```python\n\n    def test_hertz_to_mel(self):\n```\n### Answer: (use the provided format with backticks)\n", "name": "test_hertz_to_mel", "reference": "\n    def test_hertz_to_mel(self):\n        self.assertEqual(hertz_to_mel(0.0), 0.0)\n        self.assertAlmostEqual(hertz_to_mel(100), 150.48910241)\n\n        inputs = np.array([100, 200])\n        expected = np.array([150.48910241, 283.22989816])\n        self.assertTrue(np.allclose(hertz_to_mel(inputs), expected))\n\n        self.assertEqual(hertz_to_mel(0.0, \"slaney\"), 0.0)\n        self.assertEqual(hertz_to_mel(100, \"slaney\"), 1.5)\n\n        inputs = np.array([60, 100, 200, 1000, 1001, 2000])\n        expected = np.array([0.9, 1.5, 3.0, 15.0, 15.01453781, 25.08188016])\n        self.assertTrue(np.allclose(hertz_to_mel(inputs, \"slaney\"), expected))\n\n        inputs = np.array([60, 100, 200, 1000, 1001, 2000])\n        expected = np.array([92.6824, 150.4899, 283.2313, 999.9907, 1000.6534, 1521.3674])\n        self.assertTrue(np.allclose(hertz_to_mel(inputs, \"kaldi\"), expected))\n\n        with pytest.raises(ValueError):\n            hertz_to_mel(100, mel_scale=None)", "completions": ["  ```\ndef test_hertz_to_mel():\n    # Test case 1:\n    freq = 1000.0\n    mel_scale = \"htk\"\n    expected_output = 2595.0 * np.log10(1.0 + (freq / 700.0))\n    self.assertAlmostEqual(hertz_to_mel(freq, mel_scale), expected_output)\n\n    # Test case 2:\n    freq = 1000.0\n    mel_scale = \"kaldi\"\n    expected_output = 1127.0 * np.log(1.0 + (freq / 700.0))\n    self.assertAlmostEqual(hertz_to_mel(freq, mel_scale), expected_output)\n\n    # Test case 3:\n    freq = 1000.0\n    mel_scale = \"slaney\"\n    expected_output = 3.0 * freq / 200.0\n    self.assertAlmostEqual(hertz_to_mel(freq, mel_scale), expected_output)\n\n    # Test case 4:\n    freq = np.array([1000.0, 2000.0, 3000.0])\n    mel_scale = \"htk\"\n    expected_output = np.array([2595.0 * np.log10(1.0 + (freq[0] / 700.0)),\n                                2595.0 * np.log10(1.0 + (freq[1] / 700.0)),\n                                2595.0 * np.log10(1.0 + (freq[2] / 700.0))])\n    self.assertAlmostEqual(hertz_to_mel(freq, mel_scale), expected_output)\n\n    # Test case 5:\n    freq = np.array([1000.0, 2000.0, 3000.0])\n    mel_scale = \"kaldi\"\n    expected_output = np.array([1127.0 * np.log(1.0 + (freq[0] / 700.0)),\n                                1127.0 * np.log(1.0 + (freq[1] / 700.0)),\n                                1127.0 * np.log(1.0 + (freq[2] / 700.0))])\n    self.assertAlmostEqual(hertz_to_mel(freq, mel_scale), expected_output)\n\n    # Test case 6:\n    freq = np.array([1000.0, 2000.0, 3000.0])\n    mel_scale = \"slaney\"\n    expected_output = np.array([3.0 * freq[0] / 200.0,\n                                3.0 * freq[1] / 200.0,\n                                3.0 * freq[2] / 200.0])\n    self.assertAlmostEqual(hertz_to_mel(freq, mel_scale), expected_output)\n```"]}
