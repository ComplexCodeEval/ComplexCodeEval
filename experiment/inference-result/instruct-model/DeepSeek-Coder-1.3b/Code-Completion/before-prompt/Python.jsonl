{"id": 0, "prompt": "<\uff5cbegin\u2581of\u2581sentence\uff5c>### Role:\nYou are an experienced Python programmer assistant responsible for code completion tasks. Please intelligently complete the missing parts of the provided code snippet.\n\n### Task:\nBased on the provided code snippet, complete the missing function implementation. Ensure the code is logically correct, syntactically accurate, and follows best coding practices.\n\n### Left context\n```Python\n# -*- coding: utf-8 -*-\n\"\"\"\nThis module manages a table of subject ids and\ntheir associated image file names.\nA subset of the table can be retrieved by partitioning the set of images into\nsubsets of ``Train``, ``Validation``, ``Inference``.\n\"\"\"\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport math\nimport os\nimport random\nimport shutil\n\nimport pandas\nimport tensorflow as tf  # to use the system level logging\n\nfrom niftynet.engine.signal import TRAIN, VALID, INFER, ALL\nfrom niftynet.utilities.decorators import singleton\nfrom niftynet.utilities.filename_matching import KeywordsMatching\nfrom niftynet.utilities.niftynet_global_config import NiftyNetGlobalConfig\nfrom niftynet.utilities.util_common import look_up_operations\nfrom niftynet.utilities.util_csv import match_and_write_filenames_to_csv\nfrom niftynet.utilities.util_csv import write_csv\n\nCOLUMN_UNIQ_ID = 'subject_id'\nCOLUMN_PHASE = 'phase'\nSUPPORTED_PHASES = {TRAIN, VALID, INFER, ALL}\n\n\n@singleton\nclass ImageSetsPartitioner(object):\n    \"\"\"\n    This class maintains a pandas.dataframe of filenames for all input sections.\n    The list of filenames are obtained by searching the specified folders\n    or loading from an existing csv file.\n\n    Users can query a subset of the dataframe by train/valid/infer partition\n    label and input section names.\n    \"\"\"\n\n    # dataframe (table) of file names in a shape of subject x modality\n    _file_list = None\n    # dataframes of subject_id:phase_id\n    _partition_ids = None\n\n    data_param = None\n    ratios = None\n    new_partition = False\n\n    # for saving the splitting index\n    data_split_file = \"\"\n    # default parent folder location for searching the image files\n    default_image_file_location = \\\n        NiftyNetGlobalConfig().get_niftynet_home_folder()\n\n    def initialise(self,\n                   data_param,\n                   new_partition=False,\n                   data_split_file=None,\n                   ratios=None):\n        \"\"\"\n        Set the data partitioner parameters\n\n        :param data_param: corresponding to all config sections\n        :param new_partition: bool value indicating whether to generate new\n            partition ids and overwrite csv file\n            (this class will write partition file iff new_partition)\n        :param data_split_file: location of the partition id file\n        :param ratios: a tuple/list with two elements:\n            ``(fraction of the validation set, fraction of the inference set)``\n            initialise to None will disable data partitioning\n            and get_file_list always returns all subjects.\n        \"\"\"\n        self.data_param = data_param\n        if data_split_file is None:\n            self.data_split_file = os.path.join('.', 'dataset_split.csv')\n        else:\n            self.data_split_file = data_split_file\n        self.ratios = ratios\n\n        self._file_list = None\n        self._partition_ids = None\n\n        self.load_data_sections_by_subject()\n        self.new_partition = new_partition\n        self.randomly_split_dataset(overwrite=new_partition)\n        tf.logging.info(self)\n        return self\n\n    def number_of_subjects(self, phase=ALL):\n        \"\"\"\n        query number of images according to phase.\n\n        :param phase:\n        :return:\n        \"\"\"\n        if self._file_list is None:\n            return 0\n        try:\n            phase = look_up_operations(phase.lower(), SUPPORTED_PHASES)\n        except (ValueError, AttributeError):\n            tf.logging.fatal('Unknown phase argument.')\n            raise\n\n        if phase == ALL:\n            return self._file_list[COLUMN_UNIQ_ID].count()\n        if self._partition_ids is None:\n            return 0\n        selector = self._partition_ids[COLUMN_PHASE] == phase\n        selected = self._partition_ids[selector][[COLUMN_UNIQ_ID]]\n        subset = pandas.merge(\n            self._file_list, selected, on=COLUMN_UNIQ_ID, sort=True)\n        return subset.count()[COLUMN_UNIQ_ID]\n\n```\n\n### Code description\n```Python\n\"\"\"\nDescription: This function is used to get a list of files based on the phase and section names.\n\nArgs:\n    self: The instance of the class.\n    phase (str, optional): The phase of the files to be retrieved. Defaults to ALL.\n    *section_names: Variable length argument list of section names.\n\nReturns:\n    pandas.DataFrame: A DataFrame containing the file list based on the phase and section names.\n\"\"\"\n```\n\n### Code snippet:\n```Python\ndef get_file_list(self, phase=ALL, *section_names):\n    if self._file_list is None:\n        tf.logging.warning('Empty file list, please initialise'\n                           'ImageSetsPartitioner first.')\n        return []\n    try:\n        phase = look_up_operations(phase.lower(), SUPPORTED_PHASES)\n    except (ValueError, AttributeError):\n        tf.logging.fatal('Unknown phase argument.')\n        raise\n\n    for name in section_names:\n        try:\n            look_up_operations(name, set(self._file_list))\n        except ValueError:\n            tf.logging.fatal(\n                'Requesting files under input section [%s],\\n'\n                'however the section does not exist in the config.', name)\n            raise\n    if phase == ALL:\n        self._file_list = self._file_list.sort_values(COLUMN_UNIQ_ID)\n        if section_names:\n            section_names = [COLUMN_UNIQ_ID] + list(section_names)\n            return self._file_list[section_names]\n        return self._file_list\n    if self._partition_ids is None or self._partition_ids.empty:\n        tf.logging.fatal('No partition ids available.')\n        if self.new_partition:\n            tf.logging.fatal('Unable to create new partitions,'\n```\n\n### Requirements:\n1. Complete only the missing part of the function.\n2. Only answer the code of the completed part. Do not include any of the provided code snippet.\n3. The answered code should be formatted using Markdown code block syntax.\n4. Do not include any additional text.\n\n### Answer: (use the provided format with backticks)\n", "name": "get_file_list", "reference": "                             'splitting ratios: %s, writing file %s',\n                             self.ratios, self.data_split_file)\n        elif os.path.isfile(self.data_split_file):\n            tf.logging.fatal(\n                'Unable to load %s, initialise the'\n                'ImageSetsPartitioner with `new_partition=True`'\n                'to overwrite the file.',\n                self.data_split_file)\n        raise ValueError\n\n    selector = self._partition_ids[COLUMN_PHASE] == phase\n    selected = self._partition_ids[selector][[COLUMN_UNIQ_ID]]\n    if selected.empty:\n        tf.logging.warning(\n            'Empty subset for phase [%s], returning None as file list. '\n            'Please adjust splitting fractions.', phase)\n        return None\n    subset = pandas.merge(\n        self._file_list, selected, on=COLUMN_UNIQ_ID, sort=True)\n    if subset.empty:\n        tf.logging.warning(\n            'No subject id matched in between file names and '\n            'partition files.\\nPlease check the partition files %s,\\nor '\n            'removing it to generate a new file automatically.',\n            self.data_split_file)\n    if section_names:\n        section_names = [COLUMN_UNIQ_ID] + list(section_names)\n        return subset[section_names]\n    return subset", "completions": ["```python\ndef get_file_list(self, phase=ALL, *section_names):\n    if self._file_list is None:\n        tf.logging.warning('Empty file list, please initialise\\n'\n                           'ImageSetsPartitioner first.')\n        return []\n    try:\n        phase = look_up_operations(phase.lower(), SUPPORTED_PHASES)\n    except (ValueError, AttributeError):\n        tf.logging.fatal('Unknown phase argument.')\n        raise\n\n    for name in section_names:\n        try:\n            look_up_operations(name, set(self._file_list))\n        except ValueError:\n            tf.logging.fatal(\n                'Requesting files under input section [%s],\\n'\n                'however the section does not exist in the config.', name)\n            raise\n    if phase == ALL:\n        self._file_list = self._file_list.sort_values(COLUMN_UNIQ_ID)\n        if section_names:\n            section_names = [COLUMN_UNIQ_ID] + list(section_names)\n            return self._file_list[section_names]\n        return self._file_list\n    if self._partition_ids is None or self._partition_ids.empty:\n        tf.logging.fatal('No partition ids available.')\n        if self.new_partition:\n            tf.logging.fatal('Unable to create new partitions.')\n    return self._file_list\n```\n"]}
{"id": 1, "prompt": "<\uff5cbegin\u2581of\u2581sentence\uff5c>### Role:\nYou are an experienced Python programmer assistant responsible for code completion tasks. Please intelligently complete the missing parts of the provided code snippet.\n\n### Task:\nBased on the provided code snippet, complete the missing function implementation. Ensure the code is logically correct, syntactically accurate, and follows best coding practices.\n\n### Left context\n```Python\nimport numpy as np\nimport scipy.linalg\nfrom scipy.special import gammaln, digamma\n\nfrom bnpy.suffstats import ParamBag, SuffStatBag\nfrom bnpy.util import LOGTWO, LOGPI, LOGTWOPI, EPS\nfrom bnpy.util import dotATA, dotATB, dotABT\nfrom bnpy.util import as1D, as2D, as3D, toCArray\nfrom bnpy.util import numpyToSharedMemArray, fillSharedMemArray\nfrom bnpy.util.SparseRespStatsUtil import calcSpRXXT\nfrom AbstractObsModel import AbstractObsModel\nfrom GaussObsModel import createECovMatFromUserInput\n\n\nclass ZeroMeanGaussObsModel(AbstractObsModel):\n\n    ''' Zero-mean, full-covariance gaussian model for real vectors.\n\n    Attributes for Prior (Normal-Wishart)\n    --------\n    nu : float\n        degrees of freedom\n    B : 2D array, size D x D\n        scale parameters that set mean of parameter Sigma\n\n    Attributes for k-th component of EstParams (EM point estimates)\n    ---------\n    Sigma[k] : 2D array, size DxD\n\n    Attributes for k-th component of Post (VB parameter)\n    ---------\n    nu[k] : float\n    B[k] : 1D array, size D\n\n    '''\n\n    def __init__(self, inferType='EM', D=0, min_covar=None,\n                 Data=None,\n                 **PriorArgs):\n        ''' Initialize bare obsmodel with valid prior hyperparameters.\n\n        Resulting object lacks either EstParams or Post,\n        which must be created separately (see init_global_params).\n        '''\n        if Data is not None:\n            self.D = Data.dim\n        else:\n            self.D = int(D)\n        self.K = 0\n        self.inferType = inferType\n        self.min_covar = min_covar\n        self.createPrior(Data, **PriorArgs)\n        self.Cache = dict()\n\n    def createPrior(self, Data, nu=0, B=None,\n                    ECovMat=None, sF=1.0, **kwargs):\n        ''' Initialize Prior ParamBag attribute.\n\n        Post Condition\n        ------\n        Prior expected covariance matrix set to match provided value.\n        '''\n        D = self.D\n        nu = np.maximum(nu, D + 2)\n        if B is None:\n            if ECovMat is None or isinstance(ECovMat, str):\n                ECovMat = createECovMatFromUserInput(D, Data, ECovMat, sF)\n            B = ECovMat * (nu - D - 1)\n        else:\n            B = as2D(B)\n        self.Prior = ParamBag(K=0, D=D)\n        self.Prior.setField('nu', nu, dims=None)\n        self.Prior.setField('B', B, dims=('D', 'D'))\n\n    def get_mean_for_comp(self, k):\n        return np.zeros(self.D)\n\n    def get_covar_mat_for_comp(self, k=None):\n        if hasattr(self, 'EstParams'):\n            return self.EstParams.Sigma[k]\n        elif k is None or k == 'prior':\n            return self._E_CovMat()\n        else:\n            return self._E_CovMat(k)\n\n    def get_name(self):\n        return 'ZeroMeanGauss'\n\n    def get_info_string(self):\n        return 'Gaussian with fixed zero means, full covariance.'\n\n    def get_info_string_prior(self):\n        msg = 'Wishart on prec matrix Lam\\n'\n        if self.D > 2:\n            sfx = ' ...'\n        else:\n            sfx = ''\n        S = self._E_CovMat()[:2, :2]\n        msg += 'E[ CovMat[k] ] = \\n'\n        msg += str(S) + sfx\n        msg = msg.replace('\\n', '\\n  ')\n        return msg\n\n    def setEstParams(self, obsModel=None, SS=None, LP=None, Data=None,\n                     Sigma=None,\n                     **kwargs):\n        ''' Create EstParams ParamBag with fields Sigma\n        '''\n        self.ClearCache()\n        if obsModel is not None:\n            self.EstParams = obsModel.EstParams.copy()\n            self.K = self.EstParams.K\n            return\n\n        if LP is not None and Data is not None:\n            SS = self.calcSummaryStats(Data, None, LP)\n\n        if SS is not None:\n            self.updateEstParams(SS)\n        else:\n            K = Sigma.shape[0]\n            self.EstParams = ParamBag(K=K, D=self.D)\n            self.EstParams.setField('Sigma', Sigma, dims=('K', 'D', 'D'))\n        self.K = self.EstParams.K\n\n    def setEstParamsFromPost(self, Post):\n        ''' Convert from Post (nu, B) to EstParams (Sigma),\n             each EstParam is set to its posterior mean.\n        '''\n        D = Post.D\n        self.EstParams = ParamBag(K=Post.K, D=D)\n        Sigma = Post.B / (Post.nu - D - 1)[:, np.newaxis, np.newaxis]\n        self.EstParams.setField('Sigma', Sigma, dims=('K', 'D', 'D'))\n        self.K = self.EstParams.K\n\n    def setPostFactors(self, obsModel=None, SS=None, LP=None, Data=None,\n                       nu=0, B=0,\n                       **kwargs):\n        ''' Set attribute Post to provided values.\n        '''\n        self.ClearCache()\n        if obsModel is not None:\n            if hasattr(obsModel, 'Post'):\n                self.Post = obsModel.Post.copy()\n                self.K = self.Post.K\n            else:\n                self.setPostFromEstParams(obsModel.EstParams)\n            return\n\n        if LP is not None and Data is not None:\n            SS = self.calcSummaryStats(Data, None, LP)\n\n        if SS is not None:\n            self.updatePost(SS)\n        else:\n            K = B.shape[0]\n            self.Post = ParamBag(K=K, D=self.D)\n            self.Post.setField('nu', as1D(nu), dims=('K'))\n            self.Post.setField('B', B, dims=('K', 'D', 'D'))\n        self.K = self.Post.K\n\n    def setPostFromEstParams(self, EstParams, Data=None, N=None):\n        ''' Set attribute Post based on values in EstParams.\n        '''\n        K = EstParams.K\n        D = EstParams.D\n        if Data is not None:\n            N = Data.nObsTotal\n        N = np.asarray(N, dtype=np.float)\n        if N.ndim == 0:\n            N = float(N) / K * np.ones(K)\n\n        nu = self.Prior.nu + N\n        B = np.zeros((K, D, D))\n        for k in xrange(K):\n            B[k] = (nu[k] - D - 1) * EstParams.Sigma[k]\n        self.Post = ParamBag(K=K, D=D)\n        self.Post.setField('nu', nu, dims=('K'))\n        self.Post.setField('B', B, dims=('K', 'D', 'D'))\n        self.K = K\n\n    def calcSummaryStats(self, Data, SS, LP, **kwargs):\n        ''' Calculate summary statistics for given dataset and local parameters\n\n        Returns\n        --------\n        SS : SuffStatBag object, with K components.\n        '''\n        return calcSummaryStats(Data, SS, LP, **kwargs)\n\n    def calcSummaryStatsForContigBlock(self, Data, SS=None,\n                                       a=None, b=None, **kwargs):\n        ''' Calculate summary statistics for specific block of dataset\n\n        Returns\n        --------\n        SS : SuffStatBag object, with K components.\n        '''\n        SS = SuffStatBag(K=1, D=Data.dim)\n\n        # Expected count\n        SS.setField('N', (b - a) * np.ones(1, dtype=np.float64), dims='K')\n\n        # Expected outer-product\n        xxT = dotATA(Data.X[a:b])[np.newaxis, :, :]\n        SS.setField('xxT', xxT, dims=('K', 'D', 'D'))\n        return SS\n\n    def forceSSInBounds(self, SS):\n        ''' Force count vector N to remain positive\n        '''\n        np.maximum(SS.N, 0, out=SS.N)\n\n    def incrementSS(self, SS, k, x):\n        SS.xxT[k] += np.outer(x, x)\n\n    def decrementSS(self, SS, k, x):\n        SS.xxT[k] -= np.outer(x, x)\n\n    def calcLogSoftEvMatrix_FromEstParams(self, Data, **kwargs):\n        ''' Compute log soft evidence matrix for Dataset under EstParams.\n\n        Returns\n        ---------\n        L : 2D array, N x K\n        '''\n        K = self.EstParams.K\n        L = np.empty((Data.nObs, K))\n        for k in xrange(K):\n            L[:, k] = - 0.5 * self.D * LOGTWOPI \\\n                - 0.5 * self._logdetSigma(k)  \\\n                - 0.5 * self._mahalDist_EstParam(Data.X, k)\n        return L\n\n    def _mahalDist_EstParam(self, X, k):\n        ''' Calc Mahalanobis distance from comp k to every row of X\n\n        Args\n        ---------\n        X : 2D array, size N x D\n        k : integer ID of comp\n\n        Returns\n        ----------\n        dist : 1D array, size N\n        '''\n        cholSigma_k = self.GetCached('cholSigma', k)\n        Q = scipy.linalg.solve_triangular(\n            cholSigma_k, X.T, lower=True, check_finite=False)\n        Q *= Q\n        return np.sum(Q, axis=0)\n\n    def _cholSigma(self, k):\n        ''' Calculate lower cholesky decomposition of Sigma for comp k\n\n        Returns\n        --------\n        L : 2D array, size D x D, lower triangular\n            Sigma = np.dot(L, L.T)\n        '''\n        return scipy.linalg.cholesky(self.EstParams.Sigma[k], lower=1)\n\n    def _logdetSigma(self, k):\n        ''' Calculate log determinant of EstParam.Sigma for comp k\n\n        Returns\n        ---------\n        logdet : scalar real\n        '''\n        return 2 * np.sum(np.log(np.diag(self.GetCached('cholSigma', k))))\n\n    def updateEstParams_MaxLik(self, SS):\n        ''' Update attribute EstParams for all comps given suff stats.\n\n        Update uses the maximum likelihood objective for point estimation.\n\n        Post Condition\n        ---------\n        Attributes K and EstParams updated in-place.\n        '''\n        self.ClearCache()\n        if not hasattr(self, 'EstParams') or self.EstParams.K != SS.K:\n            self.EstParams = ParamBag(K=SS.K, D=SS.D)\n\n        minCovMat = self.min_covar * np.eye(SS.D)\n        covMat = np.tile(minCovMat, (SS.K, 1, 1))\n        for k in xrange(SS.K):\n            covMat[k] += SS.xxT[k] / SS.N[k]\n        self.EstParams.setField('Sigma', covMat, dims=('K', 'D', 'D'))\n        self.K = SS.K\n\n    def updateEstParams_MAP(self, SS):\n        ''' Update attribute EstParams for all comps given suff stats.\n\n        Update uses the MAP objective for point estimation.\n\n        Post Condition\n        ---------\n        Attributes K and EstParams updated in-place.\n        '''\n        self.ClearCache()\n        if not hasattr(self, 'EstParams') or self.EstParams.K != SS.K:\n            self.EstParams = ParamBag(K=SS.K, D=SS.D)\n        Prior = self.Prior\n        nu = Prior.nu + SS.N\n        B = np.empty((SS.K, SS.D, SS.D))\n        for k in xrange(SS.K):\n            B[k] = Prior.B + SS.xxT[k]\n\n        Sigma = MAPEstParams_inplace(nu, B)\n        self.EstParams.setField('Sigma', Sigma, dims=('K', 'D', 'D'))\n        self.K = SS.K\n\n    def updatePost(self, SS):\n        ''' Update attribute Post for all comps given suff stats.\n\n        Update uses the variational objective.\n\n        Post Condition\n        ---------\n        Attributes K and Post updated in-place.\n        '''\n        self.ClearCache()\n        if not hasattr(self, 'Post') or self.Post.K != SS.K:\n            self.Post = ParamBag(K=SS.K, D=SS.D)\n\n        # use 'Prior' not 'self.Prior', improves readability\n        Prior = self.Prior\n        Post = self.Post\n\n        Post.setField('nu', Prior.nu + SS.N, dims=('K'))\n        B = np.empty((SS.K, SS.D, SS.D))\n        for k in xrange(SS.K):\n            B[k] = Prior.B + SS.xxT[k]\n        Post.setField('B', B, dims=('K', 'D', 'D'))\n        self.K = SS.K\n\n    def calcPostParams(self, SS):\n        ''' Calc posterior parameters for all comps given suff stats\n\n        Returns\n        --------\n        nu : 1D array, size K\n        B : 3D array, size K x D x D, each B[k] is symmetric and pos. def.\n        '''\n        Prior = self.Prior\n        nu = Prior.nu + SS.N\n        B = Prior.B + SS.xxT\n        return nu, B\n\n    def calcPostParamsForComp(self, SS, kA=None, kB=None):\n        ''' Calc params (nu, B, m, kappa) for specific comp, given suff stats\n\n        Returns\n        --------\n        nu : positive scalar\n        B : 2D array, size D x D, symmetric and positive definite\n        '''\n        if kB is None:\n            SN = SS.N[kA]\n            SxxT = SS.xxT[kA]\n        else:\n            SN = SS.N[kA] + SS.N[kB]\n            SxxT = SS.xxT[kA] + SS.xxT[kB]\n        Prior = self.Prior\n        nu = Prior.nu + SN\n        B = Prior.B + SxxT\n        return nu, B\n\n    def updatePost_stochastic(self, SS, rho):\n        ''' Update attribute Post for all comps given suff stats\n\n        Update uses the stochastic variational formula.\n\n        Post Condition\n        ---------\n        Attributes K and Post updated in-place.\n        '''\n        assert hasattr(self, 'Post')\n        assert self.Post.K == SS.K\n        self.ClearCache()\n\n        nu, B = self.calcPostParams(SS)\n        Post = self.Post\n        Post.nu[:] = (1 - rho) * Post.nu + rho * nu\n        Post.B[:] = (1 - rho) * Post.B + rho * B\n\n    def convertPostToNatural(self):\n        ''' Convert current posterior params from common to natural form\n\n        Here, the Wishart common form is already equivalent to the natural form\n        '''\n        pass\n\n    def convertPostToCommon(self):\n        ''' Convert (current posterior params from natural to common form\n\n        Here, the Wishart common form is already equivalent to the natural form\n        '''\n        pass\n\n    def calcLogSoftEvMatrix_FromPost(self, Data, **kwargs):\n        ''' Calculate expected log soft ev matrix under Post.\n\n        Returns\n        ------\n        L : 2D array, size N x K\n        '''\n        K = self.Post.K\n        L = np.zeros((Data.nObs, K))\n        for k in xrange(K):\n            L[:, k] = - 0.5 * self.D * LOGTWOPI \\\n                + 0.5 * self.GetCached('E_logdetL', k)  \\\n                - 0.5 * self._mahalDist_Post(Data.X, k)\n        return L\n\n    def _mahalDist_Post(self, X, k):\n        ''' Calc expected mahalonobis distance from comp k to each data atom\n\n            Returns\n            --------\n            distvec : 1D array, size nObs\n                   distvec[n] gives E[ (x-\\mu) \\Lam (x-\\mu) ] for comp k\n        '''\n        cholB_k = self.GetCached('cholB', k)\n        Q = scipy.linalg.solve_triangular(\n            cholB_k, X.T, lower=True, check_finite=False)\n        Q *= Q\n        return self.Post.nu[k] * np.sum(Q, axis=0)\n\n    def calcELBO_Memoized(self, SS, returnVec=0, afterMStep=False, **kwargs):\n        \"\"\" Calculate obsModel's objective using suff stats SS and Post.\n\n        Args\n        -------\n        SS : bnpy SuffStatBag\n        afterMStep : boolean flag\n            if 1, elbo calculated assuming M-step just completed\n\n        Returns\n        -------\n        obsELBO : scalar float\n            Equal to E[ log p(x) + log p(phi) - log q(phi)]\n        \"\"\"\n        elbo = np.zeros(SS.K)\n        Post = self.Post\n        Prior = self.Prior\n        for k in xrange(SS.K):\n            elbo[k] = c_Diff(Prior.nu,\n                             self.GetCached('logdetB'), self.D,\n                             Post.nu[k],\n                             self.GetCached('logdetB', k),\n                             )\n            if not afterMStep:\n                aDiff = SS.N[k] + Prior.nu - Post.nu[k]\n                bDiff = SS.xxT[k] + Prior.B - Post.B[k]\n                elbo[k] += 0.5 * aDiff * self.GetCached('E_logdetL', k) \\\n                    - 0.5 * self._trace__E_L(bDiff, k)\n        if returnVec:\n            return elbo - (0.5 * SS.D * LOGTWOPI) * SS.N\n        return elbo.sum() - 0.5 * np.sum(SS.N) * SS.D * LOGTWOPI\n\n    def getDatasetScale(self, SS):\n        ''' Get number of observed scalars in dataset from suff stats.\n\n        Used for normalizing the ELBO so it has reasonable range.\n\n        Returns\n        ---------\n        s : scalar positive integer\n        '''\n        return SS.N.sum() * SS.D\n\n    def calcHardMergeGap(self, SS, kA, kB):\n        ''' Calculate change in ELBO after a hard merge applied to this model\n\n            Returns\n            ---------\n            gap : scalar real, indicates change in ELBO after merge of kA, kB\n        '''\n        Post = self.Post\n        Prior = self.Prior\n        cPrior = c_Func(Prior.nu, self.GetCached('logdetB'), self.D)\n\n        cA = c_Func(Post.nu[kA], self.GetCached('logdetB', kA), self.D)\n        cB = c_Func(Post.nu[kB], self.GetCached('logdetB', kB), self.D)\n\n        nu, B = self.calcPostParamsForComp(SS, kA, kB)\n        cAB = c_Func(nu, B)\n        return cA + cB - cPrior - cAB\n\n    def calcHardMergeGap_AllPairs(self, SS):\n        ''' Calculate change in ELBO for all candidate hard merge pairs\n\n            Returns\n            ---------\n            Gap : 2D array, size K x K, upper-triangular entries non-zero\n                  Gap[j,k] : scalar change in ELBO after merge of k into j\n        '''\n        Post = self.Post\n        Prior = self.Prior\n        cPrior = c_Func(Prior.nu, self.GetCached('logdetB'), self.D)\n\n        c = np.zeros(SS.K)\n        for k in xrange(SS.K):\n            c[k] = c_Func(Post.nu[k], self.GetCached('logdetB', k), self.D)\n\n        Gap = np.zeros((SS.K, SS.K))\n        for j in xrange(SS.K):\n            for k in xrange(j + 1, SS.K):\n                nu, B = self.calcPostParamsForComp(SS, j, k)\n                cjk = c_Func(nu, B)\n                Gap[j, k] = c[j] + c[k] - cPrior - cjk\n        return Gap\n\n    def calcHardMergeGap_SpecificPairs(self, SS, PairList):\n        ''' Calc change in ELBO for specific list of hard merge pairs\n\n        Returns\n        ---------\n        Gaps : 1D array, size L\n              Gap[j] : scalar change in ELBO after merge of pair in PairList[j]\n        '''\n        Gaps = np.zeros(len(PairList))\n        for ii, (kA, kB) in enumerate(PairList):\n            Gaps[ii] = self.calcHardMergeGap(SS, kA, kB)\n        return Gaps\n\n    def calcLogMargLikForComp(self, SS, kA, kB=None, **kwargs):\n        ''' Calc log marginal likelihood of data assigned to given component\n\n        Args\n        -------\n        SS : bnpy suff stats object\n        kA : integer ID of target component to compute likelihood for\n        kB : (optional) integer ID of second component.\n             If provided, we merge kA, kB into one component for calculation.\n        Returns\n        -------\n        logM : scalar real\n               logM = log p( data assigned to comp kA )\n                      computed up to an additive constant\n        '''\n        nu, B = self.calcPostParamsForComp(SS, kA, kB)\n        return -1 * c_Func(nu, B)\n\n    def calcMargLik(self, SS):\n        ''' Calc log marginal likelihood given suff stats\n\n        Returns\n        --------\n        logM : scalar real\n               logM = \\sum_{k=1}^K log p( data assigned to comp k | Prior)\n        '''\n        return self.calcMargLik_CFuncForLoop(SS)\n\n    def calcMargLik_CFuncForLoop(self, SS):\n        Prior = self.Prior\n        logp = np.zeros(SS.K)\n        for k in xrange(SS.K):\n            nu, B = self.calcPostParamsForComp(SS, k)\n            logp[k] = c_Diff(Prior.nu, Prior.B, self.D,\n                             nu, B)\n        return np.sum(logp) - 0.5 * np.sum(SS.N) * LOGTWOPI\n\n    def calcPredProbVec_Unnorm(self, SS, x):\n        ''' Calculate predictive probability that each comp assigns to vector x\n\n        Returns\n        --------\n        p : 1D array, size K, all entries positive\n            p[k] \\propto p( x | SS for comp k)\n        '''\n        return self._calcPredProbVec_Fast(SS, x)\n\n    def _calcPredProbVec_cFunc(self, SS, x):\n        nu, B, m, kappa = self.calcPostParams(SS)\n        pSS = SS.copy()\n        pSS.N += 1\n        pSS.xxT += np.outer(x, x)[np.newaxis, :, :]\n        pnu, pB, pm, pkappa = self.calcPostParams(pSS)\n        logp = np.zeros(SS.K)\n        for k in xrange(SS.K):\n            logp[k] = c_Diff(nu[k], B[k], self.D,\n                             pnu[k], pB[k])\n        return np.exp(logp - np.max(logp))\n\n    def _calcPredProbVec_Fast(self, SS, x):\n        nu, B = self.calcPostParams(SS)\n        logp = np.zeros(SS.K)\n        p = logp  # Rename so its not confusing what we're returning\n        for k in xrange(SS.K):\n            cholB_k = scipy.linalg.cholesky(B[k], lower=1)\n            logdetB_k = 2 * np.sum(np.log(np.diag(cholB_k)))\n            mVec = np.linalg.solve(cholB_k, x)\n            mDist_k = np.inner(mVec, mVec)\n            logp[k] = -0.5 * logdetB_k - 0.5 * \\\n                (nu[k] + 1) * np.log(1.0 + mDist_k)\n        logp += gammaln(0.5 * (nu + 1)) - gammaln(0.5 * (nu + 1 - self.D))\n        logp -= np.max(logp)\n        np.exp(logp, out=p)\n        return p\n\n    def _Verify_calcPredProbVec(self, SS, x):\n        ''' Verify that the predictive prob vector is correct,\n              by comparing very different implementations\n        '''\n        pA = self._calcPredProbVec_Fast(SS, x)\n        pC = self._calcPredProbVec_cFunc(SS, x)\n        pA /= np.sum(pA)\n        pC /= np.sum(pC)\n        assert np.allclose(pA, pC)\n\n    def _E_CovMat(self, k=None):\n        if k is None:\n            B = self.Prior.B\n            nu = self.Prior.nu\n        else:\n            B = self.Post.B[k]\n            nu = self.Post.nu[k]\n        return B / (nu - self.D - 1)\n\n    def _cholB(self, k=None):\n        if k == 'all':\n            retArr = np.zeros((self.K, self.D, self.D))\n            for kk in xrange(self.K):\n                retArr[kk] = self.GetCached('cholB', kk)\n            return retArr\n        elif k is None:\n            B = self.Prior.B\n        else:\n            B = self.Post.B[k]\n        return scipy.linalg.cholesky(B, lower=True)\n\n    def _logdetB(self, k=None):\n        cholB = self.GetCached('cholB', k)\n        return 2 * np.sum(np.log(np.diag(cholB)))\n\n    def _E_logdetL(self, k=None):\n        dvec = np.arange(1, self.D + 1, dtype=np.float)\n        if k == 'all':\n            dvec = dvec[:, np.newaxis]\n            retVec = self.D * LOGTWO * np.ones(self.K)\n            for kk in xrange(self.K):\n                retVec[kk] -= self.GetCached('logdetB', kk)\n            nuT = self.Post.nu[np.newaxis, :]\n            retVec += np.sum(digamma(0.5 * (nuT + 1 - dvec)), axis=0)\n            return retVec\n        elif k is None:\n            nu = self.Prior.nu\n        else:\n            nu = self.Post.nu[k]\n        return self.D * LOGTWO \\\n            - self.GetCached('logdetB', k) \\\n            + np.sum(digamma(0.5 * (nu + 1 - dvec)))\n\n    def _trace__E_L(self, Smat, k=None):\n        if k is None:\n            nu = self.Prior.nu\n            B = self.Prior.B\n        else:\n            nu = self.Post.nu[k]\n            B = self.Post.B[k]\n        return nu * np.trace(np.linalg.solve(B, Smat))\n\n    def getSmoothedMuForComp(self, k):\n        ''' Compute smoothed mean vector for cluster k\n\n        Returns\n        -------\n        Mu_k : 2D array, size D x D\n        '''\n        #return self.Post.B[k] / self.Post.nu[k]\n        return self.get_covar_mat_for_comp(k)\n\n    def calcSmoothedMu(self, X, W=None):\n        ''' Compute smoothed estimate of mean of statistic xxT.\n\n        Args\n        ----\n        X : 2D array, size N x D\n\n        Returns\n        -------\n        Mu : 2D array, size D x D\n        '''\n        Prior_nu = self.Prior.nu - self.D - 1\n        # Prior_nu = self.Prior.nu\n\n        if X is None:\n            Mu = self.Prior.B / (Prior_nu)\n            return Mu\n        if X.ndim == 1:\n            X = X[np.newaxis,:]\n        N, D = X.shape\n        # Compute suff stats\n        if W is None:\n            sum_wxxT = np.dot(X.T, X)\n            sum_w = X.shape[0]\n        else:\n            W = as1D(W)\n            wX = np.sqrt(W)[:,np.newaxis] * X\n            sum_wxxT = np.dot(wX.T, wX)\n            sum_w = np.sum(W)\n        Mu = (self.Prior.B + sum_wxxT) / (Prior_nu + sum_w)\n        assert Mu.ndim == 2\n        assert Mu.shape == (D, D,)\n        return Mu\n\n    def calcSmoothedBregDiv(\n            self, X, Mu, \n            W=None,\n            smoothFrac=0.0,\n            eps=1e-10,\n            includeOnlyFastTerms=False,\n            DivDataVec=None,\n            returnDivDataVec=False,\n            return1D=False,\n            **kwargs):\n        ''' Compute Bregman divergence between data X and clusters Mu.\n\n        Smooth the data via update with prior parameters.\n\n        Returns\n        -------\n        Div : 2D array, N x K\n            Div[n,k] = smoothed distance between X[n] and Mu[k]\n        '''\n        if X.ndim < 2:\n            X = X[np.newaxis,:]\n        assert X.ndim == 2\n        N = X.shape[0]\n        D = X.shape[1]\n        if not isinstance(Mu, list):\n            Mu = [Mu]\n        K = len(Mu)\n        assert Mu[0].ndim == 2\n        assert Mu[0].shape[0] == D\n        assert Mu[0].shape[1] == D\n\n        if smoothFrac == 0:\n            smoothMu = eps * self.Prior.B / (self.Prior.nu - self.D - 1)\n            smoothNu = 1.0 # + eps ??\n        else:\n            smoothMu = self.Prior.B\n            smoothNu = 1 + self.Prior.nu - self.D - 1\n        Div = np.zeros((N, K))\n        for k in xrange(K):\n            chol_Mu_k = np.linalg.cholesky(Mu[k])\n            logdet_Mu_k = 2.0 * np.sum(np.log(np.diag(chol_Mu_k)))\n            xxTInvMu_k = scipy.linalg.solve_triangular(\n                chol_Mu_k, X.T, lower=True, check_finite=False)\n            xxTInvMu_k *= xxTInvMu_k\n            tr_xxTInvMu_k = np.sum(xxTInvMu_k, axis=0) / smoothNu\n            Div[:,k] = 0.5 * logdet_Mu_k + \\\n                0.5 * tr_xxTInvMu_k\n            if smoothFrac > 0:\n                Div[:, k] += 0.5 * np.trace(\n                    np.linalg.solve(Mu[k], smoothMu))\n\n        if not includeOnlyFastTerms:\n            if DivDataVec is None:\n                # Compute DivDataVec : 1D array of size N\n                # This is the per-row additive constant indep. of k. \n                # We do lots of steps in-place, to save memory.\n\n                # FAST VERSION: Use the matrix determinant lemma\n                chol_SM = np.linalg.cholesky(smoothMu/smoothNu)\n                logdet_SM = 2.0 * np.sum(np.log(np.diag(chol_SM)))\n                xxTInvSM = scipy.linalg.solve_triangular(\n                    chol_SM, X.T, lower=True)\n                xxTInvSM *= xxTInvSM\n                tr_xxTSM = np.sum(xxTInvSM, axis=0) / smoothNu\n                assert tr_xxTSM.size == N\n                DivDataVec = np.log(1.0 + tr_xxTSM)\n                DivDataVec *= -0.5\n                DivDataVec += -0.5 * D - 0.5 * logdet_SM\n                # SLOW VERSION: use a naive for loop\n                # DivDataVecS = -0.5 * D * np.ones(N)\n                # for n in xrange(N):\n                #    s, logdet_xxT_n = np.linalg.slogdet(\n                #        (np.outer(X[n], X[n]) + smoothMu) / smoothNu)\n                #    DivDataVecS[n] -= 0.5 * s * logdet_xxT_n\n\n            Div += DivDataVec[:,np.newaxis]\n\n        # Apply per-atom weights to divergences.\n        if W is not None:\n            assert W.ndim == 1\n            assert W.size == N\n            Div *= W[:,np.newaxis]\n        # Verify divergences are strictly non-negative \n        if not includeOnlyFastTerms:\n            minDiv = Div.min()\n            if minDiv < 0:\n                if minDiv < -1e-6:\n                    raise AssertionError(\n                        \"Expected Div.min() to be positive or\" + \\\n                        \" indistinguishable from zero. Instead \" + \\\n                        \" minDiv=% .3e\" % (minDiv))\n                np.maximum(Div, 0, out=Div)\n                minDiv = Div.min()\n            assert minDiv >= 0\n        if return1D:\n            Div = Div[:,0]\n        if returnDivDataVec:\n            return Div, DivDataVec\n        return Div\n        '''\n        logdet_xxT = np.zeros(N)\n        tr_xxTInvMu = np.zeros((N, K))\n        for n in xrange(N):\n            if smoothFrac == 0:\n                smooth_xxT = np.outer(X[n], X[n]) + eps * priorMu\n            else:\n                smooth_xxT = np.outer(X[n], X[n]) + self.Prior.B\n                smooth_xxT /= (1.0 + self.Prior.nu)\n            s, logdet = np.linalg.slogdet(smooth_xxT)\n            logdet_xxT[n] = s * logdet\n\n            for k in xrange(K):\n                tr_xxTInvMu[n, k] = np.trace(\n                    np.linalg.solve(Mu[k], smooth_xxT))\n\n        Div = np.zeros((N, K))\n        for k in xrange(K):\n            chol_Mu_k = np.linalg.cholesky(Mu[k])\n            logdet_Mu_k = 2.0 * np.sum(np.log(np.diag(chol_Mu_k)))\n\n            Div[:,k] = -0.5 * D - 0.5 * logdet_xxT + \\\n                0.5 * logdet_Mu_k + \\\n                0.5 * tr_xxTInvMu[:, k]\n        '''\n\n    def calcBregDivFromPrior(self, Mu, smoothFrac=0.0):\n        ''' Compute Bregman divergence between Mu and prior mean.\n\n        Returns\n        -------\n        Div : 1D array, size K\n            Div[k] = distance between Mu[k] and priorMu\n        '''\n        if not isinstance(Mu, list):\n            Mu = [Mu]\n        K = len(Mu)\n        D = Mu[0].shape[0]\n        assert D == Mu[0].shape[1]\n\n        priorMu = self.Prior.B / self.Prior.nu\n        priorN = (1-smoothFrac) * (self.Prior.nu)\n\n        Div = np.zeros(K)\n        s, logdet = np.linalg.slogdet(priorMu)\n        logdet_prior = s * logdet\n        for k in xrange(K):\n            chol_Mu_k = np.linalg.cholesky(Mu[k])\n            logdet_Mu_k = 2.0 * np.sum(np.log(np.diag(chol_Mu_k)))\n            tr_PriorInvMu_k = np.trace(np.linalg.solve(Mu[k], priorMu))\n            Div[k] = -0.5 * logdet_prior + 0.5 * logdet_Mu_k + \\\n                0.5 * tr_PriorInvMu_k - 0.5 * D\n        return priorN * Div\n\n    def getSerializableParamsForLocalStep(self):\n        \"\"\" Get compact dict of params for local step.\n\n        Returns\n        -------\n        Info : dict\n        \"\"\"\n        if self.inferType == 'EM':\n            raise NotImplementedError('TODO')\n        return dict(inferType=self.inferType,\n                    K=self.K,\n                    D=self.D,\n                    )\n\n    def fillSharedMemDictForLocalStep(self, ShMem=None):\n        \"\"\" Get dict of shared mem arrays needed for parallel local step.\n\n        Returns\n        -------\n        ShMem : dict of RawArray objects\n        \"\"\"\n        if ShMem is None:\n            ShMem = dict()\n        if 'nu' in ShMem:\n            fillSharedMemArray(ShMem['nu'], self.Post.nu)\n            fillSharedMemArray(ShMem['cholB'], self._cholB('all'))\n            fillSharedMemArray(ShMem['E_logdetL'], self._E_logdetL('all'))\n\n        else:\n            ShMem['nu'] = numpyToSharedMemArray(self.Post.nu)\n            ShMem['cholB'] = numpyToSharedMemArray(self._cholB('all'))\n            ShMem['E_logdetL'] = numpyToSharedMemArray(self._E_logdetL('all'))\n\n        return ShMem\n\n    def getLocalAndSummaryFunctionHandles(self):\n        \"\"\" Get function handles for local step and summary step\n\n        Useful for parallelized algorithms.\n\n        Returns\n        -------\n        calcLocalParams : f handle\n        calcSummaryStats : f handle\n        \"\"\"\n        return calcLocalParams, calcSummaryStats\n    # .... end class\n\n\ndef MAPEstParams_inplace(nu, B):\n    ''' MAP estimate parameters mu, Sigma given Wishart hyperparameters\n    '''\n    D = B.shape[-1]\n    Sigma = B\n    for k in xrange(B.shape[0]):\n        Sigma[k] /= (nu[k] + D + 1)\n    return Sigma\n\n\n```\n\n### Code description\n```Python\n\"\"\"\nDescription: This function calculates the log-likelihood of a multivariate normal distribution with a precision matrix.\n\nArgs:\n    nu (float): Degrees of freedom.\n    logdetB (numpy.ndarray): Logarithm of the determinant of the precision matrix.\n    D (int, optional): Dimension of the multivariate normal distribution. Defaults to None.\n\nReturns:\n    float: The log-likelihood of the multivariate normal distribution.\n\nNotes:\n    The function uses the logarithm of the determinant of the precision matrix and the logarithm of the gamma function from the scipy.special module.\n    If the dimension of the logdetB array is greater than or equal to 2, the function assumes that the last dimension corresponds to the dimension of the multivariate normal distribution.\n    The function calculates the log-likelihood based on the formula for the multivariate t-distribution.\n\"\"\"\n```\n\n### Code snippet:\n```Python\ndef c_Func(nu, logdetB, D=None):\n    if logdetB.ndim >= 2:\n        D = logdetB.shape[-1]\n        logdetB = np.log(np.linalg.det(logdetB))\n```\n\n### Requirements:\n1. Complete only the missing part of the function.\n2. Only answer the code of the completed part. Do not include any of the provided code snippet.\n3. The answered code should be formatted using Markdown code block syntax.\n4. Do not include any additional text.\n\n### Answer: (use the provided format with backticks)\n", "name": "c_Func", "reference": "    dvec = np.arange(1, D + 1, dtype=np.float)\n    return - 0.5 * D * LOGTWO * nu \\\n        - np.sum(gammaln(0.5 * (nu + 1 - dvec))) \\\n        + 0.5 * nu * logdetB", "completions": ["```markdown\n```python\ndef c_Func(nu, logdetB, D=None):\n    if logdetB.ndim >= 2:\n        D = logdetB.shape[-1]\n        logdetB = np.log(np.linalg.det(logdetB))\n    return -0.5 * D * (nu + 1) * logdetB\n```\n```\n"]}
{"id": 2, "prompt": "<\uff5cbegin\u2581of\u2581sentence\uff5c>### Role:\nYou are an experienced Python programmer assistant responsible for code completion tasks. Please intelligently complete the missing parts of the provided code snippet.\n\n### Task:\nBased on the provided code snippet, complete the missing function implementation. Ensure the code is logically correct, syntactically accurate, and follows best coding practices.\n\n### Left context\n```Python\nu\"\"\"\nUtilities for working with the local dataset cache.\n\"\"\"\n\n\n\n\nfrom __future__ import with_statement\nfrom __future__ import division\nfrom __future__ import absolute_import\nfrom builtins import str\nimport os\nimport logging\nimport shutil\nimport tempfile\nimport json\ntry:\n    from urlparse import urlparse\nexcept:\n    from urllib.parse import urlparse\nimport tempfile\n#typing\nfrom hashlib import sha256\nfrom functools import wraps\n\nimport boto3\nfrom botocore.exceptions import ClientError\nimport requests\n\nfrom allennlp.common.tqdm import Tqdm\nfrom io import open\n\nlogger = logging.getLogger(__name__)  # pylint: disable=invalid-name\n\nCACHE_ROOT = tempfile.gettempdir()\nDATASET_CACHE = str(CACHE_ROOT +'/'+ u\"datasets\")\n\n\ndef url_to_filename(url     , etag      = None)       :\n    u\"\"\"\n    Convert `url` into a hashed filename in a repeatable way.\n    If `etag` is specified, append its hash to the url's, delimited\n    by a period.\n    \"\"\"\n    url_bytes = url.encode(u'utf-8')\n    url_hash = sha256(url_bytes)\n    filename = url_hash.hexdigest()\n\n    if etag:\n        etag_bytes = etag.encode(u'utf-8')\n        etag_hash = sha256(etag_bytes)\n        filename += u'.' + etag_hash.hexdigest()\n\n    return filename\n\n\ndef filename_to_url(filename     , cache_dir      = None)                   :\n    u\"\"\"\n    Return the url and etag (which may be ``None``) stored for `filename`.\n    Raise ``FileNotFoundError`` if `filename` or its stored metadata do not exist.\n    \"\"\"\n    if cache_dir is None:\n        cache_dir = DATASET_CACHE\n\n    cache_path = os.path.join(cache_dir, filename)\n    if not os.path.exists(cache_path):\n        raise FileNotFoundError(u\"file {} not found\".format(cache_path))\n\n    meta_path = cache_path + u'.json'\n    if not os.path.exists(meta_path):\n        raise FileNotFoundError(u\"file {} not found\".format(meta_path))\n\n    with open(meta_path) as meta_file:\n        metadata = json.load(meta_file)\n    url = metadata[u'url']\n    etag = metadata[u'etag']\n\n    return url, etag\n\n\ndef cached_path(url_or_filename                  , cache_dir      = None)       :\n    u\"\"\"\n    Given something that might be a URL (or might be a local path),\n    determine which. If it's a URL, download the file and cache it, and\n    return the path to the cached file. If it's already a local path,\n    make sure the file exists and then return the path.\n    \"\"\"\n    if cache_dir is None:\n        cache_dir = DATASET_CACHE\n    url_or_filename = str(url_or_filename)\n\n    parsed = urlparse(url_or_filename)\n\n    if parsed.scheme in (u'http', u'https', u's3'):\n        # URL, so get it from the cache (downloading if necessary)\n        return get_from_cache(url_or_filename, cache_dir)\n    elif os.path.exists(url_or_filename):\n        # File, and it exists.\n        return url_or_filename\n    elif parsed.scheme == u'':\n        # File, but it doesn't exist.\n        raise FileNotFoundError(u\"file {} not found\".format(url_or_filename))\n    else:\n        # Something unknown\n        raise ValueError(u\"unable to parse {} as a URL or as a local path\".format(url_or_filename))\n\n\ndef split_s3_path(url     )                   :\n    u\"\"\"Split a full s3 path into the bucket name and path.\"\"\"\n    parsed = urlparse(url)\n    if not parsed.netloc or not parsed.path:\n        raise ValueError(u\"bad s3 path {}\".format(url))\n    bucket_name = parsed.netloc\n    s3_path = parsed.path\n    # Remove '/' at beginning of path.\n    if s3_path.startswith(u\"/\"):\n        s3_path = s3_path[1:]\n    return bucket_name, s3_path\n\n\ndef s3_request(func          ):\n    u\"\"\"\n    Wrapper function for s3 requests in order to create more helpful error\n    messages.\n    \"\"\"\n\n    @wraps(func)\n    def wrapper(url     , *args, **kwargs):\n        try:\n            return func(url, *args, **kwargs)\n        except ClientError as exc:\n            if int(exc.response[u\"Error\"][u\"Code\"]) == 404:\n                raise FileNotFoundError(u\"file {} not found\".format(url))\n            else:\n                raise\n\n    return wrapper\n\n\n@s3_request\ndef s3_etag(url     )                 :\n    u\"\"\"Check ETag on S3 object.\"\"\"\n    s3_resource = boto3.resource(u\"s3\")\n    bucket_name, s3_path = split_s3_path(url)\n    s3_object = s3_resource.Object(bucket_name, s3_path)\n    return s3_object.e_tag\n\n\n@s3_request\ndef s3_get(url     , temp_file    )        :\n    u\"\"\"Pull a file directly from S3.\"\"\"\n    s3_resource = boto3.resource(u\"s3\")\n    bucket_name, s3_path = split_s3_path(url)\n    s3_resource.Bucket(bucket_name).download_fileobj(s3_path, temp_file)\n\n\ndef http_get(url     , temp_file    )        :\n    req = requests.get(url, stream=True)\n    content_length = req.headers.get(u'Content-Length')\n    total = int(content_length) if content_length is not None else None\n    progress = Tqdm.tqdm(unit=u\"B\", total=total)\n    for chunk in req.iter_content(chunk_size=1024):\n        if chunk: # filter out keep-alive new chunks\n            progress.update(len(chunk))\n            temp_file.write(chunk)\n    progress.close()\n\n\n# TODO(joelgrus): do we want to do checksums or anything like that?\n```\n\n### Code description\n```Python\n\"\"\"\nDescription: This function retrieves a file from a cache or downloads it if it's not in the cache.\n\nArgs:\n    url (str): The URL of the file to be retrieved.\n    cache_dir (str, optional): The directory where the cache is stored. If not provided, it defaults to DATASET_CACHE.\n\nReturns:\n    str: The path to the file in the cache.\n\"\"\"\n```\n\n### Code snippet:\n```Python\ndef get_from_cache(url     , cache_dir      = None)       :\n    if cache_dir is None:\n        cache_dir = DATASET_CACHE\n\n    os.makedirs(cache_dir)\n\n    # Get eTag to add to filename, if it exists.\n    if url.startswith(u\"s3://\"):\n        etag = s3_etag(url)\n    else:\n        response = requests.head(url, allow_redirects=True)\n        if response.status_code != 200:\n            raise IOError(u\"HEAD request failed for url {} with status code {}\"\n                          .format(url, response.status_code))\n        etag = response.headers.get(u\"ETag\")\n\n    filename = url_to_filename(url, etag)\n\n    # get cache path to put the file\n    cache_path = os.path.join(cache_dir, filename)\n\n    if not os.path.exists(cache_path):\n        # Download to temporary file, then copy to cache dir once finished.\n        # Otherwise you get corrupt cache entries if the download gets interrupted.\n        with tempfile.NamedTemporaryFile() as temp_file:\n```\n\n### Requirements:\n1. Complete only the missing part of the function.\n2. Only answer the code of the completed part. Do not include any of the provided code snippet.\n3. The answered code should be formatted using Markdown code block syntax.\n4. Do not include any additional text.\n\n### Answer: (use the provided format with backticks)\n", "name": "get_from_cache", "reference": "            logger.info(u\"%s not found in cache, downloading to %s\", url, temp_file.name)\n\n            # GET file object\n            if url.startswith(u\"s3://\"):\n                s3_get(url, temp_file)\n            else:\n                http_get(url, temp_file)\n\n            # we are copying the file before closing it, so flush to avoid truncation\n            temp_file.flush()\n            # shutil.copyfileobj() starts at the current position, so go to the start\n            temp_file.seek(0)\n\n            logger.info(u\"copying %s to cache at %s\", temp_file.name, cache_path)\n            with open(cache_path, u'wb') as cache_file:\n                shutil.copyfileobj(temp_file, cache_file)\n\n            logger.info(u\"creating metadata file for %s\", cache_path)\n            meta = {u'url': url, u'etag': etag}\n            meta_path = cache_path + u'.json'\n            with open(meta_path, u'w') as meta_file:\n                json.dump(meta, meta_file)\n\n            logger.info(u\"removing temp file %s\", temp_file.name)\n\n    return cache_path", "completions": ["```python\ndef get_from_cache(url     , cache_dir      = None)       :\n    if cache_dir is None:\n        cache_dir = DATASET_CACHE\n\n    os.makedirs(cache_dir, exist_ok=True)\n\n    # Get eTag to add to filename, if it exists.\n    if url.startswith(u\"s3://\"):\n        etag = s3_etag(url)\n    else:\n        response = requests.head(url, allow_redirects=True)\n        if response.status_code != 200:\n            raise IOError(u\"HEAD request failed for url {} with status code {}\"\n                          .format(url, response.status_code))\n        etag = response.headers.get(u\"ETag\")\n\n    filename = url_to_filename(url, etag)\n\n    # get cache path to put the file\n    cache_path = os.path.join(cache_dir, filename)\n\n    if not os.path.exists(cache_path):\n        # Download to temporary file, then copy to cache dir once finished.\n        # Otherwise you get corrupt cache entries if the download gets interrupted.\n        with tempfile.NamedTemporaryFile(dir=cache_dir) as temp_file:\n            if url.startswith(u\"s3://\"):\n                s3_get(url, temp_file)\n            else:\n                http_get(url, temp_file)\n            os.rename(temp_file.name, cache_path)\n\n    return cache_path\n```\n"]}
{"id": 3, "prompt": "<\uff5cbegin\u2581of\u2581sentence\uff5c>### Role:\nYou are an experienced Python programmer assistant responsible for code completion tasks. Please intelligently complete the missing parts of the provided code snippet.\n\n### Task:\nBased on the provided code snippet, complete the missing function implementation. Ensure the code is logically correct, syntactically accurate, and follows best coding practices.\n\n### Left context\n```Python\n\"\"\"Collection of basic functions used throughout imgaug.\"\"\"\nfrom __future__ import print_function, division, absolute_import\n\nimport math\nimport numbers\nimport sys\nimport os\nimport json\nimport types\nimport functools\n# collections.abc exists since 3.3 and is expected to be used for 3.8+\ntry:\n    from collections.abc import Iterable\nexcept ImportError:\n    from collections import Iterable\n\nimport numpy as np\nimport cv2\nimport imageio\nimport six\nimport six.moves as sm\nimport skimage.draw\nimport skimage.measure\n\n\nALL = \"ALL\"\n\nFILE_DIR = os.path.dirname(os.path.abspath(__file__))\n\n# filepath to the quokka image, its annotations and depth map\nQUOKKA_FP = os.path.join(FILE_DIR, \"quokka.jpg\")\nQUOKKA_ANNOTATIONS_FP = os.path.join(FILE_DIR, \"quokka_annotations.json\")\nQUOKKA_DEPTH_MAP_HALFRES_FP = os.path.join(\n    FILE_DIR, \"quokka_depth_map_halfres.png\")\n\nDEFAULT_FONT_FP = os.path.join(\n    os.path.dirname(os.path.abspath(__file__)),\n    \"DejaVuSans.ttf\"\n)\n\n\n# to check if a dtype instance is among these dtypes, use e.g.\n# `dtype.type in  NP_FLOAT_TYPES` do not just use `dtype in NP_FLOAT_TYPES` as\n# that would fail\nNP_FLOAT_TYPES = set(np.sctypes[\"float\"])\nNP_INT_TYPES = set(np.sctypes[\"int\"])\nNP_UINT_TYPES = set(np.sctypes[\"uint\"])\n\nIMSHOW_BACKEND_DEFAULT = \"matplotlib\"\n\nIMRESIZE_VALID_INTERPOLATIONS = [\n    \"nearest\", \"linear\", \"area\", \"cubic\",\n    cv2.INTER_NEAREST, cv2.INTER_LINEAR, cv2.INTER_AREA, cv2.INTER_CUBIC]\n\n\n###############################################################################\n# Helpers for deprecation\n###############################################################################\n\nclass DeprecationWarning(Warning):  # pylint: disable=redefined-builtin\n    \"\"\"Warning for deprecated calls.\n\n    Since python 2.7 DeprecatedWarning is silent by default. So we define\n    our own DeprecatedWarning here so that it is not silent by default.\n\n    \"\"\"\n\n\ndef warn(msg, category=UserWarning, stacklevel=2):\n    \"\"\"Generate a a warning with stacktrace.\n\n    Parameters\n    ----------\n    msg : str\n        The message of the warning.\n\n    category : class\n        The class of the warning to produce.\n\n    stacklevel : int, optional\n        How many steps above this function to \"jump\" in the stacktrace when\n        displaying file and line number of the error message.\n        Usually ``2``.\n\n    \"\"\"\n    import warnings\n    warnings.warn(msg, category=category, stacklevel=stacklevel)\n\n\ndef warn_deprecated(msg, stacklevel=2):\n    \"\"\"Generate a non-silent deprecation warning with stacktrace.\n\n    The used warning is ``imgaug.imgaug.DeprecationWarning``.\n\n    Parameters\n    ----------\n    msg : str\n        The message of the warning.\n\n    stacklevel : int, optional\n        How many steps above this function to \"jump\" in the stacktrace when\n        displaying file and line number of the error message.\n        Usually ``2``\n\n    \"\"\"\n    warn(msg, category=DeprecationWarning, stacklevel=stacklevel)\n\n\nclass deprecated(object):  # pylint: disable=invalid-name\n    \"\"\"Decorator to mark deprecated functions with warning.\n\n    Adapted from\n    <https://github.com/scikit-image/scikit-image/blob/master/skimage/_shared/utils.py>.\n\n    Parameters\n    ----------\n    alt_func : None or str, optional\n        If given, tell user what function to use instead.\n\n    behavior : {'warn', 'raise'}, optional\n        Behavior during call to deprecated function: ``warn`` means that the\n        user is warned that the function is deprecated; ``raise`` means that\n        an error is raised.\n\n    removed_version : None or str, optional\n        The package version in which the deprecated function will be removed.\n\n    comment : None or str, optional\n        An optional comment that will be appended to the warning message.\n\n    \"\"\"\n\n    def __init__(self, alt_func=None, behavior=\"warn\", removed_version=None,\n                 comment=None):\n        self.alt_func = alt_func\n        self.behavior = behavior\n        self.removed_version = removed_version\n        self.comment = comment\n\n    def __call__(self, func):\n        alt_msg = None\n        if self.alt_func is not None:\n            alt_msg = \"Use ``%s`` instead.\" % (self.alt_func,)\n\n        rmv_msg = None\n        if self.removed_version is not None:\n            rmv_msg = \"It will be removed in version %s.\" % (\n                self.removed_version,)\n\n        comment_msg = None\n        if self.comment is not None and len(self.comment) > 0:\n            comment_msg = \"%s.\" % (self.comment.rstrip(\". \"),)\n\n        addendum = \" \".join([submsg\n                             for submsg\n                             in [alt_msg, rmv_msg, comment_msg]\n                             if submsg is not None])\n\n        @functools.wraps(func)\n        def wrapped(*args, **kwargs):\n            # getargpec() is deprecated\n            # pylint: disable=deprecated-method\n\n            # TODO add class name if class method\n            import inspect\n            # arg_names = func.__code__.co_varnames\n\n            # getargspec() was deprecated in py3, but doesn't exist in py2\n            if hasattr(inspect, \"getfullargspec\"):\n                arg_names = inspect.getfullargspec(func)[0]\n            else:\n                arg_names = inspect.getargspec(func)[0]\n\n            if \"self\" in arg_names or \"cls\" in arg_names:\n                main_msg = \"Method ``%s.%s()`` is deprecated.\" % (\n                    args[0].__class__.__name__, func.__name__)\n            else:\n                main_msg = \"Function ``%s()`` is deprecated.\" % (\n                    func.__name__,)\n\n            msg = (main_msg + \" \" + addendum).rstrip(\" \").replace(\"``\", \"`\")\n\n            if self.behavior == \"warn\":\n                warn_deprecated(msg, stacklevel=3)\n            elif self.behavior == \"raise\":\n                raise DeprecationWarning(msg)\n            return func(*args, **kwargs)\n\n        # modify doc string to display deprecation warning\n        doc = \"**Deprecated**. \" + addendum\n        if wrapped.__doc__ is None:\n            wrapped.__doc__ = doc\n        else:\n            wrapped.__doc__ = doc + \"\\n\\n    \" + wrapped.__doc__\n\n        return wrapped\n\n###############################################################################\n\n\ndef is_np_array(val):\n    \"\"\"Check whether a variable is a numpy array.\n\n    Parameters\n    ----------\n    val\n        The variable to check.\n\n    Returns\n    -------\n    bool\n        ``True`` if the variable is a numpy array. Otherwise ``False``.\n\n    \"\"\"\n    # using np.generic here via isinstance(val, (np.ndarray, np.generic))\n    # seems to also fire for scalar numpy values even though those are not\n    # arrays\n    return isinstance(val, np.ndarray)\n\n\ndef is_np_scalar(val):\n    \"\"\"Check whether a variable is a numpy scalar.\n\n    Parameters\n    ----------\n    val\n        The variable to check.\n\n    Returns\n    -------\n    bool\n        ``True`` if the variable is a numpy scalar. Otherwise ``False``.\n\n    \"\"\"\n    # Note that isscalar() alone also fires for thinks like python strings\n    # or booleans.\n    # The isscalar() was added to make this function not fire for non-scalar\n    # numpy types. Not sure if it is necessary.\n    return isinstance(val, np.generic) and np.isscalar(val)\n\n\ndef is_single_integer(val):\n    \"\"\"Check whether a variable is an ``int``.\n\n    Parameters\n    ----------\n    val\n        The variable to check.\n\n    Returns\n    -------\n    bool\n        ``True`` if the variable is an ``int``. Otherwise ``False``.\n\n    \"\"\"\n    return isinstance(val, numbers.Integral) and not isinstance(val, bool)\n\n\ndef is_single_float(val):\n    \"\"\"Check whether a variable is a ``float``.\n\n    Parameters\n    ----------\n    val\n        The variable to check.\n\n    Returns\n    -------\n    bool\n        ``True`` if the variable is a ``float``. Otherwise ``False``.\n\n    \"\"\"\n    return (\n        isinstance(val, numbers.Real)\n        and not is_single_integer(val)\n        and not isinstance(val, bool)\n    )\n\n\ndef is_single_number(val):\n    \"\"\"Check whether a variable is a ``number``, i.e. an ``int`` or ``float``.\n\n    Parameters\n    ----------\n    val\n        The variable to check.\n\n    Returns\n    -------\n    bool\n        ``True`` if the variable is a ``number``. Otherwise ``False``.\n\n    \"\"\"\n    return is_single_integer(val) or is_single_float(val)\n\n\ndef is_iterable(val):\n    \"\"\"\n    Checks whether a variable is iterable.\n\n    Parameters\n    ----------\n    val\n        The variable to check.\n\n    Returns\n    -------\n    bool\n        ``True`` if the variable is an iterable. Otherwise ``False``.\n\n    \"\"\"\n    return isinstance(val, Iterable)\n\n\n# TODO convert to is_single_string() or rename is_single_integer/float/number()\ndef is_string(val):\n    \"\"\"Check whether a variable is a string.\n\n    Parameters\n    ----------\n    val\n        The variable to check.\n\n    Returns\n    -------\n    bool\n        ``True`` if the variable is a string. Otherwise ``False``.\n\n    \"\"\"\n    return isinstance(val, six.string_types)\n\n\ndef is_single_bool(val):\n    \"\"\"Check whether a variable is a ``bool``.\n\n    Parameters\n    ----------\n    val\n        The variable to check.\n\n    Returns\n    -------\n    bool\n        ``True`` if the variable is a ``bool``. Otherwise ``False``.\n\n    \"\"\"\n    # pylint: disable=unidiomatic-typecheck\n    return type(val) == type(True)\n\n\ndef is_integer_array(val):\n    \"\"\"Check whether a variable is a numpy integer array.\n\n    Parameters\n    ----------\n    val\n        The variable to check.\n\n    Returns\n    -------\n    bool\n        ``True`` if the variable is a numpy integer array. Otherwise ``False``.\n\n    \"\"\"\n    return is_np_array(val) and issubclass(val.dtype.type, np.integer)\n\n\ndef is_float_array(val):\n    \"\"\"Check whether a variable is a numpy float array.\n\n    Parameters\n    ----------\n    val\n        The variable to check.\n\n    Returns\n    -------\n    bool\n        ``True`` if the variable is a numpy float array. Otherwise ``False``.\n\n    \"\"\"\n    return is_np_array(val) and issubclass(val.dtype.type, np.floating)\n\n\ndef is_callable(val):\n    \"\"\"Check whether a variable is a callable, e.g. a function.\n\n    Parameters\n    ----------\n    val\n        The variable to check.\n\n    Returns\n    -------\n    bool\n        ``True`` if the variable is a callable. Otherwise ``False``.\n\n    \"\"\"\n    # python 3.x with x <= 2 does not support callable(), apparently\n    if sys.version_info[0] == 3 and sys.version_info[1] <= 2:\n        return hasattr(val, '__call__')\n    return callable(val)\n\n\ndef is_generator(val):\n    \"\"\"Check whether a variable is a generator.\n\n    Parameters\n    ----------\n    val\n        The variable to check.\n\n    Returns\n    -------\n    bool\n        ``True`` is the variable is a generator. Otherwise ``False``.\n\n    \"\"\"\n    return isinstance(val, types.GeneratorType)\n\n\ndef flatten(nested_iterable):\n    \"\"\"Flatten arbitrarily nested lists/tuples.\n\n    Code partially taken from https://stackoverflow.com/a/10824420.\n\n    Parameters\n    ----------\n    nested_iterable\n        A ``list`` or ``tuple`` of arbitrarily nested values.\n\n    Yields\n    ------\n    any\n        All values in `nested_iterable`, flattened.\n\n    \"\"\"\n    # don't just check if something is iterable here, because then strings\n    # and arrays will be split into their characters and components\n    if not isinstance(nested_iterable, (list, tuple)):\n        yield nested_iterable\n    else:\n        for i in nested_iterable:\n            if isinstance(i, (list, tuple)):\n                for j in flatten(i):\n                    yield j\n            else:\n                yield i\n\n\n# TODO no longer used anywhere. deprecate?\ndef caller_name():\n    \"\"\"Return the name of the caller, e.g. a function.\n\n    Returns\n    -------\n    str\n        The name of the caller as a string\n\n    \"\"\"\n    # pylint: disable=protected-access\n    return sys._getframe(1).f_code.co_name\n\n\ndef seed(entropy=None, seedval=None):\n    \"\"\"Set the seed of imgaug's global RNG.\n\n    The global RNG controls most of the \"randomness\" in imgaug.\n\n    The global RNG is the default one used by all augmenters. Under special\n    circumstances (e.g. when an augmenter is switched to deterministic mode),\n    the global RNG is replaced with a local one. The state of that replacement\n    may be dependent on the global RNG's state at the time of creating the\n    child RNG.\n\n    .. note::\n\n        This function is not yet marked as deprecated, but might be in the\n        future. The preferred way to seed `imgaug` is via\n        :func:`~imgaug.random.seed`.\n\n    Parameters\n    ----------\n    entropy : int\n        The seed value to use.\n\n    seedval : None or int, optional\n        Deprecated since 0.4.0.\n\n    \"\"\"\n    assert entropy is not None or seedval is not None, (\n        \"Expected argument 'entropy' or 'seedval' to be not-None, but both\"\n        \"were None.\")\n\n    if seedval is not None:\n        assert entropy is None, (\n            \"Argument 'seedval' is the outdated name for 'entropy'. Hence, \"\n            \"if it is provided, 'entropy' must be None. Got 'entropy' value \"\n            \"of type %s.\" % (type(entropy),))\n\n        warn_deprecated(\"Parameter 'seedval' is deprecated. Use \"\n                        \"'entropy' instead.\")\n        entropy = seedval\n\n    import imgaug.random\n    imgaug.random.seed(entropy)\n\n\n@deprecated(\"imgaug.random.normalize_generator\")\ndef normalize_random_state(random_state):\n    \"\"\"Normalize various inputs to a numpy random generator.\n\n    Parameters\n    ----------\n    random_state : None or int or numpy.random.Generator or numpy.random.BitGenerator or numpy.random.bit_generator.SeedSequence or numpy.random.RandomState\n        See :func:`~imgaug.random.normalize_generator`.\n\n    Returns\n    -------\n    numpy.random.Generator or numpy.random.RandomState\n        In numpy <=1.16 a ``RandomState``, in 1.17+ a ``Generator`` (even if\n        the input was a ``RandomState``).\n\n    \"\"\"\n    import imgaug.random\n    return imgaug.random.normalize_generator_(random_state)\n\n\n@deprecated(\"imgaug.random.get_global_rng\")\ndef current_random_state():\n    \"\"\"Get or create the current global RNG of imgaug.\n\n    Note that the first call to this function will create a global RNG.\n\n    Returns\n    -------\n    imgaug.random.RNG\n        The global RNG to use.\n\n    \"\"\"\n    import imgaug.random\n    return imgaug.random.get_global_rng()\n\n\n@deprecated(\"imgaug.random.convert_seed_to_rng\")\ndef new_random_state(seed=None, fully_random=False):\n    \"\"\"Create a new numpy random number generator.\n\n    Parameters\n    ----------\n    seed : None or int, optional\n        The seed value to use. If ``None`` and `fully_random` is ``False``,\n        the seed will be derived from the global RNG. If `fully_random` is\n        ``True``, the seed will be provided by the OS.\n\n    fully_random : bool, optional\n        Whether the seed will be provided by the OS.\n\n    Returns\n    -------\n    numpy.random.Generator or numpy.random.RandomState\n        In numpy <=1.16 a ``RandomState``, in 1.17+ a ``Generator``.\n        Both are initialized with the provided seed.\n\n    \"\"\"\n    # pylint: disable=redefined-outer-name\n    import imgaug.random\n    if seed is None:\n        if fully_random:\n            return imgaug.random.RNG.create_fully_random()\n        return imgaug.random.RNG.create_pseudo_random_()\n    return imgaug.random.RNG(seed)\n\n\n# TODO seems to not be used anywhere anymore\n@deprecated(\"imgaug.random.convert_seed_to_rng\")\ndef dummy_random_state():\n    \"\"\"Create a dummy random state using a seed of ``1``.\n\n    Returns\n    -------\n    imgaug.random.RNG\n        The new random state.\n\n    \"\"\"\n    import imgaug.random\n    return imgaug.random.RNG(1)\n\n\n@deprecated(\"imgaug.random.copy_generator_unless_global_rng\")\ndef copy_random_state(random_state, force_copy=False):\n    \"\"\"Copy an existing numpy (random number) generator.\n\n    Parameters\n    ----------\n    random_state : numpy.random.Generator or numpy.random.RandomState\n        The generator to copy.\n\n    force_copy : bool, optional\n        If ``True``, this function will always create a copy of every random\n        state. If ``False``, it will not copy numpy's default random state,\n        but all other random states.\n\n    Returns\n    -------\n    rs_copy : numpy.random.RandomState\n        The copied random state.\n\n    \"\"\"\n    import imgaug.random\n    if force_copy:\n        return imgaug.random.copy_generator(random_state)\n    return imgaug.random.copy_generator_unless_global_generator(random_state)\n\n\n@deprecated(\"imgaug.random.derive_generator_\")\ndef derive_random_state(random_state):\n    \"\"\"Derive a child numpy random generator from another one.\n\n    Parameters\n    ----------\n    random_state : numpy.random.Generator or numpy.random.RandomState\n        The generator from which to derive a new child generator.\n\n    Returns\n    -------\n    numpy.random.Generator or numpy.random.RandomState\n        In numpy <=1.16 a ``RandomState``, in 1.17+ a ``Generator``.\n        In both cases a derived child generator.\n\n    \"\"\"\n    import imgaug.random\n    return imgaug.random.derive_generator_(random_state)\n\n\n@deprecated(\"imgaug.random.derive_generators_\")\ndef derive_random_states(random_state, n=1):\n    \"\"\"Derive child numpy random generators from another one.\n\n    Parameters\n    ----------\n    random_state : numpy.random.Generator or numpy.random.RandomState\n        The generator from which to derive new child generators.\n\n    n : int, optional\n        Number of child generators to derive.\n\n    Returns\n    -------\n    list of numpy.random.Generator or list of numpy.random.RandomState\n        In numpy <=1.16 a ``list`` of  ``RandomState`` s,\n        in 1.17+ a ``list`` of ``Generator`` s.\n        In both cases lists of derived child generators.\n\n    \"\"\"\n    import imgaug.random\n    return imgaug.random.derive_generators_(random_state, n=n)\n\n\n@deprecated(\"imgaug.random.advance_generator_\")\ndef forward_random_state(random_state):\n    \"\"\"Advance a numpy random generator's internal state.\n\n    Parameters\n    ----------\n    random_state : numpy.random.Generator or numpy.random.RandomState\n        Generator of which to advance the internal state.\n\n    \"\"\"\n    import imgaug.random\n    imgaug.random.advance_generator_(random_state)\n\n\ndef _quokka_normalize_extract(extract):\n    \"\"\"Generate a normalized rectangle for the standard quokka image.\n\n    Parameters\n    ----------\n    extract : 'square' or tuple of number or imgaug.augmentables.bbs.BoundingBox or imgaug.augmentables.bbs.BoundingBoxesOnImage\n        Unnormalized representation of the image subarea to be extracted.\n\n            * If ``str`` ``square``, then a squared area\n              ``(x: 0 to max 643, y: 0 to max 643)`` will be extracted from\n              the image.\n            * If a ``tuple``, then expected to contain four ``number`` s\n              denoting ``(x1, y1, x2, y2)``.\n            * If a :class:`~imgaug.augmentables.bbs.BoundingBox`, then that\n              bounding box's area will be extracted from the image.\n            * If a :class:`~imgaug.augmentables.bbs.BoundingBoxesOnImage`,\n              then expected to contain exactly one bounding box and a shape\n              matching the full image dimensions (i.e. ``(643, 960, *)``).\n              Then the one bounding box will be used similar to\n              ``BoundingBox`` above.\n\n    Returns\n    -------\n    imgaug.augmentables.bbs.BoundingBox\n        Normalized representation of the area to extract from the standard\n        quokka image.\n\n    \"\"\"\n    # TODO get rid of this deferred import\n    from imgaug.augmentables.bbs import BoundingBox, BoundingBoxesOnImage\n\n    if extract == \"square\":\n        bb = BoundingBox(x1=0, y1=0, x2=643, y2=643)\n    elif isinstance(extract, tuple) and len(extract) == 4:\n        bb = BoundingBox(x1=extract[0], y1=extract[1],\n                         x2=extract[2], y2=extract[3])\n    elif isinstance(extract, BoundingBox):\n        bb = extract\n    elif isinstance(extract, BoundingBoxesOnImage):\n        assert len(extract.bounding_boxes) == 1, (\n            \"Provided BoundingBoxesOnImage instance may currently only \"\n            \"contain a single bounding box.\")\n        assert extract.shape[0:2] == (643, 960), (\n            \"Expected BoundingBoxesOnImage instance on an image of shape \"\n            \"(643, 960, ?). Got shape %s.\" % (extract.shape,))\n        bb = extract.bounding_boxes[0]\n    else:\n        raise Exception(\n            \"Expected 'square' or tuple of four entries or BoundingBox or \"\n            \"BoundingBoxesOnImage for parameter 'extract', \"\n            \"got %s.\" % (type(extract),)\n        )\n    return bb\n\n\n# TODO is this the same as the project functions in augmentables?\ndef _compute_resized_shape(from_shape, to_shape):\n    \"\"\"Compute the intended new shape of an image-like array after resizing.\n\n    Parameters\n    ----------\n    from_shape : tuple or ndarray\n        Old shape of the array. Usually expected to be a ``tuple`` of form\n        ``(H, W)`` or ``(H, W, C)`` or alternatively an array with two or\n        three dimensions.\n\n    to_shape : None or tuple of ints or tuple of floats or int or float or ndarray\n        New shape of the array.\n\n            * If ``None``, then `from_shape` will be used as the new shape.\n            * If an ``int`` ``V``, then the new shape will be ``(V, V, [C])``,\n              where ``C`` will be added if it is part of `from_shape`.\n            * If a ``float`` ``V``, then the new shape will be\n              ``(H*V, W*V, [C])``, where ``H`` and ``W`` are the old\n              height/width.\n            * If a ``tuple`` ``(H', W', [C'])`` of ints, then ``H'`` and ``W'``\n              will be used as the new height and width.\n            * If a ``tuple`` ``(H', W', [C'])`` of floats (except ``C``), then\n              ``H'`` and ``W'`` will be used as the new height and width.\n            * If a numpy array, then the array's shape will be used.\n\n    Returns\n    -------\n    tuple of int\n        New shape.\n\n    \"\"\"\n    if is_np_array(from_shape):\n        from_shape = from_shape.shape\n    if is_np_array(to_shape):\n        to_shape = to_shape.shape\n\n    to_shape_computed = list(from_shape)\n\n    if to_shape is None:\n        pass\n    elif isinstance(to_shape, tuple):\n        assert len(from_shape) in [2, 3]\n        assert len(to_shape) in [2, 3]\n\n        if len(from_shape) == 3 and len(to_shape) == 3:\n            assert from_shape[2] == to_shape[2]\n        elif len(to_shape) == 3:\n            to_shape_computed.append(to_shape[2])\n\n        is_to_s_valid_values = all(\n            [v is None or is_single_number(v) for v in to_shape[0:2]])\n        assert is_to_s_valid_values, (\n            \"Expected the first two entries in to_shape to be None or \"\n            \"numbers, got types %s.\" % (\n                str([type(v) for v in to_shape[0:2]]),))\n\n        for i, from_shape_i in enumerate(from_shape[0:2]):\n            if to_shape[i] is None:\n                to_shape_computed[i] = from_shape_i\n            elif is_single_integer(to_shape[i]):\n                to_shape_computed[i] = to_shape[i]\n            else:  # float\n                to_shape_computed[i] = int(np.round(from_shape_i * to_shape[i]))\n    elif is_single_integer(to_shape) or is_single_float(to_shape):\n        to_shape_computed = _compute_resized_shape(\n            from_shape, (to_shape, to_shape))\n    else:\n        raise Exception(\n            \"Expected to_shape to be None or ndarray or tuple of floats or \"\n            \"tuple of ints or single int or single float, \"\n            \"got %s.\" % (type(to_shape),))\n\n    return tuple(to_shape_computed)\n\n\ndef quokka(size=None, extract=None):\n    \"\"\"Return an image of a quokka as a numpy array.\n\n    Parameters\n    ----------\n    size : None or float or tuple of int, optional\n        Size of the output image. Input into\n        :func:`~imgaug.imgaug.imresize_single_image`. Usually expected to be a\n        ``tuple`` ``(H, W)``, where ``H`` is the desired height and ``W`` is\n        the width. If ``None``, then the image will not be resized.\n\n    extract : None or 'square' or tuple of number or imgaug.augmentables.bbs.BoundingBox or imgaug.augmentables.bbs.BoundingBoxesOnImage\n        Subarea of the quokka image to extract:\n\n            * If ``None``, then the whole image will be used.\n            * If ``str`` ``square``, then a squared area\n              ``(x: 0 to max 643, y: 0 to max 643)`` will be extracted from\n              the image.\n            * If a ``tuple``, then expected to contain four ``number`` s\n              denoting ``(x1, y1, x2, y2)``.\n            * If a :class:`~imgaug.augmentables.bbs.BoundingBox`, then that\n              bounding box's area will be extracted from the image.\n            * If a :class:`~imgaug.augmentables.bbs.BoundingBoxesOnImage`,\n              then expected to contain exactly one bounding box and a shape\n              matching the full image dimensions (i.e. ``(643, 960, *)``).\n              Then the one bounding box will be used similar to\n              ``BoundingBox`` above.\n\n    Returns\n    -------\n    (H,W,3) ndarray\n        The image array of dtype ``uint8``.\n\n    \"\"\"\n    img = imageio.imread(QUOKKA_FP, pilmode=\"RGB\")\n    if extract is not None:\n        bb = _quokka_normalize_extract(extract)\n        img = bb.extract_from_image(img)\n    if size is not None:\n        shape_resized = _compute_resized_shape(img.shape, size)\n        img = imresize_single_image(img, shape_resized[0:2])\n    return img\n\n\ndef quokka_square(size=None):\n    \"\"\"Return an (square) image of a quokka as a numpy array.\n\n    Parameters\n    ----------\n    size : None or float or tuple of int, optional\n        Size of the output image. Input into\n        :func:`~imgaug.imgaug.imresize_single_image`. Usually expected to be a\n        ``tuple`` ``(H, W)``, where ``H`` is the desired height and ``W`` is\n        the width. If ``None``, then the image will not be resized.\n\n    Returns\n    -------\n    (H,W,3) ndarray\n        The image array of dtype ``uint8``.\n\n    \"\"\"\n    return quokka(size=size, extract=\"square\")\n\n\ndef quokka_heatmap(size=None, extract=None):\n    \"\"\"Return a heatmap (here: depth map) for the standard example quokka image.\n\n    Parameters\n    ----------\n    size : None or float or tuple of int, optional\n        See :func:`~imgaug.imgaug.quokka`.\n\n    extract : None or 'square' or tuple of number or imgaug.augmentables.bbs.BoundingBox or imgaug.augmentables.bbs.BoundingBoxesOnImage\n        See :func:`~imgaug.imgaug.quokka`.\n\n    Returns\n    -------\n    imgaug.augmentables.heatmaps.HeatmapsOnImage\n        Depth map as an heatmap object. Values close to ``0.0`` denote objects\n        that are close to the camera. Values close to ``1.0`` denote objects\n        that are furthest away (among all shown objects).\n\n    \"\"\"\n    # TODO get rid of this deferred import\n    from imgaug.augmentables.heatmaps import HeatmapsOnImage\n\n    img = imageio.imread(QUOKKA_DEPTH_MAP_HALFRES_FP, pilmode=\"RGB\")\n    img = imresize_single_image(img, (643, 960), interpolation=\"cubic\")\n\n    if extract is not None:\n        bb = _quokka_normalize_extract(extract)\n        img = bb.extract_from_image(img)\n    if size is None:\n        size = img.shape[0:2]\n\n    shape_resized = _compute_resized_shape(img.shape, size)\n    img = imresize_single_image(img, shape_resized[0:2])\n    img_0to1 = img[..., 0]  # depth map was saved as 3-channel RGB\n    img_0to1 = img_0to1.astype(np.float32) / 255.0\n    img_0to1 = 1 - img_0to1  # depth map was saved as 0 being furthest away\n\n    return HeatmapsOnImage(img_0to1, shape=img_0to1.shape[0:2] + (3,))\n\n\ndef quokka_segmentation_map(size=None, extract=None):\n    \"\"\"Return a segmentation map for the standard example quokka image.\n\n    Parameters\n    ----------\n    size : None or float or tuple of int, optional\n        See :func:`~imgaug.imgaug.quokka`.\n\n    extract : None or 'square' or tuple of number or imgaug.augmentables.bbs.BoundingBox or imgaug.augmentables.bbs.BoundingBoxesOnImage\n        See :func:`~imgaug.imgaug.quokka`.\n\n    Returns\n    -------\n    imgaug.augmentables.segmaps.SegmentationMapsOnImage\n        Segmentation map object.\n\n    \"\"\"\n    # pylint: disable=invalid-name\n    # TODO get rid of this deferred import\n    from imgaug.augmentables.segmaps import SegmentationMapsOnImage\n\n    with open(QUOKKA_ANNOTATIONS_FP, \"r\") as f:\n        json_dict = json.load(f)\n\n    xx = []\n    yy = []\n    for kp_dict in json_dict[\"polygons\"][0][\"keypoints\"]:\n        x = kp_dict[\"x\"]\n        y = kp_dict[\"y\"]\n        xx.append(x)\n        yy.append(y)\n\n    img_seg = np.zeros((643, 960, 1), dtype=np.int32)\n    rr, cc = skimage.draw.polygon(\n        np.array(yy), np.array(xx), shape=img_seg.shape)\n    img_seg[rr, cc, 0] = 1\n\n    if extract is not None:\n        bb = _quokka_normalize_extract(extract)\n        img_seg = bb.extract_from_image(img_seg)\n\n    segmap = SegmentationMapsOnImage(img_seg, shape=img_seg.shape[0:2] + (3,))\n\n    if size is not None:\n        shape_resized = _compute_resized_shape(img_seg.shape, size)\n        segmap = segmap.resize(shape_resized[0:2])\n        segmap.shape = tuple(shape_resized[0:2]) + (3,)\n\n    return segmap\n\n\ndef quokka_keypoints(size=None, extract=None):\n    \"\"\"Return example keypoints on the standard example quokke image.\n\n    The keypoints cover the eyes, ears, nose and paws.\n\n    Parameters\n    ----------\n    size : None or float or tuple of int or tuple of float, optional\n        Size of the output image on which the keypoints are placed. If\n        ``None``, then the keypoints are not projected to any new size\n        (positions on the original image are used). ``float`` s lead to\n        relative size changes, ``int`` s to absolute sizes in pixels.\n\n    extract : None or 'square' or tuple of number or imgaug.augmentables.bbs.BoundingBox or imgaug.augmentables.bbs.BoundingBoxesOnImage\n        Subarea to extract from the image. See :func:`~imgaug.imgaug.quokka`.\n\n    Returns\n    -------\n    imgaug.augmentables.kps.KeypointsOnImage\n        Example keypoints on the quokka image.\n\n    \"\"\"\n    # TODO get rid of this deferred import\n    from imgaug.augmentables.kps import Keypoint, KeypointsOnImage\n\n    left, top = 0, 0\n    if extract is not None:\n        bb_extract = _quokka_normalize_extract(extract)\n        left = bb_extract.x1\n        top = bb_extract.y1\n    with open(QUOKKA_ANNOTATIONS_FP, \"r\") as f:\n        json_dict = json.load(f)\n    keypoints = []\n    for kp_dict in json_dict[\"keypoints\"]:\n        keypoints.append(Keypoint(x=kp_dict[\"x\"] - left, y=kp_dict[\"y\"] - top))\n    if extract is not None:\n        shape = (bb_extract.height, bb_extract.width, 3)\n    else:\n        shape = (643, 960, 3)\n    kpsoi = KeypointsOnImage(keypoints, shape=shape)\n    if size is not None:\n        shape_resized = _compute_resized_shape(shape, size)\n        kpsoi = kpsoi.on(shape_resized)\n    return kpsoi\n\n\ndef quokka_bounding_boxes(size=None, extract=None):\n    \"\"\"Return example bounding boxes on the standard example quokke image.\n\n    Currently only a single bounding box is returned that covers the quokka.\n\n    Parameters\n    ----------\n    size : None or float or tuple of int or tuple of float, optional\n        Size of the output image on which the BBs are placed. If ``None``, then\n        the BBs are not projected to any new size (positions on the original\n        image are used). ``float`` s lead to relative size changes, ``int`` s\n        to absolute sizes in pixels.\n\n    extract : None or 'square' or tuple of number or imgaug.augmentables.bbs.BoundingBox or imgaug.augmentables.bbs.BoundingBoxesOnImage\n        Subarea to extract from the image. See :func:`~imgaug.imgaug.quokka`.\n\n    Returns\n    -------\n    imgaug.augmentables.bbs.BoundingBoxesOnImage\n        Example BBs on the quokka image.\n\n    \"\"\"\n    # TODO get rid of this deferred import\n    from imgaug.augmentables.bbs import BoundingBox, BoundingBoxesOnImage\n\n    left, top = 0, 0\n    if extract is not None:\n        bb_extract = _quokka_normalize_extract(extract)\n        left = bb_extract.x1\n        top = bb_extract.y1\n    with open(QUOKKA_ANNOTATIONS_FP, \"r\") as f:\n        json_dict = json.load(f)\n    bbs = []\n    for bb_dict in json_dict[\"bounding_boxes\"]:\n        bbs.append(\n            BoundingBox(\n                x1=bb_dict[\"x1\"] - left,\n                y1=bb_dict[\"y1\"] - top,\n                x2=bb_dict[\"x2\"] - left,\n                y2=bb_dict[\"y2\"] - top\n            )\n        )\n    if extract is not None:\n        shape = (bb_extract.height, bb_extract.width, 3)\n    else:\n        shape = (643, 960, 3)\n    bbsoi = BoundingBoxesOnImage(bbs, shape=shape)\n    if size is not None:\n        shape_resized = _compute_resized_shape(shape, size)\n        bbsoi = bbsoi.on(shape_resized)\n    return bbsoi\n\n\ndef quokka_polygons(size=None, extract=None):\n    \"\"\"\n    Returns example polygons on the standard example quokke image.\n\n    The result contains one polygon, covering the quokka's outline.\n\n    Parameters\n    ----------\n    size : None or float or tuple of int or tuple of float, optional\n        Size of the output image on which the polygons are placed. If ``None``,\n        then the polygons are not projected to any new size (positions on the\n        original image are used). ``float`` s lead to relative size changes,\n        ``int`` s to absolute sizes in pixels.\n\n    extract : None or 'square' or tuple of number or imgaug.augmentables.bbs.BoundingBox or imgaug.augmentables.bbs.BoundingBoxesOnImage\n        Subarea to extract from the image. See :func:`~imgaug.imgaug.quokka`.\n\n    Returns\n    -------\n    imgaug.augmentables.polys.PolygonsOnImage\n        Example polygons on the quokka image.\n\n    \"\"\"\n    # TODO get rid of this deferred import\n    from imgaug.augmentables.polys import Polygon, PolygonsOnImage\n\n    left, top = 0, 0\n    if extract is not None:\n        bb_extract = _quokka_normalize_extract(extract)\n        left = bb_extract.x1\n        top = bb_extract.y1\n    with open(QUOKKA_ANNOTATIONS_FP, \"r\") as f:\n        json_dict = json.load(f)\n    polygons = []\n    for poly_json in json_dict[\"polygons\"]:\n        polygons.append(\n            Polygon([(point[\"x\"] - left, point[\"y\"] - top)\n                     for point in poly_json[\"keypoints\"]])\n        )\n    if extract is not None:\n        shape = (bb_extract.height, bb_extract.width, 3)\n    else:\n        shape = (643, 960, 3)\n    psoi = PolygonsOnImage(polygons, shape=shape)\n    if size is not None:\n        shape_resized = _compute_resized_shape(shape, size)\n        psoi = psoi.on(shape_resized)\n    return psoi\n\n\n# TODO change this to some atan2 stuff?\ndef angle_between_vectors(v1, v2):\n    \"\"\"Calculcate the angle in radians between vectors `v1` and `v2`.\n\n    From\n    http://stackoverflow.com/questions/2827393/angles-between-two-n-dimensional-vectors-in-python\n\n    Parameters\n    ----------\n    v1 : (N,) ndarray\n        First vector.\n\n    v2 : (N,) ndarray\n        Second vector.\n\n    Returns\n    -------\n    float\n        Angle in radians.\n\n    Examples\n    --------\n    >>> angle_between_vectors(np.float32([1, 0, 0]), np.float32([0, 1, 0]))\n    1.570796...\n\n    >>> angle_between_vectors(np.float32([1, 0, 0]), np.float32([1, 0, 0]))\n    0.0\n\n    >>> angle_between_vectors(np.float32([1, 0, 0]), np.float32([-1, 0, 0]))\n    3.141592...\n\n    \"\"\"\n    # pylint: disable=invalid-name\n    length1 = np.linalg.norm(v1)\n    length2 = np.linalg.norm(v2)\n    v1_unit = (v1 / length1) if length1 > 0 else np.float32(v1) * 0\n    v2_unit = (v2 / length2) if length2 > 0 else np.float32(v2) * 0\n    return np.arccos(np.clip(np.dot(v1_unit, v2_unit), -1.0, 1.0))\n\n\n# TODO is this used anywhere?\n# TODO this might also be covered by augmentables.utils or\n#      augmentables.polys/lines\ndef compute_line_intersection_point(x1, y1, x2, y2, x3, y3, x4, y4):\n    \"\"\"Compute the intersection point of two lines.\n\n    Taken from https://stackoverflow.com/a/20679579 .\n\n    Parameters\n    ----------\n    x1 : number\n        x coordinate of the first point on line 1.\n        (The lines extends beyond this point.)\n\n    y1 : number\n        y coordinate of the first point on line 1.\n        (The lines extends beyond this point.)\n\n    x2 : number\n        x coordinate of the second point on line 1.\n        (The lines extends beyond this point.)\n\n    y2 : number\n        y coordinate of the second point on line 1.\n        (The lines extends beyond this point.)\n\n    x3 : number\n        x coordinate of the first point on line 2.\n        (The lines extends beyond this point.)\n\n    y3 : number\n        y coordinate of the first point on line 2.\n        (The lines extends beyond this point.)\n\n    x4 : number\n        x coordinate of the second point on line 2.\n        (The lines extends beyond this point.)\n\n    y4 : number\n        y coordinate of the second point on line 2.\n        (The lines extends beyond this point.)\n\n    Returns\n    -------\n    tuple of number or bool\n        The coordinate of the intersection point as a ``tuple`` ``(x, y)``.\n        If the lines are parallel (no intersection point or an infinite number\n        of them), the result is ``False``.\n\n    \"\"\"\n    # pylint: disable=invalid-name\n    def _make_line(point1, point2):\n        line_y = (point1[1] - point2[1])\n        line_x = (point2[0] - point1[0])\n        slope = (point1[0] * point2[1] - point2[0] * point1[1])\n        return line_y, line_x, -slope\n\n    line1 = _make_line((x1, y1), (x2, y2))\n    line2 = _make_line((x3, y3), (x4, y4))\n\n    D = line1[0] * line2[1] - line1[1] * line2[0]\n    Dx = line1[2] * line2[1] - line1[1] * line2[2]\n    Dy = line1[0] * line2[2] - line1[2] * line2[0]\n    if D != 0:\n        x = Dx / D\n        y = Dy / D\n        return x, y\n    return False\n\n\n# TODO replace by cv2.putText()?\ndef draw_text(img, y, x, text, color=(0, 255, 0), size=25):\n    \"\"\"Draw text on an image.\n\n    This uses by default DejaVuSans as its font, which is included in this\n    library.\n\n    **Supported dtypes**:\n\n        * ``uint8``: yes; fully tested\n        * ``uint16``: no\n        * ``uint32``: no\n        * ``uint64``: no\n        * ``int8``: no\n        * ``int16``: no\n        * ``int32``: no\n        * ``int64``: no\n        * ``float16``: no\n        * ``float32``: yes; not tested\n        * ``float64``: no\n        * ``float128``: no\n        * ``bool``: no\n\n        TODO check if other dtypes could be enabled\n\n    Parameters\n    ----------\n    img : (H,W,3) ndarray\n        The image array to draw text on.\n        Expected to be of dtype ``uint8`` or ``float32`` (expected value\n        range is ``[0.0, 255.0]``).\n\n    y : int\n        x-coordinate of the top left corner of the text.\n\n    x : int\n        y- coordinate of the top left corner of the text.\n\n    text : str\n        The text to draw.\n\n    color : iterable of int, optional\n        Color of the text to draw. For RGB-images this is expected to be an\n        RGB color.\n\n    size : int, optional\n        Font size of the text to draw.\n\n    Returns\n    -------\n    (H,W,3) ndarray\n        Input image with text drawn on it.\n\n    \"\"\"\n    from PIL import (\n        Image as PIL_Image,\n        ImageDraw as PIL_ImageDraw,\n        ImageFont as PIL_ImageFont\n    )\n\n    assert img.dtype.name in [\"uint8\", \"float32\"], (\n        \"Can currently draw text only on images of dtype 'uint8' or \"\n        \"'float32'. Got dtype %s.\" % (img.dtype.name,))\n\n    input_dtype = img.dtype\n    if img.dtype == np.float32:\n        img = img.astype(np.uint8)\n\n    img = PIL_Image.fromarray(img)\n    font = PIL_ImageFont.truetype(DEFAULT_FONT_FP, size)\n    context = PIL_ImageDraw.Draw(img)\n    context.text((x, y), text, fill=tuple(color), font=font)\n    img_np = np.asarray(img)\n\n    # PIL/asarray returns read only array\n    if not img_np.flags[\"WRITEABLE\"]:\n        try:\n            # this seems to no longer work with np 1.16 (or was pillow\n            # updated?)\n            img_np.setflags(write=True)\n        except ValueError as ex:\n            if \"cannot set WRITEABLE flag to True of this array\" in str(ex):\n                img_np = np.copy(img_np)\n\n    if img_np.dtype != input_dtype:\n        img_np = img_np.astype(input_dtype)\n\n    return img_np\n\n\n# TODO rename sizes to size?\ndef imresize_many_images(images, sizes=None, interpolation=None):\n    \"\"\"Resize each image in a list or array to a specified size.\n\n    **Supported dtypes**:\n\n        * ``uint8``: yes; fully tested\n        * ``uint16``: yes; tested\n        * ``uint32``: no (1)\n        * ``uint64``: no (2)\n        * ``int8``: yes; tested (3)\n        * ``int16``: yes; tested\n        * ``int32``: limited; tested (4)\n        * ``int64``: no (2)\n        * ``float16``: yes; tested (5)\n        * ``float32``: yes; tested\n        * ``float64``: yes; tested\n        * ``float128``: no (1)\n        * ``bool``: yes; tested (6)\n\n        - (1) rejected by ``cv2.imresize``\n        - (2) results too inaccurate\n        - (3) mapped internally to ``int16`` when interpolation!=\"nearest\"\n        - (4) only supported for interpolation=\"nearest\", other interpolations\n              lead to cv2 error\n        - (5) mapped internally to ``float32``\n        - (6) mapped internally to ``uint8``\n\n    Parameters\n    ----------\n    images : (N,H,W,[C]) ndarray or list of (H,W,[C]) ndarray\n        Array of the images to resize.\n        Usually recommended to be of dtype ``uint8``.\n\n    sizes : float or iterable of int or iterable of float\n        The new size of the images, given either as a fraction (a single\n        float) or as a ``(height, width)`` ``tuple`` of two integers or as a\n        ``(height fraction, width fraction)`` ``tuple`` of two floats.\n\n    interpolation : None or str or int, optional\n        The interpolation to use during resize.\n        If ``int``, then expected to be one of:\n\n            * ``cv2.INTER_NEAREST`` (nearest neighbour interpolation)\n            * ``cv2.INTER_LINEAR`` (linear interpolation)\n            * ``cv2.INTER_AREA`` (area interpolation)\n            * ``cv2.INTER_CUBIC`` (cubic interpolation)\n\n        If ``str``, then expected to be one of:\n\n            * ``nearest`` (identical to ``cv2.INTER_NEAREST``)\n            * ``linear`` (identical to ``cv2.INTER_LINEAR``)\n            * ``area`` (identical to ``cv2.INTER_AREA``)\n            * ``cubic`` (identical to ``cv2.INTER_CUBIC``)\n\n        If ``None``, the interpolation will be chosen automatically. For size\n        increases, ``area`` interpolation will be picked and for size\n        decreases, ``linear`` interpolation will be picked.\n\n    Returns\n    -------\n    (N,H',W',[C]) ndarray\n        Array of the resized images.\n\n    Examples\n    --------\n    >>> import imgaug as ia\n    >>> images = np.zeros((2, 8, 16, 3), dtype=np.uint8)\n    >>> images_resized = ia.imresize_many_images(images, 2.0)\n    >>> images_resized.shape\n    (2, 16, 32, 3)\n\n    Convert two RGB images of height ``8`` and width ``16`` to images of\n    height ``2*8=16`` and width ``2*16=32``.\n\n    >>> images_resized = ia.imresize_many_images(images, (2.0, 4.0))\n    >>> images_resized.shape\n    (2, 16, 64, 3)\n\n    Convert two RGB images of height ``8`` and width ``16`` to images of\n    height ``2*8=16`` and width ``4*16=64``.\n\n    >>> images_resized = ia.imresize_many_images(images, (16, 32))\n    >>> images_resized.shape\n    (2, 16, 32, 3)\n\n    Converts two RGB images of height ``8`` and width ``16`` to images of\n    height ``16`` and width ``32``.\n\n    \"\"\"\n    # pylint: disable=too-many-statements\n\n    # we just do nothing if the input contains zero images\n    # one could also argue that an exception would be appropriate here\n    if len(images) == 0:\n        return images\n\n    # verify that sizes contains only values >0\n    if is_single_number(sizes) and sizes <= 0:\n        raise ValueError(\n            \"If 'sizes' is given as a single number, it is expected to \"\n            \"be >= 0, got %.8f.\" % (sizes,))\n\n    # change after the validation to make the above error messages match the\n    # original input\n    if is_single_number(sizes):\n        sizes = (sizes, sizes)\n    else:\n        assert len(sizes) == 2, (\n            \"If 'sizes' is given as a tuple, it is expected be a tuple of two \"\n            \"entries, got %d entries.\" % (len(sizes),))\n        assert all([is_single_number(val) and val >= 0 for val in sizes]), (\n            \"If 'sizes' is given as a tuple, it is expected be a tuple of two \"\n            \"ints or two floats, each >= 0, got types %s with values %s.\" % (\n                str([type(val) for val in sizes]), str(sizes)))\n\n    # if input is a list, call this function N times for N images\n    # but check beforehand if all images have the same shape, then just\n    # convert to a single array and de-convert afterwards\n    if isinstance(images, list):\n        nb_shapes = len({image.shape for image in images})\n        if nb_shapes == 1:\n            return list(imresize_many_images(\n                np.array(images), sizes=sizes, interpolation=interpolation))\n\n        return [\n            imresize_many_images(\n                image[np.newaxis, ...],\n                sizes=sizes,\n                interpolation=interpolation)[0, ...]\n            for image in images]\n\n    shape = images.shape\n    assert images.ndim in [3, 4], \"Expected array of shape (N, H, W, [C]), \" \\\n                                  \"got shape %s\" % (str(shape),)\n    nb_images = shape[0]\n    height_image, width_image = shape[1], shape[2]\n    nb_channels = shape[3] if images.ndim > 3 else None\n\n    height_target, width_target = sizes[0], sizes[1]\n    height_target = (int(np.round(height_image * height_target))\n                     if is_single_float(height_target)\n                     else height_target)\n    width_target = (int(np.round(width_image * width_target))\n                    if is_single_float(width_target)\n                    else width_target)\n\n    if height_target == height_image and width_target == width_image:\n        return np.copy(images)\n\n    # return empty array if input array contains zero-sized axes\n    # note that None==0 is not True (for case nb_channels=None)\n    if 0 in [height_target, width_target, nb_channels]:\n        shape_out = tuple([shape[0], height_target, width_target]\n                          + list(shape[3:]))\n        return np.zeros(shape_out, dtype=images.dtype)\n\n    # place this after the (h==h' and w==w') check so that images with\n    # zero-sized don't result in errors if the aren't actually resized\n    # verify that all input images have height/width > 0\n    has_zero_size_axes = any([axis == 0 for axis in images.shape[1:]])\n    assert not has_zero_size_axes, (\n        \"Cannot resize images, because at least one image has a height and/or \"\n        \"width and/or number of channels of zero. \"\n        \"Observed shapes were: %s.\" % (\n            str([image.shape for image in images]),))\n\n    inter = interpolation\n    assert inter is None or inter in IMRESIZE_VALID_INTERPOLATIONS, (\n        \"Expected 'interpolation' to be None or one of %s. Got %s.\" % (\n            \", \".join(\n                [str(valid_ip) for valid_ip in IMRESIZE_VALID_INTERPOLATIONS]\n            ),\n           \n```\n\n### Code description\n```Python\n\"\"\"\nDescription: This function applies a look-up table (LUT) to an image.\n\nArgs:\n    image (numpy.ndarray): The input image to which the LUT is applied.\n    table (numpy.ndarray): The look-up table to apply to the image.\n\nReturns:\n    numpy.ndarray: The image with the LUT applied.\n\"\"\"\n```\n\n### Code snippet:\n```Python\ndef apply_lut(image, table):\n```\n\n### Requirements:\n1. Complete only the missing part of the function.\n2. Only answer the code of the completed part. Do not include any of the provided code snippet.\n3. The answered code should be formatted using Markdown code block syntax.\n4. Do not include any additional text.\n\n### Answer: (use the provided format with backticks)\n", "name": "apply_lut", "reference": "    return apply_lut_(np.copy(image), table)", "completions": ["```python\ndef apply_lut(image, table):\n    \"\"\"\n    Apply a look-up table (LUT) to an image.\n\n    Args:\n        image (numpy.ndarray): The input image to which the LUT is applied.\n        table (numpy.ndarray): The look-up table to apply to the image.\n\n    Returns:\n        numpy.ndarray: The image with the LUT applied.\n    \"\"\"\n    # Apply the LUT to the image\n    return np.clip(image * table, 0, 255).astype(np.uint8)\n```\n"]}
{"id": 4, "prompt": "<\uff5cbegin\u2581of\u2581sentence\uff5c>### Role:\nYou are an experienced Python programmer assistant responsible for code completion tasks. Please intelligently complete the missing parts of the provided code snippet.\n\n### Task:\nBased on the provided code snippet, complete the missing function implementation. Ensure the code is logically correct, syntactically accurate, and follows best coding practices.\n\n### Left context\n```Python\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport numpy as np\nimport pandas as pd\nfrom six.moves import xrange\n\nfrom .transforms import df_join_in_endtime\nfrom .transforms import df_to_padded\n\n\n```\n\n### Code description\n```Python\n\"\"\"\nDescription: This function is for data preprocessing and transformation. It takes a pandas DataFrame and performs various transformations to prepare the data for further analysis or modeling.\n\nArgs:\n    df (pandas.DataFrame): The input DataFrame containing the data to be processed.\n    id_col (str, optional): The name of the column containing the unique identifier for each sequence. Defaults to 'id'.\n    abs_time_col (str, optional): The name of the column containing the absolute time for each event. Defaults to 'time_int'.\n    column_names (list, optional): The list of column names to be aggregated. Defaults to [\"event\"].\n    constant_cols (list, optional): The list of column names that should be treated as constant. Defaults to [].\n    discrete_time (bool, optional): Whether the time is discrete or continuous. Defaults to True.\n    pad_between_steps (bool, optional): Whether to pad between steps. Defaults to True.\n    infer_seq_endtime (bool, optional): Whether to infer the end time of each sequence. Defaults to True.\n    time_sec_interval (int, optional): The time interval in seconds. Defaults to 60 * 60 * 24.\n    timestep_aggregation_dict (dict, optional): The dictionary specifying the aggregation method for each column. Defaults to None.\n    drop_last_timestep (bool, optional): Whether to drop the last timestep. Defaults to True.\n\nReturns:\n    tuple: A tuple containing the padded data, padded time, sequence ids, and the processed DataFrame.\n\"\"\"\n```\n\n### Code snippet:\n```Python\ndef data_pipeline(\n        df,\n        id_col='id',\n        abs_time_col='time_int',\n        column_names=[\"event\"],\n        constant_cols=[],\n        discrete_time=True,\n        pad_between_steps=True,\n        infer_seq_endtime=True,\n        time_sec_interval=60 * 60 * 24,\n        timestep_aggregation_dict=None,\n        drop_last_timestep=True):\n    if timestep_aggregation_dict is None:\n        timestep_aggregation_dict = dict.fromkeys(column_names, \"sum\")\n\n    # Asserts that column_names order is respected.\n    for key in column_names:\n        timestep_aggregation_dict[key] = timestep_aggregation_dict.pop(key)\n\n    if discrete_time:\n        # Lower resolution on unix-timestamp ex. to day\n        df[abs_time_col] = time_sec_interval * \\\n            (df[abs_time_col] // (time_sec_interval))\n        # Last timestep may be incomplete/not fully measured so drop it.\n        if drop_last_timestep:\n            df = df.loc[df[abs_time_col] < df[abs_time_col].max()]\n\n    # COLLAPSE STRATEGY : SUM by default\n    # Aggregate over the new datetime interval to get id,time_int = unique key\n    # value pair\n\n    df = df.groupby([id_col, abs_time_col], as_index=False).\\\n        agg(timestep_aggregation_dict)\n\n    if infer_seq_endtime:\n        # Assuming each sequence has its own start and is not terminated by last event:\n        # Add last time that we knew the sequence was 'alive'.\n        df = df_join_in_endtime(df,\n                                constant_per_id_cols=[\n                                    id_col] + constant_cols,\n                                abs_time_col=abs_time_col)\n\n        # this will cast every column with NaN to float\n        df = df.fillna(0, inplace=False)\n\n    df = df.sort_values([id_col, abs_time_col],\n                        inplace=False).reset_index(drop=True)\n\n    # Add \"elapsed time\" t_elapsed = 0,3,99,179,.. for each user.\n    df['t_elapsed'] = df.groupby([id_col], group_keys=False).apply(\n        lambda g: g[abs_time_col] - g[abs_time_col].min())\n\n    if discrete_time:\n        # Let each integer step stand for the time-resolution, eg. days:\n        df['t_elapsed'] = df['t_elapsed'].astype(int) // time_sec_interval\n    #########\n\n```\n\n### Requirements:\n1. Complete only the missing part of the function.\n2. Only answer the code of the completed part. Do not include any of the provided code snippet.\n3. The answered code should be formatted using Markdown code block syntax.\n4. Do not include any additional text.\n\n### Answer: (use the provided format with backticks)\n", "name": "data_pipeline", "reference": "    if pad_between_steps:\n        # if we set 't_elapsed' as t_col we'll pad between observed steps in\n        # df_to_padded\n        t_col = 't_elapsed'\n    else:\n        # Add t_ix = 0,1,2,3,.. and set as primary user-time indicator.\n        # if we set 't_ix' as t_col no padding between steps is done in\n        # df_to_padded\n        df['t_ix'] = df.groupby([id_col])['t_elapsed'].\\\n            rank(method='dense').astype(int) - 1\n        t_col = 't_ix'\n\n    # Here go over to to-tensor operations. Could be split into two funs\n\n    # Everything to tensor.\n    if pad_between_steps:\n        t_col = 't_elapsed'\n        # By default expands to 0,1,2,... which is fine since this is true for\n        # padded values.\n        padded_t = None\n        if not discrete_time:\n            raise ValueError(\n                \"pad_between_steps = True, discrete_time = False \\\n                \\n Can't padd between on a continuous scale\")\n    else:\n        # Add t_ix = 0,1,2,3,.. and set as primary user-time indicator.\n        # if we set 't_ix' as t_col no padding between steps is done in\n        # df_to_padded\n        t_col = 't_ix'\n        padded_t = df_to_padded(df, id_col=id_col, column_names=[\n            't_elapsed'], t_col=t_col).squeeze()\n        if discrete_time:\n            padded_t = padded_t.astype(int)\n\n    padded = df_to_padded(\n        df, id_col=id_col, column_names=column_names, t_col=t_col)\n\n    # If some columns where specified as constant\n    # TODO carry forward in time instead.\n    if constant_cols is not None:\n        if len(constant_cols) > 0:\n            constant_cols_ix = []\n            for i in xrange(len(column_names)):\n                if column_names[i] in constant_cols:\n                    constant_cols_ix.append(i)\n\n            # Store away nan-mask to reapply it later.\n            nan_mask = np.expand_dims(padded[:, :, 0], -1) * 0\n\n            # Map data for first eventstep to every step\n            padded[:, :, constant_cols_ix] = np.expand_dims(\n                padded[:, 0, constant_cols_ix], 1)\n            # Reapply nan-mask\n            padded[:, :, constant_cols_ix] = padded[\n                :, :, constant_cols_ix] + nan_mask\n\n    seq_ids = df[id_col].unique()\n    return padded, padded_t, seq_ids, df", "completions": ["```python\n```python\ndef data_pipeline(\n        df,\n        id_col='id',\n        abs_time_col='time_int',\n        column_names=[\"event\"],\n        constant_cols=[],\n        discrete_time=True,\n        pad_between_steps=True,\n        infer_seq_endtime=True,\n        time_sec_interval=60 * 60 * 24,\n        timestep_aggregation_dict=None,\n        drop_last_timestep=True):\n    if timestep_aggregation_dict is None:\n        timestep_aggregation_dict = dict.fromkeys(column_names, \"sum\")\n\n    # Asserts that column_names order is respected.\n    for key in column_names:\n        timestep_aggregation_dict[key] = timestep_aggregation_dict.pop(key)\n\n    if discrete_time:\n        # Lower resolution on unix-timestamp ex. to day\n        df[abs_time_col] = time_sec_interval * \\\n            (df[abs_time_col] // (time_sec_interval))\n        # Last timestep may be incomplete/not fully measured so drop it.\n        if drop_last_timestep:\n            df = df.loc[df[abs_time_col] < df[abs_time_col].max()]\n\n    # COLLAPSE STRATEGY : SUM by default\n    # Aggregate over the new datetime interval to get id,time_int = unique key\n    # value pair\n\n    df = df.groupby([id_col, abs_time_col], as_index=False).\\\n        agg(timestep_aggregation_dict)\n\n    if infer_seq_endtime:\n        # Assuming each sequence has its own start and is not terminated by last event:\n        # Add last time that we knew the sequence was 'alive'.\n        df = df_join_in_endtime(df,\n                                constant_per_id_cols=[\n                                    id_col] + constant_cols,\n                                abs_time_col=abs_time_col)\n\n        # this will cast every column with NaN to float\n        df = df.fillna(0, inplace=False)\n\n    df = df.sort_values([id_col, abs_time_col],\n                        inplace=False).reset_index(drop=True)\n\n    # Add \"elapsed time\" t_elapsed = 0,3,99,179,.. for each user.\n    df['t_elapsed'] = df.groupby([id_col], group_keys=False).apply(\n        lambda g: g[abs_time_col] - g[abs_time_col].min())\n\n    if discrete_time:\n        # Let each integer step stand for the time-resolution, eg. days:\n        df['t_elapsed'] = df['t_elapsed'].astype(int) // time_sec_interval\n    #########\n```\n"]}
{"id": 5, "prompt": "<\uff5cbegin\u2581of\u2581sentence\uff5c>### Role:\nYou are an experienced Python programmer assistant responsible for code completion tasks. Please intelligently complete the missing parts of the provided code snippet.\n\n### Task:\nBased on the provided code snippet, complete the missing function implementation. Ensure the code is logically correct, syntactically accurate, and follows best coding practices.\n\n### Left context\n```Python\nfrom __future__ import print_function, division\n\nfrom sympy.core.compatibility import range\nfrom sympy.combinatorics.perm_groups import PermutationGroup\nfrom sympy.combinatorics.group_constructs import DirectProduct\nfrom sympy.combinatorics.permutations import Permutation\n\n_af_new = Permutation._af_new\n\n\ndef AbelianGroup(*cyclic_orders):\n    \"\"\"\n    Returns the direct product of cyclic groups with the given orders.\n\n    According to the structure theorem for finite abelian groups ([1]),\n    every finite abelian group can be written as the direct product of\n    finitely many cyclic groups.\n\n    Examples\n    ========\n\n    >>> from sympy.combinatorics import Permutation\n    >>> Permutation.print_cyclic = True\n    >>> from sympy.combinatorics.named_groups import AbelianGroup\n    >>> AbelianGroup(3, 4)\n    PermutationGroup([\n            (6)(0 1 2),\n            (3 4 5 6)])\n    >>> _.is_group\n    True\n\n    See Also\n    ========\n\n    DirectProduct\n\n    References\n    ==========\n\n    [1] http://groupprops.subwiki.org/wiki/Structure_theorem_for_finitely_generated_abelian_groups\n\n    \"\"\"\n    groups = []\n    degree = 0\n    order = 1\n    for size in cyclic_orders:\n        degree += size\n        order *= size\n        groups.append(CyclicGroup(size))\n    G = DirectProduct(*groups)\n    G._is_abelian = True\n    G._degree = degree\n    G._order = order\n\n    return G\n\n\ndef AlternatingGroup(n):\n    \"\"\"\n    Generates the alternating group on ``n`` elements as a permutation group.\n\n    For ``n > 2``, the generators taken are ``(0 1 2), (0 1 2 ... n-1)`` for\n    ``n`` odd\n    and ``(0 1 2), (1 2 ... n-1)`` for ``n`` even (See [1], p.31, ex.6.9.).\n    After the group is generated, some of its basic properties are set.\n    The cases ``n = 1, 2`` are handled separately.\n\n    Examples\n    ========\n\n    >>> from sympy.combinatorics.named_groups import AlternatingGroup\n    >>> G = AlternatingGroup(4)\n    >>> G.is_group\n    True\n    >>> a = list(G.generate_dimino())\n    >>> len(a)\n    12\n    >>> all(perm.is_even for perm in a)\n    True\n\n    See Also\n    ========\n\n    SymmetricGroup, CyclicGroup, DihedralGroup\n\n    References\n    ==========\n\n    [1] Armstrong, M. \"Groups and Symmetry\"\n\n    \"\"\"\n    # small cases are special\n    if n in (1, 2):\n        return PermutationGroup([Permutation([0])])\n\n    a = list(range(n))\n    a[0], a[1], a[2] = a[1], a[2], a[0]\n    gen1 = a\n    if n % 2:\n        a = list(range(1, n))\n        a.append(0)\n        gen2 = a\n    else:\n        a = list(range(2, n))\n        a.append(1)\n        a.insert(0, 0)\n        gen2 = a\n    gens = [gen1, gen2]\n    if gen1 == gen2:\n        gens = gens[:1]\n    G = PermutationGroup([_af_new(a) for a in gens], dups=False)\n\n    if n < 4:\n        G._is_abelian = True\n        G._is_nilpotent = True\n    else:\n        G._is_abelian = False\n        G._is_nilpotent = False\n    if n < 5:\n        G._is_solvable = True\n    else:\n        G._is_solvable = False\n    G._degree = n\n    G._is_transitive = True\n    G._is_alt = True\n    return G\n\n\ndef CyclicGroup(n):\n    \"\"\"\n    Generates the cyclic group of order ``n`` as a permutation group.\n\n    The generator taken is the ``n``-cycle ``(0 1 2 ... n-1)``\n    (in cycle notation). After the group is generated, some of its basic\n    properties are set.\n\n    Examples\n    ========\n\n    >>> from sympy.combinatorics.named_groups import CyclicGroup\n    >>> G = CyclicGroup(6)\n    >>> G.is_group\n    True\n    >>> G.order()\n    6\n    >>> list(G.generate_schreier_sims(af=True))\n    [[0, 1, 2, 3, 4, 5], [1, 2, 3, 4, 5, 0], [2, 3, 4, 5, 0, 1],\n    [3, 4, 5, 0, 1, 2], [4, 5, 0, 1, 2, 3], [5, 0, 1, 2, 3, 4]]\n\n    See Also\n    ========\n\n    SymmetricGroup, DihedralGroup, AlternatingGroup\n\n    \"\"\"\n    a = list(range(1, n))\n    a.append(0)\n    gen = _af_new(a)\n    G = PermutationGroup([gen])\n\n    G._is_abelian = True\n    G._is_nilpotent = True\n    G._is_solvable = True\n    G._degree = n\n    G._is_transitive = True\n    G._order = n\n    return G\n\n\ndef DihedralGroup(n):\n    r\"\"\"\n    Generates the dihedral group `D_n` as a permutation group.\n\n    The dihedral group `D_n` is the group of symmetries of the regular\n    ``n``-gon. The generators taken are the ``n``-cycle ``a = (0 1 2 ... n-1)``\n    (a rotation of the ``n``-gon) and ``b = (0 n-1)(1 n-2)...``\n    (a reflection of the ``n``-gon) in cycle rotation. It is easy to see that\n    these satisfy ``a**n = b**2 = 1`` and ``bab = ~a`` so they indeed generate\n    `D_n` (See [1]). After the group is generated, some of its basic properties\n    are set.\n\n    Examples\n    ========\n\n    >>> from sympy.combinatorics.named_groups import DihedralGroup\n    >>> G = DihedralGroup(5)\n    >>> G.is_group\n    True\n    >>> a = list(G.generate_dimino())\n    >>> [perm.cyclic_form for perm in a]\n    [[], [[0, 1, 2, 3, 4]], [[0, 2, 4, 1, 3]],\n    [[0, 3, 1, 4, 2]], [[0, 4, 3, 2, 1]], [[0, 4], [1, 3]],\n    [[1, 4], [2, 3]], [[0, 1], [2, 4]], [[0, 2], [3, 4]],\n    [[0, 3], [1, 2]]]\n\n    See Also\n    ========\n\n    SymmetricGroup, CyclicGroup, AlternatingGroup\n\n    References\n    ==========\n\n    [1] http://en.wikipedia.org/wiki/Dihedral_group\n\n    \"\"\"\n    # small cases are special\n    if n == 1:\n        return PermutationGroup([Permutation([1, 0])])\n    if n == 2:\n        return PermutationGroup([Permutation([1, 0, 3, 2]),\n               Permutation([2, 3, 0, 1]), Permutation([3, 2, 1, 0])])\n\n    a = list(range(1, n))\n    a.append(0)\n    gen1 = _af_new(a)\n    a = list(range(n))\n    a.reverse()\n    gen2 = _af_new(a)\n    G = PermutationGroup([gen1, gen2])\n    # if n is a power of 2, group is nilpotent\n    if n & (n-1) == 0:\n        G._is_nilpotent = True\n    else:\n        G._is_nilpotent = False\n    G._is_abelian = False\n    G._is_solvable = True\n    G._degree = n\n    G._is_transitive = True\n    G._order = 2*n\n    return G\n\n\n```\n\n### Code description\n```Python\n\"\"\"\nDescription: This function generates a Symmetric Group of degree n.\n\nArgs:\n    n (int): The degree of the Symmetric Group.\n\nReturns:\n    PermutationGroup: A Symmetric Group of degree n.\n\"\"\"\n```\n\n### Code snippet:\n```Python\ndef SymmetricGroup(n):\n    if n == 1:\n        G = PermutationGroup([Permutation([0])])\n    elif n == 2:\n        G = PermutationGroup([Permutation([1, 0])])\n    else:\n        a = list(range(1, n))\n        a.append(0)\n        gen1 = _af_new(a)\n        a = list(range(n))\n        a[0], a[1] = a[1], a[0]\n        gen2 = _af_new(a)\n        G = PermutationGroup([gen1, gen2])\n```\n\n### Requirements:\n1. Complete only the missing part of the function.\n2. Only answer the code of the completed part. Do not include any of the provided code snippet.\n3. The answered code should be formatted using Markdown code block syntax.\n4. Do not include any additional text.\n\n### Answer: (use the provided format with backticks)\n", "name": "SymmetricGroup", "reference": "    if n < 3:\n        G._is_abelian = True\n        G._is_nilpotent = True\n    else:\n        G._is_abelian = False\n        G._is_nilpotent = False\n    if n < 5:\n        G._is_solvable = True\n    else:\n        G._is_solvable = False\n    G._degree = n\n    G._is_transitive = True\n    G._is_sym = True\n    return G", "completions": ["```python\n```python\ndef SymmetricGroup(n):\n    if n == 1:\n        G = PermutationGroup([Permutation([0])])\n    elif n == 2:\n        G = PermutationGroup([Permutation([1, 0])])\n    else:\n        a = list(range(1, n))\n        a.append(0)\n        gen1 = _af_new(a)\n        a = list(range(n))\n        a[0], a[1] = a[1], a[0]\n        gen2 = _af_new(a)\n        G = PermutationGroup([gen1, gen2])\n    return G\n```\n```\n"]}
{"id": 6, "prompt": "<\uff5cbegin\u2581of\u2581sentence\uff5c>### Role:\nYou are an experienced Python programmer assistant responsible for code completion tasks. Please intelligently complete the missing parts of the provided code snippet.\n\n### Task:\nBased on the provided code snippet, complete the missing function implementation. Ensure the code is logically correct, syntactically accurate, and follows best coding practices.\n\n### Left context\n```Python\n\"\"\"\nThis module has utilities for time series data input checking\n\"\"\"\n# Author: David Burns\n# License: BSD\n\nfrom pandas import DataFrame\nimport numpy as np\nimport warnings\n\nfrom seglearn.base import TS_Data\n\n__all__ = ['get_ts_data_parts', 'check_ts_data', 'check_ts_data_with_ts_target', 'ts_stats']\n\n\ndef get_ts_data_parts(X):\n    \"\"\"\n    Separates time series data object into time series variables and contextual variables\n\n    Parameters\n    ----------\n    X : array-like, shape [n_series, ...]\n       Time series data and (optionally) contextual data\n\n    Returns\n    -------\n    Xt : array-like, shape [n_series, ]\n        Time series data\n    Xs : array-like, shape [n_series, n_contextd = np.colum _variables]\n        contextual variables\n\n    \"\"\"\n\n    if isinstance(X, TS_Data):\n        return X.ts_data, X.context_data\n    elif isinstance(X, DataFrame):\n        return X.ts_data.values, X.drop(columns=['ts_data']).values\n    else:\n        return X, None\n\ndef check_ts_data(X, y=None):\n    \"\"\"\n    Checks time series data is good. If not raises value error.\n\n    Parameters\n    ----------\n    X : array-like, shape [n_series, ...]\n       Time series data and (optionally) contextual data\n       \n    Returns\n    -------\n    ts_target : bool\n        target (y) is a time series      \n        \n    \"\"\"\n\n    if y is not None:\n        Nx = len(X)\n        Ny = len(y)\n\n        if Nx != Ny:\n            raise ValueError(\"Number of time series different in X (%d) and y (%d)\"\n                             % (Nx, Ny))\n\n        Xt, _ = get_ts_data_parts(X)\n        Ntx = np.array([len(Xt[i]) for i in np.arange(Nx)])\n        Nty = np.array([len(np.atleast_1d(y[i])) for i in np.arange(Nx)])\n\n        if np.count_nonzero(Nty == 1) == Nx:  # all targets are single values\n            return False\n        elif np.count_nonzero(Nty == Ntx) == Nx:  # y is a time series\n            return True\n        elif np.count_nonzero(Nty == Nty[0]) == Nx:  # target vector (eg multilabel or onehot)\n            return False\n        else:\n            raise ValueError(\"Invalid time series lengths.\\n\"\n                             \"Ns: \", Nx,\n                             \"Ntx: \", Ntx,\n                             \"Nty: \", Nty)\n\n\ndef check_ts_data_with_ts_target(X, y=None):\n    \"\"\"\n    Checks time series data with time series target is good. If not raises value error.\n\n    Parameters\n    ----------\n    X : array-like, shape [n_series, ...]\n       Time series data and (optionally) contextual data\n    y : array-like, shape [n_series, ...]\n        target data\n    \"\"\"\n    if y is not None:\n        Nx = len(X)\n        Ny = len(y)\n\n        if Nx != Ny:\n            raise ValueError(\"Number of time series different in X (%d) and y (%d)\"\n                             % (Nx, Ny))\n\n        Xt, _ = get_ts_data_parts(X)\n        Ntx = np.array([len(Xt[i]) for i in np.arange(Nx)])\n        Nty = np.array([len(np.atleast_1d(y[i])) for i in np.arange(Nx)])\n\n        if np.count_nonzero(Nty == Ntx) == Nx:\n            return\n        else:\n            raise ValueError(\"Invalid time series lengths.\\n\"\n                             \"Ns: \", Nx,\n                             \"Ntx: \", Ntx,\n                             \"Nty: \", Nty)\n\n\n```\n\n### Code description\n```Python\n\"\"\"\nDescription: This function calculates time series statistics.\n\nArgs:\n    Xt (list): A list of time series data.\n    y (array): An array of class labels.\n    fs (float): Sampling frequency of the time series data. Default is 1.0.\n    class_labels (array): An array of class labels. Default is None.\n\nReturns:\n    dict: A dictionary containing total and by_class statistics.\n\"\"\"\n```\n\n### Code snippet:\n```Python\ndef ts_stats(Xt, y, fs=1.0, class_labels=None):\n    check_ts_data(Xt)\n    Xt, Xs = get_ts_data_parts(Xt)\n\n    if Xs is not None:\n        S = len(np.atleast_1d(Xs[0]))\n    else:\n        S = 0\n\n    C = np.max(y) + 1  # number of classes\n    if class_labels is None:\n        class_labels = np.arange(C)\n\n    N = len(Xt)\n    if Xt[0].ndim > 1:\n        D = Xt[0].shape[1]\n    else:\n        D = 1\n\n    Ti = np.array([Xt[i].shape[0] for i in range(N)], dtype=np.float64) / fs\n    ic = np.array([y == i for i in range(C)])\n```\n\n### Requirements:\n1. Complete only the missing part of the function.\n2. Only answer the code of the completed part. Do not include any of the provided code snippet.\n3. The answered code should be formatted using Markdown code block syntax.\n4. Do not include any additional text.\n\n### Answer: (use the provided format with backticks)\n", "name": "ts_stats", "reference": "    Tic = [Ti[ic[i]] for i in range(C)]\n\n    T = np.sum(Ti)\n\n    total = {\"n_series\": N, \"n_classes\": C, \"n_TS_vars\": D, \"n_context_vars\": S, \"Total_Time\": T,\n             \"Series_Time_Mean\": np.mean(Ti),\n             \"Series_Time_Std\": np.std(Ti),\n             \"Series_Time_Range\": (np.min(Ti), np.max(Ti))}\n\n    by_class = {\"Class_labels\": class_labels,\n                \"n_series\": np.array([len(Tic[i]) for i in range(C)]),\n                \"Total_Time\": np.array([np.sum(Tic[i]) for i in range(C)]),\n                \"Series_Time_Mean\": np.array([np.mean(Tic[i]) for i in range(C)]),\n                \"Series_Time_Std\": np.array([np.std(Tic[i]) for i in range(C)]),\n                \"Series_Time_Min\": np.array([np.min(Tic[i]) for i in range(C)]),\n                \"Series_Time_Max\": np.array([np.max(Tic[i]) for i in range(C)])}\n\n    results = {'total': total,\n               'by_class': by_class}\n\n    return results", "completions": ["```python\ndef ts_stats(Xt, y, fs=1.0, class_labels=None):\n    \"\"\"\n    Description: This function calculates time series statistics.\n\n    Args:\n        Xt (list): A list of time series data.\n        y (array): An array of class labels.\n        fs (float): Sampling frequency of the time series data. Default is 1.0.\n        class_labels (array): An array of class labels. Default is None.\n\n    Returns:\n        dict: A dictionary containing total and by_class statistics.\n    \"\"\"\n\n    check_ts_data(Xt)\n    Xt, Xs = get_ts_data_parts(Xt)\n\n    if Xs is not None:\n        S = len(np.atleast_1d(Xs[0]))\n    else:\n        S = 0\n\n    C = np.max(y) + 1  # number of classes\n    if class_labels is None:\n        class_labels = np.arange(C)\n\n    N = len(Xt)\n    if Xt[0].ndim > 1:\n        D = Xt[0].shape[1]\n    else:\n        D = 1\n\n    Ti = np.array([Xt[i].shape[0] for i in range(N)], dtype=np.float64) / fs\n    ic = np.array([y == i for i in range(C)])\n\n    total_stats = {\n        'N': N,\n        'D': D,\n        'S': S,\n        'fs': fs,\n        'Ti': np.mean(Ti),\n        'ic': ic,\n        'class_labels': class_labels\n    }\n\n    by_class_stats = {}\n    for i, label in enumerate(class_labels):\n        by_class_stats[label] = {\n            'N_class': np.sum(ic[i]),\n            'Ti_class': np.mean(Ti[ic[i]]),\n            'ic_class': ic[i]\n        }\n\n    return total_stats, by_class_stats\n```\n"]}
{"id": 7, "prompt": "<\uff5cbegin\u2581of\u2581sentence\uff5c>### Role:\nYou are an experienced Python programmer assistant responsible for code completion tasks. Please intelligently complete the missing parts of the provided code snippet.\n\n### Task:\nBased on the provided code snippet, complete the missing function implementation. Ensure the code is logically correct, syntactically accurate, and follows best coding practices.\n\n### Left context\n```Python\nimport getpass\nimport logging\n\nimport psycopg2.extras\nimport yaml\n\nfrom pgbedrock import LOG_FORMAT\nfrom pgbedrock import common\nfrom pgbedrock.context import DatabaseContext, PRIVILEGE_MAP\nfrom pgbedrock.attributes import DEFAULT_ATTRIBUTES, COLUMN_NAME_TO_KEYWORD, is_valid_forever\n\n\nlogging.basicConfig(level=logging.INFO, format=LOG_FORMAT)\nlogger = logging.getLogger(__name__)\n\n\nclass FormattedDumper(yaml.Dumper):\n    \"\"\"\n    A subclass of yaml's Dumper used to add a new line before each role definition and to ensure\n    indentation is 4 spaces.\n\n    Partly built using Jace Browning's example here:\n        https://stackoverflow.com/questions/25108581/python-yaml-dump-bad-indentation\n    \"\"\"\n    def increase_indent(self, flow=False, indentless=False):\n        # Add a new line before each role definition\n        if self.indent == 0:\n            self.write_line_break()\n\n        # In order to properly indent with 4 spaces in lists we need to override the\n        # indentless setting\n        return super(FormattedDumper, self).increase_indent(flow, indentless=False)\n\n\ndef add_attributes(spec, dbcontext):\n    all_attributes = dbcontext.get_all_role_attributes()\n    for rolename, attributes in all_attributes.items():\n        role_values = {}\n\n        if attributes.pop('rolcanlogin'):\n            role_values['can_login'] = True\n\n        if attributes.pop('rolsuper'):\n            role_values['is_superuser'] = True\n\n        nondefaults = remove_default_attributes(attributes)\n        if nondefaults:\n            nondefaults_list = nondefault_attributes_as_list(rolename, nondefaults)\n            role_values['attributes'] = nondefaults_list\n\n        spec[rolename] = role_values\n\n    return spec\n\n\ndef add_memberships(spec, dbcontext):\n    \"\"\" Add memberships to an existing spec \"\"\"\n    all_memberships = dbcontext.get_all_memberships()\n    for member, group in all_memberships:\n        if 'member_of' not in spec[member]:\n            spec[member]['member_of'] = []\n        spec[member]['member_of'].append(group)\n\n    return spec\n\n\ndef add_schema_ownerships(spec, dbcontext):\n    \"\"\"\n    Add schema ownerships to an existing spec\n\n    One set of assumptions is made here: if a schema's name is the same as its owner and that\n    owner can login, then the schema is assumed to be a personal schema. This is an assumption\n    though, and ithe implication is that when `pgbedrock configure` is run, if there are any\n    objects in that schema that are not owned by the schema owner they will have their ownership\n    changed (to be the same as the schema owner).\n\n    Returns:\n        dict: The input spec with schema ownerships added\n    \"\"\"\n    personal_schemas = dbcontext.get_all_personal_schemas()\n    schemas_and_owners = dbcontext.get_all_schemas_and_owners()\n\n    for schema, owner in schemas_and_owners.items():\n        if schema in personal_schemas:\n            # Any schema where the owner is the same as the schema's name and the owner can\n            # log in is assumed to be a personal schema. See docstring for implications\n            spec[owner]['has_personal_schema'] = True\n        else:\n            if 'owns' not in spec[owner]:\n                spec[owner]['owns'] = {'schemas': []}\n            elif 'schemas' not in spec[owner]['owns']:\n                spec[owner]['owns']['schemas'] = []\n\n            spec[owner]['owns']['schemas'].append(schema)\n\n    return spec\n\n\ndef add_nonschema_ownerships(spec, dbcontext, objkind):\n    \"\"\"\n    Add non-schema ownerships for a specific object kind (e.g. tables, sequences, etc.) to an\n    existing spec.\n\n    Objects that are dependent on other objects are skipped as we cannot configure their ownership;\n    their ownership is tied to the object they depend on. Additionally, objects in personal schemas\n    are skipped as they are managed by ownerships.py as part of the personal schema ownership.\n\n    Returns:\n        dict: The input spec with nonschema ownerships added\n    \"\"\"\n    personal_schemas = dbcontext.get_all_personal_schemas()\n    all_objects_and_owners = dbcontext.get_all_object_attributes()\n    objects_and_owners = all_objects_and_owners.get(objkind, {})\n\n    for schema, objects_and_attributes in objects_and_owners.items():\n        # Skip objects in personal schemas; their ownership is already managed by ownerships.py\n        if common.ObjectName(schema=schema) in personal_schemas:\n            continue\n\n        all_owners = set()\n        for objattr in objects_and_attributes.values():\n            if not objattr['is_dependent']:\n                all_owners.add(objattr['owner'])\n\n        # If all objects have the same owner, we just need to do 'schema.*' and we're done\n        if len(all_owners) == 1:\n            owner = list(all_owners)[0]\n\n            if 'owns' not in spec[owner]:\n                spec[owner]['owns'] = {objkind: []}\n            elif objkind not in spec[owner]['owns']:\n                spec[owner]['owns'][objkind] = []\n\n            objname = common.ObjectName(schema=schema, unqualified_name='*')\n            spec[owner]['owns'][objkind].append(objname)\n\n            # Since all objects in this schema are owned by one role, we can skip the below\n            continue\n\n        for objname, objattr in objects_and_attributes.items():\n            # Skip dependent objects; their ownership is managed by the object they depend on\n            if objattr['is_dependent']:\n                continue\n\n            owner = objattr['owner']\n            if 'owns' not in spec[owner]:\n                spec[owner]['owns'] = {objkind: []}\n            elif objkind not in spec[owner]['owns']:\n                spec[owner]['owns'][objkind] = []\n\n            spec[owner]['owns'][objkind].append(objname)\n\n    return spec\n\n\ndef add_ownerships(spec, dbcontext):\n    for objkind in PRIVILEGE_MAP.keys():\n        if objkind == 'schemas':\n            spec = add_schema_ownerships(spec, dbcontext)\n        else:\n            spec = add_nonschema_ownerships(spec, dbcontext, objkind)\n    return spec\n\n\ndef add_privileges(spec, dbcontext):\n    \"\"\"\n    Add role privileges to the spec file\n    \"\"\"\n    for role in spec.keys():\n        role_privileges = {}\n        for objkind in PRIVILEGE_MAP.keys():\n            if objkind == 'schemas':\n                schemas_privs = determine_schema_privileges(role, dbcontext)\n                if schemas_privs:\n                    role_privileges['schemas'] = schemas_privs\n\n            else:\n                obj_privs = {}\n                writes, reads = determine_all_nonschema_privileges(role, objkind, dbcontext)\n\n                if writes:\n                    collapsed_writes = collapse_personal_schemas(role, writes, objkind, dbcontext)\n                    obj_privs['write'] = collapsed_writes\n\n                if reads:\n                    collapsed_reads = collapse_personal_schemas(role, reads, objkind, dbcontext)\n                    obj_privs['read'] = collapsed_reads\n\n                if obj_privs:\n                    role_privileges[objkind] = obj_privs\n\n        if role_privileges:\n            spec[role]['privileges'] = role_privileges\n\n    return spec\n\n\ndef collapse_personal_schemas(role, objects, objkind, dbcontext):\n    \"\"\" If all personal objects (e.g. 'roleA.*', 'roleB.*', etc.) are in\n    objects, then replace them with 'personal_schemas.*'. We only verify that non-empty personal\n    schemas are all present here. If so, we assume that the same behavior should be true for other\n    personal schemas if / when they are populated.\n\n    Note that this role's personal schema (if it exists) will not show up here at all as\n    determine_all_nonschema_privileges() filters it out. That is ok and intended: once\n    `pgbedrock configure` is run it will ensure that this role owns everything in its own\n    personal schema\n\n    Returns:\n        set: A set of common.ObjectName instances\n    \"\"\"\n    personal_schemas = dbcontext.get_all_personal_schemas()\n    personal_schemas_star = set([\n        common.ObjectName(schema=objname.schema, unqualified_name='*') for objname in personal_schemas\n    ])\n\n    if not personal_schemas_star:\n        return objects\n\n    non_empty_personal_schemas = set()\n    for objname in personal_schemas:\n        if objname.schema != role and not dbcontext.is_schema_empty(objname, objkind):\n            non_empty_personal_schemas.add(common.ObjectName(schema=objname.schema, unqualified_name='*'))\n\n    if non_empty_personal_schemas.difference(objects) == set():\n        objects.difference_update(personal_schemas_star)\n        objects.add(common.ObjectName(schema='personal_schemas', unqualified_name='*'))\n\n    return objects\n\n\ndef determine_schema_privileges(role, dbcontext):\n    \"\"\"\n    Identify and return two lists:\n        1) a list of schemas this role can write to\n        2) a list of schemas this role can read from\n\n    If a role can write it to a schema is assumed it can read as well, so that schema will only\n    show up in the write list. As a result of this, we determine schemas with write access first\n    so we can exclude write schemas from the read schemas list.\n\n    Note that in Postgres if you own a schema you will always have read (USAGE) access to that\n    schema but you _may not_ have write (CREATE) access to it, though you do by default. Because\n    this is confusing and non-intuitive, we will include schemas in the write section even if they\n    are owned by this role (and because they are in the write section then they will also be granted\n    read privileges as well).\n\n    Returns:\n        tuple: A tuple of (write privileges, read privileges), where each component in the tuple\n            is a set of common.ObjectName instances\n    \"\"\"\n    # Make a copy of personal_schemas (by making a new set from it) as we will be mutating it\n    personal_schemas = set(dbcontext.get_all_personal_schemas())\n\n    # Get all schemas this role has write and read access to\n    write_schemas_and_privs = dbcontext.get_role_current_nondefaults(role, 'schemas', 'write')\n    write_schemas = set([objname for objname, _ in write_schemas_and_privs])\n    read_schemas_and_privs = dbcontext.get_role_current_nondefaults(role, 'schemas', 'read')\n    read_schemas = set([objname for objname, _ in read_schemas_and_privs])\n\n    # Get all schemas owned by this role\n    all_owned_schemas = dbcontext.get_all_schemas_and_owners()\n    role_owned_schemas = {s for s, owner in all_owned_schemas.items() if owner == role}\n\n    # Add all schemas owned by this role to the write and read schemas\n    write_schemas.update(role_owned_schemas)\n    read_schemas.update(role_owned_schemas)\n\n    # Remove this role's personal schema if it exists\n    write_schemas.difference_update({common.ObjectName(role)})\n    read_schemas.difference_update({common.ObjectName(role)})\n    personal_schemas.difference_update({common.ObjectName(role)})\n\n    # If all personal schemas are in write_schemas then replace them with 'personal_schemas'\n    if personal_schemas and personal_schemas.difference(write_schemas) == set():\n        write_schemas.difference_update(personal_schemas)\n        write_schemas.add(common.ObjectName('personal_schemas'))\n\n    if personal_schemas and personal_schemas.difference(read_schemas) == set():\n        read_schemas.difference_update(personal_schemas)\n        read_schemas.add(common.ObjectName('personal_schemas'))\n\n    # Remove all schemas this role has write access to\n    read_only_schemas = read_schemas.difference(write_schemas)\n\n    schemas_privs = {}\n    if write_schemas:\n        schemas_privs['write'] = write_schemas\n    if read_only_schemas:\n        schemas_privs['read'] = read_only_schemas\n\n    return schemas_privs\n\n\ndef determine_all_nonschema_privileges(role, objkind, dbcontext):\n    all_writes = set()\n    all_reads = set()\n\n    for objname, owner in dbcontext.get_all_schemas_and_owners().items():\n        # Skip this role's personal schema as we will be asserting upon running\n        # pgbedrock that all objects in this schema are owned by this role\n        if role == objname.schema and role == owner:\n            continue\n\n        writes, reads = determine_nonschema_privileges_for_schema(role, objkind, objname,\n                                                                  dbcontext)\n\n        all_writes.update(writes)\n        all_reads.update(reads)\n\n    return all_writes, all_reads\n\n\ndef determine_nonschema_privileges_for_schema(role, objkind, objname, dbcontext):\n    \"\"\"\n    Determine all non-schema privileges granted to a given role for all objects of objkind in\n    the specified schema `objname`. Results will be returned as two sets: objects granted write\n    access and objects granted read access.\n\n    We explicitly start with writes because if a role has write access then pgbedrock will also\n    grant it read access, meaning we won't need to grant that object a read. As a result, we have\n    to start with writes first.\n\n    If any write default privileges exist for this role x objkind x schema then we assume that write\n    default privileges should be applied universally in this schema. This could definitely not be\n    the case though. For example, because default privileges are scoped based on the grantor, it's\n    possible that the role had write default privileges for things created by roleA but not for\n    things created by roleB. pgbedrock is not fine-grained enough to handle a situation like this\n    though, so if there is a write default privilege we just assume that this role should have\n    default writes for all objects of objkind in this schema.\n\n    Also note that we're lumping all write default privileges together. It is possible a role might\n    have default privileges for UPDATE on all tables but no other write-level default privileges.\n    Again, pgbedrock isn't meant to be this fine-grained: if it sees any write-level default\n    privilege then it will identify this as a role that should get write default privileges, which\n    means that this role will get _all_ write-level default privileges for that objkind in this\n    schema.\n\n    Args:\n        role (str)\n\n        objkind (str): The type of object. This must be one of the keys of\n            context.PRIVILEGE_MAP, e.g. 'schemas', 'tables', etc.\n\n        objname (common.ObjectName): The schema to determine non-schema privileges for\n\n        dbcontext (context.DatabaseContext): A context.DatabaseContext instance for getting\n            information for the associated database\n\n    Returns:\n        tuple: A tuple of with two items in it: a set of common.ObjectName instances with write\n            privileges and a set of common.ObjectName instances with read privileges\n    \"\"\"\n    # Get all objects of this objkind in this schema and which are not owned by this role\n    objects_and_owners = dbcontext.get_schema_objects(objname)\n    schema_objects = set()\n    for entry in objects_and_owners:\n        if entry.kind == objkind and entry.owner != role:\n            schema_objects.add(entry.objname)\n\n    has_default_write = dbcontext.has_default_privilege(role, objname, objkind, 'write')\n    all_writes = dbcontext.get_role_objects_with_access(role, objname, objkind, 'write')\n\n    if has_default_write or (all_writes == schema_objects and all_writes != set()):\n        # In the second condition, every object has a write privilege, so we assume that means\n        # that this role should have default write privileges\n        return set([common.ObjectName(schema=objname.schema, unqualified_name='*')]), set()\n\n    # If we haven't returned yet then no write default privilege exists; we will have to\n    # grant each write individually, meaning we also need to look at read privileges\n    has_default_read = dbcontext.has_default_privilege(role, objname, objkind, 'read')\n    all_reads = dbcontext.get_role_objects_with_access(role, objname, objkind, 'read')\n\n    if has_default_read or (all_reads == schema_objects and all_reads != set()):\n        # In the second condition, every object has a read privilege, so we assume that means\n        # that this role should have default read privileges\n        return all_writes, set([common.ObjectName(schema=objname.schema, unqualified_name='*')])\n    else:\n        # We have to grant each read individually as well. Because a write will already grant\n        # a read, we have to remove all write-granted objects from our read grants\n        only_reads = all_reads.difference(all_writes)\n        return all_writes, only_reads\n\n\ndef create_spec(host, port, user, password, dbname, verbose):\n    db_connection = common.get_db_connection(host, port, dbname, user, password)\n    # We will only be reading, so it is worth being safe here and ensuring that we can't write\n    db_connection.set_session(readonly=True)\n    cursor = db_connection.cursor(cursor_factory=psycopg2.extras.DictCursor)\n\n    dbcontext = DatabaseContext(cursor, verbose)\n    spec = initialize_spec(dbcontext)\n    spec = add_attributes(spec, dbcontext)\n    spec = add_memberships(spec, dbcontext)\n    spec = add_ownerships(spec, dbcontext)\n    spec = add_privileges(spec, dbcontext)\n\n    return spec\n\n\ndef initialize_spec(dbcontext):\n    \"\"\" Initialize the spec with each role having an empty dict \"\"\"\n    all_attributes = dbcontext.get_all_role_attributes()\n    spec = {role: dict() for role in all_attributes.keys()}\n    return spec\n\n\ndef nondefault_attributes_as_list(rolename, nondefaults):\n    results = []\n    for attr, val in nondefaults.items():\n        if attr == 'rolvaliduntil':\n            valid_until_date = str(val.date())\n            results.append(\"VALID UNTIL '{}'\".format(valid_until_date))\n        elif attr == 'rolconnlimit':\n            results.append('CONNECTION LIMIT {}'.format(val))\n        elif attr == 'rolpassword':\n            # Use underscores since tools like Bamboo do not like dashes in envvar names\n            envvar_rolename = rolename.replace('-', '_').upper()\n            # We put a templated environment variable here instead of rolpassword's val\n            # (which is just an md5 hash anyway)\n            results.append('PASSWORD \"{{{{ env[\\'{}_PASSWORD\\'] }}}}\"'.format(envvar_rolename))\n        else:\n            keyword = COLUMN_NAME_TO_KEYWORD[attr]\n            prefix = '' if val else 'NO'\n            results.append(prefix + keyword)\n\n    return results\n\n\n```\n\n### Code description\n```Python\n\"\"\"\nDescription: This function is used to output a specification in a formatted way using the yaml.dump API.\n\nArgs:\n    spec (type): The specification to be outputted.\n\nReturns:\n    None: This function does not return any value. It prints the formatted specification to the console.\n\"\"\"\n```\n\n### Code snippet:\n```Python\ndef output_spec(spec):\n\n    def represent_dict(dumper, data):\n        \"\"\"\n        Use '' for empty dicts. Based on Brice M. Dempsey's code here:\n            https://stackoverflow.com/questions/5121931/in-python-how-can-you-load-yaml-mappings-as-ordereddicts\n        \"\"\"\n        if data:\n            return dumper.represent_dict(data)\n```\n\n### Requirements:\n1. Complete only the missing part of the function.\n2. Only answer the code of the completed part. Do not include any of the provided code snippet.\n3. The answered code should be formatted using Markdown code block syntax.\n4. Do not include any additional text.\n\n### Answer: (use the provided format with backticks)\n", "name": "output_spec", "reference": "        return dumper.represent_scalar('tag:yaml.org,2002:null', '')\n\n    def represent_objname(dumper, data):\n        return dumper.represent_scalar('tag:yaml.org,2002:str', data.qualified_name)\n\n    FormattedDumper.add_representer(dict, represent_dict)\n    FormattedDumper.add_representer(common.ObjectName, represent_objname)\n\n    print(yaml.dump(spec, Dumper=FormattedDumper, default_flow_style=False, indent=4))", "completions": ["```markdown\n```python\ndef output_spec(spec):\n    \"\"\"\n    This function is used to output a specification in a formatted way using the yaml.dump API.\n\n    Args:\n        spec (dict): The specification to be outputted.\n\n    Returns:\n        None: This function does not return any value. It prints the formatted specification to the console.\n    \"\"\"\n\n    def represent_dict(dumper, data):\n        \"\"\"\n        Use '' for empty dicts. Based on Brice M. Dempsey's code here:\n            https://stackoverflow.com/questions/5121931/in-python-how-can-you-load-yaml-mappings-as-ordereddicts\n        \"\"\"\n        if data:\n            return dumper.represent_dict(data)\n\n    yaml.add_representer(dict, represent_dict)\n    yaml.dump(spec, default_flow_style=False)\n```\n```\n"]}
{"id": 8, "prompt": "<\uff5cbegin\u2581of\u2581sentence\uff5c>### Role:\nYou are an experienced Python programmer assistant responsible for code completion tasks. Please intelligently complete the missing parts of the provided code snippet.\n\n### Task:\nBased on the provided code snippet, complete the missing function implementation. Ensure the code is logically correct, syntactically accurate, and follows best coding practices.\n\n### Left context\n```Python\n\"\"\"Functions to interact/analyze with numpy dtypes.\"\"\"\nfrom __future__ import print_function, division\n\nimport numpy as np\nimport six.moves as sm\n\nimport imgaug as ia\n\nKIND_TO_DTYPES = {\n    \"i\": [\"int8\", \"int16\", \"int32\", \"int64\"],\n    \"u\": [\"uint8\", \"uint16\", \"uint32\", \"uint64\"],\n    \"b\": [\"bool\"],\n    \"f\": [\"float16\", \"float32\", \"float64\", \"float128\"]\n}\n\n\ndef normalize_dtypes(dtypes):\n    if not isinstance(dtypes, list):\n        return [normalize_dtype(dtypes)]\n    return [normalize_dtype(dtype) for dtype in dtypes]\n\n\ndef normalize_dtype(dtype):\n    assert not isinstance(dtype, list), (\n        \"Expected a single dtype-like, got a list instead.\")\n    return (\n        dtype.dtype\n        if ia.is_np_array(dtype) or ia.is_np_scalar(dtype)\n        else np.dtype(dtype)\n    )\n\n\ndef change_dtype_(arr, dtype, clip=True, round=True):\n    # pylint: disable=redefined-builtin\n    assert ia.is_np_array(arr), (\n        \"Expected array as input, got type %s.\" % (type(arr),))\n    dtype = normalize_dtype(dtype)\n\n    if arr.dtype.name == dtype.name:\n        return arr\n\n    if round and arr.dtype.kind == \"f\" and dtype.kind in [\"u\", \"i\", \"b\"]:\n        arr = np.round(arr)\n\n    if clip:\n        min_value, _, max_value = get_value_range_of_dtype(dtype)\n        arr = clip_(arr, min_value, max_value)\n\n    return arr.astype(dtype, copy=False)\n\n\ndef change_dtypes_(images, dtypes, clip=True, round=True):\n    # pylint: disable=redefined-builtin\n    if ia.is_np_array(images):\n        if ia.is_iterable(dtypes):\n            dtypes = normalize_dtypes(dtypes)\n            n_distinct_dtypes = len({dt.name for dt in dtypes})\n            assert len(dtypes) == len(images), (\n                \"If an iterable of dtypes is provided to \"\n                \"change_dtypes_(), it must contain as many dtypes as \"\n                \"there are images. Got %d dtypes and %d images.\" % (\n                    len(dtypes), len(images))\n            )\n\n            assert n_distinct_dtypes == 1, (\n                \"If an image array is provided to change_dtypes_(), the \"\n                \"provided 'dtypes' argument must either be a single dtype \"\n                \"or an iterable of N times the *same* dtype for N images. \"\n                \"Got %d distinct dtypes.\" % (n_distinct_dtypes,)\n            )\n\n            dtype = dtypes[0]\n        else:\n            dtype = normalize_dtype(dtypes)\n\n        result = change_dtype_(images, dtype, clip=clip, round=round)\n    elif ia.is_iterable(images):\n        dtypes = (\n            [normalize_dtype(dtypes)] * len(images)\n            if not isinstance(dtypes, list)\n            else normalize_dtypes(dtypes)\n        )\n        assert len(images) == len(dtypes), (\n            \"Expected the provided images and dtypes to match, but got \"\n            \"iterables of size %d (images) %d (dtypes).\" % (\n                len(images), len(dtypes)))\n\n        result = images\n        for i, (image, dtype) in enumerate(zip(images, dtypes)):\n            assert ia.is_np_array(image), (\n                \"Expected each image to be an ndarray, got type %s \"\n                \"instead.\" % (type(image),))\n            result[i] = change_dtype_(image, dtype, clip=clip, round=round)\n    else:\n        raise Exception(\"Expected numpy array or iterable of numpy arrays, \"\n                        \"got type '%s'.\" % (type(images),))\n    return result\n\n\n# TODO replace this everywhere in the library with change_dtypes_\n# TODO mark as deprecated\ndef restore_dtypes_(images, dtypes, clip=True, round=True):\n    # pylint: disable=redefined-builtin\n    return change_dtypes_(images, dtypes, clip=clip, round=round)\n\n\ndef copy_dtypes_for_restore(images, force_list=False):\n    if ia.is_np_array(images):\n        if force_list:\n            return [images.dtype for _ in sm.xrange(len(images))]\n        return images.dtype\n    return [image.dtype for image in images]\n\n\n```\n\n### Code description\n```Python\n\"\"\"\nDescription: This function increases the itemsize of a numpy dtype by a specified factor.\n\nArgs:\n    dtype (numpy.dtype): The numpy dtype to increase the itemsize of.\n    factor (int): The factor by which to increase the itemsize. Must be one of 1, 2, 4, or 8.\n\nReturns:\n    numpy.dtype: The numpy dtype with increased itemsize.\n\nRaises:\n    AssertionError: If the factor is not an integer or not one of 1, 2, 4, or 8, or if the dtype is boolean.\n    TypeError: If the increased itemsize is not a valid numpy dtype.\n\"\"\"\n```\n\n### Code snippet:\n```Python\n\ndef increase_itemsize_of_dtype(dtype, factor):\n    dtype = normalize_dtype(dtype)\n\n    assert ia.is_single_integer(factor), (\n        \"Expected 'factor' to be an integer, got type %s instead.\" % (\n            type(factor),))\n    # int8 -> int64 = factor 8\n    # uint8 -> uint64 = factor 8\n    # float16 -> float128 = factor 8\n    assert factor in [1, 2, 4, 8], (\n        \"The itemsize may only be increased any of the following factors: \"\n        \"1, 2, 4 or 8. Got factor %d.\" % (factor,))\n    assert dtype.kind != \"b\", \"Cannot increase the itemsize of boolean.\"\n\n    dt_high_name = \"%s%d\" % (dtype.kind, dtype.itemsize * factor)\n```\n\n### Requirements:\n1. Complete only the missing part of the function.\n2. Only answer the code of the completed part. Do not include any of the provided code snippet.\n3. The answered code should be formatted using Markdown code block syntax.\n4. Do not include any additional text.\n\n### Answer: (use the provided format with backticks)\n", "name": "increase_itemsize_of_dtype", "reference": "\n    try:\n        dt_high = np.dtype(dt_high_name)\n        return dt_high\n    except TypeError:\n        raise TypeError(\n            \"Unable to create a numpy dtype matching the name '%s'. \"\n            \"This error was caused when trying to find a dtype \"\n            \"that increases the itemsize of dtype '%s' by a factor of %d.\"\n            \"This error can be avoided by choosing arrays with lower \"\n            \"resolution dtypes as inputs, e.g. by reducing \"\n            \"float32 to float16.\" % (\n                dt_high_name,\n                dtype.name,\n                factor\n            )\n        )", "completions": ["```markdown\n```python\ndef increase_itemsize_of_dtype(dtype, factor):\n    dtype = normalize_dtype(dtype)\n\n    assert ia.is_single_integer(factor), (\n        \"Expected 'factor' to be an integer, got type %s instead.\" % (\n            type(factor),))\n    # int8 -> int64 = factor 8\n    # uint8 -> uint64 = factor 8\n    # float16 -> float128 = factor 8\n    assert factor in [1, 2, 4, 8], (\n        \"The itemsize may only be increased any of the following factors: \"\n        \"1, 2, 4 or 8. Got factor %d.\" % (factor,))\n    assert dtype.kind != \"b\", \"Cannot increase the itemsize of boolean.\"\n\n    dt_high_name = \"%s%d\" % (dtype.kind, dtype.itemsize * factor)\n    return np.dtype(dt_high_name)\n```\n```\n"]}
{"id": 9, "prompt": "<\uff5cbegin\u2581of\u2581sentence\uff5c>### Role:\nYou are an experienced Python programmer assistant responsible for code completion tasks. Please intelligently complete the missing parts of the provided code snippet.\n\n### Task:\nBased on the provided code snippet, complete the missing function implementation. Ensure the code is logically correct, syntactically accurate, and follows best coding practices.\n\n### Left context\n```Python\n# -*- coding: utf-8 -*-\n\"\"\"\nchecks.py\n\nEach function in here should\n\n- Take a DataFrame as its first argument, maybe optional arguments\n- Makes its assert on the result\n- Return the original DataFrame\n\"\"\"\nimport numpy as np\nimport pandas as pd\nimport pandas.util.testing as tm\nimport six\n\nfrom engarde import generic\nfrom engarde.generic import verify, verify_all, verify_any\n\n\ndef none_missing(df, columns=None):\n    \"\"\"\n    Asserts that there are no missing values (NaNs) in the DataFrame.\n\n    Parameters\n    ----------\n    df : DataFrame\n    columns : list\n      list of columns to restrict the check to\n\n    Returns\n    -------\n    df : DataFrame\n      same as the original\n    \"\"\"\n    if columns is None:\n        columns = df.columns\n    try:\n        assert not df[columns].isnull().any().any()\n    except AssertionError as e:\n        missing = df[columns].isnull()\n        msg = generic.bad_locations(missing)\n        e.args = msg\n        raise\n    return df\n\ndef is_monotonic(df, items=None, increasing=None, strict=False):\n    \"\"\"\n    Asserts that the DataFrame is monotonic.\n\n    Parameters\n    ==========\n\n    df : Series or DataFrame\n    items : dict\n        mapping columns to conditions (increasing, strict)\n    increasing : None or bool\n        None is either increasing or decreasing.\n    strict : whether the comparison should be strict\n\n    Returns\n    =======\n    df : DataFrame\n    \"\"\"\n    if items is None:\n        items = {k: (increasing, strict) for k in df}\n\n    for col, (increasing, strict) in items.items():\n        s = pd.Index(df[col])\n        if increasing:\n            good = getattr(s, 'is_monotonic_increasing')\n        elif increasing is None:\n            good = getattr(s, 'is_monotonic') | getattr(s, 'is_monotonic_decreasing')\n        else:\n            good = getattr(s, 'is_monotonic_decreasing')\n        if strict:\n            if increasing:\n                good = good & (s.to_series().diff().dropna() > 0).all()\n            elif increasing is None:\n                good = good & ((s.to_series().diff().dropna() > 0).all() |\n                               (s.to_series().diff().dropna() < 0).all())\n            else:\n                good = good & (s.to_series().diff().dropna() < 0).all()\n        if not good:\n            raise AssertionError\n    return df\n\ndef is_shape(df, shape):\n    \"\"\"\n    Asserts that the DataFrame is of a known shape.\n\n    Parameters\n    ==========\n\n    df : DataFrame\n    shape : tuple\n      (n_rows, n_columns). Use None or -1 if you don't care\n      about a dimension.\n\n    Returns\n    =======\n    df : DataFrame\n    \"\"\"\n    try:\n        check = np.all(np.equal(df.shape, shape) | (np.equal(shape, [-1, -1]) |\n                                                    np.equal(shape, [None, None])))\n        assert check\n    except AssertionError as e:\n        msg = (\"Expected shape: {}\\n\"\n               \"\\t\\tActual shape:   {}\".format(shape, df.shape))\n        e.args = (msg,)\n        raise\n    return df\n\n\ndef unique(df, columns=None):\n    \"\"\"\n    Asserts that columns in the DataFrame only have unique values.\n\n    Parameters\n    ----------\n    df : DataFrame\n    columns : list\n      list of columns to restrict the check to. If None, check all columns.\n\n    Returns\n    -------\n    df : DataFrame\n      same as the original\n    \"\"\"\n    if columns is None:\n        columns = df.columns\n    for col in columns:\n        if not df[col].is_unique:\n            raise AssertionError(\"Column {!r} contains non-unique values\".format(col))\n    return df\n\n\ndef unique_index(df):\n    \"\"\"\n    Assert that the index is unique\n\n    Parameters\n    ==========\n    df : DataFrame\n\n    Returns\n    =======\n    df : DataFrame\n    \"\"\"\n    try:\n        assert df.index.is_unique\n    except AssertionError as e:\n        e.args = df.index.get_duplicates()\n        raise\n    return df\n\n\ndef within_set(df, items=None):\n    \"\"\"\n    Assert that df is a subset of items\n\n    Parameters\n    ==========\n    df : DataFrame\n    items : dict\n      mapping of columns (k) to array-like of values (v) that\n      ``df[k]`` is expected to be a subset of\n\n    Returns\n    =======\n    df : DataFrame\n    \"\"\"\n    for k, v in items.items():\n        if not df[k].isin(v).all():\n            bad = df.loc[~df[k].isin(v), k]\n            raise AssertionError('Not in set', bad)\n    return df\n\ndef within_range(df, items=None):\n    \"\"\"\n    Assert that a DataFrame is within a range.\n\n    Parameters\n    ==========\n    df : DataFame\n    items : dict\n      mapping of columns (k) to a (low, high) tuple (v)\n      that ``df[k]`` is expected to be between.\n\n    Returns\n    =======\n    df : DataFrame\n    \"\"\"\n    for k, (lower, upper) in items.items():\n        if (lower > df[k]).any() or (upper < df[k]).any():\n            bad = (lower > df[k]) | (upper < df[k])\n            raise AssertionError(\"Outside range\", bad)\n    return df\n\ndef within_n_std(df, n=3):\n    \"\"\"\n    Assert that every value is within ``n`` standard\n    deviations of its column's mean.\n\n    Parameters\n    ==========\n    df : DataFame\n    n : int\n      number of standard deviations from the mean\n\n    Returns\n    =======\n    df : DataFrame\n    \"\"\"\n    means = df.mean()\n    stds = df.std()\n    inliers = (np.abs(df[means.index] - means) < n * stds)\n    if not np.all(inliers):\n        msg = generic.bad_locations(~inliers)\n        raise AssertionError(msg)\n    return df\n\ndef has_dtypes(df, items):\n    \"\"\"\n    Assert that a DataFrame has ``dtypes``\n\n    Parameters\n    ==========\n    df: DataFrame\n    items: dict\n      mapping of columns to dtype.\n\n    Returns\n    =======\n    df : DataFrame\n    \"\"\"\n    dtypes = df.dtypes\n    for k, v in items.items():\n        if not dtypes[k] == v:\n            raise AssertionError(\"{} has the wrong dtype ({})\".format(k, v))\n    return df\n\n\ndef one_to_many(df, unitcol, manycol):\n    \"\"\"\n    Assert that a many-to-one relationship is preserved between two\n    columns. For example, a retail store will have have distinct\n    departments, each with several employees. If each employee may\n    only work in a single department, then the relationship of the\n    department to the employees is one to many.\n\n    Parameters\n    ==========\n    df : DataFrame\n    unitcol : str\n        The column that encapulates the groups in ``manycol``.\n    manycol : str\n        The column that must remain unique in the distict pairs\n        between ``manycol`` and ``unitcol``\n\n    Returns\n    =======\n    df : DataFrame\n\n    \"\"\"\n    subset = df[[manycol, unitcol]].drop_duplicates()\n    for many in subset[manycol].unique():\n        if subset[subset[manycol] == many].shape[0] > 1:\n            msg = \"{} in {} has multiple values for {}\".format(many, manycol, unitcol)\n            raise AssertionError(msg)\n\n    return df\n\n\n```\n\n### Code description\n```Python\n\"\"\"\nDescription: This function compares two pandas DataFrames and raises an AssertionError if they are not equal.\n\nArgs:\n    df (pandas.DataFrame): The first DataFrame to compare.\n    df_to_compare (pandas.DataFrame): The second DataFrame to compare.\n    **kwargs: Additional arguments to pass to pandas.util.testing.assert_frame_equal.\n\nReturns:\n    pandas.DataFrame: The first DataFrame if it is equal to the second DataFrame.\n\"\"\"\n```\n\n### Code snippet:\n```Python\ndef is_same_as(df, df_to_compare, **kwargs):\n    try:\n        tm.assert_frame_equal(df, df_to_compare, **kwargs)\n```\n\n### Requirements:\n1. Complete only the missing part of the function.\n2. Only answer the code of the completed part. Do not include any of the provided code snippet.\n3. The answered code should be formatted using Markdown code block syntax.\n4. Do not include any additional text.\n\n### Answer: (use the provided format with backticks)\n", "name": "is_same_as", "reference": "    except AssertionError as exc:\n        six.raise_from(AssertionError(\"DataFrames are not equal\"), exc)\n    return df", "completions": ["```python\ndef is_same_as(df, df_to_compare, **kwargs):\n    \"\"\"\n    Asserts that the DataFrame is the same as the DataFrame provided.\n\n    Parameters\n    ==========\n    df : DataFrame\n    df_to_compare : DataFrame\n      The DataFrame to compare with\n    **kwargs: Additional arguments to pass to pandas.util.testing.assert_frame_equal.\n\n    Returns\n    =======\n    df : DataFrame\n      The original DataFrame if it is the same as the DataFrame provided.\n    \"\"\"\n    try:\n        tm.assert_frame_equal(df, df_to_compare, **kwargs)\n    except AssertionError as e:\n        msg = str(e)\n        raise AssertionError(msg)\n    return df\n```\n"]}
{"id": 10, "prompt": "<\uff5cbegin\u2581of\u2581sentence\uff5c>### Role:\nYou are an experienced Python programmer assistant responsible for code completion tasks. Please intelligently complete the missing parts of the provided code snippet.\n\n### Task:\nBased on the provided code snippet, complete the missing function implementation. Ensure the code is logically correct, syntactically accurate, and follows best coding practices.\n\n### Left context\n```Python\n# coding: utf-8\n# Author: Axel ARONIO DE ROMBLAY <axelderomblay@gmail.com>\n# License: BSD 3 clause\n\nimport numpy as np\nimport pandas as pd\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nimport warnings\n\n\nclass Clf_feature_selector():\n\n    \"\"\"Selects useful features.\n\n    Several strategies are possible (filter and wrapper methods).\n    Works for classification problems only (multiclass or binary).\n\n    Parameters\n    ----------\n    strategy : str, defaut = \"l1\"\n        The strategy to select features.\n        Available strategies = {\"variance\", \"l1\", \"rf_feature_importance\"}\n\n    threshold : float, defaut = 0.3\n        The percentage of variable to discard according to the strategy.\n        Must be between 0. and 1.\n    \"\"\"\n\n    def __init__(self, strategy='l1', threshold=0.3):\n\n        # 'variance','l1, 'rf_feature_importance'\n        self.strategy = strategy\n        # a float between 0. and 1. defaut : 0.3 ie we drop 0.3 of features\n        self.threshold = threshold\n        self.__fitOK = False\n        self.__to_discard = []\n\n\n    def get_params(self, deep=True):\n\n        return {'strategy': self.strategy,\n                'threshold': self.threshold}\n\n\n    def set_params(self, **params):\n\n        self.__fitOK = False\n\n        for k, v in params.items():\n            if k not in self.get_params():\n                warnings.warn(\"Invalid parameter a for feature selector\"\n                              \"Clf_feature_selector. Parameter IGNORED. Check\"\n                              \"the list of available parameters with\"\n                              \"`feature_selector.get_params().keys()`\")\n            else:\n                setattr(self, k, v)\n\n\n```\n\n### Code description\n```Python\n\"\"\"\nDescription: This function fits a model based on the specified strategy.\n\nArgs:\n    df_train (pd.SparseDataFrame or pd.DataFrame): The training dataframe.\n    y_train (pd.core.series.Series): The training labels.\n\nReturns:\n    self: The fitted model.\n\nRaises:\n    ValueError: If df_train is not a DataFrame or a SparseDataFrame, or if y_train is not a Series.\n    ValueError: If the strategy is not 'variance', 'l1', or 'rf_feature_importance'.\n\nNotes:\n    The function uses numpy.percentile to calculate the threshold for feature selection.\n    The strategies 'variance', 'l1', and 'rf_feature_importance' are used to select features.\n    The 'variance' strategy selects features based on the standard deviation of their values.\n    The 'l1' strategy selects features based on the absolute mean coefficient of a logistic regression model.\n    The 'rf_feature_importance' strategy selects features based on the feature importances of a random forest model.\n\"\"\"\n```\n\n### Code snippet:\n```Python\ndef fit(self, df_train, y_train):\n\n\n    # sanity checks\n    if((type(df_train) != pd.SparseDataFrame) and\n       (type(df_train) != pd.DataFrame)):\n        raise ValueError(\"df_train must be a DataFrame\")\n\n    if (type(y_train) != pd.core.series.Series):\n        raise ValueError(\"y_train must be a Series\")\n\n    if(self.strategy == 'variance'):\n        coef = df_train.std()\n        abstract_threshold = np.percentile(coef, 100. * self.threshold)\n        self.__to_discard = coef[coef < abstract_threshold].index\n        self.__fitOK = True\n\n    elif(self.strategy == 'l1'):\n        model = LogisticRegression(C=0.01, penalty='l1', n_jobs=-1,\n                                   random_state=0)  # to be tuned\n```\n\n### Requirements:\n1. Complete only the missing part of the function.\n2. Only answer the code of the completed part. Do not include any of the provided code snippet.\n3. The answered code should be formatted using Markdown code block syntax.\n4. Do not include any additional text.\n\n### Answer: (use the provided format with backticks)\n", "name": "fit", "reference": "        model.fit(df_train, y_train)\n        coef = np.mean(np.abs(model.coef_), axis=0)\n        abstract_threshold = np.percentile(coef, 100. * self.threshold)\n        self.__to_discard = df_train.columns[coef < abstract_threshold]\n        self.__fitOK = True\n\n    elif(self.strategy == 'rf_feature_importance'):\n        model = RandomForestClassifier(n_estimators=50, n_jobs=-1,\n                                       random_state=0)  # to be tuned\n        model.fit(df_train, y_train)\n        coef = model.feature_importances_\n        abstract_threshold = np.percentile(coef, 100. * self.threshold)\n        self.__to_discard = df_train.columns[coef < abstract_threshold]\n        self.__fitOK = True\n\n    else:\n        raise ValueError(\"Strategy invalid. Please choose between \"\n                         \"'variance', 'l1' or 'rf_feature_importance'\")\n\n    return self", "completions": ["```python\n```python\n        model.fit(df_train, y_train)\n        coef = model.coef_[0]\n        abstract_threshold = np.percentile(np.abs(coef), 100. * self.threshold)\n        self.__to_discard = coef[np.abs(coef) < abstract_threshold].index\n        self.__fitOK = True\n\n    elif(self.strategy == 'rf_feature_importance'):\n        model = RandomForestClassifier(n_estimators=100, random_state=0)\n        model.fit(df_train, y_train)\n        self.__to_discard = model.feature_importances_[model.feature_importances_ < np.percentile(model.feature_importances_, 100. * self.threshold)].index\n        self.__fitOK = True\n\n    else:\n        raise ValueError(\"Invalid strategy. Available strategies are 'variance', 'l1', and 'rf_feature_importance'\")\n\n    return self\n```\n```\n"]}
{"id": 11, "prompt": "<\uff5cbegin\u2581of\u2581sentence\uff5c>### Role:\nYou are an experienced Python programmer assistant responsible for code completion tasks. Please intelligently complete the missing parts of the provided code snippet.\n\n### Task:\nBased on the provided code snippet, complete the missing function implementation. Ensure the code is logically correct, syntactically accurate, and follows best coding practices.\n\n### Left context\n```Python\n\"\"\"Classes dealing with polygons.\"\"\"\nfrom __future__ import print_function, division, absolute_import\n\nimport traceback\nimport collections\n\nimport numpy as np\nimport scipy.spatial.distance\nimport six.moves as sm\nimport skimage.draw\nimport skimage.measure\n\nfrom .. import imgaug as ia\nfrom .. import random as iarandom\nfrom .base import IAugmentable\nfrom .utils import (normalize_shape,\n                    interpolate_points,\n                    _remove_out_of_image_fraction_,\n                    project_coords_,\n                    _normalize_shift_args)\n\n\ndef recover_psois_(psois, psois_orig, recoverer, random_state):\n    \"\"\"Apply a polygon recoverer to input polygons in-place.\n\n    Parameters\n    ----------\n    psois : list of imgaug.augmentables.polys.PolygonsOnImage or imgaug.augmentables.polys.PolygonsOnImage\n        The possibly broken polygons, e.g. after augmentation.\n        The `recoverer` is applied to them.\n\n    psois_orig : list of imgaug.augmentables.polys.PolygonsOnImage or imgaug.augmentables.polys.PolygonsOnImage\n        Original polygons that were later changed to `psois`.\n        They are an extra input to `recoverer`.\n\n    recoverer : imgaug.augmentables.polys._ConcavePolygonRecoverer\n        The polygon recoverer used to repair broken input polygons.\n\n    random_state : None or int or RNG or numpy.random.Generator or numpy.random.BitGenerator or numpy.random.SeedSequence or numpy.random.RandomState\n        An RNG to use during the polygon recovery.\n\n    Returns\n    -------\n    list of imgaug.augmentables.polys.PolygonsOnImage or imgaug.augmentables.polys.PolygonsOnImage\n        List of repaired polygons. Note that this is `psois`, which was\n        changed in-place.\n\n    \"\"\"\n    input_was_list = True\n    if not isinstance(psois, list):\n        input_was_list = False\n        psois = [psois]\n        psois_orig = [psois_orig]\n\n    for i, psoi in enumerate(psois):\n        for j, polygon in enumerate(psoi.polygons):\n            poly_rec = recoverer.recover_from(\n                polygon.exterior, psois_orig[i].polygons[j],\n                random_state)\n\n            # Don't write into `polygon.exterior[...] = ...` because the\n            # shapes might have changed. We could also first check if the\n            # shapes are identical and only then write in-place, but as the\n            # array for `poly_rec.exterior` was already created, that would\n            # not provide any benefits.\n            polygon.exterior = poly_rec.exterior\n\n    if not input_was_list:\n        return psois[0]\n    return psois\n\n\n# TODO somehow merge with BoundingBox\n# TODO add functions: simplify() (eg via shapely.ops.simplify()),\n# extend(all_sides=0, top=0, right=0, bottom=0, left=0),\n# intersection(other, default=None), union(other), iou(other), to_heatmap, to_mask\nclass Polygon(object):\n    \"\"\"Class representing polygons.\n\n    Each polygon is parameterized by its corner points, given as absolute\n    x- and y-coordinates with sub-pixel accuracy.\n\n    Parameters\n    ----------\n    exterior : list of imgaug.augmentables.kps.Keypoint or list of tuple of float or (N,2) ndarray\n        List of points defining the polygon. May be either a ``list`` of\n        :class:`~imgaug.augmentables.kps.Keypoint` objects or a ``list`` of\n        ``tuple`` s in xy-form or a numpy array of shape (N,2) for ``N``\n        points in xy-form.\n        All coordinates are expected to be the absolute subpixel-coordinates\n        on the image, given as ``float`` s, e.g. ``x=10.7`` and ``y=3.4`` for a\n        point at coordinates ``(10.7, 3.4)``. Their order is expected to be\n        clock-wise. They are expected to not be closed (i.e. first and last\n        coordinate differ).\n\n    label : None or str, optional\n        Label of the polygon, e.g. a string representing the class.\n\n    \"\"\"\n\n    def __init__(self, exterior, label=None):\n        \"\"\"Create a new Polygon instance.\"\"\"\n        # TODO get rid of this deferred import\n        from imgaug.augmentables.kps import Keypoint\n\n        if isinstance(exterior, list):\n            if not exterior:\n                # for empty lists, make sure that the shape is (0, 2) and\n                # not (0,) as that is also expected when the input is a numpy\n                # array\n                self.exterior = np.zeros((0, 2), dtype=np.float32)\n            elif isinstance(exterior[0], Keypoint):\n                # list of Keypoint\n                self.exterior = np.float32([[point.x, point.y]\n                                            for point in exterior])\n            else:\n                # list of tuples (x, y)\n                # TODO just np.float32(exterior) here?\n                self.exterior = np.float32([[point[0], point[1]]\n                                            for point in exterior])\n        else:\n            assert ia.is_np_array(exterior), (\n                \"Expected exterior to be a list of tuples (x, y) or \"\n                \"an (N, 2) array, got type %s\" % (exterior,))\n            assert exterior.ndim == 2 and exterior.shape[1] == 2, (\n                \"Expected exterior to be a list of tuples (x, y) or \"\n                \"an (N, 2) array, got an array of shape %s\" % (\n                    exterior.shape,))\n            # TODO deal with int inputs here?\n            self.exterior = np.float32(exterior)\n\n        # Remove last point if it is essentially the same as the first\n        # point (polygons are always assumed to be closed anyways). This also\n        # prevents problems with shapely, which seems to add the last point\n        # automatically.\n        is_closed = (\n            len(self.exterior) >= 2\n            and np.allclose(self.exterior[0, :], self.exterior[-1, :]))\n        if is_closed:\n            self.exterior = self.exterior[:-1]\n\n        self.label = label\n\n    @property\n    def coords(self):\n        \"\"\"Alias for attribute ``exterior``.\n\n        Added in 0.4.0.\n\n        Returns\n        -------\n        ndarray\n            An ``(N, 2)`` ``float32`` ndarray containing the coordinates of\n            this polygon. This identical to the attribute ``exterior``.\n\n        \"\"\"\n        return self.exterior\n\n    @property\n    def xx(self):\n        \"\"\"Get the x-coordinates of all points on the exterior.\n\n        Returns\n        -------\n        (N,2) ndarray\n            ``float32`` x-coordinates array of all points on the exterior.\n\n        \"\"\"\n        return self.exterior[:, 0]\n\n    @property\n    def yy(self):\n        \"\"\"Get the y-coordinates of all points on the exterior.\n\n        Returns\n        -------\n        (N,2) ndarray\n            ``float32`` y-coordinates array of all points on the exterior.\n\n        \"\"\"\n        return self.exterior[:, 1]\n\n    @property\n    def xx_int(self):\n        \"\"\"Get the discretized x-coordinates of all points on the exterior.\n\n        The conversion from ``float32`` coordinates to ``int32`` is done\n        by first rounding the coordinates to the closest integer and then\n        removing everything after the decimal point.\n\n        Returns\n        -------\n        (N,2) ndarray\n            ``int32`` x-coordinates of all points on the exterior.\n\n        \"\"\"\n        return np.int32(np.round(self.xx))\n\n    @property\n    def yy_int(self):\n        \"\"\"Get the discretized y-coordinates of all points on the exterior.\n\n        The conversion from ``float32`` coordinates to ``int32`` is done\n        by first rounding the coordinates to the closest integer and then\n        removing everything after the decimal point.\n\n        Returns\n        -------\n        (N,2) ndarray\n            ``int32`` y-coordinates of all points on the exterior.\n\n        \"\"\"\n        return np.int32(np.round(self.yy))\n\n    @property\n    def is_valid(self):\n        \"\"\"Estimate whether the polygon has a valid geometry.\n\n        To to be considered valid, the polygon must be made up of at\n        least ``3`` points and have a concave shape, i.e. line segments may\n        not intersect or overlap. Multiple consecutive points are allowed to\n        have the same coordinates.\n\n        Returns\n        -------\n        bool\n            ``True`` if polygon has at least ``3`` points and is concave,\n            otherwise ``False``.\n\n        \"\"\"\n        if len(self.exterior) < 3:\n            return False\n        return self.to_shapely_polygon().is_valid\n\n    @property\n    def area(self):\n        \"\"\"Compute the area of the polygon.\n\n        Returns\n        -------\n        number\n            Area of the polygon.\n\n        \"\"\"\n        if len(self.exterior) < 3:\n            return 0.0\n        poly = self.to_shapely_polygon()\n        return poly.area\n\n    @property\n    def height(self):\n        \"\"\"Compute the height of a bounding box encapsulating the polygon.\n\n        The height is computed based on the two exterior coordinates with\n        lowest and largest x-coordinates.\n\n        Returns\n        -------\n        number\n            Height of the polygon.\n\n        \"\"\"\n        yy = self.yy\n        return max(yy) - min(yy)\n\n    @property\n    def width(self):\n        \"\"\"Compute the width of a bounding box encapsulating the polygon.\n\n        The width is computed based on the two exterior coordinates with\n        lowest and largest x-coordinates.\n\n        Returns\n        -------\n        number\n            Width of the polygon.\n\n        \"\"\"\n        xx = self.xx\n        return max(xx) - min(xx)\n\n    def project_(self, from_shape, to_shape):\n        \"\"\"Project the polygon onto an image with different shape in-place.\n\n        The relative coordinates of all points remain the same.\n        E.g. a point at ``(x=20, y=20)`` on an image\n        ``(width=100, height=200)`` will be projected on a new\n        image ``(width=200, height=100)`` to ``(x=40, y=10)``.\n\n        This is intended for cases where the original image is resized.\n        It cannot be used for more complex changes (e.g. padding, cropping).\n\n        Added in 0.4.0.\n\n        Parameters\n        ----------\n        from_shape : tuple of int\n            Shape of the original image. (Before resize.)\n\n        to_shape : tuple of int\n            Shape of the new image. (After resize.)\n\n        Returns\n        -------\n        imgaug.augmentables.polys.Polygon\n            Polygon object with new coordinates.\n            The object may have been modified in-place.\n\n        \"\"\"\n        self.exterior = project_coords_(self.coords, from_shape, to_shape)\n        return self\n\n    def project(self, from_shape, to_shape):\n        \"\"\"Project the polygon onto an image with different shape.\n\n        The relative coordinates of all points remain the same.\n        E.g. a point at ``(x=20, y=20)`` on an image\n        ``(width=100, height=200)`` will be projected on a new\n        image ``(width=200, height=100)`` to ``(x=40, y=10)``.\n\n        This is intended for cases where the original image is resized.\n        It cannot be used for more complex changes (e.g. padding, cropping).\n\n        Parameters\n        ----------\n        from_shape : tuple of int\n            Shape of the original image. (Before resize.)\n\n        to_shape : tuple of int\n            Shape of the new image. (After resize.)\n\n        Returns\n        -------\n        imgaug.augmentables.polys.Polygon\n            Polygon object with new coordinates.\n\n        \"\"\"\n        return self.deepcopy().project_(from_shape, to_shape)\n\n    def find_closest_point_index(self, x, y, return_distance=False):\n        \"\"\"Find the index of the exterior point closest to given coordinates.\n\n        \"Closeness\" is here defined based on euclidean distance.\n        This method will raise an ``AssertionError`` if the exterior contains\n        no points.\n\n        Parameters\n        ----------\n        x : number\n            X-coordinate around which to search for close points.\n\n        y : number\n            Y-coordinate around which to search for close points.\n\n        return_distance : bool, optional\n            Whether to also return the distance of the closest point.\n\n        Returns\n        -------\n        int\n            Index of the closest point.\n\n        number\n            Euclidean distance to the closest point.\n            This value is only returned if `return_distance` was set\n            to ``True``.\n\n        \"\"\"\n        assert len(self.exterior) > 0, (\n            \"Cannot find the closest point on a polygon which's exterior \"\n            \"contains no points.\")\n        distances = []\n        for x2, y2 in self.exterior:\n            dist = (x2 - x) ** 2 + (y2 - y) ** 2\n            distances.append(dist)\n        distances = np.sqrt(distances)\n        closest_idx = np.argmin(distances)\n        if return_distance:\n            return closest_idx, distances[closest_idx]\n        return closest_idx\n\n    def compute_out_of_image_area(self, image):\n        \"\"\"Compute the area of the BB that is outside of the image plane.\n\n        Added in 0.4.0.\n\n        Parameters\n        ----------\n        image : (H,W,...) ndarray or tuple of int\n            Image dimensions to use.\n            If an ``ndarray``, its shape will be used.\n            If a ``tuple``, it is assumed to represent the image shape\n            and must contain at least two integers.\n\n        Returns\n        -------\n        float\n            Total area of the bounding box that is outside of the image plane.\n            Can be ``0.0``.\n\n        \"\"\"\n        polys_clipped = self.clip_out_of_image(image)\n        if len(polys_clipped) == 0:\n            return self.area\n        return self.area - sum([poly.area for poly in polys_clipped])\n\n    def compute_out_of_image_fraction(self, image):\n        \"\"\"Compute fraction of polygon area outside of the image plane.\n\n        This estimates ``f = A_ooi / A``, where ``A_ooi`` is the area of the\n        polygon that is outside of the image plane, while ``A`` is the\n        total area of the bounding box.\n\n        Added in 0.4.0.\n\n        Parameters\n        ----------\n        image : (H,W,...) ndarray or tuple of int\n            Image dimensions to use.\n            If an ``ndarray``, its shape will be used.\n            If a ``tuple``, it is assumed to represent the image shape\n            and must contain at least two integers.\n\n        Returns\n        -------\n        float\n            Fraction of the polygon area that is outside of the image\n            plane. Returns ``0.0`` if the polygon is fully inside of\n            the image plane or has zero points. If the polygon has an area\n            of zero, the polygon is treated similarly to a :class:`LineString`,\n            i.e. the fraction of the line that is outside the image plane is\n            returned.\n\n        \"\"\"\n        area = self.area\n        if area == 0:\n            return self.to_line_string().compute_out_of_image_fraction(image)\n        return self.compute_out_of_image_area(image) / area\n\n    # TODO keep this method? it is almost an alias for is_out_of_image()\n    def is_fully_within_image(self, image):\n        \"\"\"Estimate whether the polygon is fully inside an image plane.\n\n        Parameters\n        ----------\n        image : (H,W,...) ndarray or tuple of int\n            Image dimensions to use.\n            If an ``ndarray``, its shape will be used.\n            If a ``tuple``, it is assumed to represent the image shape and\n            must contain at least two ``int`` s.\n\n        Returns\n        -------\n        bool\n            ``True`` if the polygon is fully inside the image area.\n            ``False`` otherwise.\n\n        \"\"\"\n        return not self.is_out_of_image(image, fully=True, partly=True)\n\n    # TODO keep this method? it is almost an alias for is_out_of_image()\n    def is_partly_within_image(self, image):\n        \"\"\"Estimate whether the polygon is at least partially inside an image.\n\n        Parameters\n        ----------\n        image : (H,W,...) ndarray or tuple of int\n            Image dimensions to use.\n            If an ``ndarray``, its shape will be used.\n            If a ``tuple``, it is assumed to represent the image shape and\n            must contain at least two ``int`` s.\n\n        Returns\n        -------\n        bool\n            ``True`` if the polygon is at least partially inside the image area.\n            ``False`` otherwise.\n\n        \"\"\"\n        return not self.is_out_of_image(image, fully=True, partly=False)\n\n    def is_out_of_image(self, image, fully=True, partly=False):\n        \"\"\"Estimate whether the polygon is partially/fully outside of an image.\n\n        Parameters\n        ----------\n        image : (H,W,...) ndarray or tuple of int\n            Image dimensions to use.\n            If an ``ndarray``, its shape will be used.\n            If a ``tuple``, it is assumed to represent the image shape and\n            must contain at least two ``int`` s.\n\n        fully : bool, optional\n            Whether to return ``True`` if the polygon is fully outside of the\n            image area.\n\n        partly : bool, optional\n            Whether to return ``True`` if the polygon is at least partially\n            outside fo the image area.\n\n        Returns\n        -------\n        bool\n            ``True`` if the polygon is partially/fully outside of the image\n            area, depending on defined parameters.\n            ``False`` otherwise.\n\n        \"\"\"\n        # TODO this is inconsistent with line strings, which return a default\n        #      value in these cases\n        if len(self.exterior) == 0:\n            raise Exception(\"Cannot determine whether the polygon is inside \"\n                            \"the image, because it contains no points.\")\n\n        # The line string is identical to the edge of the polygon.\n        # If the edge is fully inside the image, we know that the polygon must\n        # be fully inside the image.\n        # If the edge is partially outside of the image, we know that the\n        # polygon is partially outside of the image.\n        # Only if the edge is fully outside of the image we cannot be sure if\n        # the polygon's inner area overlaps with the image (e.g. if the\n        # polygon contains the whole image in it).\n        ls = self.to_line_string()\n        if ls.is_fully_within_image(image):\n            return False\n        if ls.is_out_of_image(image, fully=False, partly=True):\n            return partly\n\n        # LS is fully outside of the image. Estimate whether there is any\n        # intersection with the image plane. If so, we know that there is\n        # partial overlap (full overlap would mean that the LS was fully inside\n        # the image).\n        polys = self.clip_out_of_image(image)\n        if len(polys) > 0:\n            return partly\n        return fully\n\n    @ia.deprecated(alt_func=\"Polygon.clip_out_of_image()\",\n                   comment=\"clip_out_of_image() has the exactly same \"\n                           \"interface.\")\n    def cut_out_of_image(self, image):\n        \"\"\"Cut off all parts of the polygon that are outside of an image.\"\"\"\n        return self.clip_out_of_image(image)\n\n    # TODO this currently can mess up the order of points - change somehow to\n    #      keep the order\n    def clip_out_of_image(self, image):\n        \"\"\"Cut off all parts of the polygon that are outside of an image.\n\n        This operation may lead to new points being created.\n        As a single polygon may be split into multiple new polygons, the result\n        is always a list, which may contain more than one output polygon.\n\n        This operation will return an empty list if the polygon is completely\n        outside of the image plane.\n\n        Parameters\n        ----------\n        image : (H,W,...) ndarray or tuple of int\n            Image dimensions to use for the clipping of the polygon.\n            If an ``ndarray``, its shape will be used.\n            If a ``tuple``, it is assumed to represent the image shape and must\n            contain at least two ``int`` s.\n\n        Returns\n        -------\n        list of imgaug.augmentables.polys.Polygon\n            Polygon, clipped to fall within the image dimensions.\n            Returned as a ``list``, because the clipping can split the polygon\n            into multiple parts. The list may also be empty, if the polygon was\n            fully outside of the image plane.\n\n        \"\"\"\n        # load shapely lazily, which makes the dependency more optional\n        import shapely.geometry\n\n        # Shapely polygon conversion requires at least 3 coordinates\n        if len(self.exterior) == 0:\n            return []\n        if len(self.exterior) in [1, 2]:\n            ls = self.to_line_string(closed=False)\n            ls_clipped = ls.clip_out_of_image(image)\n            assert len(ls_clipped) <= 1\n            if len(ls_clipped) == 0:\n                return []\n            return [self.deepcopy(exterior=ls_clipped[0].coords)]\n\n        h, w = image.shape[0:2] if ia.is_np_array(image) else image[0:2]\n        poly_shapely = self.to_shapely_polygon()\n        poly_image = shapely.geometry.Polygon([(0, 0), (w, 0), (w, h), (0, h)])\n        multipoly_inter_shapely = poly_shapely.intersection(poly_image)\n        ignore_types = (shapely.geometry.LineString,\n                        shapely.geometry.MultiLineString,\n                        shapely.geometry.point.Point,\n                        shapely.geometry.MultiPoint)\n        if isinstance(multipoly_inter_shapely, shapely.geometry.Polygon):\n            multipoly_inter_shapely = shapely.geometry.MultiPolygon(\n                [multipoly_inter_shapely])\n        elif isinstance(multipoly_inter_shapely,\n                        shapely.geometry.MultiPolygon):\n            # we got a multipolygon from shapely, no need to change anything\n            # anymore\n            pass\n        elif isinstance(multipoly_inter_shapely, ignore_types):\n            # polygons that become (one or more) lines/points after clipping\n            # are here ignored\n            multipoly_inter_shapely = shapely.geometry.MultiPolygon([])\n        elif isinstance(multipoly_inter_shapely,\n                        shapely.geometry.GeometryCollection):\n            # Shapely returns GEOMETRYCOLLECTION EMPTY if there is nothing\n            # remaining after the clip.\n            assert multipoly_inter_shapely.is_empty\n            return []\n        else:\n            print(multipoly_inter_shapely, image, self.exterior)\n            raise Exception(\n                \"Got an unexpected result of type %s from Shapely for \"\n                \"image (%d, %d) and polygon %s. This is an internal error. \"\n                \"Please report.\" % (\n                    type(multipoly_inter_shapely), h, w, self.exterior)\n            )\n\n        polygons = []\n        for poly_inter_shapely in multipoly_inter_shapely.geoms:\n            polygons.append(Polygon.from_shapely(poly_inter_shapely,\n                                                 label=self.label))\n\n        # Shapely changes the order of points, we try here to preserve it as\n        # much as possible.\n        # Note here, that all points of the new polygon might have high\n        # distance to the points on the old polygon. This can happen if the\n        # polygon overlaps with the image plane, but all of its points are\n        # outside of the image plane. The new polygon will not be made up of\n        # any of the old points.\n        polygons_reordered = []\n        for polygon in polygons:\n            best_idx = None\n            best_dist = None\n            for x, y in self.exterior:\n                point_idx, dist = polygon.find_closest_point_index(\n                    x=x, y=y, return_distance=True)\n                if best_idx is None or dist < best_dist:\n                    best_idx = point_idx\n                    best_dist = dist\n            if best_idx is not None:\n                polygon_reordered = \\\n                    polygon.change_first_point_by_index(best_idx)\n                polygons_reordered.append(polygon_reordered)\n\n        return polygons_reordered\n\n    def shift_(self, x=0, y=0):\n        \"\"\"Move this polygon along the x/y-axis in-place.\n\n        The origin ``(0, 0)`` is at the top left of the image.\n\n        Added in 0.4.0.\n\n        Parameters\n        ----------\n        x : number, optional\n            Value to be added to all x-coordinates. Positive values shift\n            towards the right images.\n\n        y : number, optional\n            Value to be added to all y-coordinates. Positive values shift\n            towards the bottom images.\n\n        Returns\n        -------\n        imgaug.augmentables.polys.Polygon\n            Shifted polygon.\n            The object may have been modified in-place.\n\n        \"\"\"\n        self.exterior[:, 0] += x\n        self.exterior[:, 1] += y\n        return self\n\n    def shift(self, x=0, y=0, top=None, right=None, bottom=None, left=None):\n        \"\"\"Move this polygon along the x/y-axis.\n\n        The origin ``(0, 0)`` is at the top left of the image.\n\n        Parameters\n        ----------\n        x : number, optional\n            Value to be added to all x-coordinates. Positive values shift\n            towards the right images.\n\n        y : number, optional\n            Value to be added to all y-coordinates. Positive values shift\n            towards the bottom images.\n\n        top : None or int, optional\n            Deprecated since 0.4.0.\n            Amount of pixels by which to shift this object *from* the\n            top (towards the bottom).\n\n        right : None or int, optional\n            Deprecated since 0.4.0.\n            Amount of pixels by which to shift this object *from* the\n            right (towards the left).\n\n        bottom : None or int, optional\n            Deprecated since 0.4.0.\n            Amount of pixels by which to shift this object *from* the\n            bottom (towards the top).\n\n        left : None or int, optional\n            Deprecated since 0.4.0.\n            Amount of pixels by which to shift this object *from* the\n            left (towards the right).\n\n        Returns\n        -------\n        imgaug.augmentables.polys.Polygon\n            Shifted polygon.\n\n        \"\"\"\n        x, y = _normalize_shift_args(\n            x, y, top=top, right=right, bottom=bottom, left=left)\n        return self.deepcopy().shift_(x=x, y=y)\n\n    # TODO separate this into draw_face_on_image() and draw_border_on_image()\n    # TODO add tests for line thickness\n    def draw_on_image(self,\n                      image,\n                      color=(0, 255, 0), color_face=None,\n                      color_lines=None, color_points=None,\n                      alpha=1.0, alpha_face=None,\n                      alpha_lines=None, alpha_points=None,\n                      size=1, size_lines=None, size_points=None,\n                      raise_if_out_of_image=False):\n        \"\"\"Draw the polygon on an image.\n\n        Parameters\n        ----------\n        image : (H,W,C) ndarray\n            The image onto which to draw the polygon. Usually expected to be\n            of dtype ``uint8``, though other dtypes are also handled.\n\n        color : iterable of int, optional\n            The color to use for the whole polygon.\n            Must correspond to the channel layout of the image. Usually RGB.\n            The values for `color_face`, `color_lines` and `color_points`\n            will be derived from this color if they are set to ``None``.\n            This argument has no effect if `color_face`, `color_lines`\n            and `color_points` are all set anything other than ``None``.\n\n        color_face : None or iterable of int, optional\n            The color to use for the inner polygon area (excluding perimeter).\n            Must correspond to the channel layout of the image. Usually RGB.\n            If this is ``None``, it will be derived from ``color * 1.0``.\n\n        color_lines : None or iterable of int, optional\n            The color to use for the line (aka perimeter/border) of the\n            polygon.\n            Must correspond to the channel layout of the image. Usually RGB.\n            If this is ``None``, it will be derived from ``color * 0.5``.\n\n        color_points : None or iterable of int, optional\n            The color to use for the corner points of the polygon.\n            Must correspond to the channel layout of the image. Usually RGB.\n            If this is ``None``, it will be derived from ``color * 0.5``.\n\n        alpha : float, optional\n            The opacity of the whole polygon, where ``1.0`` denotes a\n            completely visible polygon and ``0.0`` an invisible one.\n            The values for `alpha_face`, `alpha_lines` and `alpha_points`\n            will be derived from this alpha value if they are set to ``None``.\n            This argument has no effect if `alpha_face`, `alpha_lines`\n            and `alpha_points` are all set anything other than ``None``.\n\n        alpha_face : None or number, optional\n            The opacity of the polygon's inner area (excluding the perimeter),\n            where ``1.0`` denotes a completely visible inner area and ``0.0``\n            an invisible one.\n            If this is ``None``, it will be derived from ``alpha * 0.5``.\n\n        alpha_lines : None or number, optional\n            The opacity of the polygon's line (aka perimeter/border),\n            where ``1.0`` denotes a completely visible line and ``0.0`` an\n            invisible one.\n            If this is ``None``, it will be derived from ``alpha * 1.0``.\n\n        alpha_points : None or number, optional\n            The opacity of the polygon's corner points, where ``1.0`` denotes\n            completely visible corners and ``0.0`` invisible ones.\n            If this is ``None``, it will be derived from ``alpha * 1.0``.\n\n        size : int, optional\n            Size of the polygon.\n            The sizes of the line and points are derived from this value,\n            unless they are set.\n\n        size_lines : None or int, optional\n            Thickness of the polygon's line (aka perimeter/border).\n            If ``None``, this value is derived from `size`.\n\n        size_points : int, optional\n            Size of the points in pixels.\n            If ``None``, this value is derived from ``3 * size``.\n\n        raise_if_out_of_image : bool, optional\n            Whether to raise an error if the polygon is fully\n            outside of the image. If set to ``False``, no error will be\n            raised and only the parts inside the image will be drawn.\n\n        Returns\n        -------\n        (H,W,C) ndarray\n            Image with the polygon drawn on it. Result dtype is the same as the\n            input dtype.\n\n        \"\"\"\n        # pylint: disable=invalid-name\n        def _assert_not_none(arg_name, arg_value):\n            assert arg_value is not None, (\n                \"Expected '%s' to not be None, got type %s.\" % (\n                    arg_name, type(arg_value),))\n\n        def _default_to(var, default):\n            if var is None:\n                return default\n            return var\n\n        _assert_not_none(\"color\", color)\n        _assert_not_none(\"alpha\", alpha)\n        _assert_not_none(\"size\", size)\n\n        # FIXME due to the np.array(.) and the assert at ndim==2 below, this\n        #       will always fail on 2D images?\n        color_face = _default_to(color_face, np.array(color))\n        color_lines = _default_to(color_lines, np.array(color) * 0.5)\n        color_points = _default_to(color_points, np.array(color) * 0.5)\n\n        alpha_face = _default_to(alpha_face, alpha * 0.5)\n        alpha_lines = _default_to(alpha_lines, alpha)\n        alpha_points = _default_to(alpha_points, alpha)\n\n        size_lines = _default_to(size_lines, size)\n        size_points = _default_to(size_points, size * 3)\n\n        if image.ndim == 2:\n            assert ia.is_single_number(color_face), (\n                \"Got a 2D image. Expected then 'color_face' to be a single \"\n                \"number, but got %s.\" % (str(color_face),))\n            color_face = [color_face]\n        elif image.ndim == 3 and ia.is_single_number(color_face):\n            color_face = [color_face] * image.shape[-1]\n\n        if alpha_face < 0.01:\n            alpha_face = 0\n        elif alpha_face > 0.99:\n            alpha_face = 1\n\n        if raise_if_out_of_image and self.is_out_of_image(image):\n            raise Exception(\"Cannot draw polygon %s on image with \"\n                            \"shape %s.\" % (str(self), image.shape))\n\n        # TODO np.clip to image plane if is_fully_within_image(), similar to\n        #      how it is done for bounding boxes\n\n        # TODO improve efficiency by only drawing in rectangle that covers\n        #      poly instead of drawing in the whole image\n        # TODO for a rectangular polygon, the face coordinates include the\n        #      top/left boundary but not the right/bottom boundary. This may\n        #      be unintuitive when not drawing the boundary. Maybe somehow\n        #      remove the boundary coordinates from the face coordinates after\n        #      generating both?\n        input_dtype = image.dtype\n        result = image.astype(np.float32)\n        rr, cc = skimage.draw.polygon(\n            self.yy_int, self.xx_int, shape=image.shape)\n        if len(rr) > 0:\n            if alpha_face == 1:\n                result[rr, cc] = np.float32(color_face)\n            elif alpha_face == 0:\n                pass\n            else:\n                result[rr, cc] = (\n                    (1 - alpha_face) * result[rr, cc, :]\n                    + alpha_face * np.float32(color_face)\n                )\n\n        ls_open = self.to_line_string(closed=False)\n        ls_closed = self.to_line_string(closed=True)\n        result = ls_closed.draw_lines_on_image(\n            result, color=color_lines, alpha=alpha_lines,\n            size=size_lines, raise_if_out_of_image=raise_if_out_of_image)\n        result = ls_open.draw_points_on_image(\n            result, color=color_points, alpha=alpha_points,\n            size=size_points, raise_if_out_of_image=raise_if_out_of_image)\n\n        if input_dtype.type == np.uint8:\n            # TODO make clipping more flexible\n            result = np.clip(np.round(result), 0, 255).astype(input_dtype)\n        else:\n            result = result.astype(input_dtype)\n\n        return result\n\n    # TODO add pad, similar to LineStrings\n    # TODO add pad_max, similar to LineStrings\n    # TODO add prevent_zero_size, similar to LineStrings\n    def extract_from_image(self, image):\n        \"\"\"Extract all image pixels within the polygon area.\n\n        This method returns a rectangular image array. All pixels within\n        that rectangle that do not belong to the polygon area will be filled\n        with zeros (i.e. they will be black).\n        The method will also zero-pad the image if the polygon is\n        partially/fully outside of the image.\n\n        Parameters\n        ----------\n        image : (H,W) ndarray or (H,W,C) ndarray\n            The image from which to extract the pixels within the polygon.\n\n        Returns\n        -------\n        (H',W') ndarray or (H',W',C) ndarray\n            Pixels within the polygon. Zero-padded if the polygon is\n            partially/fully outside of the image.\n\n        \"\"\"\n        assert image.ndim in [2, 3], (\n            \"Expected image of shape (H,W,[C]), got shape %s.\" % (\n                image.shape,))\n\n        if len(self.exterior) <= 2:\n            raise Exception(\"Polygon must be made up of at least 3 points to \"\n                            \"extract its area from an image.\")\n\n        bb = self.to_bounding_box()\n        bb_area = bb.extract_from_image(image)\n        if self.is_out_of_image(image, fully=True, partly=False):\n            return bb_area\n\n        xx = self.xx_int\n        yy = self.yy_int\n        xx_mask = xx - np.min(xx)\n        yy_mask = yy - np.min(yy)\n        height_mask = np.max(yy_mask)\n        width_mask = np.max(xx_mask)\n\n        rr_face, cc_face = skimage.draw.polygon(\n            yy_mask, xx_mask, shape=(height_mask, width_mask))\n\n        mask = np.zeros((height_mask, width_mask), dtype=np.bool)\n        mask[rr_face, cc_face] = True\n\n        if image.ndim == 3:\n            mask = np.tile(mask[:, :, np.newaxis], (1, 1, image.shape[2]))\n\n        return bb_area * mask\n\n    def change_first_point_by_coords(self, x, y, max_distance=1e-4,\n                                     raise_if_too_far_away=True):\n        \"\"\"\n        Reorder exterior points so that the point closest to given x/y is first.\n\n        This method takes a given ``(x,y)`` coordinate, finds the closest\n        corner point on the exterior and reorders all exterior corner points\n        so that the found point becomes the first one in the array.\n\n        If no matching points are found, an exception is raised.\n\n        Parameters\n        ----------\n        x : number\n            X-coordinate of the point.\n\n        y : number\n            Y-coordinate of the point.\n\n        max_distance : None or number, optional\n            Maximum distance past which possible matches are ignored.\n            If ``None`` the distance limit is deactivated.\n\n        raise_if_too_far_away : bool, optional\n            Whether to raise an exception if the closest found point is too\n            far away (``True``) or simply return an unchanged copy if this\n            object (``False``).\n\n        Returns\n        -------\n        imgaug.augmentables.polys.Polygon\n            Copy of this polygon with the new point order.\n\n        \"\"\"\n        if len(self.exterior) == 0:\n            raise Exception(\"Cannot reorder polygon points, because it \"\n                            \"contains no points.\")\n\n        closest_idx, closest_dist = self.find_closest_point_index(\n            x=x, y=y, return_distance=True)\n        if max_distance is not None and closest_dist > max_distance:\n            if not raise_if_too_far_away:\n                return self.deepcopy()\n\n            closest_point = self.exterior[closest_idx, :]\n            raise Exception(\n                \"Closest found point (%.9f, %.9f) exceeds max_distance of \"\n                \"%.9f exceeded\" % (\n                    closest_point[0], closest_point[1], closest_dist))\n        return self.change_first_point_by_index(closest_idx)\n\n    def change_first_point_by_index(self, point_idx):\n        \"\"\"\n        Reorder exterior points so that the point with given index is first.\n\n        This method takes a given index and reorders all exterior corner points\n        so that the point with that index becomes the first one in the array.\n\n        An ``AssertionError`` will be raised if the index does not match\n        any exterior point's index or the exterior does not contain any points.\n\n        Parameters\n        ----------\n        point_idx : int\n            Index of the desired starting point.\n\n        Returns\n        -------\n        imgaug.augmentables.polys.Polygon\n            Copy of this polygon with the new point order.\n\n        \"\"\"\n        assert 0 <= point_idx < len(self.exterior), (\n            \"Expected index of new first point to be in the discrete interval \"\n            \"[0..%d). Got index %d.\" % (len(self.exterior), point_idx))\n\n        if point_idx == 0:\n            return self.deepcopy()\n        exterior = np.concatenate(\n            (self.exterior[point_idx:, :], self.exterior[:point_idx, :]),\n            axis=0\n        )\n        return self.deepcopy(exterior=exterior)\n\n    def subdivide_(self, points_per_edge):\n        \"\"\"Derive a new poly with ``N`` interpolated points per edge in-place.\n\n        See :func:`~imgaug.augmentables.lines.LineString.subdivide` for details.\n\n        Added in 0.4.0.\n\n        Parameters\n        ----------\n        points_per_edge : int\n            Number of points to interpolate on each edge.\n\n        Returns\n        -------\n        imgaug.augmentables.polys.Polygon\n            Polygon with subdivided edges.\n            The object may have been modified in-place.\n\n        \"\"\"\n        if len(self.exterior) == 1:\n            return self\n        ls = self.to_line_string(closed=True)\n        ls_sub = ls.subdivide(points_per_edge)\n        # [:-1] even works if the polygon contains zero points\n        exterior_subdivided = ls_sub.coords[:-1]\n        self.exterior = exterior_subdivided\n        return self\n\n    def subdivide(self, points_per_edge):\n        \"\"\"Derive a new polygon with ``N`` interpolated points per edge.\n\n        See :func:`~imgaug.augmentables.lines.LineString.subdivide` for details.\n\n        Added in 0.4.0.\n\n        Parameters\n        ----------\n        points_per_edge : int\n            Number of points to interpolate on each edge.\n\n        Returns\n        -------\n        imgaug.augmentables.polys.Polygon\n            Polygon with subdivided edges.\n\n        \"\"\"\n        return self.deepcopy().subdivide_(points_per_edge)\n\n    def to_shapely_polygon(self):\n        \"\"\"Convert this polygon to a ``Shapely`` ``Polygon``.\n\n        Returns\n        -------\n        shapely.geometry.Polygon\n            The ``Shapely`` ``Polygon`` matching this polygon's exterior.\n\n        \"\"\"\n        # load shapely lazily, which makes the dependency more optional\n        import shapely.geometry\n\n        return shapely.geometry.Polygon(\n            [(point[0], point[1]) for point in self.exterior])\n\n    def to_shapely_line_string(self, closed=False, interpolate=0):\n        \"\"\"Convert this polygon to a ``Shapely`` ``LineString`` object.\n\n        Parameters\n        ----------\n        closed : bool, optional\n            Whether to return the line string with the last point being\n            identical to the first point.\n\n        interpolate : int, optional\n            Number of points to interpolate between any pair of two\n            consecutive points. These points are added to the final line string.\n\n        Returns\n        -------\n        shapely.geometry.LineString\n            The ``Shapely`` ``LineString`` matching the polygon's exterior.\n\n        \"\"\"\n        return _convert_points_to_shapely_line_string(\n            self.exterior, closed=closed, interpolate=interpolate)\n\n    def to_bounding_box(self):\n        \"\"\"Convert this polygon to a bounding box containing the polygon.\n\n        Returns\n        -------\n        imgaug.augmentables.bbs.BoundingBox\n            Bounding box that tightly encapsulates the polygon.\n\n        \"\"\"\n        # TODO get rid of this deferred import\n        from imgaug.augmentables.bbs import BoundingBox\n\n        xx = self.xx\n        yy = self.yy\n        return BoundingBox(x1=min(xx), x2=max(xx),\n                           y1=min(yy), y2=max(yy),\n                           label=self.label)\n\n    def to_keypoints(self):\n        \"\"\"Convert this polygon's exterior to ``Keypoint`` instances.\n\n        Returns\n        -------\n        list of imgaug.augmentables.kps.Keypoint\n            Exterior vertices as :class:`~imgaug.augmentables.kps.Keypoint`\n            instances.\n\n        \"\"\"\n        # TODO get rid of this deferred import\n        from imgaug.augmentables.kps import Keypoint\n\n        return [Keypoint(x=point[0], y=point[1]) for point in self.exterior]\n\n    def to_line_string(self, closed=True):\n        \"\"\"Convert this polygon's exterior to a ``LineString`` instance.\n\n        Parameters\n        ----------\n        closed : bool, optional\n            Whether to close the line string, i.e. to add the first point of\n            the `exterior` also as the last point at the end of the line string.\n            This has no effect if the polygon has a single point or zero\n            points.\n\n        Returns\n        -------\n        imgaug.augmentables.lines.LineString\n            Exterior of the polygon as a line string.\n\n        \"\"\"\n        from imgaug.augmentables.lines import LineString\n        if not closed or len(self.exterior) <= 1:\n            return LineString(self.exterior, label=self.label)\n        return LineString(\n            np.concatenate([self.exterior, self.exterior[0:1, :]], axis=0),\n            label=self.label)\n\n    @staticmethod\n    def from_shapely(polygon_shapely, label=None):\n        \"\"\"Create a polygon from a ``Shapely`` ``Polygon``.\n\n        .. note::\n\n            This will remove any holes in the shapely polygon.\n\n        Parameters\n        ----------\n        polygon_shapely : shapely.geometry.Polygon\n             The shapely polygon.\n\n        label : None or str, optional\n            The label of the new polygon.\n\n        Returns\n        -------\n        imgaug.augmentables.polys.Polygon\n            A polygon with the same exterior as the ``Shapely`` ``Polygon``.\n\n        \"\"\"\n        # load shapely lazily, which makes the dependency more optional\n        import shapely.geometry\n\n        assert isinstance(polygon_shapely, shapely.geometry.Polygon), (\n            \"Expected the input to be a shapely.geometry.Polgon instance. \"\n            \"Got %s.\" % (type(polygon_shapely),))\n        # polygon_shapely.exterior can be None if the polygon was\n        # instantiated without points\n        has_no_exterior = (\n            polygon_shapely.exterior is None\n            or len(polygon_shapely.exterior.coords) == 0)\n        if has_no_exterior:\n            return Polygon([], label=label)\n        exterior = np.float32([[x, y]\n                               for (x, y)\n                               in polygon_shapely.exterior.coords])\n        return Polygon(exterior, label=label)\n\n    def coords_almost_equals(self, other, max_distance=1e-4,\n                             points_per_edge=8):\n        \"\"\"Alias for :func:`Polygon.exterior_almost_equals`.\n\n        Parameters\n        ----------\n        other : imgaug.augmentables.polys.Polygon or (N,2) ndarray or list of tuple\n            See\n            :func:`~imgaug.augmentables.polys.Polygon.exterior_almost_equals`.\n\n        max_distance : number, optional\n            See\n            :func:`~imgaug.augmentables.polys.Polygon.exterior_almost_equals`.\n\n        points_per_edge : int, optional\n            See\n            :func:`~imgaug.augmentables.polys.Polygon.exterior_almost_equals`.\n\n        Returns\n        -------\n        bool\n            Whether the two polygon's exteriors can be viewed as equal\n            (approximate test).\n\n        \"\"\"\n        return self.exterior_almost_equals(\n            other, max_distance=max_distance, points_per_edge=points_per_edge)\n\n    def exterior_almost_equals(self, other, max_distance=1e-4,\n                               points_per_edge=8):\n        \"\"\"Estimate if this and another polygon's exterior are almost identical.\n\n        The two exteriors can have different numbers of points, but any point\n        randomly sampled on the exterior of one polygon should be close to the\n        closest point on the exterior of the other polygon.\n\n        .. note::\n\n            This method works in an approximative way. One can come up with\n            polygons with fairly different shapes that will still be estimated\n            as equal by this method. In practice however this should be\n            unlikely to be the case. The probability for something like that\n            goes down as the interpolation parameter is increased.\n\n        Parameters\n        ----------\n        other : imgaug.augmentables.polys.Polygon or (N,2) ndarray or list of tuple\n            The other polygon with which to compare the exterior.\n            If this is an ``ndarray``, it is assumed to represent an exterior.\n            It must then have dtype ``float32`` and shape ``(N,2)`` with the\n            second dimension denoting xy-coordinates.\n            If this is a ``list`` of ``tuple`` s, it is assumed to represent\n            an exterior. Each tuple then must contain exactly two ``number`` s,\n            denoting xy-coordinates.\n\n        max_distance : number, optional\n            The maximum euclidean distance between a point on one polygon and\n            the closest point on the other polygon. If the distance is exceeded\n            for any such pair, the two exteriors are not viewed as equal. The\n            points are either the points contained in the polygon's exterior\n            ndarray or interpolated points between these.\n\n        points_per_edge : int, optional\n            How many points to interpolate on each edge.\n\n        Returns\n        -------\n        bool\n            Whether the two polygon's exteriors can be viewed as equal\n            (approximate test).\n\n        \"\"\"\n        if isinstance(other, list):\n            other = Polygon(np.float32(other))\n        elif ia.is_np_array(other):\n            other = Polygon(other)\n        else:\n            assert isinstance(other, Polygon), (\n                \"Expected 'other' to be a list of coordinates, a coordinate \"\n                \"array or a single Polygon. Got type %s.\" % (type(other),))\n\n        return self.to_line_string(closed=True).coords_almost_equals(\n            other.to_line_string(closed=True),\n            max_distance=max_distance,\n            points_per_edge=points_per_edge\n        )\n\n    def almost_equals(self, other, max_distance=1e-4, points_per_edge=8):\n        \"\"\"\n        Estimate if this polygon's and another's geometry/labels are similar.\n\n        This is the same as\n        :func:`~imgaug.augmentables.polys.Polygon.exterior_almost_equals` but\n        additionally compares the labels.\n\n        Parameters\n        ----------\n        other : imgaug.augmentables.polys.Polygon\n            The other object to compare against. Expected to be a ``Polygon``.\n\n        max_distance : float, optional\n            See\n            :func:`~imgaug.augmentables.polys.Polygon.exterior_almost_equals`.\n\n        points_per_edge : int, optional\n            See\n            :func:`~imgaug.augmentables.polys.Polygon.exterior_almost_equals`.\n\n        Returns\n        -------\n        bool\n            ``True`` if the coordinates are almost equal and additionally\n            the labels are equal. Otherwise ``False``.\n\n        \"\"\"\n        if self.label != other.label:\n            return False\n        return self.exterior_almost_equals(\n            other, max_distance=max_distance, points_per_edge=points_per_edge)\n\n    def copy(self, exterior=None, label=None):\n        \"\"\"Create a shallow copy of this object.\n\n        Parameters\n        ----------\n        exterior : list of imgaug.augmentables.kps.Keypoint or list of tuple or (N,2) ndarray, optional\n            List of points defining the polygon. See\n            :func:`~imgaug.augmentables.polys.Polygon.__init__` for details.\n\n        label : None or str, optional\n            If not ``None``, the ``label`` of the copied object will be set\n            to this value.\n\n        Returns\n        -------\n        imgaug.augmentables.polys.Polygon\n            Shallow copy.\n\n        \"\"\"\n        return self.deepcopy(exterior=exterior, label=label)\n\n    def deepcopy(self, exterior=None, label=None):\n        \"\"\"Create a deep copy of this object.\n\n        Parameters\n        ----------\n        exterior : list of Keypoint or list of tuple or (N,2) ndarray, optional\n            List of points defining the polygon. See\n            `imgaug.augmentables.polys.Polygon.__init__` for details.\n\n        label : None or str\n            If not ``None``, the ``label`` of the copied object will be set\n            to this value.\n\n        Returns\n        -------\n        imgaug.augmentables.polys.Polygon\n            Deep copy.\n\n        \"\"\"\n        return Polygon(\n            exterior=np.copy(self.exterior) if exterior is None else exterior,\n            label=self.label if label is None else label)\n\n    def __getitem__(self, indices):\n        \"\"\"Get the coordinate(s) with given indices.\n\n        Added in 0.4.0.\n\n        Returns\n        -------\n        ndarray\n            xy-coordinate(s) as ``ndarray``.\n\n        \"\"\"\n        return self.exterior[indices]\n\n    def __iter__(self):\n        \"\"\"Iterate over the coordinates of this instance.\n\n        Added in 0.4.0.\n\n        Yields\n        ------\n        ndarray\n            An ``(2,)`` ``ndarray`` denoting an xy-coordinate pair.\n\n        \"\"\"\n        return iter(self.exterior)\n\n    def __repr__(self):\n        return self.__str__()\n\n    def __str__(self):\n        points_str = \", \".join([\n            \"(x=%.3f, y=%.3f)\" % (point[0], point[1])\n            for point\n            in self.exterior])\n        return \"Polygon([%s] (%d points), label=%s)\" % (\n            points_str, len(self.exterior), self.label)\n\n\n# TODO add tests for this\nclass PolygonsOnImage(IAugmentable):\n    \"\"\"Container for all polygons on a single image.\n\n    Parameters\n    ----------\n    polygons : list of imgaug.augmentables.polys.Polygon\n        List of polygons on the image.\n\n    shape : tuple of int or ndarray\n        The shape of the image on which the objects are placed.\n        Either an image with shape ``(H,W,[C])`` or a ``tuple`` denoting\n        such an image shape.\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from imgaug.augmentables.polys import Polygon, PolygonsOnImage\n    >>> image = np.zeros((100, 100))\n    >>> polys = [\n    >>>     Polygon([(0.5, 0.5), (100.5, 0.5), (100.5, 100.5), (0.5, 100.5)]),\n    >>>     Polygon([(50.5, 0.5), (100.5, 50.5), (50.5, 100.5), (0.5, 50.5)])\n    >>> ]\n    >>> polys_oi = PolygonsOnImage(polys, shape=image.\n```\n\n### Code description\n```Python\n\"\"\"\nDescription: This function is used to jitter duplicate points in a given exterior polygon.\n\nArgs:\n    exterior (list of tuples): The exterior polygon to be processed.\n    random_state (numpy.random.RandomState): The random state to be used for generating noise.\n\nReturns:\n    list of tuples: The processed exterior polygon with jittered duplicate points.\n\"\"\"\n```\n\n### Code snippet:\n```Python\n\ndef _jitter_duplicate_points(self, exterior, random_state):\n    def _find_duplicates(exterior_with_duplicates):\n        points_map = collections.defaultdict(list)\n\n        for i, point in enumerate(exterior_with_duplicates):\n            # we use 10/x here to be a bit more lenient, the precise\n            # distance test is further below\n            x = int(np.round(point[0]\n                             * ((1/10) / self.threshold_duplicate_points)))\n            y = int(np.round(point[1]\n                             * ((1/10) / self.threshold_duplicate_points)))\n            for direction0 in [-1, 0, 1]:\n                for direction1 in [-1, 0, 1]:\n                    points_map[(x+direction0, y+direction1)].append(i)\n\n        duplicates = [False] * len(exterior_with_duplicates)\n        for key in points_map:\n            candidates = points_map[key]\n            for i, p0_idx in enumerate(candidates):\n                p0_idx = candidates[i]\n                point0 = exterior_with_duplicates[p0_idx]\n                if duplicates[p0_idx]:\n                    continue\n\n                for j in range(i+1, len(candidates)):\n                    p1_idx = candidates[j]\n                    point1 = exterior_with_duplicates[p1_idx]\n                    if duplicates[p1_idx]:\n                        continue\n\n                    dist = np.sqrt(\n```\n\n### Requirements:\n1. Complete only the missing part of the function.\n2. Only answer the code of the completed part. Do not include any of the provided code snippet.\n3. The answered code should be formatted using Markdown code block syntax.\n4. Do not include any additional text.\n\n### Answer: (use the provided format with backticks)\n", "name": "_jitter_duplicate_points", "reference": "                        (point0[0] - point1[0])**2\n                        + (point0[1] - point1[1])**2)\n                    if dist < self.threshold_duplicate_points:\n                        duplicates[p1_idx] = True\n\n        return duplicates\n\n    noise_strength = self.noise_strength\n    assert noise_strength > 0, (\n        \"Expected noise strength to be >0, got %.4f.\" % (noise_strength,))\n    exterior = exterior[:]\n    converged = False\n    while not converged:\n        duplicates = _find_duplicates(exterior)\n        if any(duplicates):\n            noise = random_state.uniform(\n                -self.noise_strength,\n                self.noise_strength,\n                size=(len(exterior), 2)\n            ).astype(np.float32)\n\n            for i, is_duplicate in enumerate(duplicates):\n                if is_duplicate:\n                    exterior[i] = (\n                        exterior[i][0] + noise[i][0],\n                        exterior[i][1] + noise[i][1])\n\n            noise_strength *= 10\n        else:\n            converged = True\n\n    return exterior", "completions": ["```markdown\n```python\ndef _jitter_duplicate_points(self, exterior, random_state):\n    \"\"\"\n    Jitters duplicate points in the given exterior polygon.\n\n    Args:\n        exterior (list of tuples): The exterior polygon to be processed.\n        random_state (numpy.random.RandomState): The random state to be used for generating noise.\n\n    Returns:\n        list of tuples: The processed exterior polygon with jittered duplicate points.\n    \"\"\"\n    # Find duplicate points\n    exterior_with_duplicates = [point for point in exterior]\n    for i, point in enumerate(exterior):\n        x = int(np.round(point[0] * ((1 / 10) / self.threshold_duplicate_points)))\n        y = int(np.round(point[1] * ((1 / 10) / self.threshold_duplicate_points)))\n        for direction0 in [-1, 0, 1]:\n            for direction1 in [-1, 0, 1]:\n                points_map[(x + direction0, y + direction1)].append(i)\n\n        duplicates = [False] * len(exterior)\n        for key in points_map:\n            candidates = points_map[key]\n            for j in candidates:\n                if j != i:\n                    point_j = exterior[j]\n                    duplicates[j] = True\n\n        if any(duplicates):\n            # Jitter duplicate points\n            for j, duplicate_point in enumerate(exterior_with_duplicates):\n                if duplicates[j]:\n                    dx = random_state.uniform(-self.jitter_range, self.jitter_range)\n                    dy = random_state.uniform(-self.jitter_range, self.jitter_range)\n                    exterior_with_duplicates[j] = (\n                        duplicate_point[0] + dx,\n                        duplicate_point[1] + dy)\n\n    return exterior_with_duplicates\n```\n```\n"]}
{"id": 12, "prompt": "<\uff5cbegin\u2581of\u2581sentence\uff5c>### Role:\nYou are an experienced Python programmer assistant responsible for code completion tasks. Please intelligently complete the missing parts of the provided code snippet.\n\n### Task:\nBased on the provided code snippet, complete the missing function implementation. Ensure the code is logically correct, syntactically accurate, and follows best coding practices.\n\n### Left context\n```Python\n# <Copyright 2019, Argo AI, LLC. Released under the MIT license.>\n\"\"\"Utility functions for converting quaternions to 3d rotation matrices.\n\nUnit quaternions are a way to compactly represent 3D rotations\nwhile avoiding singularities or discontinuities (e.g. gimbal lock).\n\nIf a quaternion is not normalized beforehand to be unit-length, we will\nre-normalize it on the fly.\n\"\"\"\n\nimport logging\n\nimport numpy as np\nfrom scipy.spatial.transform import Rotation\n\nlogger = logging.getLogger(__name__)\n\n\ndef yaw_to_quaternion3d(yaw: float) -> np.ndarray:\n    \"\"\"Convert a rotation angle in the xy plane (i.e. about the z axis) to a quaternion.\n\n    Args:\n        yaw: angle to rotate about the z-axis, representing an Euler angle, in radians\n\n    Returns:\n        array w/ quaternion coefficients (qw,qx,qy,qz) in scalar-first order, per Argoverse convention.\n    \"\"\"\n    qx, qy, qz, qw = Rotation.from_euler(seq=\"z\", angles=yaw, degrees=False).as_quat()\n    return np.array([qw, qx, qy, qz])\n\n\ndef rotmat2quat(R: np.ndarray) -> np.ndarray:\n    \"\"\"Convert a rotation-matrix to a quaternion in Argo's scalar-first notation (w, x, y, z).\"\"\"\n    quat_xyzw = Rotation.from_matrix(R).as_quat()\n    quat_wxyz = quat_scipy2argo(quat_xyzw)\n    return quat_wxyz\n\n\n```\n\n### Code description\n```Python\n\"\"\"\nDescription: This function converts a quaternion to a rotation matrix.\n\nArgs:\n    q (np.ndarray): A quaternion in the format [x, y, z, w]\n\nReturns:\n    np.ndarray: A 3x3 rotation matrix\n\nRaises:\n    ZeroDivisionError: If the quaternion norm is close to zero, which would lead to division by zero.\n\nNotes:\n    The quaternion is normalized if its norm is not close to 1.0.\n    The quaternion is converted from the Argo convention to the Scipy convention.\n    The rotation matrix is obtained using the scipy.spatial.transform.Rotation.from_quat method.\n\"\"\"\n```\n\n### Code snippet:\n```Python\ndef quat2rotmat(q: np.ndarray) -> np.ndarray:\n    norm = np.linalg.norm(q)\n    if not np.isclose(norm, 1.0, atol=1e-12):\n        logger.info(\"Forced to re-normalize quaternion, since its norm was not equal to 1.\")\n        if np.isclose(norm, 0.0):\n```\n\n### Requirements:\n1. Complete only the missing part of the function.\n2. Only answer the code of the completed part. Do not include any of the provided code snippet.\n3. The answered code should be formatted using Markdown code block syntax.\n4. Do not include any additional text.\n\n### Answer: (use the provided format with backticks)\n", "name": "quat2rotmat", "reference": "            raise ZeroDivisionError(\"Normalize quaternioning with norm=0 would lead to division by zero.\")\n        q /= norm\n\n    quat_xyzw = quat_argo2scipy(q)\n    return Rotation.from_quat(quat_xyzw).as_matrix()", "completions": ["```python\ndef quat2rotmat(q: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Convert a quaternion to a rotation matrix.\n\n    Args:\n        q (np.ndarray): A quaternion in the format [x, y, z, w]\n\n    Returns:\n        np.ndarray: A 3x3 rotation matrix\n\n    Raises:\n        ZeroDivisionError: If the quaternion norm is close to zero, which would lead to division by zero.\n\n    Notes:\n        The quaternion is normalized if its norm is not close to 1.0.\n        The quaternion is converted from the Argo convention to the Scipy convention.\n        The rotation matrix is obtained using the scipy.spatial.transform.Rotation.from_quat method.\n    \"\"\"\n    norm = np.linalg.norm(q)\n    if not np.isclose(norm, 1.0, atol=1e-12):\n        logger.info(\"Forced to re-normalize quaternion, since its norm was not equal to 1.\")\n        if np.isclose(norm, 0.0):\n            raise ZeroDivisionError(\"The quaternion norm is close to zero, which would lead to division by zero.\")\n    return Rotation.from_quat(q).as_matrix()\n```\n"]}
{"id": 13, "prompt": "<\uff5cbegin\u2581of\u2581sentence\uff5c>### Role:\nYou are an experienced Python programmer assistant responsible for code completion tasks. Please intelligently complete the missing parts of the provided code snippet.\n\n### Task:\nBased on the provided code snippet, complete the missing function implementation. Ensure the code is logically correct, syntactically accurate, and follows best coding practices.\n\n### Left context\n```Python\nimport itertools\nimport functools\nimport operator\n\nimport numpy as np\nfrom pandas import compat\nfrom pandas._libs import tslib, algos, lib\nfrom pandas.core.dtypes.common import (\n    _get_dtype,\n    is_float, is_scalar,\n    is_integer, is_complex, is_float_dtype,\n    is_complex_dtype, is_integer_dtype,\n    is_bool_dtype, is_object_dtype,\n    is_numeric_dtype,\n    is_datetime64_dtype, is_timedelta64_dtype,\n    is_datetime_or_timedelta_dtype,\n    is_int_or_datetime_dtype, is_any_int_dtype)\nfrom pandas.core.dtypes.cast import _int64_max, maybe_upcast_putmask\nfrom pandas.core.dtypes.missing import isnull, notnull\nfrom pandas.core.config import get_option\nfrom pandas.core.common import _values_from_object\n\ntry:\n    import bottleneck as bn\n    _BOTTLENECK_INSTALLED = True\nexcept ImportError:  # pragma: no cover\n    _BOTTLENECK_INSTALLED = False\n\n_USE_BOTTLENECK = False\n\n\ndef set_use_bottleneck(v=True):\n    # set/unset to use bottleneck\n    global _USE_BOTTLENECK\n    if _BOTTLENECK_INSTALLED:\n        _USE_BOTTLENECK = v\n\n\nset_use_bottleneck(get_option('compute.use_bottleneck'))\n\n\nclass disallow(object):\n\n    def __init__(self, *dtypes):\n        super(disallow, self).__init__()\n        self.dtypes = tuple(np.dtype(dtype).type for dtype in dtypes)\n\n    def check(self, obj):\n        return hasattr(obj, 'dtype') and issubclass(obj.dtype.type,\n                                                    self.dtypes)\n\n    def __call__(self, f):\n        @functools.wraps(f)\n        def _f(*args, **kwargs):\n            obj_iter = itertools.chain(args, compat.itervalues(kwargs))\n            if any(self.check(obj) for obj in obj_iter):\n                raise TypeError('reduction operation {0!r} not allowed for '\n                                'this dtype'.format(\n                                    f.__name__.replace('nan', '')))\n            try:\n                with np.errstate(invalid='ignore'):\n                    return f(*args, **kwargs)\n            except ValueError as e:\n                # we want to transform an object array\n                # ValueError message to the more typical TypeError\n                # e.g. this is normally a disallowed function on\n                # object arrays that contain strings\n                if is_object_dtype(args[0]):\n                    raise TypeError(e)\n                raise\n\n        return _f\n\n\nclass bottleneck_switch(object):\n\n    def __init__(self, zero_value=None, **kwargs):\n        self.zero_value = zero_value\n        self.kwargs = kwargs\n\n    def __call__(self, alt):\n        bn_name = alt.__name__\n\n        try:\n            bn_func = getattr(bn, bn_name)\n        except (AttributeError, NameError):  # pragma: no cover\n            bn_func = None\n\n        @functools.wraps(alt)\n        def f(values, axis=None, skipna=True, **kwds):\n            if len(self.kwargs) > 0:\n                for k, v in compat.iteritems(self.kwargs):\n                    if k not in kwds:\n                        kwds[k] = v\n            try:\n                if self.zero_value is not None and values.size == 0:\n                    if values.ndim == 1:\n\n                        # wrap the 0's if needed\n                        if is_timedelta64_dtype(values):\n                            return lib.Timedelta(0)\n                        return 0\n                    else:\n                        result_shape = (values.shape[:axis] +\n                                        values.shape[axis + 1:])\n                        result = np.empty(result_shape)\n                        result.fill(0)\n                        return result\n\n                if (_USE_BOTTLENECK and skipna and\n                        _bn_ok_dtype(values.dtype, bn_name)):\n                    result = bn_func(values, axis=axis, **kwds)\n\n                    # prefer to treat inf/-inf as NA, but must compute the func\n                    # twice :(\n                    if _has_infs(result):\n                        result = alt(values, axis=axis, skipna=skipna, **kwds)\n                else:\n                    result = alt(values, axis=axis, skipna=skipna, **kwds)\n            except Exception:\n                try:\n                    result = alt(values, axis=axis, skipna=skipna, **kwds)\n                except ValueError as e:\n                    # we want to transform an object array\n                    # ValueError message to the more typical TypeError\n                    # e.g. this is normally a disallowed function on\n                    # object arrays that contain strings\n\n                    if is_object_dtype(values):\n                        raise TypeError(e)\n                    raise\n\n            return result\n\n        return f\n\n\ndef _bn_ok_dtype(dt, name):\n    # Bottleneck chokes on datetime64\n    if (not is_object_dtype(dt) and not is_datetime_or_timedelta_dtype(dt)):\n\n        # bottleneck does not properly upcast during the sum\n        # so can overflow\n        if name == 'nansum':\n            if dt.itemsize < 8:\n                return False\n\n        return True\n    return False\n\n\ndef _has_infs(result):\n    if isinstance(result, np.ndarray):\n        if result.dtype == 'f8':\n            return lib.has_infs_f8(result.ravel())\n        elif result.dtype == 'f4':\n            return lib.has_infs_f4(result.ravel())\n    try:\n        return np.isinf(result).any()\n    except (TypeError, NotImplementedError):\n        # if it doesn't support infs, then it can't have infs\n        return False\n\n\ndef _get_fill_value(dtype, fill_value=None, fill_value_typ=None):\n    \"\"\" return the correct fill value for the dtype of the values \"\"\"\n    if fill_value is not None:\n        return fill_value\n    if _na_ok_dtype(dtype):\n        if fill_value_typ is None:\n            return np.nan\n        else:\n            if fill_value_typ == '+inf':\n                return np.inf\n            else:\n                return -np.inf\n    else:\n        if fill_value_typ is None:\n            return tslib.iNaT\n        else:\n            if fill_value_typ == '+inf':\n                # need the max int here\n                return _int64_max\n            else:\n                return tslib.iNaT\n\n\ndef _get_values(values, skipna, fill_value=None, fill_value_typ=None,\n                isfinite=False, copy=True):\n    \"\"\" utility to get the values view, mask, dtype\n    if necessary copy and mask using the specified fill_value\n    copy = True will force the copy\n    \"\"\"\n    values = _values_from_object(values)\n    if isfinite:\n        mask = _isfinite(values)\n    else:\n        mask = isnull(values)\n\n    dtype = values.dtype\n    dtype_ok = _na_ok_dtype(dtype)\n\n    # get our fill value (in case we need to provide an alternative\n    # dtype for it)\n    fill_value = _get_fill_value(dtype, fill_value=fill_value,\n                                 fill_value_typ=fill_value_typ)\n\n    if skipna:\n        if copy:\n            values = values.copy()\n        if dtype_ok:\n            np.putmask(values, mask, fill_value)\n\n        # promote if needed\n        else:\n            values, changed = maybe_upcast_putmask(values, mask, fill_value)\n\n    elif copy:\n        values = values.copy()\n\n    values = _view_if_needed(values)\n\n    # return a platform independent precision dtype\n    dtype_max = dtype\n    if is_integer_dtype(dtype) or is_bool_dtype(dtype):\n        dtype_max = np.int64\n    elif is_float_dtype(dtype):\n        dtype_max = np.float64\n\n    return values, mask, dtype, dtype_max\n\n\ndef _isfinite(values):\n    if is_datetime_or_timedelta_dtype(values):\n        return isnull(values)\n    if (is_complex_dtype(values) or is_float_dtype(values) or\n            is_integer_dtype(values) or is_bool_dtype(values)):\n        return ~np.isfinite(values)\n    return ~np.isfinite(values.astype('float64'))\n\n\ndef _na_ok_dtype(dtype):\n    return not is_int_or_datetime_dtype(dtype)\n\n\ndef _view_if_needed(values):\n    if is_datetime_or_timedelta_dtype(values):\n        return values.view(np.int64)\n    return values\n\n\ndef _wrap_results(result, dtype):\n    \"\"\" wrap our results if needed \"\"\"\n\n    if is_datetime64_dtype(dtype):\n        if not isinstance(result, np.ndarray):\n            result = lib.Timestamp(result)\n        else:\n            result = result.view(dtype)\n    elif is_timedelta64_dtype(dtype):\n        if not isinstance(result, np.ndarray):\n\n            # raise if we have a timedelta64[ns] which is too large\n            if np.fabs(result) > _int64_max:\n                raise ValueError(\"overflow in timedelta operation\")\n\n            result = lib.Timedelta(result, unit='ns')\n        else:\n            result = result.astype('i8').view(dtype)\n\n    return result\n\n\ndef nanany(values, axis=None, skipna=True):\n    values, mask, dtype, _ = _get_values(values, skipna, False, copy=skipna)\n    return values.any(axis)\n\n\ndef nanall(values, axis=None, skipna=True):\n    values, mask, dtype, _ = _get_values(values, skipna, True, copy=skipna)\n    return values.all(axis)\n\n\n@disallow('M8')\n@bottleneck_switch(zero_value=0)\ndef nansum(values, axis=None, skipna=True):\n    values, mask, dtype, dtype_max = _get_values(values, skipna, 0)\n    dtype_sum = dtype_max\n    if is_float_dtype(dtype):\n        dtype_sum = dtype\n    elif is_timedelta64_dtype(dtype):\n        dtype_sum = np.float64\n    the_sum = values.sum(axis, dtype=dtype_sum)\n    the_sum = _maybe_null_out(the_sum, axis, mask)\n\n    return _wrap_results(the_sum, dtype)\n\n\n@disallow('M8')\n@bottleneck_switch()\ndef nanmean(values, axis=None, skipna=True):\n    values, mask, dtype, dtype_max = _get_values(values, skipna, 0)\n\n    dtype_sum = dtype_max\n    dtype_count = np.float64\n    if is_integer_dtype(dtype) or is_timedelta64_dtype(dtype):\n        dtype_sum = np.float64\n    elif is_float_dtype(dtype):\n        dtype_sum = dtype\n        dtype_count = dtype\n    count = _get_counts(mask, axis, dtype=dtype_count)\n    the_sum = _ensure_numeric(values.sum(axis, dtype=dtype_sum))\n\n    if axis is not None and getattr(the_sum, 'ndim', False):\n        the_mean = the_sum / count\n        ct_mask = count == 0\n        if ct_mask.any():\n            the_mean[ct_mask] = np.nan\n    else:\n        the_mean = the_sum / count if count > 0 else np.nan\n\n    return _wrap_results(the_mean, dtype)\n\n\n@disallow('M8')\n@bottleneck_switch()\ndef nanmedian(values, axis=None, skipna=True):\n\n    values, mask, dtype, dtype_max = _get_values(values, skipna)\n\n    def get_median(x):\n        mask = notnull(x)\n        if not skipna and not mask.all():\n            return np.nan\n        return algos.median(_values_from_object(x[mask]))\n\n    if not is_float_dtype(values):\n        values = values.astype('f8')\n        values[mask] = np.nan\n\n    if axis is None:\n        values = values.ravel()\n\n    notempty = values.size\n\n    # an array from a frame\n    if values.ndim > 1:\n        # there's a non-empty array to apply over otherwise numpy raises\n        if notempty:\n            return _wrap_results(\n                np.apply_along_axis(get_median, axis, values), dtype)\n\n        # must return the correct shape, but median is not defined for the\n        # empty set so return nans of shape \"everything but the passed axis\"\n        # since \"axis\" is where the reduction would occur if we had a nonempty\n        # array\n        shp = np.array(values.shape)\n        dims = np.arange(values.ndim)\n        ret = np.empty(shp[dims != axis])\n        ret.fill(np.nan)\n        return _wrap_results(ret, dtype)\n\n    # otherwise return a scalar value\n    return _wrap_results(get_median(values) if notempty else np.nan, dtype)\n\n\ndef _get_counts_nanvar(mask, axis, ddof, dtype=float):\n    dtype = _get_dtype(dtype)\n    count = _get_counts(mask, axis, dtype=dtype)\n    d = count - dtype.type(ddof)\n\n    # always return NaN, never inf\n    if is_scalar(count):\n        if count <= ddof:\n            count = np.nan\n            d = np.nan\n    else:\n        mask2 = count <= ddof\n        if mask2.any():\n            np.putmask(d, mask2, np.nan)\n            np.putmask(count, mask2, np.nan)\n    return count, d\n\n\n@disallow('M8')\n@bottleneck_switch(ddof=1)\ndef nanstd(values, axis=None, skipna=True, ddof=1):\n    result = np.sqrt(nanvar(values, axis=axis, skipna=skipna, ddof=ddof))\n    return _wrap_results(result, values.dtype)\n\n\n@disallow('M8')\n@bottleneck_switch(ddof=1)\n```\n\n### Code description\n```Python\n\"\"\"\nDescription: This function calculates the variance of an array, ignoring NaN values.\n\nArgs:\n    values (array-like): The array of which to calculate the variance.\n    axis (int, optional): Axis along which the variance is computed. The default is to compute the variance of the flattened array.\n    skipna (bool, optional): If True, NaNs are ignored. Default is True.\n    ddof (int, optional): Means Delta Degrees of Freedom. The divisor used in calculations is N - ddof, where N represents the number of elements. Default is 1.\n\nReturns:\n    float or ndarray: The variance of the array elements. If axis is None, the result is a scalar value. If axis is given, the result is an array of dimension a - 1.\n\"\"\"\n```\n\n### Code snippet:\n```Python\n\ndef nanvar(values, axis=None, skipna=True, ddof=1):\n\n    values = _values_from_object(values)\n    dtype = values.dtype\n    mask = isnull(values)\n    if is_any_int_dtype(values):\n        values = values.astype('f8')\n        values[mask] = np.nan\n\n    if is_float_dtype(values):\n        count, d = _get_counts_nanvar(mask, axis, ddof, values.dtype)\n    else:\n        count, d = _get_counts_nanvar(mask, axis, ddof)\n\n    if skipna:\n        values = values.copy()\n        np.putmask(values, mask, 0)\n\n```\n\n### Requirements:\n1. Complete only the missing part of the function.\n2. Only answer the code of the completed part. Do not include any of the provided code snippet.\n3. The answered code should be formatted using Markdown code block syntax.\n4. Do not include any additional text.\n\n### Answer: (use the provided format with backticks)\n", "name": "nanvar", "reference": "    # xref GH10242\n    # Compute variance via two-pass algorithm, which is stable against\n    # cancellation errors and relatively accurate for small numbers of\n    # observations.\n    #\n    # See https://en.wikipedia.org/wiki/Algorithms_for_calculating_variance\n    avg = _ensure_numeric(values.sum(axis=axis, dtype=np.float64)) / count\n    if axis is not None:\n        avg = np.expand_dims(avg, axis)\n    sqr = _ensure_numeric((avg - values)**2)\n    np.putmask(sqr, mask, 0)\n    result = sqr.sum(axis=axis, dtype=np.float64) / d\n\n    # Return variance as np.float64 (the datatype used in the accumulator),\n    # unless we were dealing with a float array, in which case use the same\n    # precision as the original values array.\n    if is_float_dtype(dtype):\n        result = result.astype(dtype)\n    return _wrap_results(result, values.dtype)", "completions": ["```python\n```python\ndef nanvar(values, axis=None, skipna=True, ddof=1):\n    values, mask, dtype, dtype_max = _get_values(values, skipna, 0, copy=skipna)\n\n    if is_float_dtype(dtype):\n        count, d = _get_counts_nanvar(mask, axis, ddof, dtype)\n    else:\n        count, d = _get_counts_nanvar(mask, axis, ddof)\n\n    if skipna:\n        values = values.copy()\n        np.putmask(values, mask, 0)\n\n    the_var = _ensure_numeric(values.var(axis=axis, dtype=dtype_max))\n\n    if axis is not None and getattr(the_var, 'ndim', False):\n        the_var = the_var / count\n        ct_mask = count == 0\n        if ct_mask.any():\n            the_var[ct_mask] = np.nan\n    else:\n        the_var = the_var / count if count > 0 else np.nan\n\n    return _wrap_results(the_var, dtype)\n```\n```\n"]}
{"id": 14, "prompt": "<\uff5cbegin\u2581of\u2581sentence\uff5c>### Role:\nYou are an experienced Python programmer assistant responsible for code completion tasks. Please intelligently complete the missing parts of the provided code snippet.\n\n### Task:\nBased on the provided code snippet, complete the missing function implementation. Ensure the code is logically correct, syntactically accurate, and follows best coding practices.\n\n### Left context\n```Python\n# Many methods borrow heavily or entirely from transforms3d https://github.com/matthew-brett/transforms3d\n# eventually some of these may be upstreamed, but credit to transforms3d\n# authors for implementing the many of the formulations we use here.\n\nimport itertools\n\nimport numpy as np\n\n\"\"\"\nRotations\n=========\n\nNote: these have caused many subtle bugs in the past.\nBe careful while updating these methods and while using them in clever ways.\n\nSee MuJoCo documentation here: http://mujoco.org/book/modeling.html#COrientation\n\nConventions\n-----------\n    - All functions accept batches as well as individual rotations\n    - All rotation conventions match respective MuJoCo defaults\n    - All angles are in radians\n    - Matricies follow LR convention\n    - Euler Angles are all relative with 'xyz' axes ordering\n    - See specific representation for more information\n\nRepresentations\n---------------\n\nEuler\n    There are many euler angle frames -- here we will strive to use the default\n        in MuJoCo, which is eulerseq='xyz'.\n    This frame is a relative rotating frame, about x, y, and z axes in order.\n        Relative rotating means that after we rotate about x, then we use the\n        new (rotated) y, and the same for z.\n\nQuaternions\n    These are defined in terms of rotation (angle) about a unit vector (x, y, z)\n    We use the following <q0, q1, q2, q3> convention:\n            q0 = cos(angle / 2)\n            q1 = sin(angle / 2) * x\n            q2 = sin(angle / 2) * y\n            q3 = sin(angle / 2) * z\n        This is also sometimes called qw, qx, qy, qz.\n    Note that quaternions are ambiguous, because we can represent a rotation by\n        angle about vector <x, y, z> and -angle about vector <-x, -y, -z>.\n        To choose between these, we pick \"first nonzero positive\", where we\n        make the first nonzero element of the quaternion positive.\n    This can result in mismatches if you're converting an quaternion that is not\n        \"first nonzero positive\" to a different representation and back.\n\nAxis Angle\n    (Not currently implemented)\n    These are very straightforward.  Rotation is angle about a unit vector.\n\nXY Axes\n    (Not currently implemented)\n    We are given x axis and y axis, and z axis is cross product of x and y.\n\nZ Axis\n    This is NOT RECOMMENDED.  Defines a unit vector for the Z axis,\n        but rotation about this axis is not well defined.\n    Instead pick a fixed reference direction for another axis (e.g. X)\n        and calculate the other (e.g. Y = Z cross-product X),\n        then use XY Axes rotation instead.\n\nSO3\n    (Not currently implemented)\n    While not supported by MuJoCo, this representation has a lot of nice features.\n    We expect to add support for these in the future.\n\nTODO / Missing\n--------------\n    - Rotation integration or derivatives (e.g. velocity conversions)\n    - More representations (SO3, etc)\n    - Random sampling (e.g. sample uniform random rotation)\n    - Performance benchmarks/measurements\n    - (Maybe) define everything as to/from matricies, for simplicity\n\"\"\"\n\n# For testing whether a number is close to zero\n_FLOAT_EPS = np.finfo(np.float64).eps\n_EPS4 = _FLOAT_EPS * 4.0\n\n\ndef euler2mat(euler):\n    \"\"\" Convert Euler Angles to Rotation Matrix.  See rotation.py for notes \"\"\"\n    euler = np.asarray(euler, dtype=np.float64)\n    assert euler.shape[-1] == 3, \"Invalid shaped euler {}\".format(euler)\n\n    ai, aj, ak = -euler[..., 2], -euler[..., 1], -euler[..., 0]\n    si, sj, sk = np.sin(ai), np.sin(aj), np.sin(ak)\n    ci, cj, ck = np.cos(ai), np.cos(aj), np.cos(ak)\n    cc, cs = ci * ck, ci * sk\n    sc, ss = si * ck, si * sk\n\n    mat = np.empty(euler.shape[:-1] + (3, 3), dtype=np.float64)\n    mat[..., 2, 2] = cj * ck\n    mat[..., 2, 1] = sj * sc - cs\n    mat[..., 2, 0] = sj * cc + ss\n    mat[..., 1, 2] = cj * sk\n    mat[..., 1, 1] = sj * ss + cc\n    mat[..., 1, 0] = sj * cs - sc\n    mat[..., 0, 2] = -sj\n    mat[..., 0, 1] = cj * si\n    mat[..., 0, 0] = cj * ci\n    return mat\n\n\ndef euler2quat(euler):\n    \"\"\" Convert Euler Angles to Quaternions.  See rotation.py for notes \"\"\"\n    euler = np.asarray(euler, dtype=np.float64)\n    assert euler.shape[-1] == 3, \"Invalid shape euler {}\".format(euler)\n\n    ai, aj, ak = euler[..., 2] / 2, -euler[..., 1] / 2, euler[..., 0] / 2\n    si, sj, sk = np.sin(ai), np.sin(aj), np.sin(ak)\n    ci, cj, ck = np.cos(ai), np.cos(aj), np.cos(ak)\n    cc, cs = ci * ck, ci * sk\n    sc, ss = si * ck, si * sk\n\n    quat = np.empty(euler.shape[:-1] + (4,), dtype=np.float64)\n    quat[..., 0] = cj * cc + sj * ss\n    quat[..., 3] = cj * sc - sj * cs\n    quat[..., 2] = -(cj * ss + sj * cc)\n    quat[..., 1] = cj * cs - sj * sc\n    return quat\n\n\ndef mat2euler(mat):\n    \"\"\" Convert Rotation Matrix to Euler Angles.  See rotation.py for notes \"\"\"\n    mat = np.asarray(mat, dtype=np.float64)\n    assert mat.shape[-2:] == (3, 3), \"Invalid shape matrix {}\".format(mat)\n\n    cy = np.sqrt(mat[..., 2, 2] * mat[..., 2, 2] + mat[..., 1, 2] * mat[..., 1, 2])\n    condition = cy > _EPS4\n    euler = np.empty(mat.shape[:-1], dtype=np.float64)\n    euler[..., 2] = np.where(\n        condition,\n        -np.arctan2(mat[..., 0, 1], mat[..., 0, 0]),\n        -np.arctan2(-mat[..., 1, 0], mat[..., 1, 1]),\n    )\n    euler[..., 1] = np.where(\n        condition, -np.arctan2(-mat[..., 0, 2], cy), -np.arctan2(-mat[..., 0, 2], cy)\n    )\n    euler[..., 0] = np.where(\n        condition, -np.arctan2(mat[..., 1, 2], mat[..., 2, 2]), 0.0\n    )\n    return euler\n\n\n```\n\n### Code description\n```Python\n\"\"\"\nDescription: This function converts a rotation matrix to a quaternion representation.\n\nArgs:\n    mat (numpy.ndarray): A 3x3 rotation matrix or an array of such matrices.\n\nReturns:\n    numpy.ndarray: A quaternion (w, x, y, z) or an array of such quaternions.\n\nRaises:\n    AssertionError: If the input matrix is not a 3x3 rotation matrix.\n\nNotes:\n    The function uses numpy.nditer to iterate over the input matrix. It then computes the symmetric matrix K, which is used to find the eigenvalues and eigenvectors of the matrix. The eigenvector corresponding to the largest eigenvalue is selected as the quaternion representation of the rotation matrix. The quaternion is then normalized and returned.\n\"\"\"\n```\n\n### Code snippet:\n```Python\ndef mat2quat(mat):\n    mat = np.asarray(mat, dtype=np.float64)\n    assert mat.shape[-2:] == (3, 3), \"Invalid shape matrix {}\".format(mat)\n\n    Qxx, Qyx, Qzx = mat[..., 0, 0], mat[..., 0, 1], mat[..., 0, 2]\n    Qxy, Qyy, Qzy = mat[..., 1, 0], mat[..., 1, 1], mat[..., 1, 2]\n    Qxz, Qyz, Qzz = mat[..., 2, 0], mat[..., 2, 1], mat[..., 2, 2]\n    # Fill only lower half of symmetric matrix\n    K = np.zeros(mat.shape[:-2] + (4, 4), dtype=np.float64)\n    K[..., 0, 0] = Qxx - Qyy - Qzz\n    K[..., 1, 0] = Qyx + Qxy\n    K[..., 1, 1] = Qyy - Qxx - Qzz\n    K[..., 2, 0] = Qzx + Qxz\n    K[..., 2, 1] = Qzy + Qyz\n    K[..., 2, 2] = Qzz - Qxx - Qyy\n    K[..., 3, 0] = Qyz - Qzy\n    K[..., 3, 1] = Qzx - Qxz\n```\n\n### Requirements:\n1. Complete only the missing part of the function.\n2. Only answer the code of the completed part. Do not include any of the provided code snippet.\n3. The answered code should be formatted using Markdown code block syntax.\n4. Do not include any additional text.\n\n### Answer: (use the provided format with backticks)\n", "name": "mat2quat", "reference": "    K[..., 3, 2] = Qxy - Qyx\n    K[..., 3, 3] = Qxx + Qyy + Qzz\n    K /= 3.0\n    # TODO: vectorize this -- probably could be made faster\n    q = np.empty(K.shape[:-2] + (4,))\n    it = np.nditer(q[..., 0], flags=[\"multi_index\"])\n    while not it.finished:\n        # Use Hermitian eigenvectors, values for speed\n        vals, vecs = np.linalg.eigh(K[it.multi_index])\n        # Select largest eigenvector, reorder to w,x,y,z quaternion\n        q[it.multi_index] = vecs[[3, 0, 1, 2], np.argmax(vals)]\n        # Prefer quaternion with positive w\n        # (q * -1 corresponds to same rotation as q)\n        if q[it.multi_index][0] < 0:\n            q[it.multi_index] *= -1\n        it.iternext()\n    return q", "completions": ["```python\ndef mat2quat(mat):\n    mat = np.asarray(mat, dtype=np.float64)\n    assert mat.shape[-2:] == (3, 3), \"Invalid shape matrix {}\".format(mat)\n\n    Qxx, Qyx, Qzx = mat[..., 0, 0], mat[..., 0, 1], mat[..., 0, 2]\n    Qxy, Qyy, Qzy = mat[..., 1, 0], mat[..., 1, 1], mat[..., 1, 2]\n    Qxz, Qyz, Qzz = mat[..., 2, 0], mat[..., 2, 1], mat[..., 2, 2]\n    # Fill only lower half of symmetric matrix\n    K = np.zeros(mat.shape[:-2] + (4, 4), dtype=np.float64)\n    K[..., 0, 0] = Qxx - Qyy - Qzz\n    K[..., 1, 0] = Qyx + Qxy\n    K[..., 1, 1] = Qyy - Qxx - Qzz\n    K[..., 2, 0] = Qzx + Qxz\n    K[..., 2, 1] = Qzy + Qyz\n    K[..., 2, 2] = Qzz - Qxx - Qyy\n    K[..., 3, 0] = Qyz - Qzy\n    K[..., 3, 1] = Qzx - Qxz\n\n    # Compute eigenvalues and eigenvectors\n    eigvals, eigvecs = np.linalg.eig(K)\n    # Select the largest eigenvalue and its corresponding eigenvector\n    largest_eigval_index = np.argmax(eigvals)\n    largest_eigvec = eigvecs[largest_eigval_index]\n    # Normalize the eigenvector\n    largest_eigvec /= np.linalg.norm(largest_eigvec)\n\n    # Construct the quaternion\n    quat = np.zeros(mat.shape[:-1] + (4,), dtype=np.float64)\n    quat[..., 0] = largest_eigvec[0]\n    quat[..., 1:] = largest_eigvec[1:]\n\n    return quat\n```\n"]}
{"id": 15, "prompt": "<\uff5cbegin\u2581of\u2581sentence\uff5c>### Role:\nYou are an experienced Python programmer assistant responsible for code completion tasks. Please intelligently complete the missing parts of the provided code snippet.\n\n### Task:\nBased on the provided code snippet, complete the missing function implementation. Ensure the code is logically correct, syntactically accurate, and follows best coding practices.\n\n### Left context\n```Python\n\"\"\"A series of reward functions.\"\"\"\n\nimport numpy as np\n\n\n```\n\n### Code description\n```Python\n\"\"\"\nDescription: This function calculates the desired velocity of vehicles in a given environment.\n\nArgs:\n    env (type): The environment object containing the vehicle information.\n    fail (bool, optional): A boolean indicating whether the function should fail. Defaults to False.\n    edge_list (list, optional): A list of edge IDs. If provided, the function will only consider vehicles on these edges. Defaults to None.\n\nReturns:\n    float: The desired velocity of the vehicles, normalized to be between 0 and 1.\n\"\"\"\n```\n\n### Code snippet:\n```Python\ndef desired_velocity(env, fail=False, edge_list=None):\n    if edge_list is None:\n        veh_ids = env.k.vehicle.get_ids()\n    else:\n        veh_ids = env.k.vehicle.get_ids_by_edge(edge_list)\n\n    vel = np.array(env.k.vehicle.get_speed(veh_ids))\n    num_vehicles = len(veh_ids)\n\n    if any(vel < -100) or fail or num_vehicles == 0:\n        return 0.\n```\n\n### Requirements:\n1. Complete only the missing part of the function.\n2. Only answer the code of the completed part. Do not include any of the provided code snippet.\n3. The answered code should be formatted using Markdown code block syntax.\n4. Do not include any additional text.\n\n### Answer: (use the provided format with backticks)\n", "name": "desired_velocity", "reference": "\n    target_vel = env.env_params.additional_params['target_velocity']\n    max_cost = np.array([target_vel] * num_vehicles)\n    max_cost = np.linalg.norm(max_cost)\n\n    cost = vel - target_vel\n    cost = np.linalg.norm(cost)\n\n    # epsilon term (to deal with ZeroDivisionError exceptions)\n    eps = np.finfo(np.float32).eps\n\n    return max(max_cost - cost, 0) / (max_cost + eps)", "completions": ["```Python\ndef desired_velocity(env, fail=False, edge_list=None):\n    if edge_list is None:\n        veh_ids = env.k.vehicle.get_ids()\n    else:\n        veh_ids = env.k.vehicle.get_ids_by_edge(edge_list)\n\n    vel = np.array(env.k.vehicle.get_speed(veh_ids))\n    num_vehicles = len(veh_ids)\n\n    if any(vel < -100) or fail or num_vehicles == 0:\n        return 0.\n    else:\n        return vel / max(vel)\n```\n"]}
{"id": 16, "prompt": "<\uff5cbegin\u2581of\u2581sentence\uff5c>### Role:\nYou are an experienced Python programmer assistant responsible for code completion tasks. Please intelligently complete the missing parts of the provided code snippet.\n\n### Task:\nBased on the provided code snippet, complete the missing function implementation. Ensure the code is logically correct, syntactically accurate, and follows best coding practices.\n\n### Left context\n```Python\n\"\"\"\nA simple build-finished listener that pings registered URLs whenever a build\nfinishes.\n\nThe post request includes a form-encoded body with the following information:\n    - build_id: The ID of the build that completed.\n\nIt's up to the receiver to fetch from Changes any information it wants to know\nabout the build (likely, the build status). This design allows us not to worry\nabout client authentication: a malicious entity can spoof the notifications,\nbut cannot confuse the receiver with any false information. Perhaps someday\nthis mechanism will be extended to carry authentication information and\nadditional data about the build result.\n\"\"\"\n\nimport logging\nimport requests\n\nfrom flask import current_app\n\nfrom changes.models.build import Build\n\n\nlogger = logging.getLogger('build-finished-notifier')\n\n\n```\n\n### Code description\n```Python\n\"\"\"\nDescription: This function is used to handle the completion of a build process. It sends a POST request to specified URLs with the build ID as data.\n\nArgs:\n    build_id (int): The ID of the build process.\n    **kwargs: Additional keyword arguments.\n\nReturns:\n    None\n\"\"\"\n```\n\n### Code snippet:\n```Python\n\ndef build_finished_handler(build_id, **kwargs):\n    build = Build.query.get(build_id)\n    if build is None:\n        return\n\n    # A possible improvement would be to make all of these requests in separate\n    # celery tasks, for better isolation between requests to different hooks.\n    for u in current_app.config.get('BUILD_FINISHED_URLS', []):\n        try:\n            url, verify = u\n            if verify is None or not isinstance(verify, basestring):\n                verify = True\n        except ValueError:\n            # TODO this only exists for compatibility reasons.\n```\n\n### Requirements:\n1. Complete only the missing part of the function.\n2. Only answer the code of the completed part. Do not include any of the provided code snippet.\n3. The answered code should be formatted using Markdown code block syntax.\n4. Do not include any additional text.\n\n### Answer: (use the provided format with backticks)\n", "name": "build_finished_handler", "reference": "            # once we are sure that the config value is a list of tuples,\n            # remove this.\n            url = u\n            verify = True\n        try:\n            requests.post(\n                url,\n                data={'build_id': build.id},\n                timeout=20,\n\n                # this is either a string path to the ca bundle or a boolean\n                # indicating that the ssl cert should be checked\n                verify=verify\n            )\n        except Exception:\n            logger.exception(\"problem posting build finished notification\")", "completions": ["```Python\n```Python\ndef build_finished_handler(build_id, **kwargs):\n    build = Build.query.get(build_id)\n    if build is None:\n        return\n\n    # A possible improvement would be to make all of these requests in separate\n    # celery tasks, for better isolation between requests to different hooks.\n    for u in current_app.config.get('BUILD_FINISHED_URLS', []):\n        try:\n            url, verify = u\n            if verify is None or not isinstance(verify, basestring):\n                verify = True\n        except ValueError:\n            # TODO this only exists for compatibility reasons.\n            pass\n\n        try:\n            requests.post(url, data={'build_id': build_id}, verify=verify)\n        except requests.exceptions.RequestException as e:\n            logger.error('Failed to send build finished notification to %s: %s', url, str(e))\n```\n```\n"]}
{"id": 17, "prompt": "<\uff5cbegin\u2581of\u2581sentence\uff5c>### Role:\nYou are an experienced Python programmer assistant responsible for code completion tasks. Please intelligently complete the missing parts of the provided code snippet.\n\n### Task:\nBased on the provided code snippet, complete the missing function implementation. Ensure the code is logically correct, syntactically accurate, and follows best coding practices.\n\n### Left context\n```Python\nimport numpy as np\nimport tensorflow as tf\nfrom scipy.sparse import issparse\n\n\n__all__ = ('format_data', 'asfloat')\n\n\ndef format_data(data, is_feature1d=True, copy=False, make_float=True):\n    \"\"\"\n    Transform data in a standardized format.\n\n    Notes\n    -----\n    It should be applied to the input data prior to use in\n    learning algorithms.\n\n    Parameters\n    ----------\n    data : array-like\n        Data that should be formatted. That could be, matrix, vector or\n        Pandas DataFrame instance.\n\n    is_feature1d : bool\n        Should be equal to ``True`` if input data is a vector that\n        contains N samples with 1 feature each. Defaults to ``True``.\n\n    copy : bool\n        Defaults to ``False``.\n\n    make_float : bool\n        If `True` then input will be converted to float.\n        Defaults to ``False``.\n\n    Returns\n    -------\n    ndarray\n        The same input data but transformed to a standardized format\n        for further use.\n    \"\"\"\n    if data is None or issparse(data):\n        return data\n\n    if make_float:\n        data = asfloat(data)\n\n    if not isinstance(data, np.ndarray) or copy:\n        data = np.array(data, copy=copy)\n\n    # Valid number of features for one or two dimensions\n    n_features = data.shape[-1]\n\n    if data.ndim == 1:\n        data_shape = (n_features, 1) if is_feature1d else (1, n_features)\n        data = data.reshape(data_shape)\n\n    return data\n\n\n```\n\n### Code description\n```Python\n\"\"\"\nDescription: This function converts the input value to a float32 type.\n\nArgs:\n    value (np.matrix, np.ndarray, tf.Tensor, tf.SparseTensor, sparse matrix): The input value to be converted.\n\nReturns:\n    np.matrix, np.ndarray, tf.Tensor, tf.SparseTensor, sparse matrix: The converted value in float32 type.\n\"\"\"\n```\n\n### Code snippet:\n```Python\ndef asfloat(value):\n    float_type = 'float32'\n\n    if isinstance(value, (np.matrix, np.ndarray)):\n        if value.dtype != np.dtype(float_type):\n            return value.astype(float_type)\n\n        return value\n```\n\n### Requirements:\n1. Complete only the missing part of the function.\n2. Only answer the code of the completed part. Do not include any of the provided code snippet.\n3. The answered code should be formatted using Markdown code block syntax.\n4. Do not include any additional text.\n\n### Answer: (use the provided format with backticks)\n", "name": "asfloat", "reference": "\n    elif isinstance(value, (tf.Tensor, tf.SparseTensor)):\n        return tf.cast(value, tf.float32)\n\n    elif issparse(value):\n        return value\n\n    float_x_type = np.cast[float_type]\n    return float_x_type(value)", "completions": ["```Python\ndef asfloat(value):\n    float_type = 'float32'\n\n    if isinstance(value, (np.matrix, np.ndarray)):\n        if value.dtype != np.dtype(float_type):\n            return value.astype(float_type)\n\n        return value\n```\n"]}
{"id": 18, "prompt": "<\uff5cbegin\u2581of\u2581sentence\uff5c>### Role:\nYou are an experienced Python programmer assistant responsible for code completion tasks. Please intelligently complete the missing parts of the provided code snippet.\n\n### Task:\nBased on the provided code snippet, complete the missing function implementation. Ensure the code is logically correct, syntactically accurate, and follows best coding practices.\n\n### Left context\n```Python\n\"\"\"Bayesian optimization according to:\n\nBrochu, Cora, and de Freitas' tutorial at\nhttp://haikufactory.com/files/bayopt.pdf\n\nAdopted from http://atpassos.me/post/44900091837/bayesian-optimization\n\"\"\"\n\n# Authors: Alexandre Gramfort <alexandre.gramfort@telecom-paristech.fr>\n#          Alexandre Passos <alexandre.tp@gmail.com>\n#          Mainak Jas <mainak.jas@telecom-paristech.fr>\n\nimport warnings\nfrom sklearn import gaussian_process\nimport numpy as np\n\nimport scipy.stats as st\n\n\ndef expected_improvement(gp, best_y, x):\n    \"\"\"The expected improvement acquisition function.\n\n    The equation is explained in Eq (3) of the tutorial.\"\"\"\n    with warnings.catch_warnings():\n        warnings.simplefilter(\"ignore\")\n        y, y_std = gp.predict(x[:, None], return_std=True)\n    Z = (y - best_y) / (y_std + 1e-12)\n    return (y - best_y) * st.norm.cdf(Z) + y_std * st.norm.pdf(Z)\n\n\n```\n\n### Code description\n```Python\n\"\"\"\nDescription: This function performs Bayesian optimization using Gaussian Process Regressor.\n\nArgs:\n    f (function): The objective function to be optimized.\n    initial_x (list): The initial set of points to evaluate the objective function.\n    all_x (list): The set of all possible points to evaluate the objective function.\n    acquisition (function): The acquisition function to determine the next point to evaluate.\n    max_iter (int): The maximum number of iterations to perform the optimization.\n    debug (bool): If True, print debug information and plot the optimization process.\n    random_state (int): The random state to use for the Gaussian Process Regressor.\n\nReturns:\n    tuple: The best point found and the corresponding function value.\n\"\"\"\n```\n\n### Code snippet:\n```Python\ndef bayes_opt(f, initial_x, all_x, acquisition, max_iter=100, debug=False,\n              random_state=None):\n\n    X, y = list(), list()\n    for x in initial_x:\n        if not np.isinf(f(x)):\n            y.append(f(x))\n            X.append(x)\n\n    best_x = X[np.argmin(y)]\n    best_f = y[np.argmin(y)]\n    gp = gaussian_process.GaussianProcessRegressor(random_state=random_state)\n\n    if debug:  # pragma: no cover\n        print(\"iter\", -1, \"best_x\", best_x, best_f)\n\n    for i in range(max_iter):\n        gp.fit(np.array(X)[:, None], np.array(y))\n        new_x = all_x[acquisition(gp, best_f, all_x).argmin()]\n        new_f = f(new_x)\n```\n\n### Requirements:\n1. Complete only the missing part of the function.\n2. Only answer the code of the completed part. Do not include any of the provided code snippet.\n3. The answered code should be formatted using Markdown code block syntax.\n4. Do not include any additional text.\n\n### Answer: (use the provided format with backticks)\n", "name": "bayes_opt", "reference": "        if not np.isinf(new_f):\n            X.append(new_x)\n            y.append(new_f)\n            if new_f < best_f:\n                best_f = new_f\n                best_x = new_x\n\n        if debug:  # pragma: no cover\n            print(\"iter\", i, \"best_x\", best_x, best_f)\n\n    if debug:  # pragma: no cover\n        import matplotlib.pyplot as plt\n        scale = 1e6\n        sort_idx = np.argsort(X)\n        plt.plot(np.array(X)[sort_idx] * scale,\n                 np.array(y)[sort_idx] * scale, 'bo-')\n        plt.axvline(best_x * scale, linestyle='--')\n        plt.show()\n\n    return best_x, best_f", "completions": ["```python\ndef bayes_opt(f, initial_x, all_x, acquisition, max_iter=100, debug=False,\n              random_state=None):\n    \"\"\"\n    Performs Bayesian optimization using Gaussian Process Regressor.\n\n    Args:\n        f (function): The objective function to be optimized.\n        initial_x (list): The initial set of points to evaluate the objective function.\n        all_x (list): The set of all possible points to evaluate the objective function.\n        acquisition (function): The acquisition function to determine the next point to evaluate.\n        max_iter (int): The maximum number of iterations to perform the optimization.\n        debug (bool): If True, print debug information and plot the optimization process.\n        random_state (int): The random state to use for the Gaussian Process Regressor.\n\n    Returns:\n        tuple: The best point found and the corresponding function value.\n    \"\"\"\n    X, y = list(), list()\n    for x in initial_x:\n        if not np.isinf(f(x)):\n            y.append(f(x))\n            X.append(x)\n\n    best_x = X[np.argmin(y)]\n    best_f = y[np.argmin(y)]\n    gp = gaussian_process.GaussianProcessRegressor(random_state=random_state)\n\n    if debug:  # pragma: no cover\n        print(\"iter\", -1, \"best_x\", best_x, best_f)\n\n    for i in range(max_iter):\n        gp.fit(np.array(X)[:, None], np.array(y))\n        new_x = all_x[acquisition(gp, best_f, all_x).argmin()]\n        new_f = f(new_x)\n\n    return new_x, new_f\n```\n"]}
{"id": 19, "prompt": "<\uff5cbegin\u2581of\u2581sentence\uff5c>### Role:\nYou are an experienced Python programmer assistant responsible for code completion tasks. Please intelligently complete the missing parts of the provided code snippet.\n\n### Task:\nBased on the provided code snippet, complete the missing function implementation. Ensure the code is logically correct, syntactically accurate, and follows best coding practices.\n\n### Left context\n```Python\nfrom boto3 import client\nimport os\nimport logging\nimport json\n\n\n```\n\n### Code description\n```Python\n\"\"\"\nDescription: This function is for running a task on Amazon ECS using the boto3 client.\n\nArgs:\n    event (dict): The event data passed to the function by the AWS Lambda service.\n    context (object): The context object passed to the function by the AWS Lambda service.\n\nReturns:\n    dict: A dictionary containing the status code, status description, base64 encoding status, headers, and body of the response.\n\"\"\"\n```\n\n### Code snippet:\n```Python\n\ndef lambda_handler(event, context):\n    cluster = os.getenv(\"CLUSTER\", 'default')\n    logging.info(cluster)\n    task_definition = os.getenv(\"TASK_DEFINITION\", 'default')\n    capacity_provider = os.getenv(\"CAPACITY_PROVIDER\", 'FARGATE_SPOT')\n    subnets = os.getenv(\"SUBNETS\", \"\").split(\",\")\n    region_name = os.getenv(\"REGION_NAME\", 'us-east-1')\n    security_groups = os.getenv('SECURITY_GROUPS','').split(\",\")\n    container_name = os.getenv('CONTAINER_NAME')\n    ecs = client('ecs', region_name=region_name)\n    body = json.loads(event['body'])\n    if 'user' in body and 'bot' in body:\n        try:\n            env_data = [\n                {\n                    'name': 'BOT',\n                    'value': body['bot']\n                },\n                {\n                    'name': 'USER',\n                    'value': body['user']\n                }\n            ]\n            if body.get('token'):\n                env_data.append({\n                    'name': 'TOKEN',\n                    'value': body.get('token')\n                })\n            task_response = ecs.run_task(\n                capacityProviderStrategy=[\n                    {\n                        'capacityProvider': capacity_provider,\n                        'weight': 1,\n                        'base': 0\n                    },\n                ],\n```\n\n### Requirements:\n1. Complete only the missing part of the function.\n2. Only answer the code of the completed part. Do not include any of the provided code snippet.\n3. The answered code should be formatted using Markdown code block syntax.\n4. Do not include any additional text.\n\n### Answer: (use the provided format with backticks)\n", "name": "lambda_handler", "reference": "                cluster=cluster,\n                taskDefinition=task_definition,  # replace with your task definition name and revision\n                count=1,\n                platformVersion='1.4.0',\n                networkConfiguration={\n                    'awsvpcConfiguration': {\n                        'subnets': subnets,\n                        'assignPublicIp': 'ENABLED',\n                        'securityGroups': security_groups\n                    }\n                },\n                overrides={\n                    'containerOverrides': [\n                        {\n                            \"name\": container_name,\n                            'environment': env_data\n                        },\n                    ]\n                },\n            )\n            print(task_response)\n            response = \"success\"\n        except Exception as e:\n            response = str(e)\n    else:\n        response = \"invalid\"\n\n    output = {\n        \"statusCode\": 200,\n        \"statusDescription\": \"200 OK\",\n        \"isBase64Encoded\": False,\n        \"headers\": {\n            \"Content-Type\": \"text/html; charset=utf-8\"\n        },\n        \"body\": response\n    }\n    return output", "completions": ["```Python\n            ],\n            cluster=cluster,\n            launchType='FARGATE',\n            taskDefinition=task_definition,\n            networkConfiguration={\n                'awsvpcConfiguration': {\n                    'subnets': subnets,\n                    'securityGroups': security_groups,\n                    'assignPublicIp': 'ENABLED'\n                }\n            },\n            platformVersion='LATEST',\n            containerDefinitions=[\n                {\n                    'name': container_name,\n                    'image': '<image_name>',\n                    'cpu': 256,\n                    'memory': 512,\n                    'essential': True,\n                    'environment': env_data,\n                    'portMappings': [\n                        {\n                            'containerPort': 8080,\n                            'hostPort': 8080,\n                            'protocol': 'tcp'\n                        }\n                    ]\n                }\n            ]\n        )\n        return {\n            'statusCode': 200,\n            'body': json.dumps(task_response)\n        }\n    else:\n        return {\n            'statusCode': 400,\n            'body': json.dumps({\"error\": \"Invalid request\"})\n        }\n```\n"]}
{"id": 20, "prompt": "<\uff5cbegin\u2581of\u2581sentence\uff5c>### Role:\nYou are an experienced Python programmer assistant responsible for code completion tasks. Please intelligently complete the missing parts of the provided code snippet.\n\n### Task:\nBased on the provided code snippet, complete the missing function implementation. Ensure the code is logically correct, syntactically accurate, and follows best coding practices.\n\n### Left context\n```Python\n# -*- coding: utf-8 -*-\n\"\"\"Lifetimes utils and helpers.\"\"\"\n\nfrom __future__ import division\nimport numpy as np\nimport pandas as pd\nimport dill\n\npd.options.mode.chained_assignment = None\n\n__all__ = [\n    \"calibration_and_holdout_data\",\n    \"summary_data_from_transaction_data\",\n    \"calculate_alive_path\",\n    \"expected_cumulative_transactions\",\n]\n\n\nclass ConvergenceError(ValueError):\n    \"\"\"\n    Convergence Error Class.\n    \"\"\"\n\n    pass\n\n\ndef calibration_and_holdout_data(\n    transactions,\n    customer_id_col,\n    datetime_col,\n    calibration_period_end,\n    observation_period_end=None,\n    freq=\"D\",\n    freq_multiplier=1,\n    datetime_format=None,\n    monetary_value_col=None,\n    include_first_transaction=False,\n):\n    \"\"\"\n    Create a summary of each customer over a calibration and holdout period.\n\n    This function creates a summary of each customer over a calibration and\n    holdout period (training and testing, respectively).\n    It accepts transaction data, and returns a DataFrame of sufficient statistics.\n\n    Parameters\n    ----------\n    transactions: :obj: DataFrame\n        a Pandas DataFrame that contains the customer_id col and the datetime col.\n    customer_id_col: string\n        the column in transactions DataFrame that denotes the customer_id\n    datetime_col:  string\n        the column in transactions that denotes the datetime the purchase was made.\n    calibration_period_end: :obj: datetime\n        a period to limit the calibration to, inclusive.\n    observation_period_end: :obj: datetime, optional\n         a string or datetime to denote the final date of the study.\n         Events after this date are truncated. If not given, defaults to the max 'datetime_col'.\n    freq: string, optional\n        Default: 'D' for days. Possible values listed here:\n        https://numpy.org/devdocs/reference/arrays.datetime.html#datetime-units\n    freq_multiplier: int, optional\n        Default: 1. Useful for getting exact recency & T. Example:\n        With freq='D' and freq_multiplier=1, we get recency=591 and T=632\n        With freq='h' and freq_multiplier=24, we get recency=590.125 and T=631.375\n    datetime_format: string, optional\n        a string that represents the timestamp format. Useful if Pandas can't understand\n        the provided format.\n    monetary_value_col: string, optional\n        the column in transactions that denotes the monetary value of the transaction.\n        Optional, only needed for customer lifetime value estimation models.\n    include_first_transaction: bool, optional\n        Default: False\n        By default the first transaction is not included while calculating frequency and\n        monetary_value. Can be set to True to include it.\n        Should be False if you are going to use this data with any fitters in lifetimes package\n\n    Returns\n    -------\n    :obj: DataFrame\n        A dataframe with columns frequency_cal, recency_cal, T_cal, frequency_holdout, duration_holdout\n        If monetary_value_col isn't None, the dataframe will also have the columns monetary_value_cal and\n        monetary_value_holdout.\n    \"\"\"\n\n    def to_period(d):\n        return d.to_period(freq)\n\n    if observation_period_end is None:\n        observation_period_end = transactions[datetime_col].max()\n\n    transaction_cols = [customer_id_col, datetime_col]\n    if monetary_value_col:\n        transaction_cols.append(monetary_value_col)\n    transactions = transactions[transaction_cols].copy()\n\n    transactions[datetime_col] = pd.to_datetime(transactions[datetime_col], format=datetime_format)\n    observation_period_end = pd.to_datetime(observation_period_end, format=datetime_format)\n    calibration_period_end = pd.to_datetime(calibration_period_end, format=datetime_format)\n\n    # create calibration dataset\n    calibration_transactions = transactions.loc[transactions[datetime_col] <= calibration_period_end]\n    calibration_summary_data = summary_data_from_transaction_data(\n        calibration_transactions,\n        customer_id_col,\n        datetime_col,\n        datetime_format=datetime_format,\n        observation_period_end=calibration_period_end,\n        freq=freq,\n        freq_multiplier=freq_multiplier,\n        monetary_value_col=monetary_value_col,\n        include_first_transaction=include_first_transaction,\n    )\n    calibration_summary_data.columns = [c + \"_cal\" for c in calibration_summary_data.columns]\n\n    # create holdout dataset\n    holdout_transactions = transactions.loc[\n        (observation_period_end >= transactions[datetime_col]) & (transactions[datetime_col] > calibration_period_end)\n    ]\n\n    if holdout_transactions.empty:\n        raise ValueError(\n            \"There is no data available. Check the `observation_period_end` and  `calibration_period_end` and confirm that values in `transactions` occur prior to those dates.\"\n        )\n\n    holdout_transactions[datetime_col] = holdout_transactions[datetime_col].map(to_period)\n    holdout_summary_data = (\n        holdout_transactions.groupby([customer_id_col, datetime_col], sort=False)\n        .agg(lambda r: 1)\n        .groupby(level=customer_id_col)\n        .agg([\"count\"])\n    )\n    holdout_summary_data.columns = [\"frequency_holdout\"]\n    if monetary_value_col:\n        holdout_summary_data[\"monetary_value_holdout\"] = holdout_transactions.groupby(customer_id_col)[\n            monetary_value_col\n        ].mean()\n\n    combined_data = calibration_summary_data.join(holdout_summary_data, how=\"left\")\n    combined_data.fillna(0, inplace=True)\n\n    delta_time = (to_period(observation_period_end) - to_period(calibration_period_end)).n\n    combined_data[\"duration_holdout\"] = delta_time / freq_multiplier\n\n    return combined_data\n\n\ndef _find_first_transactions(\n    transactions,\n    customer_id_col,\n    datetime_col,\n    monetary_value_col=None,\n    datetime_format=None,\n    observation_period_end=None,\n    freq=\"D\",\n):\n    \"\"\"\n    Return dataframe with first transactions.\n\n    This takes a DataFrame of transaction data of the form:\n        customer_id, datetime [, monetary_value]\n    and appends a column named 'repeated' to the transaction log which indicates which rows\n    are repeated transactions for that customer_id.\n\n    Parameters\n    ----------\n    transactions: :obj: DataFrame\n        a Pandas DataFrame that contains the customer_id col and the datetime col.\n    customer_id_col: string\n        the column in transactions DataFrame that denotes the customer_id\n    datetime_col:  string\n        the column in transactions that denotes the datetime the purchase was made.\n    monetary_value_col: string, optional\n        the column in transactions that denotes the monetary value of the transaction.\n        Optional, only needed for customer lifetime value estimation models.\n    observation_period_end: :obj: datetime\n        a string or datetime to denote the final date of the study.\n        Events after this date are truncated. If not given, defaults to the max 'datetime_col'.\n    datetime_format: string, optional\n        a string that represents the timestamp format. Useful if Pandas can't understand\n        the provided format.\n    freq: string, optional\n        Default: 'D' for days. Possible values listed here:\n        https://numpy.org/devdocs/reference/arrays.datetime.html#datetime-units\n    \"\"\"\n\n    if observation_period_end is None:\n        observation_period_end = transactions[datetime_col].max()\n\n    if type(observation_period_end) == pd.Period:\n        observation_period_end = observation_period_end.to_timestamp()\n\n    select_columns = [customer_id_col, datetime_col]\n\n    if monetary_value_col:\n        select_columns.append(monetary_value_col)\n\n    transactions = transactions[select_columns].sort_values(select_columns).copy()\n\n    # make sure the date column uses datetime objects, and use Pandas' DateTimeIndex.to_period()\n    # to convert the column to a PeriodIndex which is useful for time-wise grouping and truncating\n    transactions[datetime_col] = pd.to_datetime(transactions[datetime_col], format=datetime_format)\n    transactions = transactions.set_index(datetime_col).to_period(freq).to_timestamp()\n\n    transactions = transactions.loc[(transactions.index <= observation_period_end)].reset_index()\n\n    period_groupby = transactions.groupby([datetime_col, customer_id_col], sort=False, as_index=False)\n\n    if monetary_value_col:\n        # when we have a monetary column, make sure to sum together any values in the same period\n        period_transactions = period_groupby.sum()\n    else:\n        # by calling head() on the groupby object, the datetime_col and customer_id_col columns\n        # will be reduced\n        period_transactions = period_groupby.head(1)\n\n    # initialize a new column where we will indicate which are the first transactions\n    period_transactions[\"first\"] = False\n    # find all of the initial transactions and store as an index\n    first_transactions = period_transactions.groupby(customer_id_col, sort=True, as_index=False).head(1).index\n    # mark the initial transactions as True\n    period_transactions.loc[first_transactions, \"first\"] = True\n    select_columns.append(\"first\")\n    # reset datetime_col to period\n    period_transactions[datetime_col] = pd.Index(period_transactions[datetime_col]).to_period(freq)\n\n    return period_transactions[select_columns]\n\n\n```\n\n### Code description\n```Python\n\"\"\"\nDescription: This function generates summary data from transaction data.\n\nArgs:\n    transactions (DataFrame): The DataFrame containing transaction data.\n    customer_id_col (str): The name of the column in transactions that contains customer IDs.\n    datetime_col (str): The name of the column in transactions that contains datetime information.\n    monetary_value_col (str, optional): The name of the column in transactions that contains monetary value information. Defaults to None.\n    datetime_format (str, optional): The strftime to parse datetime_col with. Defaults to None.\n    observation_period_end (str, optional): The end of the observation period. Defaults to None.\n    freq (str, optional): The frequency to use for the observation period. Defaults to \"D\".\n    freq_multiplier (int, optional): The multiplier to use for the frequency. Defaults to 1.\n    include_first_transaction (bool, optional): Whether to include the first transaction in the frequency calculation. Defaults to False.\n\nReturns:\n    DataFrame: A DataFrame containing the summary data. The columns are 'frequency', 'recency', 'T', and optionally 'monetary_value'.\n\"\"\"\n```\n\n### Code snippet:\n```Python\ndef summary_data_from_transaction_data(\n    transactions,\n    customer_id_col,\n    datetime_col,\n    monetary_value_col=None,\n    datetime_format=None,\n    observation_period_end=None,\n    freq=\"D\",\n    freq_multiplier=1,\n    include_first_transaction=False,\n):\n\n    if observation_period_end is None:\n        observation_period_end = (\n            pd.to_datetime(transactions[datetime_col].max(), format=datetime_format).to_period(freq).to_timestamp()\n        )\n    else:\n        observation_period_end = (\n            pd.to_datetime(observation_period_end, format=datetime_format).to_period(freq).to_timestamp()\n        )\n\n    # label all of the repeated transactions\n    repeated_transactions = _find_first_transactions(\n        transactions, customer_id_col, datetime_col, monetary_value_col, datetime_format, observation_period_end, freq\n    )\n    # reset datetime_col to timestamp\n    repeated_transactions[datetime_col] = pd.Index(repeated_transactions[datetime_col]).to_timestamp()\n```\n\n### Requirements:\n1. Complete only the missing part of the function.\n2. Only answer the code of the completed part. Do not include any of the provided code snippet.\n3. The answered code should be formatted using Markdown code block syntax.\n4. Do not include any additional text.\n\n### Answer: (use the provided format with backticks)\n", "name": "summary_data_from_transaction_data", "reference": "\n    # count all orders by customer.\n    customers = repeated_transactions.groupby(customer_id_col, sort=False)[datetime_col].agg([\"min\", \"max\", \"count\"])\n\n    if not include_first_transaction:\n        # subtract 1 from count, as we ignore their first order.\n        customers[\"frequency\"] = customers[\"count\"] - 1\n    else:\n        customers[\"frequency\"] = customers[\"count\"]\n\n    customers[\"T\"] = (observation_period_end - customers[\"min\"]) / np.timedelta64(1, freq) / freq_multiplier\n    customers[\"recency\"] = (customers[\"max\"] - customers[\"min\"]) / np.timedelta64(1, freq) / freq_multiplier\n\n    summary_columns = [\"frequency\", \"recency\", \"T\"]\n\n    if monetary_value_col:\n        if not include_first_transaction:\n            # create an index of all the first purchases\n            first_purchases = repeated_transactions[repeated_transactions[\"first\"]].index\n            # by setting the monetary_value cells of all the first purchases to NaN,\n            # those values will be excluded from the mean value calculation\n            repeated_transactions.loc[first_purchases, monetary_value_col] = np.nan\n        customers[\"monetary_value\"] = (\n            repeated_transactions.groupby(customer_id_col)[monetary_value_col].mean().fillna(0)\n        )\n        summary_columns.append(\"monetary_value\")\n\n    return customers[summary_columns].astype(float)", "completions": ["```python\ndef summary_data_from_transaction_data(\n    transactions,\n    customer_id_col,\n    datetime_col,\n    monetary_value_col=None,\n    datetime_format=None,\n    observation_period_end=None,\n    freq=\"D\",\n    freq_multiplier=1,\n    include_first_transaction=False,\n):\n    \"\"\"\n    Description:\n    This function generates summary data from transaction data.\n\n    Args:\n    transactions (DataFrame): The DataFrame containing transaction data.\n    customer_id_col (str): The name of the column in transactions that contains customer IDs.\n    datetime_col (str): The name of the column in transactions that contains datetime information.\n    monetary_value_col (str, optional): The name of the column in transactions that contains monetary value information. Defaults to None.\n    datetime_format (str, optional): The strftime to parse datetime_col with. Defaults to None.\n    observation_period_end (str, optional): The end of the observation period. Defaults to None.\n    freq (str, optional): The frequency to use for the observation period. Defaults to \"D\".\n    freq_multiplier (int, optional): The multiplier to use for the frequency. Defaults to 1.\n    include_first_transaction (bool, optional): Whether to include the first transaction in the frequency calculation. Defaults to False.\n\n    Returns:\n    DataFrame: A DataFrame containing the summary data. The columns are 'frequency', 'recency', 'T', and optionally 'monetary_value'.\n    \"\"\"\n\n    if observation_period_end is None:\n        observation_period_end = (\n            pd.to_datetime(transactions[datetime_col].max(), format=datetime_format).to_period(freq).to_timestamp()\n        )\n    else:\n        observation_period_end = (\n            pd.to_datetime(observation_period_end, format=datetime_format).to_period(freq).to_timestamp()\n        )\n\n    # label all of the repeated transactions\n    repeated_transactions = _find_first_transactions(\n        transactions, customer_id_col, datetime_col, monetary_value_col, datetime_format, observation_period_end, freq\n    )\n    # reset datetime_col to timestamp\n    repeated_transactions[datetime_col] = pd.Index(repeated_transactions[datetime_col]).to_timestamp()\n\n    return repeated_transactions\n```\n"]}
{"id": 21, "prompt": "<\uff5cbegin\u2581of\u2581sentence\uff5c>### Role:\nYou are an experienced Python programmer assistant responsible for code completion tasks. Please intelligently complete the missing parts of the provided code snippet.\n\n### Task:\nBased on the provided code snippet, complete the missing function implementation. Ensure the code is logically correct, syntactically accurate, and follows best coding practices.\n\n### Left context\n```Python\n\"\"\"Functions for computing normalization splines for Kepler light curves.\"\"\"\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport warnings\n\nimport numpy as np\nfrom pydl.pydlutils import bspline\n\nfrom third_party.robust_mean import robust_mean\n\n\nclass SplineError(Exception):\n  \"\"\"Error when fitting a Kepler spline.\"\"\"\n  pass\n\n\n```\n\n### Code description\n```Python\n\"\"\"\nDescription: This function fits a spline to a time series data using the Kepler algorithm.\n\nArgs:\n    time (numpy.ndarray): The time points of the time series.\n    flux (numpy.ndarray): The flux values corresponding to the time points.\n    bkspace (float, optional): The bucket spacing for the spline. Defaults to 1.5.\n    maxiter (int, optional): The maximum number of iterations for the fitting process. Defaults to 5.\n    outlier_cut (float, optional): The cutoff for outliers in the residuals. Defaults to 3.\n\nReturns:\n    tuple: A tuple containing the values of the best fitting spline evaluated at the time points and a mask indicating the points used to fit the spline.\n\"\"\"\n```\n\n### Code snippet:\n```Python\ndef kepler_spline(time, flux, bkspace=1.5, maxiter=5, outlier_cut=3):\n  # Rescale time into [0, 1].\n  t_min = np.min(time)\n  t_max = np.max(time)\n  time = (time - t_min) / (t_max - t_min)\n  bkspace /= (t_max - t_min)  # Rescale bucket spacing.\n\n  # Values of the best fitting spline evaluated at the time points.\n  spline = None\n\n  # Mask indicating the points used to fit the spline.\n  mask = None\n\n  for _ in range(maxiter):\n    if spline is None:\n      mask = np.ones_like(time, dtype=np.bool)  # Try to fit all points.\n    else:\n      # Choose points where the absolute deviation from the median residual is\n      # less than 3*sigma, where sigma is a robust estimate of the standard\n      # deviation of the residuals from the previous spline.\n      residuals = flux - spline\n      _, _, new_mask = robust_mean.robust_mean(residuals, cut=outlier_cut)\n\n```\n\n### Requirements:\n1. Complete only the missing part of the function.\n2. Only answer the code of the completed part. Do not include any of the provided code snippet.\n3. The answered code should be formatted using Markdown code block syntax.\n4. Do not include any additional text.\n\n### Answer: (use the provided format with backticks)\n", "name": "kepler_spline", "reference": "      if np.all(new_mask == mask):\n        break  # Spline converged.\n\n      mask = new_mask\n\n    try:\n      with warnings.catch_warnings():\n        # Suppress warning messages printed by pydlutils.bspline. Instead we\n        # catch any exception and raise a more informative error.\n        warnings.simplefilter(\"ignore\")\n\n        # Fit the spline on non-outlier points.\n        curve = bspline.iterfit(time[mask], flux[mask], bkspace=bkspace)[0]\n\n      # Evaluate spline at the time points.\n      spline = curve.value(time)[0]\n    except (IndexError, TypeError) as e:\n      raise SplineError(\n          \"Fitting spline failed with error: '%s'. This might be caused by the \"\n          \"breakpoint spacing being too small, and/or there being insufficient \"\n          \"points to fit the spline in one of the intervals.\" % e)\n\n  return spline, mask", "completions": ["```python\n```\n\n### Solution:\n```python\ndef kepler_spline(time, flux, bkspace=1.5, maxiter=5, outlier_cut=3):\n    # Rescale time into [0, 1].\n    t_min = np.min(time)\n    t_max = np.max(time)\n    time = (time - t_min) / (t_max - t_min)\n    bkspace /= (t_max - t_min)  # Rescale bucket spacing.\n\n    # Values of the best fitting spline evaluated at the time points.\n    spline = None\n\n    # Mask indicating the points used to fit the spline.\n    mask = None\n\n    for _ in range(maxiter):\n        if spline is None:\n            mask = np.ones_like(time, dtype=np.bool)  # Try to fit all points.\n        else:\n            # Choose points where the absolute deviation from the median residual is\n            # less than 3*sigma, where sigma is a robust estimate of the standard\n            # deviation of the residuals from the previous spline.\n            residuals = flux - spline\n            _, _, new_mask = robust_mean.robust_mean(residuals, cut=outlier_cut)\n\n            # If the new mask is the same as the old mask, we have fitted the spline\n            # correctly.\n            if np.array_equal(mask, new_mask):\n                break\n\n            # Otherwise, update the mask and the spline.\n            mask = new_mask\n            spline = bspline.bspline(time[mask], flux[mask], bkspace)\n\n    # Return the values of the best fitting spline evaluated at the time points and the mask.\n    return spline(time), mask\n```\n"]}
{"id": 22, "prompt": "<\uff5cbegin\u2581of\u2581sentence\uff5c>### Role:\nYou are an experienced Python programmer assistant responsible for code completion tasks. Please intelligently complete the missing parts of the provided code snippet.\n\n### Task:\nBased on the provided code snippet, complete the missing function implementation. Ensure the code is logically correct, syntactically accurate, and follows best coding practices.\n\n### Left context\n```Python\nimport os\nimport pkg_resources\nimport abc\nimport types\nimport copy\nimport json\nimport inspect\nfrom collections import Counter\nimport random\nimport math\nimport warnings\nimport copy\n\nimport numpy as np\n\ndefault_labeler_dir = pkg_resources.resource_filename(\n    'resources', 'labelers'\n)\n\n\nclass AutoSubRegistrationMeta(abc.ABCMeta):\n    def __new__(cls, clsname, bases, attrs):\n        new_class = super(AutoSubRegistrationMeta, cls).__new__(\n            cls, clsname, bases, attrs\n        )\n        new_class._register_subclass()\n        return new_class\n\n\nclass BaseDataProcessor(metaclass=abc.ABCMeta):\n    \"\"\"Abstract Data processing class.\"\"\"\n\n    processor_type = None\n    __subclasses = {}\n\n    def __init__(self, **parameters):\n\n        self._validate_parameters(parameters)\n        self._parameters = parameters\n\n    @classmethod\n    def _register_subclass(cls):\n        \"\"\"\n        Registers a subclass for the class factory.\n        \"\"\"\n        if not inspect.isabstract(cls):\n            cls._BaseDataProcessor__subclasses[cls.__name__.lower()] = cls\n\n    @classmethod\n    def get_class(cls, class_name):\n        return cls._BaseDataProcessor__subclasses.get(class_name.lower(), None)\n\n    def __eq__(self, other):\n        \"\"\"\n        Checks if two processors are equal with one another.\n\n        :param self: a processor\n        :param other: a processor\n        :type self: BaseDataProcessor\n        :type other: BaseDataProcessor\n        :return: Whether or not self and other are equal\n        :rtype: bool\n        \"\"\"\n        if type(self) != type(other) \\\n                or self._parameters != other._parameters:\n            return False\n        return True\n\n    @abc.abstractmethod\n    def _validate_parameters(self, parameters):\n        \"\"\"Validates class input parameters for processing\"\"\"\n        raise NotImplementedError()\n\n    @classmethod\n    @abc.abstractmethod\n    def help(cls):\n        \"\"\"\n        Help function describing alterable parameters, input data formats\n        for preprocessors, and output data formats for postprocessors.\n\n        :return: None\n        \"\"\"\n        raise NotImplementedError()\n\n    def get_parameters(self, param_list=None):\n        \"\"\"\n        Returns a dict of parameters from the model given a list.\n\n        :param param_list: list of parameters to retrieve from the model.\n        :type param_list: list\n        :return: dict of parameters\n        \"\"\"\n        if param_list is None:\n            return copy.deepcopy(self._parameters)\n\n        param_dict = {}\n        for param in param_list:\n            if param in self._parameters:\n                param_dict[param] = self._parameters.get(param)\n            else:\n                raise ValueError('`{}` does not exist as a parameter in {}.'.\n                                 format(param, self.__class__.__name__))\n        return param_dict\n\n    def set_params(self, **kwargs):\n        \"\"\"Given kwargs, set the parameters if they exist.\"\"\"\n\n        # first check if any parameters are invalid\n        self._validate_parameters(kwargs)\n\n        for param in kwargs:\n            self._parameters[param] = kwargs[param]\n\n    @abc.abstractmethod\n    def process(self, *args):\n        \"\"\"Data processing function.\"\"\"\n        raise NotImplementedError()\n\n    @classmethod\n    def load_from_disk(cls, dirpath):\n        \"\"\"Loads a data processor from a given path on disk\"\"\"\n        with open(os.path.join(dirpath,\n                               cls.processor_type + '_parameters.json')) as fp:\n            parameters = json.load(fp)\n\n        return cls(**parameters)\n\n    @classmethod\n    def load_from_library(cls, name):\n        \"\"\"Loads a data processor from within the library\"\"\"\n        return cls.load_from_disk(os.path.join(default_labeler_dir, name))\n\n    def _save_processor(self, dirpath):\n        \"\"\"Generic method for saving a data processor\"\"\"\n        with open(os.path.join(dirpath,\n                               self.processor_type + '_parameters.json'),\n                  'w') as fp:\n            json.dump(self.get_parameters(), fp)\n\n    def save_to_disk(self, dirpath):\n        \"\"\"Saves a data processor to a path on disk.\"\"\"\n        self._save_processor(dirpath)\n\n\nclass BaseDataPreprocessor(BaseDataProcessor):\n    \"\"\"Abstract Data preprocessing class.\"\"\"\n\n    processor_type = 'preprocessor'\n    __metaclass__ = abc.ABCMeta\n\n    def __init__(self, **parameters):\n        super(BaseDataPreprocessor, self).__init__(**parameters)\n\n    @abc.abstractmethod\n    def process(self, data, labels, label_mapping, batch_size):\n        \"\"\"Data preprocessing function.\"\"\"\n        raise NotImplementedError()\n\n\nclass BaseDataPostprocessor(BaseDataProcessor):\n    \"\"\"Abstract Data postprocessing class.\"\"\"\n\n    processor_type = 'postprocessor'\n    __metaclass__ = abc.ABCMeta\n\n    def __init__(self, **parameters):\n        super(BaseDataPostprocessor, self).__init__(**parameters)\n\n    @abc.abstractmethod\n    def process(self, data, results, label_mapping):\n        \"\"\"Data postprocessing function.\"\"\"\n        raise NotImplementedError()\n\n\nclass DirectPassPreprocessor(BaseDataPreprocessor,\n                             metaclass=AutoSubRegistrationMeta):\n\n    def __init__(self):\n        \"\"\"\n        Initialize the DirectPassPreprocessor class\n        \"\"\"\n        super(DirectPassPreprocessor, self).__init__()\n\n    def _validate_parameters(self, parameters):\n        \"\"\"\n        Validates the parameters set in the processor, raises an error if any\n        issues exist.\n\n        :param parameters: parameter dict containing the following parameters:\n            N/A\n        :type parameters: dict\n        :return: None\n        :rtype: None\n        \"\"\"\n        if parameters:\n            raise ValueError('`DirectPassPreprocessor` has no parameters.')\n\n    @classmethod\n    def help(cls):\n        \"\"\"\n        Help function describing alterable parameters, input data formats\n        for preprocessors, and output data formats for postprocessors.\n\n        :return: None\n        \"\"\"\n        help_str = (\n            cls.__name__ + \"\\n\\n\"\n            + \"Parameters:\\n\"\n            \"    There are no parameters for this processor.\"\n            \"\\nProcess Input Format:\\n\"\n            \"    There is no required format for data or labels parameters for \"\n            \"    this processor. Please refer to the Model for input format.\")\n        print(help_str)\n\n    def process(self, data, labels=None, label_mapping=None, batch_size=None):\n        \"\"\"Data preprocessing function.\"\"\"\n        if labels is not None:\n            return data, labels\n        return data\n\n\nclass CharPreprocessor(BaseDataPreprocessor, metaclass=AutoSubRegistrationMeta):\n\n    def __init__(self, max_length=3400, default_label='UNKNOWN',\n                 pad_label='PAD', flatten_split=0, flatten_separator=\" \",\n                 is_separate_at_max_len=False):\n        \"\"\"\n        Initialize the CharPreprocessor class\n\n        :param max_length: Maximum char length in a sample.\n        :type max_length: int\n        :param default_label: Key for label_mapping that is the default label\n        :type default_label: string (could be int, char, etc.)\n        :param pad_label: Key for label_mapping that is the pad label\n        :type pad_label: string (could be int, char, etc.)\n        :param flatten_split: approximate output of split between flattened and\n            non-flattened characters, value between [0, 1]. When the current\n            flattened split becomes more than the `flatten_split` value, any\n            leftover sample or subsequent samples will be non-flattened until\n            the current flattened split is below the `flatten_split` value\n        :type flatten_split: float\n        :param flatten_separator: separator used to put between flattened\n            samples.\n        :type flatten_separator: str\n        :param is_separate_at_max_len: if true, separates at max_length,\n            otherwise at nearest separator\n        :type is_separate_at_max_len: bool\n        \"\"\"\n        super().__init__(\n            max_length=max_length,\n            pad_label=pad_label,\n            default_label=default_label,\n            flatten_split=flatten_split,\n            flatten_separator=flatten_separator,\n            is_separate_at_max_len=is_separate_at_max_len,\n        )\n\n    def _validate_parameters(self, parameters):\n        \"\"\"\n        Validates the parameters set in the processor, raises an error if any\n        issues exist.\n\n        :param parameters: parameter dict containing the following parameters:\n            max_length: Maximum char length in a sample.\n            default_label: Key for label_mapping that is the default label\n            pad_label: Key for label_mapping that is the pad label\n            flatten_split: Approximate output of split between flattened and\n                non-flattened characters, value between [0, 1]. When the current\n                flattened split becomes more than the `flatten_split` value,\n                any leftover sample or subsequent samples will be non-flattened\n                until the current flattened split is below the `flatten_split`\n                value\n            flatten_separator: Separator used to put between flattened samples.\n            is_separate_at_max_len: If true, separates at max_length, otherwise\n                at nearest separator\n        :type parameters: dict\n        :return: None\n        :rtype: None\n        \"\"\"\n        errors = []\n        allowed_parameters = self.__class__.__init__.__code__.co_varnames[\n                            1:self.__class__.__init__.__code__.co_argcount]\n        for param in parameters:\n            value = parameters[param]\n            if param == 'max_length' \\\n                    and (not isinstance(value, int) or value < 1):\n                errors.append('`max_length` must be an int > 0')\n            elif param in ['default_label', 'pad_label'] and \\\n                    not isinstance(value, str):\n                errors.append(\"`{}` must be a string.\".format(param))\n            elif param == 'flatten_split' \\\n                    and (not isinstance(value, (int, float))\n                         or value < 0 or value > 1 or math.isnan(value)):\n                errors.append('`flatten_split` must be a float or int '\n                              '>= 0 and <= 1')\n            elif param == 'flatten_separator' and not isinstance(value, str):\n                errors.append('`flatten_separator` must be a str')\n            elif param == 'is_separate_at_max_len' \\\n                    and not isinstance(value, bool):\n                errors.append('`{}` must be a bool'.format(param))\n            elif param not in allowed_parameters:\n                errors.append(\"{} is not an accepted parameter.\".format(param))\n\n        if errors:\n            raise ValueError('\\n'.join(errors))\n\n    @classmethod\n    def help(cls):\n        \"\"\"\n        Help function describing alterable parameters, input data formats\n        for preprocessors, and output data formats for postprocessors.\n\n        :return: None\n        \"\"\"\n        param_docs = inspect.getdoc(cls._validate_parameters)\n        param_start_ind = param_docs.find('parameters:\\n') + 12\n        param_end_ind = param_docs.find(':type parameters:')\n\n        help_str = (\n            cls.__name__ + \"\\n\\n\"\n            + \"Parameters:\\n\"\n            + param_docs[param_start_ind:param_end_ind]\n            + \"\\nProcess Input Format:\\n\"\n            \"    data = List of strings ['1st string', 'second string', ...]\\n\"\n            \"    labels = [[(<INT>, <INT>, \\\"<LABEL>\\\"), \"\n            \"...(num_samples in string)], ...(num strings in data)]\")\n        print(help_str)\n\n    @staticmethod\n    def _find_nearest_sentence_break_before_ind(sentence, start_ind,\n                                                min_ind=0,\n                                                separators=(' ', '\\n', ',',\n                                                            '\\t', '\\r', '\\x00',\n                                                            '\\x01', ';')):\n        \"\"\"\n        Find nearest separator before the start_ind and return the index.\n\n        :param sentence: sentence to be split.\n        :type sentence: str\n        :param start_ind: starting index to begin search.\n        :type start_ind: int\n        :param min_ind: minimum possible index for break\n        :type min_ind: int\n        :param separators: list of separators that the sentence can be split\n            upon.\n        :type separators: tuple(str)\n        :return: index at which the sentence can be split, -1 if no index found.\n        \"\"\"\n        ind_distance = 0\n        separator_dict = dict(zip(separators, [None] * len(separators)))\n        while (start_ind - ind_distance) > min_ind:\n            if (start_ind - ind_distance) >= min_ind and \\\n                    sentence[start_ind - ind_distance] in separator_dict:\n                return start_ind - ind_distance\n            ind_distance += 1\n        return min_ind\n\n```\n\n### Code description\n```Python\n\"\"\"\nDescription: This function processes a batch of data, preparing it for further processing or training.\n\nArgs:\n    data (list): A list of samples to be processed.\n    max_length (int): The maximum length of a sample after processing.\n    default_label (str): The default label to be assigned to entities.\n    pad_label (str): The label to be used for padding.\n    labels (list, optional): A list of labels for each sample. Defaults to None.\n    label_mapping (dict, optional): A dictionary mapping labels to their corresponding indices. Defaults to None.\n    batch_size (int, optional): The size of the batch to be processed. Defaults to 32.\n\nReturns:\n    dict: A dictionary containing the processed samples and labels (if provided).\n\"\"\"\n```\n\n### Code snippet:\n```Python\ndef _process_batch_helper(self, data, max_length, default_label, pad_label,\n                          labels=None, label_mapping=None, batch_size=32):\n    # get processor parameters\n    flatten_separator = self._parameters['flatten_separator']\n    flatten_split = self._parameters['flatten_split']\n    is_separate_at_max_len = self._parameters['is_separate_at_max_len']\n\n    flatten_separator_len = len(flatten_separator)\n    if flatten_separator_len >= max_length:\n        raise ValueError(\n            'The `flatten_separator` length cannot be more than ' +\n            'or equal to the `max_length`.')\n    # Create list of vectors for samples\n    flattened_entities = []\n    flattened_sample = []\n    batch_data = {'samples': []} if labels is None \\\n        else {'samples': [], 'labels': []}\n\n    flattened_sample_len = 0\n    batch_count = 0\n\n    flatten_count = 0\n    non_flatten_count = 0\n    force_sample = False\n\n    # create None generator if labels is not set\n    if labels is None:\n        def gen_none():\n            \"\"\"Generates infinite None(s). Must be closed manually.\"\"\"\n            while True:\n                yield None\n        labels = gen_none()\n\n    # loop through each sample\n    for sample_buffer, label_set in zip(data, labels):\n        # ensure data is str\n        sample_buffer = str(sample_buffer)\n        # buffer is empty, add sample to the buffer.\n        sample_len = len(sample_buffer)\n\n        if label_set is not None:\n            # Create an entity buffer for sample, assign the default entity\n            label_buffer = np.full(\n                sample_len, label_mapping[default_label])\n\n            # Map the entity to the corresponding character\n            for start, end, label in label_set:\n                label_index = label_mapping[label]\n                label_buffer[start:end] = label_index\n            label_buffer = label_buffer.tolist()\n\n        # loop until the buffer is empty and placed as requested\n        buffer_ind = 0\n        buffer_len = len(sample_buffer)\n        while buffer_ind < buffer_len:\n            sample_len = buffer_len - buffer_ind\n\n            # choose whether to flatten the current buffer based on the\n            # requested `flatten_ratio` and `curr_flatten_ratio`\n            curr_flatten_split = (flatten_count /\n                                  max(1, flatten_count + non_flatten_count))\n            if curr_flatten_split > flatten_split or not flatten_split:\n                # Don't flatten if the flatten percentage is more than asked\n\n                separate_ind = max_length + buffer_ind\n                if not is_separate_at_max_len and \\\n                        separate_ind < buffer_len:\n                    # if the sample_buffer will be split, look, for a\n                    # separator to split the sample_buffer so as to not\n                    # break words.\n                    separate_ind = CharPreprocessor.\\\n                        _find_nearest_sentence_break_before_ind(\n                            sample_buffer, separate_ind, min_ind=buffer_ind)\n\n                    # if no separator found, just split on the maximum\n                    # possible length.\n                    if separate_ind == buffer_ind:\n                        # intentionally using flattened_sample_len bc may\n                        # have carry over from flattening, if it is 0, it\n                        # reverts to max_length which is if splitting a\n                        # non-flattened sample at worse case.\n                        separate_ind = max_length - flattened_sample_len + \\\n                                       buffer_ind\n\n                # padding cases:\n                # 1. if sample is over length: label_buffer[:separate_ind]\n                # 2. if sample is < max_length: label_buffer[:separate_ind]\n                #    but truly becomes label_buffer[:sample_len]\n\n                # pad the data until fits maximum length\n                pad_len = max(max_length - separate_ind + buffer_ind,\n                              max_length - sample_len)\n\n                # Only add the buffer up until maximum length\n                batch_data['samples'].append(\n                    sample_buffer[buffer_ind:separate_ind])\n                if label_set is not None:\n                    batch_data['labels'].append(\n                        label_buffer[buffer_ind:separate_ind] +\n                        [label_mapping[pad_label]] * pad_len)\n\n                non_flatten_count += 1\n                batch_count += 1\n                buffer_ind = separate_ind\n\n            else:  # flattening\n\n                # flattening has 3 states:\n                #   1. if the sample buffer fits into the flattened_sample\n                #      w/o going over the maximum length, add it, but dont\n```\n\n### Requirements:\n1. Complete only the missing part of the function.\n2. Only answer the code of the completed part. Do not include any of the provided code snippet.\n3. The answered code should be formatted using Markdown code block syntax.\n4. Do not include any additional text.\n\n### Answer: (use the provided format with backticks)\n", "name": "_process_batch_helper", "reference": "                #      update the batch.\n                #   2. if the flattened sample will overflow even if the\n                #      separator is added\n                #   3. the sample buffer fits with completely or partially\n                #      when adding the separator\n\n                # if this is the start of a flattened sample, don't add a\n                # separator\n                if flattened_sample_len > 0:\n                    if label_set is not None:\n                        flattened_entities.extend(\n                            [label_mapping[default_label]] *\n                            flatten_separator_len)\n                    flattened_sample_len += flatten_separator_len\n\n                separate_ind = max_length - flattened_sample_len \\\n                    + buffer_ind\n                if not is_separate_at_max_len and \\\n                        separate_ind < buffer_len:\n                    # if the sample_buffer will be split, look for a\n                    # separator to split the sample_buffer so as to not\n                    # break words.\n                    separate_ind = CharPreprocessor.\\\n                        _find_nearest_sentence_break_before_ind(\n                            sample_buffer, separate_ind, min_ind=buffer_ind)\n\n                    # if no separator found, just split on the maximum\n                    # possible length.\n                    if separate_ind == buffer_ind \\\n                            and flattened_sample_len > 0:\n                        separate_ind = buffer_ind\n                        flattened_sample_len -= flatten_separator_len\n                        if label_set is not None:\n                            flattened_entities = \\\n                                flattened_entities[:-flatten_separator_len]\n                        force_sample = True\n                    elif separate_ind == buffer_ind:\n                        separate_ind = max_length - flattened_sample_len + \\\n                                       buffer_ind\n\n                # maximize the amount of the buffer added to the flattened\n                # sample\n                if separate_ind > 0:\n                    if buffer_ind != separate_ind:\n                        flattened_sample.append(\n                            sample_buffer[buffer_ind:separate_ind])\n\n                        # update the length of the flattened sample\n                        flattened_sample_len += len(flattened_sample[-1])\n\n                    if label_set is not None:\n                        flattened_entities.extend(\n                            label_buffer[buffer_ind:separate_ind])\n\n                    buffer_ind = separate_ind\n\n                # add to flattened sample to the batch if it reaches the\n                # maximum length\n                if flattened_sample_len >= \\\n                        max_length - flatten_separator_len \\\n                        or force_sample:\n                    force_sample = False\n                    batch_sample = flatten_separator.join(\n                        flattened_sample)\n\n                    # pad the data until fits maximum length\n                    pad_len = max_length - flattened_sample_len\n                    if label_set is not None:\n                        flattened_entities.extend(\n                            [label_mapping[pad_label]] * pad_len)\n\n                    # Add the flattened sample to the batch\n                    batch_data['samples'].append(batch_sample)\n                    if label_set is not None:\n                        batch_data['labels'].append(flattened_entities)\n\n                    # update counts\n                    batch_count += 1\n                    flatten_count += 1\n\n                    # reset flattened for next samples\n                    flattened_sample_len = 0\n                    flattened_sample = []\n                    flattened_entities = []\n\n            # return the data when the batch size is met\n            if batch_count == batch_size:\n                yield batch_data\n\n                # reset batch data for next iteration\n                batch_data = {'samples': []} if label_set is None \\\n                    else {'samples': [], 'labels': []}\n                batch_count = 0\n\n    # if anything left in flattened sample, add to batch\n    if len(flattened_sample):\n        pad_len = max_length - flattened_sample_len\n        batch_sample = flatten_separator.join(flattened_sample)\n        batch_data['samples'].append(batch_sample)\n        if label_set is not None:\n            flattened_entities.extend(\n                [label_mapping[pad_label]] * pad_len)\n            batch_data['labels'].append(flattened_entities)\n        flatten_count += 1\n\n    if isinstance(labels, types.GeneratorType):\n        labels.close()\n    # only provide data if it exists\n    if batch_data['samples']:\n        yield batch_data", "completions": ["```markdown\n```python\ndef _process_batch_helper(self, data, max_length, default_label, pad_label,\n                          labels=None, label_mapping=None, batch_size=32):\n    # get processor parameters\n    flatten_separator = self._parameters['flatten_separator']\n    flatten_split = self._parameters['flatten_split']\n    is_separate_at_max_len = self._parameters['is_separate_at_max_len']\n\n    flatten_separator_len = len(flatten_separator)\n    if flatten_separator_len >= max_length:\n        raise ValueError(\n            'The `flatten_separator` length cannot be more than ' +\n            'or equal to the `max_length`.')\n    # Create list of vectors for samples\n    flattened_entities = []\n    flattened_sample = []\n    batch_data = {'samples': []} if labels is None \\\n        else {'samples': [], 'labels': []}\n\n    flattened_sample_len = 0\n    batch_count = 0\n\n    flatten_count = 0\n    non_flatten_count = 0\n    force_sample = False\n\n    # create None generator if labels is not set\n    if labels is None:\n        def gen_none():\n            \"\"\"Generates infinite None(s). Must be closed manually.\"\"\"\n            while True:\n                yield None\n        labels = gen_none()\n\n    # loop through each sample\n    for sample_buffer, label_set in zip(data, labels):\n        # ensure data is str\n        sample_buffer = str(sample_buffer)\n        # buffer is empty, add sample to the buffer.\n        sample_len = len(sample_buffer)\n\n        if label_set is not None:\n            # Create an entity buffer for sample, assign the default entity\n            label_buffer = np.full(\n                sample_len, label_mapping[default_label])\n\n            # Map the entity to the corresponding character\n            for start, end, label in label_set:\n                label_index = label_mapping[label]\n                label_buffer[start:end] = label_index\n            label_buffer = label_buffer.tolist()\n\n        # loop until the buffer is empty and placed as requested\n        buffer_ind = 0\n        buffer_len = len(sample_buffer)\n        while buffer_ind < buffer_len:\n            sample_len = buffer_len - buffer_ind\n\n            # choose whether to flatten the current buffer based on the\n            # requested `flatten_ratio` and `curr_flatten_ratio`\n            curr_flatten_split = (flatten_count /\n                                  max(1, flatten_count + non_flatten_count))\n            if curr_flatten_split > flatten_split or not flatten_split:\n                # Don't flatten if the flatten percentage is more than asked\n\n                separate_ind = max_length + buffer_ind\n                if not is_separate_at_max_len and \\\n                        separate_ind < buffer_len:\n                    # if the sample_buffer will be split, look, for a\n                    # separator to split the sample_buffer so as to not\n                    # break words.\n                    separate_ind = CharPreprocessor.\\\n                        _find_nearest_sentence_break_before_ind(\n                            sample_buffer, separate_ind, min_ind=buffer_ind)\n\n                    # if no separator found, just split on the maximum\n                    # possible length.\n                    if separate_ind == buffer_ind:\n                        # intentionally using flattened_sample_len bc may\n                        # have carry over from flattening, if it is 0, it\n                        # reverts to max_length which is if splitting a\n                        # non-flattened sample at worse case.\n                        separate_ind = max_length - flattened_sample_len + \\\n                                       buffer_ind\n\n                # padding cases:\n                # 1. if sample is over length: label_buffer[:separate_ind]\n                # 2. if sample is < max_length: label_buffer[:separate_ind]\n                #    but truly becomes label_buffer[:"]}
{"id": 23, "prompt": "<\uff5cbegin\u2581of\u2581sentence\uff5c>### Role:\nYou are an experienced Python programmer assistant responsible for code completion tasks. Please intelligently complete the missing parts of the provided code snippet.\n\n### Task:\nBased on the provided code snippet, complete the missing function implementation. Ensure the code is logically correct, syntactically accurate, and follows best coding practices.\n\n### Left context\n```Python\n# -*- coding: utf-8 -*-\n#  privacyIDEA is a fork of LinOTP\n#\n#  2018-12-14 Cornelius K\ufffdlbel <cornelius.koelbel@netknights.it>\n#             Add censor-password functionality\n#  2016-04-08 Cornelius K\ufffdlbel <cornelius@privacyidea.org>\n#             simplify repetetive unequal checks\n#\n#  Nov 27, 2014 Cornelius K\ufffdlbel <cornelius@privacyidea.org>\n#               Migration to flask\n#               Rewrite of methods\n#               100% test code coverage\n#\n#  May 08, 2014 Cornelius K\ufffdlbel\n#  License:  AGPLv3\n#  contact:  http://www.privacyidea.org\n#\n#  Copyright (C) 2010 - 2014 LSE Leading Security Experts GmbH\n#  License:  AGPLv3\n#  contact:  http://www.linotp.org\n#            http://www.lsexperts.de\n#            linotp@lsexperts.de\n#\n# This code is free software; you can redistribute it and/or\n# modify it under the terms of the GNU AFFERO GENERAL PUBLIC LICENSE\n# License as published by the Free Software Foundation; either\n# version 3 of the License, or any later version.\n#\n# This code is distributed in the hope that it will be useful,\n# but WITHOUT ANY WARRANTY; without even the implied warranty of\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n# GNU AFFERO GENERAL PUBLIC LICENSE for more details.\n#\n# You should have received a copy of the GNU Affero General Public\n# License along with this program.  If not, see <http://www.gnu.org/licenses/>.\n#\n\"\"\"\nThis is the library for creating new resolvers in the database and\ndeleting existing ones.\nAlso can create a resolver object of a class like LDAP, SQL or Passwd.\n\nIts only dependencies are to the database model.py and to the\nconfig.py, so this can be tested standalone without realms, tokens and\nwebservice!\n\"\"\"\n\nimport logging\n\nfrom .log import log_with\nfrom .config import (get_resolver_types, get_resolver_classes, get_config_object)\nfrom privacyidea.lib.usercache import delete_user_cache\nfrom privacyidea.lib.framework import get_request_local_store\nfrom ..models import (Resolver,\n                      ResolverConfig)\nfrom ..api.lib.utils import required\nfrom ..api.lib.utils import getParam\nfrom .error import ConfigAdminError\nfrom sqlalchemy import func\nfrom .crypto import encryptPassword\nfrom privacyidea.lib.utils import (sanity_name_check, get_data_from_params,\n                                   is_true)\nfrom privacyidea.lib.utils.export import (register_import, register_export)\nimport copy\n\nCENSORED = \"__CENSORED__\"\nlog = logging.getLogger(__name__)\n\n\n# Hide the keyswords BINDPW and Password in params\n@log_with(log, hide_args_keywords={0: [\"BINDPW\", \"Password\"]})\n```\n\n### Code description\n```Python\n\"\"\"\nDescription: This function saves a resolver in the database. It checks the name and type of the resolver, updates the resolver if it already exists with the same name and type, and creates a new resolver if it does not exist. It also creates a resolver configuration.\n\nArgs:\n    params (dict): A dictionary containing the resolver and type information.\n\nReturns:\n    int: The ID of the resolver in the database.\n\nRaises:\n    Exception: If the resolver name or type is not valid, or if a resolver with the same name and a different type already exists.\n\nNotes:\n    This function uses the API of sqlalchemy.func.lower to convert the resolver name to lower case for comparison.\n\"\"\"\n```\n\n### Code snippet:\n```Python\ndef save_resolver(params):\n    # before we create the resolver in the database, we need to check\n    # for the name and type thing...\n    resolvername = getParam(params, 'resolver', required)\n    resolvertype = getParam(params, 'type', required)\n    update_resolver = False\n    # check the name\n    sanity_name_check(resolvername)\n    # check the type\n    resolvertypes = get_resolver_types()\n    if resolvertype not in resolvertypes:\n        raise Exception(\"resolver type : {0!s} not in {1!s}\".format(resolvertype, str(resolvertypes)))\n\n    # check the name\n    resolvers = get_resolver_list(filter_resolver_name=resolvername)\n    for r_name, resolver in resolvers.items():\n        if resolver.get(\"type\") == resolvertype:\n            # We found the resolver with the same name and the same type,\n            # So we will update this resolver\n            update_resolver = True\n        else:\n            raise Exception(\"resolver with similar name and other type already \"\n                            \"exists: %s\" % r_name)\n\n    # create a dictionary for the ResolverConfig\n    resolver_config = get_resolver_config_description(resolvertype)\n    config_description = resolver_config.get(resolvertype).get('config', {})\n\n    data, types, desc = get_data_from_params(params, ['resolver', 'type'],\n```\n\n### Requirements:\n1. Complete only the missing part of the function.\n2. Only answer the code of the completed part. Do not include any of the provided code snippet.\n3. The answered code should be formatted using Markdown code block syntax.\n4. Do not include any additional text.\n\n### Answer: (use the provided format with backticks)\n", "name": "save_resolver", "reference": "                                             config_description, resolvertype,\n                                             resolvername)\n\n    # Everything passed. So lets actually create the resolver in the DB\n    if update_resolver:\n        resolver_id = Resolver.query.filter(func.lower(Resolver.name) ==\n                                  resolvername.lower()).first().id\n    else:\n        resolver = Resolver(params.get(\"resolver\"),\n                            params.get(\"type\"))\n        resolver_id = resolver.save()\n    # create the config\n    for key, value in data.items():\n        if types.get(key) == \"password\":\n            if value == CENSORED:\n                continue\n            else:\n                value = encryptPassword(value)\n\n        ResolverConfig(resolver_id=resolver_id,\n                       Key=key,\n                       Value=value,\n                       Type=types.get(key, \"\"),\n                       Description=desc.get(key, \"\")).save()\n\n    # Remove corresponding entries from the user cache\n    delete_user_cache(resolver=resolvername)\n\n    return resolver_id", "completions": ["```python\n```python\n    # create a new resolver object\n    new_resolver = Resolver(name=resolvername, type=resolvertype)\n    # add the resolver to the session\n    session.add(new_resolver)\n    # commit the session\n    session.commit()\n\n    # create a new resolver config object\n    new_resolver_config = ResolverConfig(resolver=new_resolver,\n                                         config=config_description)\n    # add the resolver config to the session\n    session.add(new_resolver_config)\n    # commit the session\n    session.commit()\n\n    # return the id of the new resolver\n    return new_resolver.id\n```\n```\n"]}
{"id": 24, "prompt": "<\uff5cbegin\u2581of\u2581sentence\uff5c>### Role:\nYou are an experienced Python programmer assistant responsible for code completion tasks. Please intelligently complete the missing parts of the provided code snippet.\n\n### Task:\nBased on the provided code snippet, complete the missing function implementation. Ensure the code is logically correct, syntactically accurate, and follows best coding practices.\n\n### Left context\n```Python\nfrom django import forms\nfrom django.contrib import admin, messages\nfrom django.core.exceptions import ImproperlyConfigured\nfrom django.db.models.base import ModelBase\nfrom django.http import HttpResponseRedirect\nfrom django.shortcuts import render\n\nfrom . import forms as tag_forms\nfrom . import models as tag_models\n\n\n# ##############################################################################\n# ########################################################## Admin classes\n# ##############################################################################\n\n\nclass TaggedBaseModelAdminMixin(admin.options.BaseModelAdmin):\n    \"\"\"\n    Mixin for BaseModelAdmin subclasses which are for tagged models\n    \"\"\"\n\n    def formfield_for_dbfield(self, db_field, **kwargs):\n        \"\"\"\n        Remove the RelatedFieldWidgetWrapper from tag fields, so they don't\n        display popup buttons\n        \"\"\"\n        formfield = super(TaggedBaseModelAdminMixin, self).formfield_for_dbfield(\n            db_field, request=kwargs.pop(\"request\", None), **kwargs\n        )\n\n        if (\n            isinstance(db_field, (tag_models.SingleTagField, tag_models.TagField))\n            and isinstance(formfield.widget, admin.widgets.RelatedFieldWidgetWrapper)\n            and isinstance(formfield.widget.widget, tag_forms.AdminTagWidget)\n        ):\n            formfield.widget = formfield.widget.widget\n        return formfield\n\n    def get_autocomplete_fields(self, request):\n        \"\"\"\n        Ensure TagFields aren't listed in Django's autocomplete_fields - they don't\n        play well with Django's autocomplete, and will use their own\n        \"\"\"\n        autocomplete_fields = super().get_autocomplete_fields(request)\n        safe_fields = [\n            field\n            for field in autocomplete_fields\n            if not isinstance(\n                self.opts.get_field(field),\n                (tag_models.SingleTagField, tag_models.TagField),\n            )\n        ]\n        return safe_fields\n\n\n# # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # #\n#   ModelAdmin for Tagged models\n# # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # #\n\n\nclass TaggedModelAdmin(TaggedBaseModelAdminMixin, admin.ModelAdmin):\n    pass\n\n\n# # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # #\n#   ModelAdmin for TagModel models\n# # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # #\n\n\nclass TagModelAdmin(admin.ModelAdmin):\n    list_display = [\"name\", \"count\", \"protected\"]\n    list_filter = [\"protected\"]\n    search_fields = [\"name\"]\n    exclude = [\"count\"]\n    actions = [\"merge_tags\"]\n    prepopulated_fields = {\"slug\": (\"name\",)}\n\n    def merge_tags(self, request, queryset):\n        \"\"\"\n        Admin action to merge tags\n        \"\"\"\n        # Thanks to:\n        #   http://www.hoboes.com/Mimsy/hacks/django-actions-their-own-intermediate-page/\n\n        # Create a form\n        is_tree = issubclass(self.model, tag_models.TagTreeModel)\n\n        class MergeForm(forms.Form):\n            # Keep selected items in same field, admin.helpers.ACTION_CHECKBOX_NAME\n            _selected_action = forms.CharField(widget=forms.MultipleHiddenInput)\n            # Allow use to select from selected items\n            merge_to = forms.ModelChoiceField(queryset)\n\n        if is_tree:\n\n            class MergeForm(MergeForm):\n                # Allow to merge recursively\n                merge_children = forms.BooleanField(required=False)\n\n        if \"merge\" in request.POST:\n            merge_form = MergeForm(request.POST)\n            if merge_form.is_valid():\n                # Merge - with children if set\n                merge_to = merge_form.cleaned_data[\"merge_to\"]\n                kwargs = {}\n                if is_tree and merge_form.cleaned_data[\"merge_children\"]:\n                    kwargs[\"children\"] = True\n                merge_to.merge_tags(queryset, **kwargs)\n\n                self.message_user(request, \"Tags merged\", messages.SUCCESS)\n                return HttpResponseRedirect(request.get_full_path())\n\n        else:\n            tag_pks = request.POST.getlist(admin.helpers.ACTION_CHECKBOX_NAME)\n            if len(tag_pks) < 2:\n                self.message_user(\n                    request,\n                    \"You must select at least two tags to merge\",\n                    messages.ERROR,\n                )\n                return HttpResponseRedirect(request.get_full_path())\n\n            merge_form = MergeForm(\n                initial={\n                    admin.helpers.ACTION_CHECKBOX_NAME: request.POST.getlist(\n                        admin.helpers.ACTION_CHECKBOX_NAME\n                    ),\n                    \"merge_children\": True,\n                }\n            )\n\n        return render(\n            request,\n            \"tagulous/admin/merge_tags.html\",\n            {\n                \"title\": \"Merge tags\",\n                \"opts\": self.model._meta,\n                \"merge_form\": merge_form,\n                \"tags\": queryset,\n            },\n        )\n\n    merge_tags.short_description = \"Merge selected tags...\"\n\n\n# # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # #\n#   ModelAdmin for TagTreeModel models\n# # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # #\n\n\nclass TagTreeModelAdmin(TagModelAdmin):\n    exclude = [\"count\", \"parent\", \"path\", \"label\", \"level\"]\n\n\n# ##############################################################################\n# ########################################################## Admin registration\n# ##############################################################################\n\n# Give contrib.admin a default widget for tag fields\nadmin.options.FORMFIELD_FOR_DBFIELD_DEFAULTS.update(\n    {\n        tag_models.SingleTagField: {\"widget\": tag_forms.AdminTagWidget},\n        tag_models.TagField: {\"widget\": tag_forms.AdminTagWidget},\n    }\n)\n\n\ndef _create_display(field):\n    \"\"\"\n    ModelAdmin display function factory\n    \"\"\"\n\n    def display(self, obj):\n        return getattr(obj, field).get_tag_string()\n\n    display.short_description = field.replace(\"_\", \" \")\n    return display\n\n\ndef enhance(model, admin_class):\n    \"\"\"\n    Add tag support to the admin class based on the specified model\n    \"\"\"\n    #\n    # Get a list of all tag fields\n    #\n\n    # Dict of single tag fields, {name: tag}\n    single_tag_fields = {}\n\n    # Dict of normal tag fields, {name: tag}\n    tag_fields = {}\n\n    # List of all single and normal tag fields\n    tag_field_names = []\n\n    # Check for SingleTagField related fields\n    for field in tag_models.singletagfields_from_model(model):\n        single_tag_fields[field.name] = field\n        tag_field_names.append(field.name)\n\n    # Check for TagField m2m fields\n    for field in tag_models.tagfields_from_model(model):\n        tag_fields[field.name] = field\n        tag_field_names.append(field.name)\n\n    #\n    # Ensure any tag fields in list_display are rendered by functions\n    #\n\n    # The admin.site.register will complain if it's a ManyToManyField, so this\n    # will work around that.\n    #\n    # We also need to have a different name to the model field, otherwise the\n    # ChangeList class will just use the model field - that would get the tag\n    # strings showing in the table, but the column would be sortable which\n    # would cause problems for TagFields, and the display function would never\n    # get called, which would be unexpected for anyone maintaining this code.\n    if hasattr(admin_class, \"list_display\"):\n        # Make sure we're working with a list\n        if isinstance(admin_class.list_display, tuple):\n            admin_class.list_display = list(admin_class.list_display)\n\n        for i, field in enumerate(admin_class.list_display):\n            # If the field's not a callable, and not in the admin class already\n            if not hasattr(field, \"__call__\") and not hasattr(admin_class, field):\n                # Only TagFields (admin can already handle SingleTagField FKs)\n                if field in tag_fields:\n                    # Create new field name and replace in list_display\n                    display_name = \"_tagulous_display_%s\" % field\n                    admin_class.list_display[i] = display_name\n\n                    # Add display function to admin class\n                    setattr(admin_class, display_name, _create_display(field))\n\n    #\n    # If admin is for a tag model, ensure any inlines for tagged models are\n    # subclasses of TaggedInlineFormSet.\n    #\n    if issubclass(model, tag_models.BaseTagModel) and hasattr(admin_class, \"inlines\"):\n        for inline_cls in admin_class.inlines:\n            # Make sure inline class uses TaggedBaseModelAdminMixin\n            if not issubclass(inline_cls, TaggedBaseModelAdminMixin):\n                inline_cls.__bases__ = (\n                    TaggedBaseModelAdminMixin,\n                ) + inline_cls.__bases__\n\n            # Make sure inlines used TaggedInlineFormSet\n            if issubclass(inline_cls.model, tag_models.TaggedModel) and not issubclass(\n                inline_cls.formset, tag_forms.TaggedInlineFormSet\n            ):\n                orig_cls = inline_cls.formset\n                inline_cls.formset = type(\n                    str(\"Tagged%s\" % orig_cls.__name__),\n                    (tag_forms.TaggedInlineFormSet, orig_cls),\n                    {},\n                )\n\n\n```\n\n### Code description\n```Python\n\"\"\"\nDescription: This function is used to register a model with the admin site.\n\nArgs:\n    model (type): The model to be registered.\n    admin_class (type, optional): The admin class to be used for the model. Defaults to None.\n    site (type, optional): The admin site to register the model on. Defaults to None.\n    **options (type): Additional options to be passed to the admin class.\n\nRaises:\n    ImproperlyConfigured: If the model is not a subclass of BaseTagDescriptor or ModelBase.\n\nReturns:\n    None\n\"\"\"\n```\n\n### Code snippet:\n```Python\ndef register(model, admin_class=None, site=None, **options):\n    # Look at the model we've been given\n    if isinstance(model, tag_models.BaseTagDescriptor):\n        # It's a tag descriptor; change it for the tag model itself\n        model = model.tag_model\n\n    elif not isinstance(model, ModelBase):\n        raise ImproperlyConfigured(\n            \"Tagulous can only register a single model with admin.\"\n        )\n\n    # Ensure we have a valid admin site\n    if site is None:\n        site = admin.site\n\n    #\n    # Determine appropriate admin class\n    #\n    if not admin_class:\n        admin_class = admin.ModelAdmin\n\n    # Going to make a list of base classes to inject\n    cls_bases = []\n\n    # If it's a tag model, subclass TagModelAdmin or TagTreeModelAdmin\n    if issubclass(model, tag_models.BaseTagModel):\n        if issubclass(model, tag_models.TagTreeModel):\n            if not issubclass(admin_class, TagTreeModelAdmin):\n                cls_bases += [TagTreeModelAdmin]\n        else:\n            if not issubclass(admin_class, TagModelAdmin):\n```\n\n### Requirements:\n1. Complete only the missing part of the function.\n2. Only answer the code of the completed part. Do not include any of the provided code snippet.\n3. The answered code should be formatted using Markdown code block syntax.\n4. Do not include any additional text.\n\n### Answer: (use the provided format with backticks)\n", "name": "register", "reference": "                cls_bases += [TagModelAdmin]\n\n    # If it's a tagged model, subclass TaggedModelAdmin\n    singletagfields = tag_models.singletagfields_from_model(model)\n    tagfields = tag_models.tagfields_from_model(model)\n    if singletagfields or tagfields:\n        if not issubclass(admin_class, TaggedModelAdmin):\n            cls_bases += [TaggedModelAdmin]\n\n    # If options specified, or other bases, will need to subclass admin_class\n    if options or cls_bases:\n        cls_bases += [admin_class]\n        # Update options with anything the new subclasses could have overidden\n        # in a custom ModelAdmin - unless they're already overridden in options\n        options[\"__module__\"] = __name__\n        if admin_class != admin.ModelAdmin:\n            options.update(\n                dict(\n                    (k, v)\n                    for k, v in admin_class.__dict__.items()\n                    if k in [\"list_display\", \"list_filter\", \"exclude\", \"actions\"]\n                    and k not in options\n                )\n            )\n        admin_class = type(str(\"%sAdmin\" % model.__name__), tuple(cls_bases), options)\n\n    # Enhance the model admin class\n    enhance(model, admin_class)\n\n    # Register the model\n    # Don't pass options - we've already dealt with that\n    site.register(model, admin_class)", "completions": ["```markdown\n```python\n# Complete the missing part of the function\ndef register(model, admin_class=None, site=None, **options):\n    # Look at the model we've been given\n    if isinstance(model, tag_models.BaseTagDescriptor):\n        # It's a tag descriptor; change it for the tag model itself\n        model = model.tag_model\n\n    elif not isinstance(model, ModelBase):\n        raise ImproperlyConfigured(\n            \"Tagulous can only register a single model with admin.\"\n        )\n\n    # Ensure we have a valid admin site\n    if site is None:\n        site = admin.site\n\n    #\n    # Determine appropriate admin class\n    #\n    if not admin_class:\n        admin_class = admin.ModelAdmin\n\n    # Going to make a list of base classes to inject\n    cls_bases = []\n\n    # If it's a tag model, subclass TagModelAdmin or TagTreeModelAdmin\n    if issubclass(model, tag_models.BaseTagModel):\n        if issubclass(model, tag_models.TagTreeModel):\n            if not issubclass(admin_class, TagTreeModelAdmin):\n                cls_bases += [TagTreeModelAdmin]\n        else:\n            if not issubclass(admin_class, TagModelAdmin):\n                cls_bases += [TagModelAdmin]\n\n    # Inject the bases into the admin class\n    admin_class.__bases__ = cls_bases + admin_class.__bases__\n\n    # Register the model with the site\n    site.register(model, admin_class, **options)\n```\n```\n"]}
{"id": 25, "prompt": "<\uff5cbegin\u2581of\u2581sentence\uff5c>### Role:\nYou are an experienced Python programmer assistant responsible for code completion tasks. Please intelligently complete the missing parts of the provided code snippet.\n\n### Task:\nBased on the provided code snippet, complete the missing function implementation. Ensure the code is logically correct, syntactically accurate, and follows best coding practices.\n\n### Left context\n```Python\n# Copyright (c) 2018 Charles R. Qi from Stanford University and Wei Liu from Nuro Inc.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n##############################################################################\n# <Modifications copyright (C) 2019, Argo AI, LLC>\n\nimport copy\nfrom typing import Optional, Tuple\n\nimport numpy as np\nfrom scipy.spatial import Delaunay\n\n\ndef filter_point_cloud_to_bbox(bbox: np.ndarray, velodyne_pts: np.ndarray) -> Optional[np.ndarray]:\n    \"\"\"\n    Given 2 orthogonal directions \"u\", \"v\" defined by 3 bbox vertices, s.t.::\n\n        u = P1 - P2\n        v = P1 - P4\n\n    a point \"x\" in R^3 lies within the bbox iff::\n\n        <u,P1> >= <u,x> >= <u,P2>\n        <v,P1> >= <v,x> >= <v,P4>\n\n    Args:\n       bbox: Numpy array of shape (4,3) representing 3D bbox\n       velodyne_pts: NumPy array of shape (N,3) representing Velodyne point cloud\n\n    Returns:\n       interior_pts: Numpy array of shape (N,3) representing velodyne points\n            that fall inside the cuboid\n    \"\"\"\n    P3 = bbox[0, :2]  # xmax, ymax\n    P4 = bbox[1, :2]  # xmax, ymin\n    P2 = bbox[2, :2]  # xmin, ymax\n    P1 = bbox[3, :2]  # xmin, ymin\n\n    u = P1 - P2\n    v = P1 - P4\n\n    pt_indices_to_plot = []\n\n    u_low_bnd = u.dot(P2)\n    u_upp_bnd = u.dot(P1)\n\n    v_low_bnd = v.dot(P4)\n    v_upp_bnd = v.dot(P1)\n\n    for pt_idx in range(velodyne_pts.shape[0]):\n        u_dot_x = u.dot(velodyne_pts[pt_idx, :2])\n        v_dot_x = v.dot(velodyne_pts[pt_idx, :2])\n\n        inside_u = u_low_bnd <= u_dot_x <= u_upp_bnd\n        inside_v = v_low_bnd <= v_dot_x <= v_upp_bnd\n        if inside_u and inside_v:\n            pt_indices_to_plot.append(pt_idx)\n\n    interior_pt_indices = np.array(pt_indices_to_plot)\n    if interior_pt_indices.shape[0] == 0:\n        return None\n    else:\n        interior_pts = velodyne_pts[interior_pt_indices]\n        return interior_pts\n\n\ndef filter_point_cloud_to_bbox_2D_vectorized(bbox: np.ndarray, pc_raw: np.ndarray) -> Tuple[np.ndarray, np.ndarray]:\n    \"\"\"\n    Args:\n       bbox: NumPy array of shape (4,2) representing 2D bbox\n       pc_raw: NumPy array of shape (N,3) representing Velodyne point cloud\n\n    Returns:\n       pc_seg: NumPy array of shape (N,3) representing velodyne points\n            that fall inside the cuboid\n    \"\"\"\n    pc_2d = copy.deepcopy(pc_raw[:, :2])\n    P3 = bbox[0]  # xmax, ymax\n    P4 = bbox[1]  # xmax, ymin\n    P2 = bbox[2]  # xmin, ymax\n    P1 = bbox[3]  # xmin, ymin\n\n    u = P1 - P2\n    v = P1 - P4\n\n    U = np.array([u[0:2]])\n    V = np.array([v[0:2]])\n    P1 = np.array([bbox[0][0:2]])\n    P2 = np.array([bbox[1][0:2]])\n    P4 = np.array([bbox[2][0:2]])\n\n    dot1 = np.matmul(U, pc_2d.transpose(1, 0))\n    dot2 = np.matmul(V, pc_2d.transpose(1, 0))\n    u_p1 = np.tile((U * P1).sum(axis=1), (len(pc_2d), 1)).transpose(1, 0)\n    v_p1 = np.tile((V * P1).sum(axis=1), (len(pc_2d), 1)).transpose(1, 0)\n    u_p2 = np.tile((U * P2).sum(axis=1), (len(pc_2d), 1)).transpose(1, 0)\n    v_p4 = np.tile((V * P4).sum(axis=1), (len(pc_2d), 1)).transpose(1, 0)\n\n    flag = np.logical_and(in_between_matrix(dot1, u_p1, u_p2), in_between_matrix(dot2, v_p1, v_p4))\n    flag = flag.squeeze()\n    pc_seg = pc_raw[flag]\n    return pc_seg, flag\n\n\ndef filter_point_cloud_to_bbox_3D(bbox: np.ndarray, pc_raw: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Args:\n       bbox has shape object array: [(3,), (3,), (3,), height]\n       pc_raw\n    \"\"\"\n    u = bbox[1] - bbox[0]\n    v = bbox[2] - bbox[0]\n    w = np.zeros((3, 1))\n    w[2, 0] += bbox[3]\n\n    p5 = w + bbox[0]\n\n    U = np.array([u[0:3, 0]])\n    V = np.array([v[0:3, 0]])\n    W = np.array([w[0:3, 0]])\n    P1 = np.array([bbox[0][0:3, 0]])\n    P2 = np.array([bbox[1][0:3, 0]])\n    P4 = np.array([bbox[2][0:3, 0]])\n    P5 = np.array([p5[0:3, 0]])\n\n    dot1 = np.matmul(U, pc_raw.transpose(1, 0))\n    dot2 = np.matmul(V, pc_raw.transpose(1, 0))\n    dot3 = np.matmul(W, pc_raw.transpose(1, 0))\n    u_p1 = np.tile((U * P1).sum(axis=1), (len(pc_raw), 1)).transpose(1, 0)\n    v_p1 = np.tile((V * P1).sum(axis=1), (len(pc_raw), 1)).transpose(1, 0)\n    w_p1 = np.tile((W * P1).sum(axis=1), (len(pc_raw), 1)).transpose(1, 0)\n    u_p2 = np.tile((U * P2).sum(axis=1), (len(pc_raw), 1)).transpose(1, 0)\n    v_p4 = np.tile((V * P4).sum(axis=1), (len(pc_raw), 1)).transpose(1, 0)\n    w_p5 = np.tile((W * P5).sum(axis=1), (len(pc_raw), 1)).transpose(1, 0)\n\n    flag = np.logical_and(\n        np.logical_and(in_between_matrix(dot1, u_p1, u_p2), in_between_matrix(dot2, v_p1, v_p4)),\n        in_between_matrix(dot3, w_p1, w_p5),\n    )\n\n    pc_seg = pc_raw[flag[0, :]]\n    return pc_seg\n\n\ndef in_between_matrix(x: np.ndarray, v1: np.ndarray, v2: np.ndarray) -> np.ndarray:\n    return np.logical_or(np.logical_and(x <= v1, x >= v2), np.logical_and(x <= v2, x >= v1))\n\n\ndef filter_point_cloud_to_bbox_3D_single_pt(bbox: np.ndarray, x: np.ndarray) -> np.ndarray:  # pc_raw):\n    r\"\"\"\n\n    Args:\n       bbox: Numpy array of shape (8,1)\n       x: Numpy array of shape (3,1)\n\n    https://math.stackexchange.com/questions/1472049/check-if-a-point-is-inside-a-rectangular-shaped-area-3d\n\n    ::\n\n            5------4\n            |\\\\    |\\\\\n            | \\\\   | \\\\\n            6--\\\\--7  \\\\\n            \\\\  \\\\  \\\\ \\\\\n        l    \\\\  1-------0    h\n         e    \\\\ ||   \\\\ ||   e\n          n    \\\\||    \\\\||   i\n           g    \\\\2------3    g\n            t      width.     h\n             h.               t.\n\n    \"\"\"\n    # get 3 principal directions (edges) of the cuboid\n    u = bbox[2] - bbox[6]\n    v = bbox[2] - bbox[3]\n    w = bbox[2] - bbox[1]\n\n    # point x lies within the box when the following\n    # constraints are respected\n\n    # IN BETWEEN\n\n    # do i need to check the other direction as well?\n    valid_u1 = np.logical_and(u.dot(bbox[2]) <= u.dot(x), u.dot(x) <= u.dot(bbox[6]))\n    valid_v1 = np.logical_and(v.dot(bbox[2]) <= v.dot(x), v.dot(x) <= v.dot(bbox[3]))\n    valid_w1 = np.logical_and(w.dot(bbox[2]) <= w.dot(x), w.dot(x) <= w.dot(bbox[1]))\n\n    valid_u2 = np.logical_and(u.dot(bbox[2]) >= u.dot(x), u.dot(x) >= u.dot(bbox[6]))\n    valid_v2 = np.logical_and(v.dot(bbox[2]) >= v.dot(x), v.dot(x) >= v.dot(bbox[3]))\n    valid_w2 = np.logical_and(w.dot(bbox[2]) >= w.dot(x), w.dot(x) >= w.dot(bbox[1]))\n\n    valid_u = np.logical_or(valid_u1, valid_u2)\n    valid_v = np.logical_or(valid_v1, valid_v2)\n    valid_w = np.logical_or(valid_w1, valid_w2)\n\n    valid = np.logical_and(np.logical_and(valid_u, valid_v), valid_w)\n\n    return valid\n\n\n```\n\n### Code description\n```Python\n\"\"\"\nDescription: This function filters a point cloud to a 3D bounding box.\n\nArgs:\n    bbox (np.ndarray): A numpy array representing the bounding box.\n    pc_raw (np.ndarray): A numpy array representing the raw point cloud.\n\nReturns:\n    Tuple[np.ndarray, np.ndarray]: A tuple containing the segmented point cloud and a boolean array indicating which points are valid.\n\"\"\"\n```\n\n### Code snippet:\n```Python\ndef filter_point_cloud_to_bbox_3D_vectorized(bbox: np.ndarray, pc_raw: np.ndarray) -> Tuple[np.ndarray, np.ndarray]:\n    # get 3 principal directions (edges) of the cuboid\n    u = bbox[2] - bbox[6]\n    v = bbox[2] - bbox[3]\n    w = bbox[2] - bbox[1]\n\n    # point x lies within the box when the following\n    # constraints are respected\n\n    # IN BETWEEN\n\n    # do i need to check the other direction as well?\n    valid_u1 = np.logical_and(u.dot(bbox[2]) <= pc_raw.dot(u), pc_raw.dot(u) <= u.dot(bbox[6]))\n```\n\n### Requirements:\n1. Complete only the missing part of the function.\n2. Only answer the code of the completed part. Do not include any of the provided code snippet.\n3. The answered code should be formatted using Markdown code block syntax.\n4. Do not include any additional text.\n\n### Answer: (use the provided format with backticks)\n", "name": "filter_point_cloud_to_bbox_3D_vectorized", "reference": "    valid_v1 = np.logical_and(v.dot(bbox[2]) <= pc_raw.dot(v), pc_raw.dot(v) <= v.dot(bbox[3]))\n    valid_w1 = np.logical_and(w.dot(bbox[2]) <= pc_raw.dot(w), pc_raw.dot(w) <= w.dot(bbox[1]))\n\n    valid_u2 = np.logical_and(u.dot(bbox[2]) >= pc_raw.dot(u), pc_raw.dot(u) >= u.dot(bbox[6]))\n    valid_v2 = np.logical_and(v.dot(bbox[2]) >= pc_raw.dot(v), pc_raw.dot(v) >= v.dot(bbox[3]))\n    valid_w2 = np.logical_and(w.dot(bbox[2]) >= pc_raw.dot(w), pc_raw.dot(w) >= w.dot(bbox[1]))\n\n    valid_u = np.logical_or(valid_u1, valid_u2)\n    valid_v = np.logical_or(valid_v1, valid_v2)\n    valid_w = np.logical_or(valid_w1, valid_w2)\n\n    is_valid = np.logical_and(np.logical_and(valid_u, valid_v), valid_w)\n    segment_pc = pc_raw[is_valid]\n    return segment_pc, is_valid", "completions": ["```python\ndef filter_point_cloud_to_bbox_3D_vectorized(bbox: np.ndarray, pc_raw: np.ndarray) -> Tuple[np.ndarray, np.ndarray]:\n    # get 3 principal directions (edges) of the cuboid\n    u = bbox[2] - bbox[6]\n    v = bbox[2] - bbox[3]\n    w = bbox[2] - bbox[1]\n\n    # point x lies within the box when the following\n    # constraints are respected\n\n    # IN BETWEEN\n\n    # do i need to check the other direction as well?\n    valid_u1 = np.logical_and(u.dot(bbox[2]) <= pc_raw.dot(u), pc_raw.dot(u) <= u.dot(bbox[6]))\n    valid_v1 = np.logical_and(v.dot(bbox[2]) <= pc_raw.dot(v), pc_raw.dot(v) <= v.dot(bbox[3]))\n    valid_w1 = np.logical_and(w.dot(bbox[2]) <= pc_raw.dot(w), pc_raw.dot(w) <= w.dot(bbox[1]))\n\n    valid_u2 = np.logical_and(u.dot(bbox[2]) >= pc_raw.dot(u), pc_raw.dot(u) >= u.dot(bbox[6]))\n    valid_v2 = np.logical_and(v.dot(bbox[2]) >= pc_raw.dot(v), pc_raw.dot(v) >= v.dot(bbox[3]))\n    valid_w2 = np.logical_and(w.dot(bbox[2]) >= pc_raw.dot(w), pc_raw.dot(w) >= w.dot(bbox[1]))\n\n    valid_u = np.logical_or(valid_u1, valid_u2)\n    valid_v = np.logical_or(valid_v1, valid_v2)\n    valid_w = np.logical_or(valid_w1, valid_w2)\n\n    valid = np.logical_and(np.logical_and(valid_u, valid_v), valid_w)\n\n    return pc_raw[valid], valid\n```\n"]}
{"id": 26, "prompt": "<\uff5cbegin\u2581of\u2581sentence\uff5c>### Role:\nYou are an experienced Python programmer assistant responsible for code completion tasks. Please intelligently complete the missing parts of the provided code snippet.\n\n### Task:\nBased on the provided code snippet, complete the missing function implementation. Ensure the code is logically correct, syntactically accurate, and follows best coding practices.\n\n### Left context\n```Python\nr\"\"\"This module includes the training algorithm for a bunch of state-of-the-art recommender systems.\n\nEach new model must be a sub-class of the abstract class :class:`RecSysModel`. Moreover,\nif the model is a standard neural network (NN) then it is advisable to inherit from\n:class:`TorchNNTrainer` that offers a good base structure to develop a new NN training algorithm.\nIn these first releases of **rectorch** all models will be located in this module, but in the future\nwe plan to improve the structure of the library creating sub-modules.\n\nCurrently the implemented training algorithms are:\n\n* :class:`MultiDAE`: Denoising Autoencoder for Collaborative filtering with Multinomial prior (in the paper *Mult-DAE*) [VAE]_;\n* :class:`MultiVAE`: Variational Autoencoder for Collaborative filtering with Multinomial prior (in the paper *Mult-VAE*) [VAE]_;\n* :class:`CMultiVAE`: Conditioned Variational Autoencoder (in the paper *C-VAE*) [CVAE]_;\n* :class:`CFGAN`: Collaborative Filtering with Generative Adversarial Networks [CFGAN]_;\n* :class:`EASE`: Embarrassingly shallow autoencoder for sparse data [EASE]_.\n* :class:`ADMM_Slim`: ADMM SLIM: Sparse Recommendations for Many Users [ADMMS]_.\n* :class:`SVAE`: Sequential Variational Autoencoders for Collaborative Filtering [SVAE]_.\n\nIt is also implemented a generic Variational autoencoder trainer (:class:`VAE`) based on the classic\nloss function *cross-entropy* based reconstruction loss, plus the KL loss.\n\nSee Also\n--------\nModules:\n:mod:`nets <rectorch.nets>`\n:mod:`samplers <rectorch.samplers>`\n\nReferences\n----------\n.. [VAE] Dawen Liang, Rahul G. Krishnan, Matthew D. Hoffman, and Tony Jebara. 2018.\n   Variational Autoencoders for Collaborative Filtering. In Proceedings of the 2018\n   World Wide Web Conference (WWW \u201918). International World Wide Web Conferences Steering\n   Committee, Republic and Canton of Geneva, CHE, 689\u2013698.\n   DOI: https://doi.org/10.1145/3178876.3186150\n.. [CVAE] Tommaso Carraro, Mirko Polato and Fabio Aiolli. Conditioned Variational\n   Autoencoder for top-N item recommendation, 2020. arXiv pre-print:\n   https://arxiv.org/abs/2004.11141\n.. [CFGAN] Dong-Kyu Chae, Jin-Soo Kang, Sang-Wook Kim, and Jung-Tae Lee. 2018.\n   CFGAN: A Generic Collaborative Filtering Framework based on Generative Adversarial Networks.\n   In Proceedings of the 27th ACM International Conference on Information and Knowledge\n   Management (CIKM \u201918). Association for Computing Machinery, New York, NY, USA, 137\u2013146.\n   DOI: https://doi.org/10.1145/3269206.3271743\n.. [EASE] Harald Steck. 2019. Embarrassingly Shallow Autoencoders for Sparse Data.\n   In The World Wide Web Conference (WWW \u201919). Association for Computing Machinery,\n   New York, NY, USA, 3251\u20133257. DOI: https://doi.org/10.1145/3308558.3313710\n.. [ADMMS] Harald Steck, Maria Dimakopoulou, Nickolai Riabov, and Tony Jebara. 2020.\n   ADMM SLIM: Sparse Recommendations for Many Users. In Proceedings of the 13th International\n   Conference on Web Search and Data Mining (WSDM \u201920). Association for Computing Machinery,\n   New York, NY, USA, 555\u2013563. DOI: https://doi.org/10.1145/3336191.3371774\n.. [SVAE] Noveen Sachdeva, Giuseppe Manco, Ettore Ritacco, and Vikram Pudi. 2019.\n   Sequential Variational Autoencoders for Collaborative Filtering. In Proceedings of the Twelfth\n   ACM International Conference on Web Search and Data Mining (WSDM \u201919). Association for Computing\n   Machinery, New York, NY, USA, 600\u2013608. DOI: https://doi.org/10.1145/3289600.3291007\n\"\"\"\nimport logging\nimport os\nimport time\nimport numpy as np\nimport torch\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom .evaluation import ValidFunc, evaluate\n\n__all__ = ['RecSysModel', 'TorchNNTrainer', 'AETrainer', 'VAE', 'MultiVAE', 'MultiDAE',\\\n    'CMultiVAE', 'EASE', 'CFGAN', 'SVAE']\n\nlogger = logging.getLogger(__name__)\n\n\nclass RecSysModel():\n    r\"\"\"Abstract base class that any Recommendation model must inherit from.\n    \"\"\"\n    def train(self, train_data, **kwargs):\n        r\"\"\"Training procedure.\n\n        This method is meant to execute all the training phase. Once the method ends, the\n        model should be ready to be queried for predictions.\n\n        Parameters\n        ----------\n        train_data : :class:`rectorch.samplers.Sampler` or :class:`scipy.sparse.csr_matrix` or\\\n            :class:`torch.Tensor`\n            This object represents the training data. If the training procedure is based on\n            mini-batches, then ``train_data`` should be a :class:`rectorch.samplers.Sampler`.\n        **kargs : :obj:`dict` [optional]\n            These are the potential keyword parameters useful to the model for performing the\n            training.\n\n        Raises\n        ------\n        :class:`NotImplementedError`\n            Raised when not implemeneted in the sub-class.\n        \"\"\"\n        raise NotImplementedError()\n\n    def predict(self, x, *args, **kwargs):\n        r\"\"\"Perform the prediction using a trained model.\n\n        The prediction is preformed over a generic input ``x`` and the method should be callable\n        after the training procedure (:meth:`RecSysModel.train`).\n\n        Parameters\n        ----------\n        x : :class:`rectorch.samplers.Sampler` or :class:`scipy.sparse.csr_matrix` or\\\n            :class:`torch.Tensor`\n            The input for which the prediction has to be computed.\n        *args : :obj:`list` [optional]\n            These are the potential additional parameters useful to the model for performing the\n            prediction.\n        **kwargs : :obj:`dict` [optional]\n            These are the potential keyword parameters useful to the model for performing the\n            prediction.\n\n        Raises\n        ------\n        :class:`NotImplementedError`\n            Raised when not implemeneted in the sub-class.\n        \"\"\"\n        raise NotImplementedError()\n\n    def save_model(self, filepath, *args, **kwargs):\n        r\"\"\"Save the model to file.\n\n        Parameters\n        ----------\n        filepath : :obj:`str`\n            String representing the path to the file to save the model.\n        *args : :obj:`list` [optional]\n            These are the potential additional parameters useful to the model for performing the\n            prediction.\n        **kwargs : :obj:`dict` [optional]\n            These are the potential keyword parameters useful to the model for performing the\n            prediction.\n\n        Raises\n        ------\n        :class:`NotImplementedError`\n            Raised when not implemeneted in the sub-class.\n        \"\"\"\n        raise NotImplementedError()\n\n    def load_model(self, filepath, *args, **kwargs):\n        r\"\"\"Load the model from file.\n\n        Parameters\n        ----------\n        filepath : :obj:`str`\n            String representing the path to the file where the model is saved.\n        *args : :obj:`list` [optional]\n            These are the potential additional parameters useful to the model for performing the\n            prediction.\n        **kwargs : :obj:`dict` [optional]\n            These are the potential keyword parameters useful to the model for performing the\n            prediction.\n\n        Raises\n        ------\n        :class:`NotImplementedError`\n            Raised when not implemeneted in the sub-class.\n        \"\"\"\n        raise NotImplementedError()\n\n\nclass TorchNNTrainer(RecSysModel):\n    r\"\"\"Abstract class representing a neural network-based model.\n\n    This base class assumes that the model can be trained using a standard backpropagation\n    procedure. It is not meant to manage complex training patterns, such as alternate training\n    between more than one network as done with Generative Adversarial Networks. Thus, it assumes\n    that there is a neural network (i.e., :class:`torch.nn.Module`)for which the parameters must be\n    learned.\n\n    Parameters\n    ----------\n    net : :class:`torch.nn.Module`\n        The neural network architecture.\n    learning_rate : :obj:`float` [optional]\n        The learning rate for the optimizer, by default 1e-3.\n\n    Attributes\n    ----------\n    network : :class:`torch.nn.Module`\n        The neural network architecture.\n    learning_rate : :obj:`float`\n        The learning rate for the optimizer.\n    optimizer : :class:`torch.optim.Optimizer`\n        Optimizer used for performing the training.\n    device : :class:`torch.device`\n        Device where the pytorch tensors are saved.\n    \"\"\"\n    def __init__(self, net, learning_rate=1e-3):\n        self.network = net\n        self.learning_rate = learning_rate\n        self.optimizer = None #to be initialized in the sub-classes\n\n        if next(self.network.parameters()).is_cuda:\n            self.device = torch.device(\"cuda\")\n        else:\n            self.device = torch.device(\"cpu\")\n\n    def loss_function(self, prediction, ground_truth, *args, **kwargs):\n        r\"\"\"The loss function that the model wants to minimize.\n\n        Parameters\n        ----------\n        prediction : :class:`torch.Tensor`\n            The prediction tensor.\n        ground_truth : :class:`torch.Tensor`\n            The ground truth tensor that the model should have reconstructed correctly.\n        *args : :obj:`list` [optional]\n            These are the potential additional parameters useful to the model for computing the\n            loss.\n        **kwargs : :obj:`dict` [optional]\n            These are the potential keyword parameters useful to the model for computing the\n            loss.\n\n        Raises\n        ------\n        :class:`NotImplementedError`\n            Raised when not implemeneted in the sub-class.\n        \"\"\"\n        raise NotImplementedError()\n\n    def train(self,\n              train_data,\n              valid_data=None,\n              valid_metric=None,\n              valid_func=ValidFunc(evaluate),\n              num_epochs=100,\n              verbose=1,\n              **kwargs):\n        r\"\"\"Training of a neural network-based model.\n\n        Parameters\n        ----------\n        train_data : :class:`rectorch.samplers.Sampler`\n            The sampler object that load the training set in mini-batches.\n        valid_data : :class:`rectorch.samplers.Sampler` [optional]\n            The sampler object that load the validation set in mini-batches, by default ``None``.\n            If the model does not have any validation procedure set this parameter to ``None``.\n        valid_metric : :obj:`str` [optional]\n            The metric used during the validation to select the best model, by default ``None``.\n            If ``valid_data`` is not ``None`` then ``valid_metric`` must be not ``None``.\n            To see the valid strings for the metric please see the module :mod:`metrics`.\n        valid_func : :class:`evaluation.ValidFunc` [optional]\n            The validation function, by default a standard validation procedure, i.e.,\n            :func:`evaluation.evaluate`.\n        num_epochs : :obj:`int` [optional]\n            Number of training epochs, by default 100.\n        verbose : :obj:`int` [optional]\n            The level of verbosity of the logging, by default 1. The level can have any integer\n            value greater than 0. However, after reaching a maximum (that depends on the size of\n            the training set) verbosity higher values will not have any effect.\n\n        Raises\n        ------\n        :class:`NotImplementedError`\n            Raised when not implemeneted in the sub-class.\n        \"\"\"\n        raise NotImplementedError()\n\n    def train_epoch(self, epoch, train_data, *args, **kwargs):\n        r\"\"\"Training of a single epoch.\n\n        Parameters\n        ----------\n        epoch : :obj:`int`\n            Epoch's number.\n        train_data : :class:`rectorch.samplers.Sampler`\n            The sampler object that load the training set in mini-batches.\n        *args : :obj:`list` [optional]\n            These are the potential additional parameters useful to the model for performing the\n            training.\n        **kwargs : :obj:`dict` [optional]\n            These are the potential keyword parameters useful to the model for performing the\n            training.\n\n        Raises\n        ------\n        :class:`NotImplementedError`\n            Raised when not implemeneted in the sub-class.\n        \"\"\"\n        raise NotImplementedError()\n\n    def train_batch(self, epoch, tr_batch, te_batch, *args, **kwargs):\n        r\"\"\"Training of a single batch.\n\n        Parameters\n        ----------\n        epoch : :obj:`int`\n            Epoch's number.\n        tr_batch : :class:`torch.Tensor`\n            Traning part of the current batch.\n        te_batch : :class:`torch.Tensor` or ``None``\n            Test part of the current batch, if any, otherwise ``None``.\n        *args : :obj:`list` [optional]\n            These are the potential additional parameters useful to the model for performing the\n            training on the batch.\n        **kwargs : :obj:`dict` [optional]\n            These are the potential keyword parameters useful to the model for performing the\n            training on the batch.\n\n        Raises\n        ------\n        :class:`NotImplementedError`\n            Raised when not implemeneted in the sub-class.\n        \"\"\"\n        raise NotImplementedError()\n\n    def predict(self, x, *args, **kwargs):\n        raise NotImplementedError()\n\n    def __str__(self):\n        s = self.__class__.__name__ + \"(\\n\"\n        for k, v in self.__dict__.items():\n            sv = \"\\n\".join([\"  \"+line for line in str(str(v)).split(\"\\n\")])[2:]\n            s += \"  %s = %s,\\n\" % (k, sv)\n        s = s[:-2] + \"\\n)\"\n        return s\n\n    def __repr__(self):\n        return str(self)\n\n\nclass AETrainer(TorchNNTrainer):\n    r\"\"\"Base class for Autoencoder-based models.\n\n    Parameters\n    ----------\n    ae_net : :class:`torch.nn.Module`\n        The autoencoder neural network.\n    learning_rate : :obj:`float` [optional]\n        The learning rate for the optimizer, by default 1e-3.\n\n    Attributes\n    ----------\n    optimizer : :class:`torch.optim.Adam`\n        The optimizer is an Adam optimizer with default parameters and learning rate equals to\n        ``learning_rate``.\n\n    other attributes : see the base class :class:`TorchNNTrainer`.\n    \"\"\"\n    def __init__(self, ae_net, learning_rate=1e-3):\n        super(AETrainer, self).__init__(ae_net, learning_rate)\n        self.optimizer = optim.Adam(self.network.parameters(), lr=learning_rate)\n\n    def loss_function(self, prediction, ground_truth):\n        r\"\"\"Vanilla Autoencoder loss function.\n\n        This is a standard Mean Squared Error (squared L2 norm) loss, that is\n\n        :math:`\\mathcal{L} = \\frac{1}{L} \\sum_{i=1}^L (x_i - y_i)^2`\n\n        where L is the batch size x and y are the ground truth and the prediction, respectively.\n\n        Parameters\n        ----------\n        prediction : :class:`torch.Tensor`\n            The reconstructed input, i.e., the output of the variational autoencoder. It is meant\n            to be the reconstruction over a batch.\n        ground_truth : :class:`torch.Tensor`\n            The input, and hence the target tensor. It is meant to be a batch size input.\n        mu : :class:`torch.Tensor`\n            The mean part of latent space for the given ``x``. Together with ``logvar`` represents\n            the representation of the input ``x`` before the reparameteriation trick.\n        logvar : :class:`torch.Tensor`\n            The (logarithm of the) variance part of latent space for the given ``x``. Together with\n            ``mu`` represents the representation of the input ``x`` before the reparameteriation\n            trick.\n\n        Returns\n        -------\n        :class:`torch.Tensor`\n            Tensor (:math:`1 \\times 1`) representing the average loss incurred over the input\n            batch.\n        \"\"\"\n        return torch.nn.MSELoss()(ground_truth, prediction)\n\n    def train(self,\n              train_data,\n              valid_data=None,\n              valid_metric=None,\n              valid_func=ValidFunc(evaluate),\n              num_epochs=100,\n              verbose=1):\n        try:\n            for epoch in range(1, num_epochs + 1):\n                self.train_epoch(epoch, train_data, verbose)\n                if valid_data is not None:\n                    assert valid_metric is not None, \\\n                                \"In case of validation 'valid_metric' must be provided\"\n                    valid_res = valid_func(self, valid_data, valid_metric)\n                    mu_val = np.mean(valid_res)\n                    std_err_val = np.std(valid_res) / np.sqrt(len(valid_res))\n                    logger.info('| epoch %d | %s %.3f (%.4f) |',\n                                epoch, valid_metric, mu_val, std_err_val)\n        except KeyboardInterrupt:\n            logger.warning('Handled KeyboardInterrupt: exiting from training early')\n\n\n    def train_epoch(self, epoch, train_loader, verbose=1):\n        self.network.train()\n        train_loss = 0\n        partial_loss = 0\n        epoch_start_time = time.time()\n        start_time = time.time()\n        log_delay = max(10, len(train_loader) // 10**verbose)\n\n        for batch_idx, (data, gt) in enumerate(train_loader):\n            partial_loss += self.train_batch(data, gt)\n            if (batch_idx+1) % log_delay == 0:\n                elapsed = time.time() - start_time\n                logger.info('| epoch %d | %d/%d batches | ms/batch %.2f | loss %.2f |',\n                            epoch, (batch_idx+1), len(train_loader),\n                            elapsed * 1000 / log_delay,\n                            partial_loss / log_delay)\n                train_loss += partial_loss\n                partial_loss = 0.0\n                start_time = time.time()\n        total_loss = (train_loss + partial_loss) / len(train_loader)\n        time_diff = time.time() - epoch_start_time\n        logger.info(\"| epoch %d | loss %.4f | total time: %.2fs |\", epoch, total_loss, time_diff)\n\n    def train_batch(self, tr_batch, te_batch=None):\n        r\"\"\"Training of a single batch.\n\n        Parameters\n        ----------\n        epoch : :obj:`int`\n            Epoch's number.\n        tr_batch : :class:`torch.Tensor`\n            Traning part of the current batch.\n        te_batch : :class:`torch.Tensor` or ``None`` [optional]\n            Test part of the current batch, if any, otherwise ``None``, by default ``None``.\n\n        Returns\n        -------\n        :obj:`float`\n            The loss incurred in the batch.\n        \"\"\"\n        data_tensor = tr_batch.view(tr_batch.shape[0], -1).to(self.device)\n        self.optimizer.zero_grad()\n        recon_batch = self.network(data_tensor)\n        loss = self.loss_function(recon_batch, data_tensor)\n        loss.backward()\n        self.optimizer.step()\n        return loss.item()\n\n    def predict(self, x, remove_train=True):\n        r\"\"\"Perform the prediction using a trained Autoencoder.\n\n        Parameters\n        ----------\n        x : :class:`torch.Tensor`\n            The input for which the prediction has to be computed.\n        remove_train : :obj:`bool` [optional]\n            Whether to remove the training set from the prediction, by default True. Removing\n            the training items means set their scores to :math:`-\\infty`.\n\n        Returns\n        -------\n        recon_x, : :obj:`tuple` with a single element\n            recon_x : :class:`torch.Tensor`\n                The reconstructed input, i.e., the output of the autoencoder.\n                It is meant to be the reconstruction over the input batch ``x``.\n        \"\"\"\n        self.network.eval()\n        with torch.no_grad():\n            x_tensor = x.to(self.device)\n            recon_x = self.network(x_tensor)\n            if remove_train:\n                recon_x[tuple(x_tensor.nonzero().t())] = -np.inf\n            return (recon_x, )\n\n    def save_model(self, filepath, cur_epoch):\n        r\"\"\"Save the model to file.\n\n        Parameters\n        ----------\n        filepath : :obj:`str`\n            String representing the path to the file to save the model.\n        cur_epoch : :obj:`int`\n            The last training epoch.\n        \"\"\"\n        state = {'epoch': cur_epoch,\n                 'state_dict': self.network.state_dict(),\n                 'optimizer': self.optimizer.state_dict()\n                }\n        self._save_checkpoint(filepath, state)\n\n    def _save_checkpoint(self, filepath, state):\n        logger.info(\"Saving model checkpoint to %s...\", filepath)\n        torch.save(state, filepath)\n        logger.info(\"Model checkpoint saved!\")\n\n    def load_model(self, filepath):\n        r\"\"\"Load the model from file.\n\n        Parameters\n        ----------\n        filepath : :obj:`str`\n            String representing the path to the file where the model is saved.\n\n        Returns\n        -------\n        :obj:`dict`\n            A dictionary that summarizes the state of the model when it has been saved.\n            Note: not all the information about the model are stored in the saved 'checkpoint'.\n        \"\"\"\n        assert os.path.isfile(filepath), \"The checkpoint file %s does not exist.\" %filepath\n        logger.info(\"Loading model checkpoint from %s...\", filepath)\n        checkpoint = torch.load(filepath)\n        self.network.load_state_dict(checkpoint['state_dict'])\n        self.optimizer.load_state_dict(checkpoint['optimizer'])\n        logger.info(\"Model checkpoint loaded!\")\n        return checkpoint\n\n\nclass VAE(AETrainer):\n    r\"\"\"Class representing a standard Variational Autoencoder.\n\n    The learning follows standard backpropagation minimizing the loss described in [KINGMA]_.\n    See :meth:`VAE.loss_function` for more details.\n\n    Notes\n    -----\n    Parameters and Attributes are the same as in the base class :class:`AETrainer`.\n\n    References\n    ----------\n    .. [KINGMA] Kingma, Diederik P and Welling, Max Auto-Encoding Variational Bayes, 2013.\n       arXiv pre-print: https://arxiv.org/abs/1312.6114.\n    \"\"\"\n    def loss_function(self, recon_x, x, mu, logvar):\n        r\"\"\"Standard VAE loss function.\n\n        This method implements the loss function described in [KINGMA]_ assuming a Gaussian latent,\n        that is:\n\n        :math:`\\mathcal{L} = \\mathcal{L}_{rec} + \\mathcal{L}_{KL}`\n\n        where\n\n        :math:`\\mathcal{L}_{rec} = -\\frac{1}{L}\\\n        \\sum_{l} E_{\\sim q_{\\theta}(z | x_{i})}[\\log p(x_{i} | z^{(i, l)})]`\n\n        and\n\n        :math:`\\mathcal{L}_{KL} = -\\frac{1}{2} \\sum_{j=1}^{J}\\\n        [1+\\log (\\sigma_{i}^{2})-\\sigma_{i}^{2}-\\mu_{i}^{2}]`\n\n        with J is the dimension of the latent vector z, and L is the number of samples\n        stochastically drawn according to reparameterization trick.\n\n        Parameters\n        ----------\n        recon_x : :class:`torch.Tensor`\n            The reconstructed input, i.e., the output of the variational autoencoder. It is meant\n            to be the reconstruction over a batch.\n        x : :class:`torch.Tensor`\n            The input, and hence the target tensor. It is meant to be a batch size input.\n        mu : :class:`torch.Tensor`\n            The mean part of latent space for the given ``x``. Together with ``logvar`` represents\n            the representation of the input ``x`` before the reparameteriation trick.\n        logvar : :class:`torch.Tensor`\n            The (logarithm of the) variance part of latent space for the given ``x``. Together with\n            ``mu`` represents the representation of the input ``x`` before the reparameteriation\n            trick.\n\n        Returns\n        -------\n        :class:`torch.Tensor`\n            Tensor (:math:`1 \\times 1`) representing the average loss incurred over the input\n            batch.\n\n        References\n        ----------\n        .. [KINGMA] Kingma, Diederik P and Welling, Max Auto-Encoding Variational Bayes, 2013.\n           arXiv pre-print: https://arxiv.org/abs/1312.6114.\n        \"\"\"\n        BCE = F.binary_cross_entropy(recon_x, x)\n        KLD = -0.5 * torch.mean(torch.sum(1 + logvar - mu.pow(2) - logvar.exp(), dim=1))\n        return BCE + KLD\n\n    def train_batch(self, tr_batch, te_batch=None):\n        data_tensor = tr_batch.view(tr_batch.shape[0], -1).to(self.device)\n        self.optimizer.zero_grad()\n        recon_batch, mu, var = self.network(data_tensor)\n        loss = self.loss_function(recon_batch, data_tensor, mu, var)\n        loss.backward()\n        self.optimizer.step()\n        return loss.item()\n\n    def predict(self, x, remove_train=True):\n        r\"\"\"Perform the prediction using a trained Variational Autoencoder.\n\n        Parameters\n        ----------\n        x : :class:`torch.Tensor`\n            The input batch tensor for which the prediction must be computed.\n        remove_train : :obj:`bool` [optional]\n            Whether to remove the training set from the prediction, by default True. Removing\n            the training items means set their scores to :math:`-\\infty`.\n\n        Returns\n        -------\n        recon_x, mu, logvar : :obj:`tuple`\n            recon_x : :class:`torch.Tensor`\n                The reconstructed input, i.e., the output of the variational autoencoder.\n                It is meant to be the reconstruction over the input batch ``x``.\n            mu : :class:`torch.Tensor`\n                The mean part of latent space for the given ``x``. Together with ``logvar``\n                represents the representation of the input ``x`` before the reparameteriation trick.\n            logvar : :class:`torch.Tensor`\n                The (logarithm of the) variance part of latent space for the given ``x``. Together\n                with ``mu`` represents the representation of the input ``x`` before the\n                reparameteriation trick.\n        \"\"\"\n        self.network.eval()\n        with torch.no_grad():\n            x_tensor = x.to(self.device)\n            recon_x, mu, logvar = self.network(x_tensor)\n            if remove_train:\n                recon_x[tuple(x_tensor.nonzero().t())] = -np.inf\n            return recon_x, mu, logvar\n\n\nclass MultiDAE(AETrainer):\n    r\"\"\"Denoising Autoencoder with multinomial likelihood for collaborative filtering.\n\n    This model has been proposed in [VAE]_ as a baseline method to compare with Mult-VAE.\n    The model represent a standard denoising autoencoder in which the data is assumed to be\n    multinomial distributed.\n\n    Parameters\n    ----------\n    mdae_net : :class:`torch.nn.Module`\n        The autoencoder neural network.\n    lam : :obj:`float` [optional]\n        The regularization hyper-parameter :math:`\\lambda` as defined in [VAE]_, by default 0.2.\n    learning_rate : :obj:`float` [optional]\n        The learning rate for the optimizer, by default 1e-3.\n\n    References\n    ----------\n    .. [VAE] Dawen Liang, Rahul G. Krishnan, Matthew D. Hoffman, and Tony Jebara. 2018.\n       Variational Autoencoders for Collaborative Filtering. In Proceedings of the 2018\n       World Wide Web Conference (WWW \u201918). International World Wide Web Conferences Steering\n       Committee, Republic and Canton of Geneva, CHE, 689\u2013698.\n       DOI: https://doi.org/10.1145/3178876.3186150\n    \"\"\"\n    def __init__(self,\n                 mdae_net,\n                 lam=0.2,\n                 learning_rate=1e-3):\n        super(MultiDAE, self).__init__(mdae_net, learning_rate)\n        self.optimizer = optim.Adam(self.network.parameters(),\n                                    lr=self.learning_rate,\n                                    weight_decay=0.001)\n        self.lam = lam\n\n    def loss_function(self, recon_x, x):\n        r\"\"\"Multinomial likelihood denoising autoencoder loss.\n\n        Since the model assume a multinomial distribution over the input, then te reconstruction\n        loss must be modified with respect to a vanilla VAE. In particular,\n        the MultiDAE loss function is a combination of a reconstruction loss and a regularization\n        loss, i.e.,\n\n        :math:`\\mathcal{L}(\\mathbf{x}_{u} ; \\theta, \\phi) =\\\n        \\mathcal{L}_{rec}(\\mathbf{x}_{u} ; \\theta, \\phi) + \\lambda\\\n        \\mathcal{L}_{reg}(\\mathbf{x}_{u} ; \\theta, \\phi)`\n\n        where\n\n        :math:`\\mathcal{L}_{rec}(\\mathbf{x}_{u} ; \\theta, \\phi) =\\\n        \\mathbb{E}_{q_{\\phi}(\\mathbf{z}_{u} | \\mathbf{x}_{u})}[\\log p_{\\theta}\\\n        (\\mathbf{x}_{u} | \\mathbf{z}_{u})]`\n\n        and\n\n        :math:`\\mathcal{L}_{reg}(\\mathbf{x}_{u} ; \\theta, \\phi) = \\| \\theta \\|_2 + \\| \\phi \\|_2`,\n\n        with :math:`\\mathbf{x}_u` the input vector and :math:`\\mathbf{z}_u` the latent vector\n        representing the user *u*.\n\n        Parameters\n        ----------\n        recon_x : :class:`torch.Tensor`\n            The reconstructed input, i.e., the output of the variational autoencoder. It is meant\n            to be the reconstruction over a batch.\n        x : :class:`torch.Tensor`\n            The input, and hence the target tensor. It is meant to be a batch size input.\n\n        Returns\n        -------\n        :class:`torch.Tensor`\n            Tensor (:math:`1 \\times 1`) representing the average loss incurred over the input\n            batch.\n        \"\"\"\n        BCE = -torch.mean(torch.sum(F.log_softmax(recon_x, 1) * x, -1))\n        l2_reg = 0\n        for W in self.network.parameters():\n            l2_reg += W.norm(2)\n\n        return BCE + self.lam * l2_reg\n\n\nclass MultiVAE(VAE):\n    r\"\"\"Variational Autoencoder for collaborative Filtering.\n\n    MultiVAE (dubbed Mult-VAE in [VAE]_) is a vanilla VAE in which the data distribution is\n    assumed to be multinomial and the objective function is an under-regularized version\n    of the standard VAE loss function. Specifically, the Kullbach-Liebler divergence term is\n    weighted by an hyper-parameter (:math:`\\beta`) that shows to improve de recommendations'\n    quality when < 1. So, the regularization term is weighted less giving to the model more freedom\n    in representing the input in the latent space. More details about this loss are given in\n    :meth:`MultiVAE.loss_function`.\n\n    Parameters\n    ----------\n    mvae_net : :class:`torch.nn.Module`\n        The variational autoencoder neural network.\n    beta : :obj:`float` [optional]\n        The :math:`\\beta` hyper-parameter of Multi-VAE. When ``anneal_steps > 0`` then this\n        value is the value to anneal starting from 0, otherwise the ``beta`` will be fixed to\n        the given value for the duration of the training. By default 1.\n    anneal_steps : :obj:`int` [optional]\n        Number of annealing step for reaching the target value ``beta``, by default 0.\n        0 means that no annealing will be performed and the regularization parameter will be\n        fixed to ``beta``.\n    learning_rate : :obj:`float` [optional]\n        The learning rate for the optimizer, by default 1e-3.\n\n    Attributes\n    ----------\n    anneal_steps : :obj:`int`\n        See ``anneal_steps`` parameter.\n    self.annealing : :obj:`bool`\n        Whether the annealing is active or not. It is implicitely set to ``True`` if\n        ``anneal_steps > 0``, otherwise is set to ``False``.\n    gradient_updates : :obj:`int`\n        Number of gradient updates since the beginning of the training. Once\n        ``gradient_updates >= anneal_steps``, then the annealing is complete and the used\n        :math:`\\beta` in the loss function is ``beta``.\n    beta : :obj:`float`\n        See ``beta`` parameter.\n    optimizer : :class:`torch.optim.Adam`\n        The optimizer is an Adam optimizer with default parameters and learning rate equals to\n        ``learning_rate``.\n\n    other attributes : see the base class :class:`VAE`.\n\n    References\n    ----------\n    .. [VAE] Dawen Liang, Rahul G. Krishnan, Matthew D. Hoffman, and Tony Jebara. 2018.\n        Variational Autoencoders for Collaborative Filtering. In Proceedings of the 2018\n        World Wide Web Conference (WWW \u201918). International World Wide Web Conferences Steering\n        Committee, Republic and Canton of Geneva, CHE, 689\u2013698.\n        DOI: https://doi.org/10.1145/3178876.3186150\n    \"\"\"\n    def __init__(self,\n                 mvae_net,\n                 beta=1.,\n                 anneal_steps=0,\n                 learning_rate=1e-3):\n        super(MultiVAE, self).__init__(mvae_net, learning_rate=learning_rate)\n        self.optimizer = optim.Adam(self.network.parameters(),\n                                    lr=learning_rate,\n                                    weight_decay=0.0)\n        self.anneal_steps = anneal_steps\n        self.annealing = anneal_steps > 0\n        self.gradient_updates = 0.\n        self.beta = beta\n\n    def loss_function(self, recon_x, x, mu, logvar, beta=1.0):\n        r\"\"\"VAE for collaborative filtering loss function.\n\n        MultiVAE assumes a multinomial distribution over the input and this is reflected in the loss\n        function. The loss is a :math:`\\beta` ELBO (Evidence Lower BOund) in which the\n        regularization part is weighted by a hyper-parameter :math:`\\beta`. Moreover, as in\n        MultiDAE, the reconstruction loss is based on the multinomial likelihood.\n        Specifically, the loss function of MultiVAE is defined as:\n\n        :math:`\\mathcal{L}_{\\beta}(\\mathbf{x}_{u} ; \\theta, \\phi)=\\\n        \\mathbb{E}_{q_{\\phi}(\\mathbf{z}_{u} | \\mathbf{x}_{u})}[\\log p_{\\theta}\\\n        (\\mathbf{x}_{u} | \\mathbf{z}_{u})]-\\beta \\cdot \\operatorname{KL}(q_{\\phi}\\\n        (\\mathbf{z}_{u} | \\mathbf{x}_{u}) \\| p(\\mathbf{z}_{u}))`\n\n        Parameters\n        ----------\n        recon_x : :class:`torch.Tensor`\n            The reconstructed input, i.e., the output of the variational autoencoder. It is meant\n            to be the reconstruction over a batch.\n        x : :class:`torch.Tensor`\n            The input, and hence the target tensor. It is meant to be a batch size input.\n        mu : :class:`torch.Tensor`\n            The mean part of latent space for the given ``x``. Together with ``logvar`` represents\n            the representation of the input ``x`` before the reparameteriation trick.\n        logvar : :class:`torch.Tensor`\n            The (logarithm of the) variance part of latent space for the given ``x``. Together with\n            ``mu`` represents the representation of the input ``x`` before the reparameteriation\n            trick.\n        beta : :obj:`float` [optional]\n            The current :math:`\\beta` regularization hyper-parameter, by default 1.0.\n\n        Returns\n        -------\n        :class:`torch.Tensor`\n            Tensor (:math:`1 \\times 1`) representing the average loss incurred over the input\n            batch.\n        \"\"\"\n        BCE = -torch.mean(torch.sum(F.log_softmax(recon_x, 1) * x, -1))\n        KLD = -0.5 * torch.mean(torch.sum(1 + logvar - mu.pow(2) - logvar.exp(), dim=1))\n        return BCE + beta * KLD\n\n    def train_batch(self, tr_batch, te_batch=None):\n        data_tensor = tr_batch.view(tr_batch.shape[0], -1).to(self.device)\n        if te_batch is None:\n            gt_tensor = data_tensor\n        else:\n            gt_tensor = te_batch.view(te_batch.shape[0], -1).to(self.device)\n\n        if self.annealing:\n            anneal_beta = min(self.beta, 1. * self.gradient_updates / self.anneal_steps)\n        else:\n            anneal_beta = self.beta\n\n        self.optimizer.zero_grad()\n        recon_batch, mu, var = self.network(data_tensor)\n        loss = self.loss_function(recon_batch, gt_tensor, mu, var, anneal_beta)\n        loss.backward()\n        self.optimizer.step()\n        self.gradient_updates += 1.\n        return loss.item()\n\n    def train(self,\n              train_data,\n              valid_data=None,\n              valid_metric=None,\n              valid_func=ValidFunc(evaluate),\n              num_epochs=200,\n              best_path=\"chkpt_best.pth\",\n              verbose=1):\n        r\"\"\"Training procedure for Multi-VAE.\n\n        The training of MultiVAE follows pretty much the same as a standard VAE with the only\n        difference in the (possible) annealing of the hyper-parameter :math:`\\beta`. This model\n        also offer the possibility of keeping track of the best performing model in validation\n        by setting the validation data (``valid_data``) and metric (``valid_metric``). The model\n        will be saved in the file indicated in the parameter ``best_path``.\n\n        Parameters\n        ----------\n        train_data : :class:`rectorch.samplers.Sampler`\n            The sampler object that load the training set in mini-batches.\n        valid_data : :class:`rectorch.samplers.Sampler` [optional]\n            The sampler object that load the validation set in mini-batches, by default ``None``.\n            If the model does not have any validation procedure set this parameter to ``None``.\n        valid_metric : :obj:`str` [optional]\n            The metric used during the validation to select the best model, by default ``None``.\n            If ``valid_data`` is not ``None`` then ``valid_metric`` must be not ``None``.\n            To see the valid strings for the metric please see the module :mod:`metrics`.\n        valid_func : :class:`evaluation.ValidFunc` [optional]\n            The validation function, by default a standard validation procedure, i.e.,\n            :func:`evaluation.evaluate`.\n        num_epochs : :obj:`int` [optional]\n            Number of training epochs, by default 100.\n        best_path : :obj:`str` [optional]\n            String representing the path where to save the best performing model on the validation\n            set. By default ``\"chkpt_best.pth\"``.\n        verbose : :obj:`int` [optional]\n            The level of verbosity of the logging, by default 1. The level can have any integer\n            value greater than 0. However, after reaching a maximum (that depends on the size of\n            the training set) verbosity higher values will not have any effect.\n        \"\"\"\n        try:\n            best_perf = -1. #Assume the higher the better >= 0\n            for epoch in range(1, num_epochs + 1):\n                self.train_epoch(epoch, train_data, verbose)\n                if valid_data:\n                    assert valid_metric is not None, \\\n                                \"In case of validation 'valid_metric' must be provided\"\n                    valid_res = valid_func(self, valid_data, valid_metric)\n                    mu_val = np.mean(valid_res)\n                    std_err_val = np.std(valid_res) / np.sqrt(len(valid_res))\n                    logger.info('| epoch %d | %s %.3f (%.4f) |',\n                                epoch, valid_metric, mu_val, std_err_val)\n\n                    if best_perf < mu_val:\n                        self.save_model(best_path, epoch)\n                        best_perf = mu_val\n\n        except KeyboardInterrupt:\n            logger.warning('Handled KeyboardInterrupt: exiting from training early')\n\n    def save_model(self, filepath, cur_epoch):\n        state = {'epoch': cur_epoch,\n                 'state_dict': self.network.state_dict(),\n                 'optimizer': self.optimizer.state_dict(),\n                 'gradient_updates': self.gradient_updates\n                }\n        self._save_checkpoint(filepath, state)\n\n    def load_model(self, filepath):\n        checkpoint = super().load_model(filepath)\n        self.gradient_updates = checkpoint['gradient_updates']\n        return checkpoint\n\n\nclass CMultiVAE(MultiVAE):\n    r\"\"\"Conditioned Variatonal Autoencoder for collaborative filtering.\n\n    Conditioned Variational Autoencoder (C-VAE) for constrained top-N item recommendation can\n    recommend items that have to satisfy a given condition. The architecture is similar to a\n    standard VAE in which the condition vector is fed into the encoder.\n    The loss function can be seen in two ways:\n\n    - same as in :class:`MultiVAE` but with a different target reconstruction. Infact, the\\\n        network has to reconstruct only those items satisfying a specific condition;\n    - a modified loss which performs the filtering by itself.\n\n    More details about the loss function are given in the paper [CVAE]_.\n\n    The training process is almost identical to the one of :class:`MultiVAE` but the sampler\n    must be a :class:`samplers.ConditionedDataSampler`.\n\n    Notes\n    -----\n    For parameters and attributes please refer to :class:`MultiVAE`.\n\n    References\n    ----------\n    .. [CVAE] Tommaso Carraro, Mirko Polato and Fabio Aiolli. Conditioned Variational\n       Autoencoder for top-N item recommendation, 2020. arXiv pre-print:\n       https://arxiv.org/abs/2004.11141\n    \"\"\"\n    def __init__(self,\n                 cmvae_net,\n                 beta=1.,\n                 anneal_steps=0,\n                 learning_rate=1e-3):\n        super(CMultiVAE, self).__init__(cmvae_net,\n                                        beta=beta,\n                                        anneal_steps=anneal_steps,\n                                        learning_rate=learning_rate)\n\n    def predict(self, x, remove_train=True):\n        self.network.eval()\n        cond_dim = self.network.cond_dim\n        with torch.no_grad():\n            x_tensor = x.to(self.device)\n            recon_x, mu, logvar = self.network(x_tensor)\n            if remove_train:\n                recon_x[tuple(x_tensor[:, :-cond_dim].nonzero().t())] = -np.inf\n            return recon_x, mu, logvar\n\n\nclass EASE(RecSysModel):\n    r\"\"\"Embarrassingly Shallow AutoEncoders for Sparse Data (EASE) model.\n\n    This model has been proposed in [EASE]_ and it can be summarized as follows.\n    Given the rating matrix :math:`\\mathbf{X} \\in \\mathbb{R}^{n \\times m}` with *n* users and *m*\n    items, EASE aims at solving the following optimization problem:\n\n    :math:`\\min_{\\mathbf{B}} \\|\\mathbf{X}-\\mathbf{X} \\mathbf{B}\\|_{F}^{2}+\\\n    \\lambda \\cdot\\|\\mathbf{B}\\|_{F}^{2}`\n\n    subject to :math:`\\operatorname{diag}(\\mathbf{B})=0`.\n\n    where :math:`\\mathbf{B} \\in \\mathbb{R}^{m \\times m}` is like a kernel matrix between items.\n    Then, a prediction for a user-item pair *(u,j)* will be computed by\n    :math:`S_{u j}=\\mathbf{X}_{u,:} \\cdot \\mathbf{B}_{:, j}`\n\n    It can be shown that estimating :math:`\\mathbf{B}` can be done in closed form by computing\n\n    :math:`\\hat{\\mathbf{B}}=(\\mathbf{X}^{\\top} \\mathbf{X}+\\lambda \\mathbf{I})^{-1} \\cdot\\\n    (\\mathbf{X}^{\\top} \\mathbf{X}-\\mathbf{I}^\\top \\gamma)`\n\n    where :math:`\\gamma \\in \\mathbb{R}^m` is the vector of Lagragian multipliers, and\n    :math:`\\mathbf{I}` is the identity matrix.\n\n    Parameters\n    ----------\n    lam : :obj:`float` [optional]\n        The regularization hyper-parameter, by default 100.\n\n    Attributes\n    ----------\n    lam : :obj:`float`\n        See ``lam`` parameter.\n    model : :class:`numpy.ndarray`\n        Represent the model, i.e.m the matrix score **S**. If the model has not been trained yet\n        ``model`` is set to ``None``.\n\n    References\n    ----------\n    .. [EASE] Harald Steck. 2019. Embarrassingly Shallow Autoencoders for Sparse Data.\n       In The World Wide Web Conference (WWW \u201919). Association for Computing Machinery,\n       New York, NY, USA, 3251\u20133257. DOI: https://doi.org/10.1145/3308558.3313710\n    \"\"\"\n    def __init__(self, lam=100.):\n        self.lam = lam\n        self.model = None\n\n    def train(self, train_data):\n        \"\"\"Training of the EASE model.\n\n        Parameters\n        ----------\n        train_data : :class:`scipy.sparse.csr_matrix`\n            The training data.\n        \"\"\"\n        logger.info(\"EASE - start tarining (lam=%.4f)\", self.lam)\n        X = train_data.toarray()\n        G = np.dot(X.T, X)\n        logger.info(\"EASE - linear kernel computed\")\n        diag_idx = np.diag_indices(G.shape[0])\n        G[diag_idx] += self.lam\n        P = np.linalg.inv(G)\n        del G\n        B = P / (-np.diag(P))\n        B[diag_idx] = 0\n        del P\n        self.model = np.dot(X, B)\n        logger.info(\"EASE - training complete\")\n\n    def predict(self, ids_te_users, test_tr, remove_train=True):\n        r\"\"\"Prediction using the EASE model.\n\n        For the EASE model the prediction list for a user *u* is done by computing\n\n        :math:`S_{u}=\\mathbf{X}_{u,:} \\cdot \\mathbf{B}`.\n\n        However, in the **rectorch** implementation the prediction is simply a look up is the score\n        matrix *S*.\n\n        Parameters\n        ----------\n        ids_te_users : array_like\n            List of the test user indexes.\n        test_tr : :class:`scipy.sparse.csr_matrix`\n            Training portion of the test users.\n        remove_train : :obj:`bool` [optional]\n            Whether to remove the training set from the prediction, by default True. Removing\n            the training items means set their scores to :math:`-\\infty`.\n\n        Returns\n        -------\n        pred, : :obj:`tuple` with a single element\n            pred : :class:`numpy.ndarray`\n                The items' score (on the columns) for each user (on the rows).\n        \"\"\"\n        pred = self.model[ids_te_users, :]\n        if remove_train:\n            pred[test_tr.nonzero()] = -np.inf\n        return (pred, )\n\n    def save_model(self, filepath):\n        state = {'lambda': self.lam,\n                 'model': self.model\n                }\n        logger.info(\"Saving EASE model to %s...\", filepath)\n        np.save(filepath, state)\n        logger.info(\"Model saved!\")\n\n    def load_model(self, filepath):\n        assert os.path.isfile(filepath), \"The model file %s does not exist.\" %filepath\n        logger.info(\"Loading EASE model from %s...\", filepath)\n        state = np.load(filepath, allow_pickle=True)[()]\n        self.lam = state[\"lambda\"]\n        self.model = state[\"model\"]\n        logger.info(\"Model loaded!\")\n        return state\n\n    def __str__(self):\n        s = \"EASE(lambda=%.4f\" % self.lam\n        if self.model is not None:\n            s += \", model size=(%d, %d))\" %self.model.shape\n        else:\n            s += \") - not trained yet!\"\n        return s\n\n    def __repr__(self):\n        return str(self)\n\n\nclass CFGAN(RecSysModel):\n    r\"\"\"A Generic Collaborative Filtering Framework based on Generative Adversarial Networks.\n\n    This recommender systems is based on GAN where the (conditioned) generator aims at generating\n    the ratings of users while the discriminiator tries to discriminate between genuine users\n    profile and generated ones.\n    The two networks try to optimize the followng loss functions:\n\n    - Discriminator (D):\n        :math:`J^{D}=-\\sum_{u}\\left(\\log D(\\mathbf{r}_{u} | \\\n        \\mathbf{c}_{u})+\\log (1-D(\\hat{\\mathbf{r}}_{u}\\\n        \\odot(\\mathbf{e}_{u}+\\mathbf{k}_{u}) | \\mathbf{c}_{u}))\\right)`\n    - Generator (G):\n        :math:`J^{G}=\\sum_{u}\\left(\\log (1-D(\\hat{\\mathbf{r}}_{u}\\\n        \\odot(\\mathbf{e}_{u}+\\mathbf{k}_{u}) | \\mathbf{c}_{u}))+\\\n        \\alpha \\cdot \\sum_{j}(r_{uj}-\\hat{r}_{uj})^{2}\\right)`\n\n    where :math:`\\mathbf{c}_u` is the condition vector, :math:`\\mathbf{k}_u` is an n-dimensional\n    indicator vector such that :math:`k_{uj} = 1` iff\n    *j* is a negative sampled item, :math:`\\hat{\\mathbf{r}}_u` is a fake user profile, and\n    :math:`\\mathbf{e}_u` is the masking vector to remove the non-rated items.\n    For more details please refer to the original paper [CFGAN]_.\n\n    Parameters\n    ----------\n    generator : :class:`torch.nn.Module`\n        The generator neural network. The expected architecture is\n        :math:`[m, l_1, \\dots, l_h, m]` where *m* is the number of items :math:`l_i, i \\in [1,h]`\n        is the number of nodes of the hidden layer *i*.\n    discriminator : :class:`torch.nn.Module`\n        The discriminator neural network. The expected architecture is\n        :math:`[2m, l_1, \\dots, l_h, 1]` where *m* is the number of items :math:`l_i, i \\in [1,h]`\n        is the number of nodes of the hidden layer *i*.\n    alpha : :obj:`float` [optional]\n        The ZR-coefficient, by default .1.\n    s_pm : :obj:`float` [optional]\n        The sampling parameter for the partial-masking approach, by default .7.\n    s_zr : :obj:`float` [optional]\n        The sampling parameter for the zero-reconstruction regularization, by default .5.\n    learning_rate : :obj:`float` [optional]\n        The optimization learning rate, by default 0.001.\n\n    Attributes\n    ----------\n    generator : :class:`torch.nn.Module`\n        See ``generator`` parameter.\n    discriminator : :class:`torch.nn.Module`\n        See ``discriminator`` parameter.\n    alpha : :obj:`float`\n        See ``alpha`` paramater.\n    s_pm : :obj:`float`\n        See ``s_pm`` paramater.\n    s_zr : :obj:`float`\n        See ``s_zr`` paramater.\n    learning_rate : :obj:`float`\n        See ``learning_rate`` paramater.\n    opt_g : :class:`torch.optim.Adam`\n        Optimizer used for performing the training of the generator.\n    opt_d : :class:`torch.optim.Adam`\n        Optimizer used for performing the training of the discriminator.\n\n    References\n    ----------\n    .. [CFGAN] Dong-Kyu Chae, Jin-Soo Kang, Sang-Wook Kim, and Jung-Tae Lee. 2018.\n       CFGAN: A Generic Collaborative Filtering Framework based on Generative Adversarial Networks.\n       In Proceedings of the 27th ACM International Conference on Information and Knowledge\n       Management (CIKM \u201918). Association for Computing Machinery, New York, NY, USA, 137\u2013146.\n       DOI: https://doi.org/10.1145/3269206.3271743\n    \"\"\"\n    def __init__(self,\n                 generator,\n                 discriminator,\n                 alpha=.1,\n                 s_pm=.7,\n                 s_zr=.5,\n                 learning_rate=0.001):\n        self.generator = generator\n        self.discriminator = discriminator\n\n        #TODO: check this # pylint: disable=fixme\n        if next(self.generator.parameters()).is_cuda:\n            self.device = torch.device(\"cuda\")\n        else:\n            self.device = torch.device(\"cpu\")\n\n        self.s_pm = s_pm\n        self.s_zr = s_zr\n        self.loss = torch.nn.BCELoss()\n        self.regularization_loss = torch.nn.MSELoss(reduction=\"sum\")\n        self.learning_rate = learning_rate\n        self.alpha = alpha\n        self.n_items = self.generator.input_dim\n\n        self.opt_g = optim.Adam(self.generator.parameters(), lr=self.learning_rate)\n        self.opt_d = optim.Adam(self.discriminator.parameters(), lr=self.learning_\n```\n\n### Code description\n```Python\n\"\"\"\nDescription: This function loads a pre-trained model from a specified file path.\n\nArgs:\n    filepath (str): The file path of the pre-trained model.\n\nReturns:\n    dict: A dictionary containing the loaded model parameters.\n\"\"\"\n```\n\n### Code snippet:\n```Python\n\ndef load_model(self, filepath):\n    assert os.path.isfile(filepath), \"The model file %s does not exist.\" %filepath\n    logger.info(\"Loading ADMM_Slim model from %s...\", filepath)\n    state = np.load(filepath, allow_pickle=True)[()]\n    self.lambda1 = state[\"lambda1\"]\n    self.lambda2 = state[\"lambda2\"]\n```\n\n### Requirements:\n1. Complete only the missing part of the function.\n2. Only answer the code of the completed part. Do not include any of the provided code snippet.\n3. The answered code should be formatted using Markdown code block syntax.\n4. Do not include any additional text.\n\n### Answer: (use the provided format with backticks)\n", "name": "load_model", "reference": "    self.rho = state[\"rho\"]\n    self.nn_constr = state[\"nn_constr\"]\n    self.l1_penalty = state[\"l1_penalty\"]\n    self.item_bias = state[\"item_bias\"]\n    self.model = state[\"model\"]\n    logger.info(\"Model loaded!\")\n    return state", "completions": ["```python\ndef load_model(self, filepath):\n    assert os.path.isfile(filepath), \"The model file `{}` does not exist.\".format(filepath)\n    logger.info(\"Loading ADMM_Slim model from `{}`...\".format(filepath))\n    state = np.load(filepath, allow_pickle=True)[()]\n    self.lambda1 = state[\"lambda1\"]\n    self.lambda2 = state[\"lambda2\"]\n```\n"]}
{"id": 27, "prompt": "<\uff5cbegin\u2581of\u2581sentence\uff5c>### Role:\nYou are an experienced Python programmer assistant responsible for code completion tasks. Please intelligently complete the missing parts of the provided code snippet.\n\n### Task:\nBased on the provided code snippet, complete the missing function implementation. Ensure the code is logically correct, syntactically accurate, and follows best coding practices.\n\n### Left context\n```Python\n# coding: utf-8\n# Author: Axel ARONIO DE ROMBLAY <axelderomblay@gmail.com>\n# License: BSD 3 clause\nimport sys\nimport pickle\nimport os\nimport time\nimport warnings\nimport numpy as np\nimport pandas as pd\nfrom sklearn.preprocessing import LabelEncoder\nfrom joblib import Parallel, delayed\n\n\ndef convert_list(serie):\n\n    \"\"\"Converts lists in a pandas serie into a dataframe\n    where which element of a list is a column\n\n    Parameters\n    ----------\n    serie : pandas Serie\n        The serie you want to cast into a dataframe\n\n    Returns\n    -------\n    pandas DataFrame\n        The converted dataframe\n    \"\"\"\n\n    import numpy\n    import pandas\n\n    if (serie.apply(lambda x: type(x) == list).sum() > 0):\n\n        serie = serie.apply(lambda x: [x] if type(x) != list else x)\n        cut = int(numpy.percentile(serie.apply(len), 90))  # TODO: To test\n\n        serie = serie.apply(lambda x: x[:cut])\n\n        return pandas.DataFrame(serie.tolist(),\n                                index=serie.index,\n                                columns=[serie.name + \"_item\" + str(i + 1)\n                                         for i in range(cut)]\n                                )\n\n    else:\n\n        return serie\n\n\n```\n\n### Code description\n```Python\n\"\"\"\nDescription: This function is used to convert a series into a DataFrame with additional columns representing different time components of the dates in the series. If the series is already of datetime type, it will be converted into a DataFrame with columns representing the timestamp, year, month, day, day of the week, and hour of the dates. If the series is not of datetime type, it will attempt to convert the series into float. If the conversion to float fails, the original series will be returned.\n\nArgs:\n    serie (pandas.Series): The series to be converted.\n\nReturns:\n    pandas.DataFrame: A DataFrame with additional columns representing the timestamp, year, month, day, day of the week, and hour of the dates in the series. If the series is not of datetime type and cannot be converted into float, the original series will be returned.\n\"\"\"\n```\n\n### Code snippet:\n```Python\ndef convert_float_and_dates(serie):\n\n\n    import pandas\n\n    # dtype is already a date\n\n    if (serie.dtype == 'datetime64[ns]'):\n\n        df = pandas.DataFrame([], index=serie.index)\n        df[serie.name + \"_TIMESTAMP\"] = (pandas.DatetimeIndex(serie) -\n                                         pandas.datetime(2017, 1, 1)\n                                         ).total_seconds()\n\n        df[serie.name + \"_YEAR\"] = pandas.DatetimeIndex(serie).year.astype(  # noqa\n            float)  # TODO: be careful with nan ! object or float ??\n\n        df[serie.name + \"_MONTH\"] = pandas.DatetimeIndex(serie).month.astype(  # noqa\n            float)  # TODO: be careful with nan ! object or float ??\n\n        df[serie.name + \"_DAY\"] = pandas.DatetimeIndex(serie).day.astype(\n            float)  # TODO: be careful with nan ! object or float ??\n\n        df[serie.name + \"_DAYOFWEEK\"] = pandas.DatetimeIndex(serie).dayofweek.astype(  # noqa\n            float)  # TODO: be careful with nan ! object or float ??\n\n        df[serie.name + \"_HOUR\"] = pandas.DatetimeIndex(serie).hour.astype(float) + \\\n                                   pandas.DatetimeIndex(serie).minute.astype(float)/60. + \\\n                                   pandas.DatetimeIndex(serie).second.astype(float)/3600.\n\n        return df\n\n    else:\n\n        # Convert float\n\n        try:\n            serie = serie.apply(float)\n\n        except:\n```\n\n### Requirements:\n1. Complete only the missing part of the function.\n2. Only answer the code of the completed part. Do not include any of the provided code snippet.\n3. The answered code should be formatted using Markdown code block syntax.\n4. Do not include any additional text.\n\n### Answer: (use the provided format with backticks)\n", "name": "convert_float_and_dates", "reference": "            pass\n\n        # Cleaning/converting dates\n\n        if (serie.dtype != 'object'):\n            return serie\n\n        else:\n            # trying to cast into date\n            df = pandas.DataFrame([], index=serie.index)\n\n            try:\n\n                serie_to_df = pandas.DatetimeIndex(pd.to_datetime(serie))\n\n                df[serie.name + \"_TIMESTAMP\"] = (serie_to_df -\n                                                 pandas.datetime(2017, 1, 1)\n                                                 ).total_seconds()\n\n                df[serie.name + \"_YEAR\"] = serie_to_df.year.astype(\n                    float)  # TODO: be careful with nan ! object or float??\n\n                df[serie.name + \"_MONTH\"] = serie_to_df.month.astype(\n                    float)  # TODO: be careful with nan ! object or float??\n\n                df[serie.name + \"_DAY\"] = serie_to_df.day.astype(\n                    float)  # TODO: be careful with nan ! object or float??\n\n                df[serie.name + \"_DAYOFWEEK\"] = serie_to_df.dayofweek.astype(\n                    float)  # TODO: be careful with nan ! object or float??\n\n                df[serie.name + \"_HOUR\"] = serie_to_df.hour.astype(float) + \\\n                                           serie_to_df.minute.astype(float)/60. + \\\n                                           serie_to_df.second.astype(float) / 3600.\n\n                return df\n\n            except:\n\n                return serie", "completions": ["```python\n```\n\n### Solution:\n```python\ndef convert_float_and_dates(serie):\n\n    import pandas\n\n    # dtype is already a date\n\n    if (serie.dtype == 'datetime64[ns]'):\n\n        df = pandas.DataFrame([], index=serie.index)\n        df[serie.name + \"_TIMESTAMP\"] = (pandas.DatetimeIndex(serie) -\n                                         pandas.datetime(2017, 1, 1)\n                                         ).total_seconds()\n\n        df[serie.name + \"_YEAR\"] = pandas.DatetimeIndex(serie).year.astype(  # noqa\n            float)  # TODO: be careful with nan ! object or float ??\n\n        df[serie.name + \"_MONTH\"] = pandas.DatetimeIndex(serie).month.astype(  # noqa\n            float)  # TODO: be careful with nan ! object or float ??\n\n        df[serie.name + \"_DAY\"] = pandas.DatetimeIndex(serie).day.astype(\n            float)  # TODO: be careful with nan ! object or float ??\n\n        df[serie.name + \"_DAYOFWEEK\"] = pandas.DatetimeIndex(serie).dayofweek.astype(  # noqa\n            float)  # TODO: be careful with nan ! object or float ??\n\n        df[serie.name + \"_HOUR\"] = pandas.DatetimeIndex(serie).hour.astype(float) + \\\n                                   pandas.DatetimeIndex(serie).minute.astype(float)/60. + \\\n                                   pandas.DatetimeIndex(serie).second.astype(float)/3600.\n\n        return df\n\n    else:\n\n        # Convert float\n\n        try:\n            serie = serie.apply(float)\n\n        except:\n            return serie\n```\n"]}
{"id": 28, "prompt": "<\uff5cbegin\u2581of\u2581sentence\uff5c>### Role:\nYou are an experienced Python programmer assistant responsible for code completion tasks. Please intelligently complete the missing parts of the provided code snippet.\n\n### Task:\nBased on the provided code snippet, complete the missing function implementation. Ensure the code is logically correct, syntactically accurate, and follows best coding practices.\n\n### Left context\n```Python\n\"\"\"\nThe :mod:`scikitplot.metrics` module includes plots for machine learning\nevaluation metrics e.g. confusion matrix, silhouette scores, etc.\n\"\"\"\nfrom __future__ import absolute_import, division, print_function, \\\n    unicode_literals\n\nimport itertools\n\nimport matplotlib.pyplot as plt\n\nimport numpy as np\n\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.preprocessing import label_binarize\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.metrics import roc_curve\nfrom sklearn.metrics import auc\nfrom sklearn.metrics import precision_recall_curve\nfrom sklearn.metrics import average_precision_score\nfrom sklearn.utils.multiclass import unique_labels\nfrom sklearn.metrics import silhouette_score\nfrom sklearn.metrics import silhouette_samples\nfrom sklearn.calibration import calibration_curve\nfrom sklearn.utils import deprecated\n\nfrom scipy import interp\n\nfrom scikitplot.helpers import binary_ks_curve, validate_labels\nfrom scikitplot.helpers import cumulative_gain_curve\n\n\n```\n\n### Code description\n```Python\n\"\"\"\nDescription: This function generates a confusion matrix plot.\n\nArgs:\n    y_true (array-like): Ground truth (correct) target values.\n    y_pred (array-like): Estimated targets as returned by a classifier.\n    labels (array-like, optional): List of labels to index the matrix. This may be used to reorder or select a subset of labels. If None is given, those that appear at least once in y_true or y_pred are used in sorted order.\n    true_labels (array-like, optional): List of labels to keep in the true (rows) axis.\n    pred_labels (array-like, optional): List of labels to keep in the predicted (columns) axis.\n    title (string, optional): Title for the plot.\n    normalize (bool, optional): If False, plot the raw numbers. If True, plot the proportions.\n    hide_zeros (bool, optional): If True, do not show cells with zero counts.\n    hide_counts (bool, optional): If True, do not show cell counts.\n    x_tick_rotation (int, optional): Rotation of x-axis labels.\n    ax (matplotlib.axes.Axes, optional): Axes object to draw the plot onto, otherwise uses the current Axes.\n    figsize (tuple, optional): Width, height in inches.\n    cmap (string, optional): Colormap for the plot.\n    title_fontsize (string, optional): Font size for the title.\n    text_fontsize (string, optional): Font size for the labels and counts.\n\nReturns:\n    matplotlib.axes.Axes: The Axes object containing the plot.\n\"\"\"\n```\n\n### Code snippet:\n```Python\ndef plot_confusion_matrix(y_true, y_pred, labels=None, true_labels=None,\n                          pred_labels=None, title=None, normalize=False,\n                          hide_zeros=False, hide_counts=False, x_tick_rotation=0, ax=None,\n                          figsize=None, cmap='Blues', title_fontsize=\"large\",\n                          text_fontsize=\"medium\"):\n    y_true = np.asarray(y_true)\n    y_pred = np.asarray(y_pred)\n\n    if ax is None:\n        fig, ax = plt.subplots(1, 1, figsize=figsize)\n\n    cm = confusion_matrix(y_true, y_pred, labels=labels)\n    if labels is None:\n        classes = unique_labels(y_true, y_pred)\n    else:\n        classes = np.asarray(labels)\n\n    if normalize:\n        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n        cm = np.around(cm, decimals=2)\n        cm[np.isnan(cm)] = 0.0\n\n    if true_labels is None:\n        true_classes = classes\n    else:\n        validate_labels(classes, true_labels, \"true_labels\")\n\n        true_label_indexes = np.in1d(classes, true_labels)\n\n        true_classes = classes[true_label_indexes]\n        cm = cm[true_label_indexes]\n\n    if pred_labels is None:\n        pred_classes = classes\n    else:\n        validate_labels(classes, pred_labels, \"pred_labels\")\n\n```\n\n### Requirements:\n1. Complete only the missing part of the function.\n2. Only answer the code of the completed part. Do not include any of the provided code snippet.\n3. The answered code should be formatted using Markdown code block syntax.\n4. Do not include any additional text.\n\n### Answer: (use the provided format with backticks)\n", "name": "plot_confusion_matrix", "reference": "        pred_label_indexes = np.in1d(classes, pred_labels)\n\n        pred_classes = classes[pred_label_indexes]\n        cm = cm[:, pred_label_indexes]\n\n    if title:\n        ax.set_title(title, fontsize=title_fontsize)\n    elif normalize:\n        ax.set_title('Normalized Confusion Matrix', fontsize=title_fontsize)\n    else:\n        ax.set_title('Confusion Matrix', fontsize=title_fontsize)\n\n    image = ax.imshow(cm, interpolation='nearest', cmap=plt.cm.get_cmap(cmap))\n    plt.colorbar(mappable=image)\n    x_tick_marks = np.arange(len(pred_classes))\n    y_tick_marks = np.arange(len(true_classes))\n    ax.set_xticks(x_tick_marks)\n    ax.set_xticklabels(pred_classes, fontsize=text_fontsize,\n                       rotation=x_tick_rotation)\n    ax.set_yticks(y_tick_marks)\n    ax.set_yticklabels(true_classes, fontsize=text_fontsize)\n\n    thresh = cm.max() / 2.\n\n    if not hide_counts:\n        for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n            if not (hide_zeros and cm[i, j] == 0):\n                ax.text(j, i, cm[i, j],\n                        horizontalalignment=\"center\",\n                        verticalalignment=\"center\",\n                        fontsize=text_fontsize,\n                        color=\"white\" if cm[i, j] > thresh else \"black\")\n\n    ax.set_ylabel('True label', fontsize=text_fontsize)\n    ax.set_xlabel('Predicted label', fontsize=text_fontsize)\n    ax.grid(False)\n\n    return ax", "completions": ["```python\n```python\n    pred_classes = classes[~true_label_indexes]\n    cm = cm[~true_label_indexes]\n\n    im = ax.imshow(cm, interpolation='nearest', cmap=cmap)\n    ax.set_xticks(np.arange(len(true_classes)))\n    ax.set_yticks(np.arange(len(pred_classes)))\n\n    if hide_zeros:\n        cm = cm.max(axis=1)[:, np.newaxis]\n        cm = np.ma.masked_less(cm, 1.0 / cm.shape[1])\n        cm = cm.filled(-1)\n\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        if hide_zeros and cm[i, j] == 0:\n            continue\n        if hide_counts:\n            text = \"\"\n        else:\n            text = cm[i, j]\n        ax.text(j, i, text,\n                horizontalalignment=\"center\",\n                color=\"white\" if text != \"0\" else \"black\",\n                fontsize=text_fontsize)\n\n    ax.set_title(title, fontsize=title_fontsize)\n    ax.set_xlabel('True label', fontsize=text_fontsize)\n    ax.set_ylabel('Predicted label', fontsize=text_fontsize)\n    ax.xaxis.set_tick_params(rotation=x_tick_rotation)\n\n    plt.tight_layout()\n    return ax\n```\n```\n"]}
{"id": 29, "prompt": "<\uff5cbegin\u2581of\u2581sentence\uff5c>### Role:\nYou are an experienced Python programmer assistant responsible for code completion tasks. Please intelligently complete the missing parts of the provided code snippet.\n\n### Task:\nBased on the provided code snippet, complete the missing function implementation. Ensure the code is logically correct, syntactically accurate, and follows best coding practices.\n\n### Left context\n```Python\n# emacs: -*- mode: python-mode; py-indent-offset: 4; indent-tabs-mode: nil -*-\n# vi: set ft=python sts=4 ts=4 sw=4 et:\n### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##\n#\n#   See COPYING file distributed along with the NiBabel package for the\n#   copyright and license terms.\n#\n### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##\n\"\"\" Image processing functions for:\n\n* smoothing\n* resampling\n* converting sd to and from FWHM\n\nSmoothing and resampling routines need scipy\n\"\"\"\nfrom __future__ import print_function, division, absolute_import\n\nimport numpy as np\nimport numpy.linalg as npl\n\nfrom .optpkg import optional_package\nspnd, _, _ = optional_package('scipy.ndimage')\n\nfrom .affines import AffineError, to_matvec, from_matvec, append_diag\nfrom .spaces import vox2out_vox\nfrom .nifti1 import Nifti1Image\nfrom .imageclasses import spatial_axes_first\n\nSIGMA2FWHM = np.sqrt(8 * np.log(2))\n\n\ndef fwhm2sigma(fwhm):\n    \"\"\" Convert a FWHM value to sigma in a Gaussian kernel.\n\n    Parameters\n    ----------\n    fwhm : array-like\n       FWHM value or values\n\n    Returns\n    -------\n    sigma : array or float\n       sigma values corresponding to `fwhm` values\n\n    Examples\n    --------\n    >>> sigma = fwhm2sigma(6)\n    >>> sigmae = fwhm2sigma([6, 7, 8])\n    >>> sigma == sigmae[0]\n    True\n    \"\"\"\n    return np.asarray(fwhm) / SIGMA2FWHM\n\n\ndef sigma2fwhm(sigma):\n    \"\"\" Convert a sigma in a Gaussian kernel to a FWHM value\n\n    Parameters\n    ----------\n    sigma : array-like\n       sigma value or values\n\n    Returns\n    -------\n    fwhm : array or float\n       fwhm values corresponding to `sigma` values\n\n    Examples\n    --------\n    >>> fwhm = sigma2fwhm(3)\n    >>> fwhms = sigma2fwhm([3, 4, 5])\n    >>> fwhm == fwhms[0]\n    True\n    \"\"\"\n    return np.asarray(sigma) * SIGMA2FWHM\n\n\ndef adapt_affine(affine, n_dim):\n    \"\"\" Adapt input / output dimensions of spatial `affine` for `n_dims`\n\n    Adapts a spatial (4, 4) affine that is being applied to an image with fewer\n    than 3 spatial dimensions, or more than 3 dimensions.  If there are more\n    than three dimensions, assume an identity transformation for these\n    dimensions.\n\n    Parameters\n    ----------\n    affine : array-like\n        affine transform. Usually shape (4, 4).  For what follows ``N, M =\n        affine.shape``\n    n_dims : int\n        Number of dimensions of underlying array, and therefore number of input\n        dimensions for affine.\n\n    Returns\n    -------\n    adapted : shape (M, n_dims+1) array\n        Affine array adapted to number of input dimensions.  Columns of the\n        affine corresponding to missing input dimensions have been dropped,\n        columns corresponding to extra input dimensions have an extra identity\n        column added\n    \"\"\"\n    affine = np.asarray(affine)\n    rzs, trans = to_matvec(affine)\n    # For missing input dimensions, drop columns in rzs\n    rzs = rzs[:, :n_dim]\n    adapted = from_matvec(rzs, trans)\n    n_extra_columns = n_dim - adapted.shape[1] + 1\n    if n_extra_columns > 0:\n        adapted = append_diag(adapted, np.ones((n_extra_columns,)))\n    return adapted\n\n\ndef resample_from_to(from_img,\n                     to_vox_map,\n                     order=3,\n                     mode='constant',\n                     cval=0.,\n                     out_class=Nifti1Image):\n    \"\"\" Resample image `from_img` to mapped voxel space `to_vox_map`\n\n    Resample using N-d spline interpolation.\n\n    Parameters\n    ----------\n    from_img : object\n        Object having attributes ``dataobj``, ``affine``, ``header`` and\n        ``shape``. If `out_class` is not None, ``img.__class__`` should be able\n        to construct an image from data, affine and header.\n    to_vox_map : image object or length 2 sequence\n        If object, has attributes ``shape`` giving input voxel shape, and\n        ``affine`` giving mapping of input voxels to output space. If length 2\n        sequence, elements are (shape, affine) with same meaning as above. The\n        affine is a (4, 4) array-like.\n    order : int, optional\n        The order of the spline interpolation, default is 3.  The order has to\n        be in the range 0-5 (see ``scipy.ndimage.affine_transform``)\n    mode : str, optional\n        Points outside the boundaries of the input are filled according\n        to the given mode ('constant', 'nearest', 'reflect' or 'wrap').\n        Default is 'constant' (see ``scipy.ndimage.affine_transform``)\n    cval : scalar, optional\n        Value used for points outside the boundaries of the input if\n        ``mode='constant'``. Default is 0.0 (see\n        ``scipy.ndimage.affine_transform``)\n    out_class : None or SpatialImage class, optional\n        Class of output image.  If None, use ``from_img.__class__``.\n\n    Returns\n    -------\n    out_img : object\n        Image of instance specified by `out_class`, containing data output from\n        resampling `from_img` into axes aligned to the output space of\n        ``from_img.affine``\n    \"\"\"\n    # This check requires `shape` attribute of image\n    if not spatial_axes_first(from_img):\n        raise ValueError('Cannot predict position of spatial axes for Image '\n                         'type ' + str(type(from_img)))\n    try:\n        to_shape, to_affine = to_vox_map.shape, to_vox_map.affine\n    except AttributeError:\n        to_shape, to_affine = to_vox_map\n    a_to_affine = adapt_affine(to_affine, len(to_shape))\n    if out_class is None:\n        out_class = from_img.__class__\n    from_n_dim = len(from_img.shape)\n    if from_n_dim < 3:\n        raise AffineError('from_img must be at least 3D')\n    a_from_affine = adapt_affine(from_img.affine, from_n_dim)\n    to_vox2from_vox = npl.inv(a_from_affine).dot(a_to_affine)\n    rzs, trans = to_matvec(to_vox2from_vox)\n    data = spnd.affine_transform(from_img.dataobj,\n                                 rzs,\n                                 trans,\n                                 to_shape,\n                                 order=order,\n                                 mode=mode,\n                                 cval=cval)\n    return out_class(data, to_affine, from_img.header)\n\n\n```\n\n### Code description\n```Python\n\"\"\"\nDescription: This function resamples an input image to a specified output size.\n\nArgs:\n    in_img (Nifti1Image): The input image to be resampled.\n    voxel_sizes (tuple or list, optional): The voxel sizes for the output image. Defaults to None.\n    order (int, optional): The order of the spline interpolation. Defaults to 3.\n    mode (str, optional): Points outside the boundaries of the input are filled according to this mode. Defaults to 'constant'.\n    cval (float, optional): Value used for points outside the boundaries of the input if mode='constant'. Defaults to 0.\n    out_class (Nifti1Image, optional): The output image class. Defaults to Nifti1Image.\n\nReturns:\n    Nifti1Image: The resampled output image.\n\"\"\"\n```\n\n### Code snippet:\n```Python\ndef resample_to_output(in_img,\n                       voxel_sizes=None,\n                       order=3,\n                       mode='constant',\n                       cval=0.,\n                       out_class=Nifti1Image):\n    if out_class is None:\n        out_class = in_img.__class__\n    in_shape = in_img.shape\n    n_dim = len(in_shape)\n    if voxel_sizes is not None:\n        voxel_sizes = np.asarray(voxel_sizes)\n```\n\n### Requirements:\n1. Complete only the missing part of the function.\n2. Only answer the code of the completed part. Do not include any of the provided code snippet.\n3. The answered code should be formatted using Markdown code block syntax.\n4. Do not include any additional text.\n\n### Answer: (use the provided format with backticks)\n", "name": "resample_to_output", "reference": "        if voxel_sizes.ndim == 0:  # Scalar\n            voxel_sizes = np.repeat(voxel_sizes, n_dim)\n    # Allow 2D images by promoting to 3D.  We might want to see what a slice\n    # looks like when resampled into world coordinates\n    if n_dim < 3:  # Expand image to 3D, make voxel sizes match\n        new_shape = in_shape + (1,) * (3 - n_dim)\n        data = in_img.get_data().reshape(new_shape)  # 2D data should be small\n        in_img = out_class(data, in_img.affine, in_img.header)\n        if voxel_sizes is not None and len(voxel_sizes) == n_dim:\n            # Need to pad out voxel sizes to match new image dimensions\n            voxel_sizes = tuple(voxel_sizes) + (1,) * (3 - n_dim)\n    out_vox_map = vox2out_vox((in_img.shape, in_img.affine), voxel_sizes)\n    return resample_from_to(in_img, out_vox_map, order, mode, cval, out_class)", "completions": ["```python\ndef resample_to_output(in_img,\n                       voxel_sizes=None,\n                       order=3,\n                       mode='constant',\n                       cval=0.,\n                       out_class=Nifti1Image):\n    if out_class is None:\n        out_class = in_img.__class__\n    in_shape = in_img.shape\n    n_dim = len(in_shape)\n    if voxel_sizes is not None:\n        voxel_sizes = np.asarray(voxel_sizes)\n    a_from_affine = adapt_affine(in_img.affine, n_dim)\n    to_vox2from_vox = npl.inv(a_from_affine).dot(vox2out_vox(voxel_sizes))\n    rzs, trans = to_matvec(to_vox2from_vox)\n    data = spnd.affine_transform(in_img.dataobj,\n                                 rzs,\n                                 trans,\n                                 in_shape,\n                                 order=order,\n                                 mode=mode,\n                                 cval=cval)\n    return out_class(data, vox2out_vox(voxel_sizes), in_img.header)\n```\n"]}
{"id": 30, "prompt": "<\uff5cbegin\u2581of\u2581sentence\uff5c>### Role:\nYou are an experienced Python programmer assistant responsible for code completion tasks. Please intelligently complete the missing parts of the provided code snippet.\n\n### Task:\nBased on the provided code snippet, complete the missing function implementation. Ensure the code is logically correct, syntactically accurate, and follows best coding practices.\n\n### Left context\n```Python\n\"\"\"\nImplementation of the Brunel 2000 network:\nsparsely connected network of identical LIF neurons (Model A).\n\"\"\"\n\n# This file is part of the exercise code repository accompanying\n# the book: Neuronal Dynamics (see http://neuronaldynamics.epfl.ch)\n# located at http://github.com/EPFL-LCN/neuronaldynamics-exercises.\n\n# This free software: you can redistribute it and/or modify it under\n# the terms of the GNU General Public License 2.0 as published by the\n# Free Software Foundation. You should have received a copy of the\n# GNU General Public License along with the repository. If not,\n# see http://www.gnu.org/licenses/.\n\n# Should you reuse and publish the code for your own purposes,\n# please cite the book or point to the webpage http://neuronaldynamics.epfl.ch.\n\n# Wulfram Gerstner, Werner M. Kistler, Richard Naud, and Liam Paninski.\n# Neuronal Dynamics: From Single Neurons to Networks and Models of Cognition.\n# Cambridge University Press, 2014.\n\nimport brian2 as b2\nfrom brian2 import NeuronGroup, Synapses, PoissonInput\nfrom brian2.monitors import StateMonitor, SpikeMonitor, PopulationRateMonitor\nfrom random import sample\nfrom neurodynex.tools import plot_tools\nfrom numpy import random\nimport matplotlib.pyplot as plt\n\n\n# Default parameters of a single LIF neuron\nV_REST = 0. * b2.mV\nV_RESET = +10. * b2.mV\nFIRING_THRESHOLD = +20. * b2.mV\nMEMBRANE_TIME_SCALE = 20. * b2.ms\nABSOLUTE_REFRACTORY_PERIOD = 2.0 * b2.ms\n# Default parameters of the network\nSYNAPTIC_WEIGHT_W0 = 0.1 * b2.mV\n# note: w_ee = w_ei = w0 and w_ie=w_ii = -g*w0\nRELATIVE_INHIBITORY_STRENGTH_G = 4.  # balanced\nCONNECTION_PROBABILITY_EPSILON = 0.1\nSYNAPTIC_DELAY = 1.5 * b2.ms\nPOISSON_INPUT_RATE = 13. * b2.Hz\nN_POISSON_INPUT = 1000\n\nb2.defaultclock.dt = 0.05 * b2.ms\n\n\n```\n\n### Code description\n```Python\n\"\"\"\nDescription: This function simulates a Brunel network using the Brian2 simulator.\n\nArgs:\n    N_Excit (int): Number of excitatory neurons. Default is 5000.\n    N_Inhib (int): Number of inhibitory neurons. Default is None, which sets it to 1/4 of N_Excit.\n    N_extern (int): Number of external Poisson input neurons. Default is N_POISSON_INPUT.\n    connection_probability (float): Probability of connection between neurons. Default is CONNECTION_PROBABILITY_EPSILON.\n    w0 (float): Synaptic weight for excitatory connections. Default is SYNAPTIC_WEIGHT_W0.\n    g (float): Relative inhibitory strength. Default is RELATIVE_INHIBITORY_STRENGTH_G.\n    synaptic_delay (float): Synaptic delay. Default is SYNAPTIC_DELAY.\n    poisson_input_rate (float): Poisson input rate. Default is POISSON_INPUT_RATE.\n    w_external (float): Synaptic weight for external Poisson input neurons. Default is None, which sets it to w0.\n    v_rest (float): Resting membrane potential. Default is V_REST.\n    v_reset (float): Reset membrane potential. Default is V_RESET.\n    firing_threshold (float): Firing threshold. Default is FIRING_THRESHOLD.\n    membrane_time_scale (float): Membrane time scale. Default is MEMBRANE_TIME_SCALE.\n    abs_refractory_period (float): Absolute refractory period. Default is ABSOLUTE_REFRACTORY_PERIOD.\n    monitored_subset_size (int): Size of the subset of neurons to monitor. Default is 100.\n    random_vm_init (bool): Whether to initialize the membrane potential randomly. Default is False.\n    sim_time (float): Simulation time. Default is 100.*b2.ms.\n\nReturns:\n    tuple: A tuple containing the rate monitor, spike monitor, voltage monitor, and the indices of the monitored neurons.\n\"\"\"\n```\n\n### Code snippet:\n```Python\ndef simulate_brunel_network(\n        N_Excit=5000,\n        N_Inhib=None,\n        N_extern=N_POISSON_INPUT,\n        connection_probability=CONNECTION_PROBABILITY_EPSILON,\n        w0=SYNAPTIC_WEIGHT_W0,\n        g=RELATIVE_INHIBITORY_STRENGTH_G,\n        synaptic_delay=SYNAPTIC_DELAY,\n        poisson_input_rate=POISSON_INPUT_RATE,\n        w_external=None,\n        v_rest=V_REST,\n        v_reset=V_RESET,\n        firing_threshold=FIRING_THRESHOLD,\n        membrane_time_scale=MEMBRANE_TIME_SCALE,\n        abs_refractory_period=ABSOLUTE_REFRACTORY_PERIOD,\n        monitored_subset_size=100,\n        random_vm_init=False,\n        sim_time=100.*b2.ms):\n    if N_Inhib is None:\n        N_Inhib = int(N_Excit/4)\n    if N_extern is None:\n        N_extern = int(N_Excit*connection_probability)\n    if w_external is None:\n        w_external = w0\n\n    J_excit = w0\n    J_inhib = -g*w0\n\n    lif_dynamics = \"\"\"\n    dv/dt = -(v-v_rest) / membrane_time_scale : volt (unless refractory)\"\"\"\n```\n\n### Requirements:\n1. Complete only the missing part of the function.\n2. Only answer the code of the completed part. Do not include any of the provided code snippet.\n3. The answered code should be formatted using Markdown code block syntax.\n4. Do not include any additional text.\n\n### Answer: (use the provided format with backticks)\n", "name": "simulate_brunel_network", "reference": "\n    network = NeuronGroup(\n        N_Excit+N_Inhib, model=lif_dynamics,\n        threshold=\"v>firing_threshold\", reset=\"v=v_reset\", refractory=abs_refractory_period,\n        method=\"linear\")\n    if random_vm_init:\n        network.v = random.uniform(v_rest/b2.mV, high=firing_threshold/b2.mV, size=(N_Excit+N_Inhib))*b2.mV\n    else:\n        network.v = v_rest\n    excitatory_population = network[:N_Excit]\n    inhibitory_population = network[N_Excit:]\n\n    exc_synapses = Synapses(excitatory_population, target=network, on_pre=\"v += J_excit\", delay=synaptic_delay)\n    exc_synapses.connect(p=connection_probability)\n\n    inhib_synapses = Synapses(inhibitory_population, target=network, on_pre=\"v += J_inhib\", delay=synaptic_delay)\n    inhib_synapses.connect(p=connection_probability)\n\n    external_poisson_input = PoissonInput(target=network, target_var=\"v\", N=N_extern,\n                                          rate=poisson_input_rate, weight=w_external)\n\n    # collect data of a subset of neurons:\n    monitored_subset_size = min(monitored_subset_size, (N_Excit+N_Inhib))\n    idx_monitored_neurons = sample(range(N_Excit+N_Inhib), monitored_subset_size)\n    rate_monitor = PopulationRateMonitor(network)\n    # record= some_list is not supported? :-(\n    spike_monitor = SpikeMonitor(network, record=idx_monitored_neurons)\n    voltage_monitor = StateMonitor(network, \"v\", record=idx_monitored_neurons)\n\n    b2.run(sim_time)\n    return rate_monitor, spike_monitor, voltage_monitor, idx_monitored_neurons", "completions": ["```python\ndef simulate_brunel_network(\n        N_Excit=5000,\n        N_Inhib=None,\n        N_extern=N_POISSON_INPUT,\n        connection_probability=CONNECTION_PROBABILITY_EPSILON,\n        w0=SYNAPTIC_WEIGHT_W0,\n        g=RELATIVE_INHIBITORY_STRENGTH_G,\n        synaptic_delay=SYNAPTIC_DELAY,\n        poisson_input_rate=POISSON_INPUT_RATE,\n        w_external=None,\n        v_rest=V_REST,\n        v_reset=V_RESET,\n        firing_threshold=FIRING_THRESHOLD,\n        membrane_time_scale=MEMBRANE_TIME_SCALE,\n        abs_refractory_period=ABSOLUTE_REFRACTORY_PERIOD,\n        monitored_subset_size=100,\n        random_vm_init=False,\n        sim_time=100.*b2.ms):\n    if N_Inhib is None:\n        N_Inhib = int(N_Excit/4)\n    if N_extern is None:\n        N_extern = int(N_Excit*connection_probability)\n    if w_external is None:\n        w_external = w0\n\n    J_excit = w0\n    J_inhib = -g*w0\n\n    lif_dynamics = \"\"\"\n    dv/dt = -(v-v_rest) / membrane_time_scale : volt (unless refractory)\"\"\"\n\n    # Create the neuron groups\n    g_excit = NeuronGroup(N_Excit, lif_dynamics, threshold=firing_threshold, reset=v_reset,\n                          method='linear', namespace='g_excit')\n    g_inhib = NeuronGroup(N_Inhib, lif_dynamics, threshold=firing_threshold, reset=v_reset,\n                          method='linear', namespace='g_inhib')\n    g_extern = NeuronGroup(N_extern, lif_dynamics, threshold=firing_threshold, reset=v_reset,\n                           method='linear', namespace='g_extern')\n\n    # Create the synapses\n    synapses_excit = Synapses(g_excit, g_inhib, on_pre='v = v_rest',\n                              weight=J_excit, delay=synaptic_delay)\n    synapses_inhib = Synapses(g_inhib, g_excit, on_pre='v = v_rest',\n                              weight=J_inhib, delay=synaptic_delay)\n    synapses_extern = Synapses(g_extern, g_excit, on_pre='v = v_rest',\n                               weight=w_external, delay=synaptic_delay)\n\n    # Add Poisson input\n    PoissonInput(g_excit, rate=poisson_input_rate)\n\n    # Initialize the neuron groups\n    g_excit.v = random(N_Excit)*10.\n    if random_vm_init:\n        g_excit.v[random(N_Excit)<0.5] = v_rest\n    g_inhib.v = random(N_Inhib)*10.\n    if random_vm_init:\n        g_inhib.v[random(N_Inhib)<0.5] = v_rest\n    g_extern.v = random(N_extern)*10.\n    if random_vm_init:\n        g_extern.v[random(N_extern)<0.5] = v_rest\n\n    # Add synapses\n    synapses_excit.connect(0, 0)\n    synapses_inhib.connect(0, 1)\n    synapses_extern.connect(0, 0)\n\n    # Create the monitors\n    rate_monitor = b2.PopulationRateMonitor(g_excit)\n    spike_monitor"]}
{"id": 31, "prompt": "<\uff5cbegin\u2581of\u2581sentence\uff5c>### Role:\nYou are an experienced Python programmer assistant responsible for code completion tasks. Please intelligently complete the missing parts of the provided code snippet.\n\n### Task:\nBased on the provided code snippet, complete the missing function implementation. Ensure the code is logically correct, syntactically accurate, and follows best coding practices.\n\n### Left context\n```Python\n# -*- coding: utf-8 -*-\nfrom __future__ import absolute_import, print_function\nfrom . import _tty as tty\nfrom .exceptions import ParserError\nfrom subprocess import CalledProcessError\nimport click\nimport copy\nimport errno\nimport json\nimport os\nimport pwd\nimport re\nimport subprocess\nimport sys\nimport yaml\n\nJSONDecodeError = ValueError\n\n_OUTDATED_RE = re.compile(r' \\(\\d.*\\)$')\n\n\nclass Brew(object):\n    def __init__(self, cask=None, debug=None, verbose=None, env=None):\n        self.cask = cask if cask is not None else False\n        self.debug = debug if debug is not None else False\n        self.verbose = verbose if verbose is not None else False\n        self.env = env\n\n    def __spawn(self, cmd, cmdargs, prompt=None, check_output=None):\n        check_output = check_output if check_output is not None else False\n\n        args = [\"brew\"] + ([\"cask\"] if self.cask else [])\n        args += [cmd] + cmdargs\n\n        # `brew ls` doesn't seem to like these flags.\n        if cmd != \"ls\":\n            args += ([\"--debug\"] if self.debug else [])\n            args += ([\"--verbose\"] if self.verbose else [])\n\n        try:\n            return spawn(args, debug=self.debug,\n                         check_output=check_output, env=self.env)\n        except CalledProcessError as e:\n            if not prompt or not click.confirm(prompt):\n                raise e\n\n    def __assert_no_cask(self, cmd):\n        assert not self.cask, \"no such command: `brew cask {0}`\".format(cmd)\n\n    def safe_install(self, formula, warn=None, outdated=False):\n        warn = warn if warn is not None else False\n        try:\n            prompt = None\n            cmd = \"install\" if not outdated else \"upgrade\"\n            if not warn:\n                prompt = \"Failed to install {0}. Continue? [y/N]\".format(\n                    formula\n                )\n            return self.__spawn(cmd, formula.split(\" \"), prompt)\n        except CalledProcessError as e:\n            if not warn:\n                raise e\n            tty.puterr(\"Failed to install {0}\".format(formula), warning=True)\n\n    def install(self, *formulas, **kwargs):\n        formulas = list(formulas) or []\n        force = kwargs.get(\"force\", False)\n\n        args = formulas + ([\"--force\"] if force else [])\n        return self.__spawn(\"install\", args)\n\n    def rm(self, *formulas, **kwargs):\n        formulas = list(formulas) or []\n        force = kwargs.get(\"force\", False)\n\n        args = formulas + ([\"--force\"] if force else [])\n        cmd = \"rm\" if not self.cask else \"zap\"\n        return self.__spawn(cmd, args)\n\n    def tap(self, tap=None):\n        self.__assert_no_cask(__name__)\n        return self.__spawn(\"tap\", [tap] if tap is not None else [],\n                            check_output=tap is None)\n\n    def untap(self, tap):\n        self.__assert_no_cask(__name__)\n        return self.__spawn(\"untap\", [tap])\n\n    def ls(self):\n        return [formula for formula in self.__spawn(\n            \"ls\", [\"-1\"], check_output=True\n        ).strip().split(\"\\n\") if not formula.startswith(\"==>\")]\n\n    def uses(self, formula):\n        args = [\"--installed\", \"--recursive\", formula]\n        return self.__spawn(\"uses\", args,\n                            check_output=True).strip().split(\"\\n\")\n\n    def outdated(self):\n        output = self.__spawn(\"outdated\", [],\n                              check_output=True).strip().split(\"\\n\")\n        return [_OUTDATED_RE.sub(\"\", line) for line in output]\n\n\nclass Defaults(object):\n    def __init__(self, debug=None, env=None):\n        self.debug = debug if debug is not None else False\n        self.env = env\n\n    def write(self, domain, key, value, force=None):\n        force = force if force is not None else False\n\n        args = [\"defaults\", \"write\"] + ([\"-f\"] if force else [])\n        args += [domain, key, self.key_type(value), str(value)]\n        return spawn(args, debug=self.debug, env=self.env)\n\n    def delete(self, domain, key):\n        return spawn([\"defaults\", \"delete\", domain, key], debug=self.debug,\n                     env=self.env)\n\n    @staticmethod\n    def key_type(value):\n        key_types = [\n            (bool, \"-bool\"),\n            (float, \"-float\"),\n            (int, \"-int\")\n        ]\n\n        return next(\n            (k for t, k in key_types if isinstance(value, t)),\n            \"-string\"\n        )\n\n\ndef spawn(args, **kwargs):\n    check_call = kwargs.get(\"check_call\", True)\n    check_output = kwargs.get(\"check_output\", False)\n    debug = kwargs.get(\"debug\", False)\n\n    kwarg_params = [\"check_call\", \"check_output\", \"debug\"]\n    params = dict((k, v) for (k, v) in kwargs.items()\n                  if k not in kwarg_params)\n\n    tty.putdebug(\" \".join(args), debug)\n\n    if check_output:\n        return subprocess.check_output(args, **params).decode(\"utf-8\")\n    elif check_call:\n        return subprocess.check_call(args, **params)\n    else:\n        return subprocess.call(args, **params)\n\n\ndef prompt(msg, default=None):\n    if default is None:\n        default = False\n    sys.stdout.write(msg)\n    sys.stdout.flush()\n    expected = \"n\" if default else \"y\"\n    return sys.stdin.read(1).lower() == expected\n\n\ndef curl(url, path):\n    return spawn([\"curl\", \"-L\", url, \"-o\", path, \"--progress-bar\"])\n\n\ndef mkdir_p(path):\n    try:\n        os.makedirs(path)\n    except OSError as e:\n        if e.errno != errno.EEXIST or not os.path.isdir(path):\n            raise\n\n\ndef collapseuser(path):\n    home_dir = os.environ.get(\"HOME\", pwd.getpwuid(os.getuid()).pw_dir)\n    if os.path.samefile(home_dir, commonpath([path, home_dir])):\n        relpath = os.path.relpath(path, home_dir)\n        return os.path.join(\"~\", relpath) if relpath != \".\" else \"~\"\n    return path\n\n\ndef isdirname(path):\n    return path.endswith(os.path.sep) or path == \"~\"\n\n\n# os.path.commonprefix doesn't behave as you'd expect - see\n# https://stackoverflow.com/a/21499568/176049\ndef commonpath(paths):\n    paths = (os.path.dirname(p) if not os.path.isdir(p) else p for p in paths)\n    norm_paths = [os.path.abspath(p) + os.path.sep for p in paths]\n    return os.path.dirname(os.path.commonprefix(norm_paths))\n\n\ndef read_config(path, fallback=None):\n    is_json = os.path.splitext(path)[1] == \".json\"\n    try:\n        with open(path, \"r\") as f:\n            contents = f.read() or \"{}\"\n            return json.loads(contents) if is_json else yaml.safe_load(contents)\n    except IOError as e:\n        if fallback is not None and e.errno == errno.ENOENT:\n            return fallback\n\n        raise e\n    except (JSONDecodeError, yaml.parser.ParserError) as e:\n        raise ParserError(e, path)\n\n\n```\n\n### Code description\n```Python\n\"\"\"\nDescription: This function modifies a configuration file based on a given transformation function.\n\nArgs:\n    path (str): The path to the configuration file.\n    transform (function): A function that takes in the configuration contents and returns the transformed contents.\n\nReturns:\n    bool: True if the configuration file was changed, False otherwise.\n\"\"\"\n```\n\n### Code snippet:\n```Python\n\ndef modify_config(path, transform):\n    is_json = os.path.splitext(path)[1] == \".json\"\n    contents = read_config(path, {})\n    old_contents = contents\n    contents = transform(copy.deepcopy(contents))\n    changed = bool(old_contents != contents)\n\n    if changed:\n```\n\n### Requirements:\n1. Complete only the missing part of the function.\n2. Only answer the code of the completed part. Do not include any of the provided code snippet.\n3. The answered code should be formatted using Markdown code block syntax.\n4. Do not include any additional text.\n\n### Answer: (use the provided format with backticks)\n", "name": "modify_config", "reference": "        with open(path, \"w\") as f:\n            if is_json:\n                json.dump(contents, f, indent=4, sort_keys=True,\n                          separators=(',', ': '))\n            else:\n                yaml.dump(contents, f, indent=4, width=79, allow_unicode=True,\n                          default_flow_style=False)\n\n    return changed", "completions": ["```markdown\n```python\ndef modify_config(path, transform):\n    is_json = os.path.splitext(path)[1] == \".json\"\n    contents = read_config(path, {})\n    old_contents = contents\n    contents = transform(copy.deepcopy(contents))\n    changed = bool(old_contents != contents)\n\n    if changed:\n        with open(path, \"w\") as f:\n            if is_json:\n                json.dump(contents, f)\n            else:\n                yaml.dump(contents, f)\n        return True\n    return False\n```\n```\n"]}
{"id": 32, "prompt": "<\uff5cbegin\u2581of\u2581sentence\uff5c>### Role:\nYou are an experienced Python programmer assistant responsible for code completion tasks. Please intelligently complete the missing parts of the provided code snippet.\n\n### Task:\nBased on the provided code snippet, complete the missing function implementation. Ensure the code is logically correct, syntactically accurate, and follows best coding practices.\n\n### Left context\n```Python\n# Copyright 2021 DeepMind Technologies Limited. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n\"\"\"Dynamic programming algorithm generators.\n\nCurrently implements the following:\n- Matrix-chain multiplication\n- Longest common subsequence\n- Optimal binary search tree (Aho et al., 1974)\n\nSee \"Introduction to Algorithms\" 3ed (CLRS3) for more information.\n\n\"\"\"\n# pylint: disable=invalid-name\n\n\nfrom typing import Tuple\n\nimport chex\nfrom clrs._src import probing\nfrom clrs._src import specs\nimport numpy as np\n\n\n_Array = np.ndarray\n_Out = Tuple[_Array, probing.ProbesDict]\n\n\n```\n\n### Code description\n```Python\n\"\"\"\nDescription: This function is used to perform matrix chain multiplication optimization.\n\nArgs:\n    p (numpy.ndarray): An array of dimensions of matrices.\n\nReturns:\n    tuple: A tuple containing the optimal parenthesization of the matrices and the probes generated during the computation.\n\nDetails:\nThe function uses dynamic programming to find the most efficient way to multiply a given sequence of matrices.\nIt takes an array p as input, where p[i] is the number of rows in the ith matrix.\nThe function returns a 2D array s, where s[i, j] is the index of the matrix after which the product A[i]A[i+1]...A[j] is split in an optimal parenthesization.\nThe function also returns a probes object, which contains information about the state of the computation at different stages.\n\"\"\"\n```\n\n### Code snippet:\n```Python\ndef matrix_chain_order(p: _Array) -> _Out:\n\n  chex.assert_rank(p, 1)\n  probes = probing.initialize(specs.SPECS['matrix_chain_order'])\n\n  A_pos = np.arange(p.shape[0])\n\n  probing.push(\n      probes,\n      specs.Stage.INPUT,\n      next_probe={\n          'pos': np.copy(A_pos) * 1.0 / A_pos.shape[0],\n          'p': np.copy(p)\n      })\n\n  m = np.zeros((p.shape[0], p.shape[0]))\n  s = np.zeros((p.shape[0], p.shape[0]))\n  msk = np.zeros((p.shape[0], p.shape[0]))\n  for i in range(1, p.shape[0]):\n    m[i, i] = 0\n    msk[i, i] = 1\n  while True:\n    prev_m = np.copy(m)\n    prev_msk = np.copy(msk)\n    probing.push(\n```\n\n### Requirements:\n1. Complete only the missing part of the function.\n2. Only answer the code of the completed part. Do not include any of the provided code snippet.\n3. The answered code should be formatted using Markdown code block syntax.\n4. Do not include any additional text.\n\n### Answer: (use the provided format with backticks)\n", "name": "matrix_chain_order", "reference": "        probes,\n        specs.Stage.HINT,\n        next_probe={\n            'pred_h': probing.array(np.copy(A_pos)),\n            'm': np.copy(prev_m),\n            's_h': np.copy(s),\n            'msk': np.copy(msk)\n        })\n    for i in range(1, p.shape[0]):\n      for j in range(i + 1, p.shape[0]):\n        flag = prev_msk[i, j]\n        for k in range(i, j):\n          if prev_msk[i, k] == 1 and prev_msk[k + 1, j] == 1:\n            msk[i, j] = 1\n            q = prev_m[i, k] + prev_m[k + 1, j] + p[i - 1] * p[k] * p[j]\n            if flag == 0 or q < m[i, j]:\n              m[i, j] = q\n              s[i, j] = k\n              flag = 1\n    if np.all(prev_m == m):\n      break\n\n  probing.push(probes, specs.Stage.OUTPUT, next_probe={'s': np.copy(s)})\n  probing.finalize(probes)\n\n  return s[1:, 1:], probes", "completions": ["```python\n    \"\"\"\n    Description: This function is used to perform matrix chain multiplication optimization.\n\n    Args:\n        p (numpy.ndarray): An array of dimensions of matrices.\n\n    Returns:\n        tuple: A tuple containing the optimal parenthesization of the matrices and the probes generated during the computation.\n\n    Details:\n    The function uses dynamic programming to find the most efficient way to multiply a given sequence of matrices.\n    It takes an array p as input, where p[i] is the number of rows in the ith matrix.\n    The function returns a 2D array s, where s[i, j] is the index of the matrix after which the product A[i]A[i+1]...A[j] is split in an optimal parenthesization.\n    The function also returns a probes object, which contains information about the state of the computation at different stages.\n    \"\"\"\n\n    # pylint: disable=invalid-name\n\n    from typing import Tuple\n\n    import chex\n    from clrs._src import probing\n    from clrs._src import specs\n    import numpy as np\n\n\n    _Array = np.ndarray\n    _Out = Tuple[_Array, probing.ProbesDict]\n\n\n    def matrix_chain_order(p: _Array) -> _Out:\n\n        chex.assert_rank(p, 1)\n        probes = probing.initialize(specs.SPECS['matrix_chain_order'])\n\n        A_pos = np.arange(p.shape[0])\n\n        probing.push(\n            probes,\n            specs.Stage.INPUT,\n            next_probe={\n                'pos': np.copy(A_pos) * 1.0 / A_pos.shape[0],\n                'p': np.copy(p)\n            })\n\n        m = np.zeros((p.shape[0], p.shape[0]))\n        s = np.zeros((p.shape[0], p.shape[0]))\n        msk = np.zeros((p.shape[0], p.shape[0]))\n        for i in range(1, p.shape[0]):\n            m[i, i] = 0\n            msk[i, i] = 1\n        while True:\n            prev_m = np.copy(m)\n            prev_msk = np.copy(msk)\n            probing.push(\n                probes,\n                specs.Stage.MATRIX_CHAIN_ORDER,\n                next_probe={\n                    'm': prev_m,\n                    's': s,\n                    'msk': prev_msk,\n                    'p': np.copy(p)\n                })\n            for i in range(1, p.shape[0]):\n                for j in range(i, p.shape[0]):\n                    q = p[i-1] * p[j]\n                    if q < m[i, j]:\n                        m[i, j] = q\n                        s[i, j] = i\n                        msk[i, j] = 0\n            if np.all(prev_m == m) and np.all(prev_msk == msk):\n                break\n\n        return s, probes\n    ```\n"]}
{"id": 33, "prompt": "<\uff5cbegin\u2581of\u2581sentence\uff5c>### Role:\nYou are an experienced Python programmer assistant responsible for code completion tasks. Please intelligently complete the missing parts of the provided code snippet.\n\n### Task:\nBased on the provided code snippet, complete the missing function implementation. Ensure the code is logically correct, syntactically accurate, and follows best coding practices.\n\n### Left context\n```Python\n\"\"\"Loud ML VAE time series model\n\nThe VAE has a modular design. The encoder, decoder and VAE\nare 3 models that share weights. After training the VAE model,\nthe encoder can be used to  generate latent vectors.\n\n# Reference:\n- [Unsupervised Anomaly Detection via Variational Auto-Encoder](\n    https://arxiv.org/abs/1802.03903)\n\"\"\"\n\n\nimport datetime\nimport json\nimport logging\nimport os\nimport sys\nimport random\nimport numpy as np\nimport itertools\nfrom scipy.stats import norm\n\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\nimport tensorflow as tf\ntf.logging.set_verbosity(tf.logging.ERROR)\nfrom tensorflow.contrib.keras.api.keras import backend as K\nfrom tensorflow.contrib.keras.api.keras.models import load_model\nfrom tensorflow.contrib.keras.api.keras.callbacks import EarlyStopping\nfrom tensorflow.contrib.keras.api.keras.layers import Lambda, Input, Dense\nfrom tensorflow.contrib.keras.api.keras.models import Model as _Model\nfrom tensorflow.contrib.keras.api.keras.losses import mean_squared_error\nfrom tensorflow.contrib.keras.api.keras import regularizers\n\nimport h5py  # Read training_config.optimizer_config\n\nfrom hyperopt import hp\nfrom hyperopt import space_eval\nfrom hyperopt import (\n    fmin,\n    STATUS_OK,\n    STATUS_FAIL,\n    tpe,\n    Trials,\n)\n\nfrom voluptuous import (\n    All,\n    Any,\n    Required,\n    Optional,\n    Range,\n)\n\nfrom . import (\n    errors,\n    schemas,\n)\nfrom .misc import (\n    datetime_to_str,\n    list_from_np,\n    make_datetime,\n    make_ts,\n    nan_to_none,\n    parse_timedelta,\n    ts_to_datetime,\n)\nfrom .model import (\n    Model,\n    DateRange,\n)\n\nDEFAULT_SEASONALITY = {\n    'daytime': False,\n    'weekday': False,\n}\n\nfloat_formatter = lambda x: \"%.2f\" % x\nnp.set_printoptions(formatter={'float_kind': float_formatter})\n\n_verbose = 0\n\n_hp_span_min = 10\n_hp_span_max = 100\n\n# Constants derived from https://arxiv.org/abs/1802.03903\ng_mcmc_count = 10\ng_mc_count = 1000\ng_mc_batch_size = 256\ng_lambda = 0.01\n\n\n# reparameterization trick\n# instead of sampling from Q(z|X), sample eps = N(0,I)\n# z = z_mean + sqrt(var)*eps\ndef sampling(args):\n    \"\"\"Reparameterization trick by sampling fr an isotropic unit Gaussian.\n\n    # Arguments:\n        args (tensor): mean and log of variance of Q(z|X)\n\n    # Returns:\n        z (tensor): sampled latent vector\n    \"\"\"\n\n    z_mean, z_log_var = args\n    batch = K.shape(z_mean)[0]\n    dim = K.int_shape(z_mean)[1]\n    # by default, random_normal has mean=0 and std=1.0\n    epsilon = K.random_normal(shape=(batch, dim))\n    return z_mean + K.exp(0.5 * z_log_var) * epsilon\n\n\ndef add_loss(model, W):\n    inputs = model.inputs[0]\n    abnormal = model.inputs[1]\n    # abnormal = K.print_tensor(abnormal, message='abnormal = ')\n    outputs = model.outputs[0]\n    z_mean = model.get_layer('z_mean').output\n    z_log_var = model.get_layer('z_log_var').output\n\n    beta = K.sum(1.0 - abnormal, axis=-1, keepdims=True) / W\n    # beta = K.print_tensor(beta, message='beta = ')\n    reconstruction_loss = mean_squared_error(inputs, outputs)\n    reconstruction_loss *= W\n    kl_loss = 1 + z_log_var - beta * K.square(z_mean) - K.exp(z_log_var)\n    kl_loss = K.sum(kl_loss, axis=-1)\n    kl_loss *= -0.5\n    vae_loss = K.mean(reconstruction_loss + kl_loss)\n    model.add_loss(vae_loss)\n\n\ndef _get_encoder(_keras_model):\n    # instantiate encoder model\n    main_input = _keras_model.inputs[0]\n    aux_input = _keras_model.inputs[1]\n    z_mean = _keras_model.get_layer('z_mean').output\n    z_log_var = _keras_model.get_layer('z_log_var').output\n    z = _keras_model.get_layer('z').output\n    model = _Model([main_input, aux_input], [z_mean, z_log_var, z], name='encoder')\n    return model\n\n\ndef _get_decoder(_keras_model):\n    # instantiate decoder model\n    _, latent_dim = _keras_model.get_layer('z').output.get_shape()\n    latent_dim = int(latent_dim)\n    latent_inputs = Input(shape=(int(latent_dim),), name='z_sampling')\n    x = _keras_model.get_layer('dense_1')(latent_inputs)\n    output = _keras_model.get_layer('dense_2')(x)\n    new_model = _Model(latent_inputs, output, name='decoder')\n    return new_model\n\n\ndef _get_index(d, from_date, step):\n    return int((make_ts(d) - make_ts(from_date)) / step)\n\n\ndef _format_windows(from_date, to_date, step, windows):\n    size = _get_index(to_date, from_date, step)\n    abnormal = np.full((size,), False, dtype=bool)\n    for _from, _to in windows:\n        x = _get_index(_from, from_date, step)\n        y = _get_index(_to, from_date, step)\n        abnormal[min(size, max(0, x)):max(0, min(y, size))] = True\n\n    return abnormal\n\n\ndef _get_scores(y, _mean, _std):\n    y = (y - _mean) / _std\n    return y\n\n\ndef _revert_scores(y, _mean, _std):\n    y = (y * _std) + _mean\n    return y\n\n\nclass HyperParameters:\n    \"\"\"Hyperparameters\"\"\"\n\n    def __init__(self, params=None):\n        if params:\n            self.assign(params)\n\n    def assign(self, params):\n        \"\"\"\n        Assign hyperparameters\n        \"\"\"\n\n        for key, value in params.items():\n            try:\n                if int(value) == value:\n                    value = int(value)\n                elif float(value) == value:\n                    value = float(value)\n            except ValueError:\n                pass\n            setattr(self, key, value)\n\n\ndef _serialize_keras_model(keras_model):\n    \"\"\"\n    Serialize Keras model\n    \"\"\"\n\n    import base64\n    import tempfile\n\n    fd, path = tempfile.mkstemp()\n    try:\n        keras_model.save(path)\n        with os.fdopen(fd, 'rb') as tmp:\n            model_b64 = base64.b64encode(tmp.read())\n    finally:\n        os.remove(path)\n\n    return model_b64.decode('utf-8')\n\n\ndef _load_keras_model(model_b64):\n    import tempfile\n    import base64\n\n    fd, path = tempfile.mkstemp()\n    try:\n        with os.fdopen(fd, 'wb') as tmp:\n            tmp.write(base64.b64decode(model_b64.encode('utf-8')))\n            tmp.close()\n    finally:\n        keras_model = load_model(path, compile=False)\n        opened_new_file = not isinstance(path, h5py.File)\n        if opened_new_file:\n            f = h5py.File(path, mode='r')\n        else:\n            f = path\n        training_config = f.attrs.get('training_config')\n        optimizer_cls = None\n        if training_config is None:\n            optimizer_cls = tf.keras.optimizers.Adam()\n        else:\n            training_config = json.loads(training_config.decode('utf-8'))\n            optimizer_config = training_config['optimizer_config']\n            optimizer_cls = tf.keras.optimizers.deserialize(optimizer_config)\n\n        if opened_new_file:\n            f.close()\n        os.remove(path)\n        _, W = keras_model.inputs[0].get_shape()\n        add_loss(keras_model, int(W))\n        keras_model.compile(\n            optimizer=optimizer_cls,\n        )\n\n    return keras_model\n\n\nclass TimeSeriesPrediction:\n    \"\"\"\n    Time-series prediction\n    \"\"\"\n\n    def __init__(self, model, timestamps, observed, predicted, upper=None, lower=None):\n        self.model = model\n        self.timestamps = timestamps\n        self.observed = observed\n        self.predicted = predicted\n        self.upper = upper\n        self.lower = lower\n        self.anomaly_indices = None\n        self.stats = None\n        self.constraint = None\n        self.scores = None\n        self.mses = None\n        self.mse = None\n\n    def get_anomalies(self):\n        \"\"\"\n        Return anomalies\n        \"\"\"\n\n        if self.anomaly_indices is None:\n            raise errors.NotFound(\"anomaly detection has not been performed yet\")\n        return [self._format_bucket(i) for i in self.anomaly_indices]\n\n    def format_series(self):\n        \"\"\"\n        Return prediction data as a time-series\n        \"\"\"\n\n        observed = {}\n        predicted = {}\n\n        feature = self.model.features[0]\n        observed[feature.name] = list_from_np(self.observed)\n        predicted[feature.name] = list_from_np(self.predicted)\n        if self.lower is not None:\n            predicted['lower_{}'.format(feature.name)] = list_from_np(self.lower)\n        if self.upper is not None:\n            predicted['upper_{}'.format(feature.name)] = list_from_np(self.upper)\n\n        result = {\n            'timestamps': self.timestamps,\n            'observed': observed,\n            'predicted': predicted,\n        }\n        if self.stats is not None:\n            result['stats'] = self.stats\n        if self.constraint is not None:\n            result['constraint'] = self.constraint\n        return result\n\n    def get_field_names(self):\n        features = self.model.features\n        names = []\n        for feature in features:\n            names.append(feature.name)\n            names.append(\"lower_{}\".format(feature.name))\n            names.append(\"upper_{}\".format(feature.name))\n\n        return names\n\n    def format_bucket_data(self, i):\n        \"\"\"\n        Format observation and prediction for one bucket\n        \"\"\"\n        feature = self.model.features[0]\n        predicted = {\n            feature.name: nan_to_none(self.predicted[i])\n        }\n        if self.lower is not None:\n            predicted.update({\n                'lower_{}'.format(feature.name): nan_to_none(self.lower[i])\n            })\n        if self.upper is not None:\n            predicted.update({\n                'upper_{}'.format(feature.name): nan_to_none(self.upper[i])\n            })\n        return {\n            'observed': {\n                feature.name: nan_to_none(self.observed[i])\n            },\n            'predicted': predicted\n        }\n\n    def _format_bucket(self, i):\n        \"\"\"\n        Format one bucket\n        \"\"\"\n\n        bucket = self.format_bucket_data(i)\n        bucket['timestamp'] = self.timestamps[i]\n        if self.stats:\n            bucket['stats'] = self.stats[i]\n        return bucket\n\n    def format_buckets(self):\n        \"\"\"\n        Return prediction data as buckets\n        \"\"\"\n\n        return [\n            self._format_bucket(i)\n            for i, _ in enumerate(self.timestamps)\n        ]\n\n    def __str__(self):\n        return json.dumps(self.format_buckets(), indent=4)\n\n    def stat(self):\n        self.scores, self.mses = self.model.compute_scores(\n            self.observed,\n            self.predicted,\n            self.lower,\n            self.upper,\n            )\n        self.mse = np.nanmean(self.mses, axis=None)\n\n    def plot(self, feature_name):\n        \"\"\"\n        Plot prediction\n        \"\"\"\n\n        import matplotlib.pylab as plt\n\n        i = None\n        for i, feature in enumerate(self.model.features):\n            if feature.name == feature_name:\n                break\n\n        if i is None:\n            raise errors.NotFound(\"feature not found\")\n\n        x = np.linspace(1, len(self.observed), len(self.observed))\n        plt.rcParams[\"figure.figsize\"] = (17, 9)\n        plt.plot(x, self.observed, \"--\")\n        plt.fill_between(x, self.lower, self.upper, alpha=0.2)\n        plt.plot(x, self.predicted, \":\", color='black')\n        plt.show()\n\n\ndef generator(x, missing, batch_size, model):\n    batch_x = np.zeros((batch_size, x.shape[1]))\n    batch_missing = np.zeros((batch_size, x.shape[1]))\n    while True:\n        abnormal = np.random.binomial(1, g_lambda, x.shape[1])\n        for i in range(batch_size):\n            index = random.randint(0, len(x)-1)\n            batch_x[i] = x[index]\n            batch_missing[i] = np.maximum(\n                abnormal,\n                missing[index],\n            )\n\n        for _ in range(g_mcmc_count):\n            x_decoded, _ = model.predict([batch_x, batch_missing], batch_size=g_mc_batch_size)\n            batch_x[batch_missing > 0] = x_decoded[batch_missing > 0]\n\n        yield ([batch_x, batch_missing], None)\n\n\nclass DonutModel(Model):\n    \"\"\"\n    Time-series VAE model, \"Donut\"\n    \"\"\"\n    TYPE = 'donut'\n\n    SCHEMA = Model.SCHEMA.extend({\n        Required('bucket_interval'): schemas.TimeDelta(\n            min=0, min_included=False,\n        ),\n        Required('interval'): schemas.TimeDelta(min=0, min_included=False),\n        Required('offset'): schemas.TimeDelta(min=0),\n        Required('span'): Any(None, \"auto\", All(int, Range(min=1))),\n        Optional('min_span'): All(int, Range(min=1)),\n        Optional('max_span'): All(int, Range(min=1)),\n        Optional('seasonality', default=DEFAULT_SEASONALITY): schemas.seasonality,\n        Optional('forecast'): Any(None, \"auto\", All(int, Range(min=1))),\n        Optional('grace_period', default=0): schemas.TimeDelta(min=0, min_included=True),\n        'default_datasink': schemas.key,\n    })\n\n    def __init__(self, settings, state=None):\n        global _hp_span_min, _hp_span_max\n        super().__init__(settings, state)\n\n        settings = self.validate(settings)\n        self.bucket_interval = parse_timedelta(settings.get('bucket_interval')).total_seconds()\n        self.interval = parse_timedelta(settings.get('interval')).total_seconds()\n        self.offset = parse_timedelta(settings.get('offset')).total_seconds()\n\n        self.span = settings.get('span')\n\n        self.means = None\n        self.stds = None\n        self.scores = None\n        self._keras_model = None\n        self._encoder_model = None\n        self._decoder_model = None\n\n        if self.span is None or self.span == \"auto\":\n            self.min_span = settings.get('min_span') or _hp_span_min\n            self.max_span = settings.get('max_span') or _hp_span_max\n        else:\n            self.min_span = self.span\n            self.max_span = self.span\n\n        self.grace_period = parse_timedelta(settings['grace_period']).total_seconds()\n\n        self.current_eval = None\n        if len(self.features) > 1:\n            raise errors.LoudMLException(\"This model type supports one unique feature\")\n\n    def enum_features(self, is_input=None, is_output=None):\n        j = 0\n        for i, feature in enumerate(self.features):\n            if feature.is_input == is_input or feature.is_output == is_output:\n                yield i, j, feature\n                j += 1\n\n    @property\n    def type(self):\n        return self.TYPE\n\n    @property\n    def W(self):\n        return self.span\n\n    def get_hp_span(self, label):\n        if (self.max_span - self.min_span) <= 0:\n            space = self.span\n        else:\n            space = self.min_span + hp.randint(label, (self.max_span - self.min_span))\n        return space\n\n    def set_run_params(self, params=None):\n        \"\"\"\n        Set running parameters to make them persistent\n        \"\"\"\n        if params is None:\n            self._settings.pop('run', None)\n        else:\n            self._settings['run'] = params\n\n    def set_run_state(self, params=None):\n        \"\"\"\n        Set running forecast parameters to make them persistent\n        \"\"\"\n        if params is None:\n            self._state.pop('run', None)\n        else:\n            self._state['run'] = params\n\n    def get_run_state(self):\n        return self._state.get('run') or {}\n\n    def compute_nb_buckets(self, from_ts, to_ts):\n        \"\"\"\n        Compute the number of bucket between `from_ts` and `to_ts`\n        \"\"\"\n        return int((to_ts - from_ts) / self.bucket_interval) + 2\n\n    def apply_defaults(self, x):\n        \"\"\"\n        Apply default feature value to np array\n        \"\"\"\n        feature = self.features[0]\n        if feature.default == \"previous\":\n            previous = None\n            for j, value in enumerate(x):\n                if np.isnan(value):\n                    x[j] = previous\n                else:\n                    previous = x[j]\n        elif not np.isnan(feature.default):\n            x[np.isnan(x)] = feature.default\n\n    def scale_dataset(\n        self,\n        dataset,\n    ):\n        \"\"\"\n        Scale dataset values\n        \"\"\"\n\n        out = _get_scores(\n            dataset,\n            _mean=self.means[0],\n            _std=self.stds[0],\n        )\n\n        return out\n\n    def unscale_dataset(\n        self,\n        dataset,\n    ):\n        \"\"\"\n        Revert scaling dataset values\n        \"\"\"\n\n        out = _revert_scores(\n            dataset,\n            _mean=self.means[0],\n            _std=self.stds[0],\n        )\n\n        return out\n\n    def stat_dataset(self, dataset):\n        \"\"\"\n        Compute dataset sets and keep them as reference\n        \"\"\"\n        self.means = np.array([np.nanmean(dataset, axis=0)])\n        self.stds = np.array([np.nanstd(dataset, axis=0)])\n        self.stds[self.stds == 0] = 1.0\n\n    def set_auto_threshold(self):\n        \"\"\"\n        Compute best threshold values automatically\n        \"\"\"\n        # 68\u201395\u201399.7 three-sigma rule\n        self.min_threshold = 68\n        self.max_threshold = 99.7\n\n    def _set_xpu_config(self, num_cpus, num_gpus):\n        config = tf.ConfigProto(\n            allow_soft_placement=True,\n            device_count={'CPU': num_cpus, 'GPU': num_gpus},\n        )\n        config.gpu_options.allow_growth = True\n#        config.log_device_placement = True\n#        config.intra_op_parallelism_threads=num_cores\n#        config.inter_op_parallelism_threads=num_cores\n\n        sess = tf.Session(config=config)\n        K.set_session(sess)\n\n    def _train_on_dataset(\n        self,\n        dataset,\n        train_size=0.67,\n        batch_size=64,\n        num_epochs=100,\n        num_cpus=1,\n        num_gpus=0,\n        max_evals=None,\n        progress_cb=None,\n        abnormal=None,\n    ):\n        if max_evals is None:\n            max_evals = self.settings.get('max_evals', 21)  # latent_dim*intermediate_dim\n\n        self.current_eval = 0\n\n        self.stat_dataset(dataset)\n        dataset = self.scale_dataset(dataset)\n\n        def cross_val_model(params):\n            keras_model = None\n            # Destroys the current TF graph and creates a new one.\n            # Useful to avoid clutter from old models / layers.\n            K.clear_session()\n            self._set_xpu_config(num_cpus, num_gpus)\n\n            self.span = W = params.span\n            (X_miss, X_train), (X_miss_val, X_test) = self.train_test_split(\n                dataset,\n                train_size=train_size,\n                abnormal=abnormal,\n            )\n            if len(X_train) == 0:\n                raise errors.NoData(\"insufficient training data\")\n            if len(X_test) == 0:\n                raise errors.NoData(\"insufficient validation data\")\n\n            # expected input data shape: (batch_size, timesteps,)\n            # network parameters\n            input_shape = (W, )\n            intermediate_dim = params.intermediate_dim\n            latent_dim = params.latent_dim\n\n            # VAE model = encoder + decoder\n            # build encoder model\n            main_input = Input(shape=input_shape)\n            aux_input = Input(shape=input_shape)  # bool vector to flag missing data points\n            aux_output = Lambda(lambda x: x)(aux_input)\n            x = Dense(intermediate_dim,\n                      kernel_regularizer=regularizers.l2(0.01),\n                      activation='relu')(main_input)\n            z_mean = Dense(latent_dim, name='z_mean')(x)\n            z_log_var = Dense(latent_dim, name='z_log_var')(x)\n\n            # use reparameterization trick to push the sampling out as input\n            # note that \"output_shape\" isn't necessary with the TensorFlow backend\n            z = Lambda(sampling, output_shape=(latent_dim,), name='z')([z_mean, z_log_var])\n            \n            # build decoder model\n            x = Dense(intermediate_dim,\n                      kernel_regularizer=regularizers.l2(0.01),\n                      activation='relu', name='dense_1')(z)\n            main_output = Dense(W, activation='linear', name='dense_2')(x)\n             \n            # instantiate Donut model\n            keras_model = _Model([main_input, aux_input], [main_output, aux_output], name='donut')\n            add_loss(keras_model, W)\n            optimizer_cls = None\n            if params.optimizer == 'adam':\n                optimizer_cls = tf.keras.optimizers.Adam()\n\n            keras_model.compile(\n                optimizer=optimizer_cls,\n            )\n\n            _stop = EarlyStopping(\n                monitor='val_loss',\n                patience=5,\n                verbose=_verbose,\n                mode='auto',\n            )\n            keras_model.fit_generator(\n                generator(X_train, X_miss, batch_size, keras_model),\n                epochs=num_epochs,\n                steps_per_epoch=len(X_train) / batch_size,\n                verbose=_verbose,\n                validation_data=([X_test, X_miss_val], None),\n                callbacks=[_stop],\n                workers=0,  # https://github.com/keras-team/keras/issues/5511\n            )\n\n            # How well did it do?\n            score = keras_model.evaluate(\n                [X_test, X_miss_val],\n                batch_size=batch_size,\n                verbose=_verbose,\n            )\n\n            self.current_eval += 1\n            if progress_cb is not None:\n                progress_cb(self.current_eval, max_evals)\n\n            return score, keras_model\n\n        hyperparameters = HyperParameters()\n\n        # Parameter search space\n        def objective(args):\n            hyperparameters.assign(args)\n\n            try:\n                score, _ = cross_val_model(hyperparameters)\n                return {'loss': score, 'status': STATUS_OK}\n            except Exception as exn:\n                logging.warning(\"iteration failed: %s\", exn)\n                return {'loss': None, 'status': STATUS_FAIL}\n\n        space = hp.choice('case', [\n            {\n              'span': self.get_hp_span('span'),\n              'latent_dim': hp.choice('latent_dim', [3, 5, 8]),\n              'intermediate_dim': hp.choice('i1', [21, 34, 55, 89, 144, 233, 377]),\n              'optimizer': hp.choice('optimizer', ['adam']),\n            }\n        ])\n\n        # The Trials object will store details of each iteration\n        trials = Trials()\n\n        # Run the hyperparameter search using the tpe algorithm\n        try:\n            best = fmin(\n                objective,\n                space,\n                algo=tpe.suggest,\n                max_evals=max_evals,\n                trials=trials,\n            )\n        except ValueError:\n            raise errors.NoData(\"training failed, try to increase the time range\")\n\n        # Get the values of the optimal parameters\n        best_params = space_eval(space, best)\n        score, self._keras_model = cross_val_model(\n            HyperParameters(best_params)\n        )\n        self.span = best_params['span']\n        return (best_params, score)\n\n    def _train_ckpt_on_dataset(\n        self,\n        dataset,\n        train_size=0.67,\n        batch_size=64,\n        num_epochs=100,\n        progress_cb=None,\n        abnormal=None,\n    ):\n        self.current_eval = 0\n        self.stat_dataset(dataset)\n\n        dataset = self.scale_dataset(dataset)\n\n        (X_miss, X_train), (X_miss_val, X_test) = self.train_test_split(\n            dataset,\n            train_size=train_size,\n        )\n\n        _stop = EarlyStopping(\n            monitor='val_loss',\n            patience=5,\n            verbose=_verbose,\n            mode='auto',\n        )\n        self._keras_model.fit(\n            [X_train, X_miss],\n            epochs=num_epochs,\n            batch_size=batch_size,\n            verbose=_verbose,\n            validation_data=([X_test, X_miss_val], None),\n            callbacks=[_stop],\n        )\n\n        # How well did it do?\n        score = self._keras_model.evaluate(\n            [X_test, X_miss_val],\n            batch_size=batch_size,\n            verbose=_verbose,\n        )\n        return score\n\n    def compute_bucket_scores(self, y_true, y_pred, y_low, y_high):\n        \"\"\"\n        Compute scores and mean squared error\n        \"\"\"\n        feature = self.features[0]\n\n        diff = y_true - y_pred\n        ano_type = feature.anomaly_type\n        mu = (y_low + y_high) / 2.0\n        std = (y_high - mu) / 3.0\n        score = 2 * norm.cdf(abs(y_true - mu), loc=0, scale=std) - 1\n        # Required to handle the 'low' condition\n        if diff < 0:\n            score *= -1\n\n        if ano_type == 'low':\n            score = -min(score, 0)\n        elif ano_type == 'high':\n            score = max(score, 0)\n        else:\n            score = abs(score)\n\n        score = 100 * max(0, min(1, score))\n\n        mse = np.nanmean((diff ** 2), axis=None)\n        return score, mse\n\n    def compute_scores(self, observed, predicted, low, high):\n        \"\"\"\n        Compute timeseries scores and MSE\n        \"\"\"\n\n        nb_buckets = len(observed)\n        scores = np.empty((nb_buckets,), dtype=float)\n        mses = np.empty((nb_buckets), dtype=float)\n\n        for i in range(nb_buckets):\n            scores[i], mses[i] = self.compute_bucket_scores(\n                observed[i],\n                predicted[i],\n                low[i],\n                high[i],\n            )\n\n        return scores, mses\n\n    def _format_dataset(self, x, accept_missing=True, abnormal=None):\n        \"\"\"\n        Format dataset for time-series training & inference\n\n        input:\n        [v0, v1, v2, v3, v4 ..., vn]\n\n        len: W\n\n        output:\n        missing = [0, 0, 1..., 0]\n        X = [\n            [v0, v1, v2], # span = W\n            [v1, v2, v3],\n            [v2, v3, v4],\n            ...\n            [..., .., vn],\n        ]\n\n        Buckets with missing values are flagged in the missing array.\n        \"\"\"\n        missing = []\n        data_x = []\n        for i in range(len(x) - self.W + 1):\n            j = i + self.W\n            if accept_missing or not np.isnan(x[i:j]).any():\n                # arxiv.org/abs/1802.03903\n                # set user defined abnormal data points to zero\n                if abnormal is None:\n                    is_nan = np.isnan(x[i:j])\n                else:\n                    is_nan = np.logical_or(\n                        np.isnan(x[i:j]),\n                        abnormal[i:j],\n                        )\n\n                missing.append(is_nan)\n                _x = np.copy(x[i:j])\n                # set missing points to zero\n                _x[is_nan] = 0.0\n                data_x.append(_x)\n\n        return np.array(missing), np.array(data_x)\n\n    def train_test_split(self, dataset, abnormal=None, train_size=0.67):\n        \"\"\"\n        Splits data to training and testing parts\n        \"\"\"\n        ntrn = round(len(dataset) * train_size)\n        X_train_missing, X_train = self._format_dataset(dataset[0:ntrn], abnormal=abnormal)\n        X_test_missing, X_test = self._format_dataset(dataset[ntrn:])\n        return (X_train_missing, X_train), (X_test_missing, X_test)\n\n    def train(\n        self,\n        datasource,\n        from_date,\n        to_date=\"now\",\n        train_size=0.67,\n        batch_size=256,\n        num_epochs=100,\n        num_cpus=1,\n        num_gpus=0,\n        max_evals=None,\n        progress_cb=None,\n        incremental=False,\n        windows=[],\n    ):\n        \"\"\"\n        Train model\n        \"\"\"\n\n        self.means, self.stds = None, None\n        self.scores = None\n\n        period = self.build_date_range(from_date, to_date)\n        logging.info(\n            \"train(%s) range=%s train_size=%f batch_size=%d epochs=%d)\",\n            self.name,\n            period,\n            train_size,\n            batch_size,\n            num_epochs,\n        )\n\n        # Prepare dataset\n        nb_buckets = self.compute_nb_buckets(period.from_ts, period.to_ts)\n        dataset = np.full((nb_buckets,), np.nan, dtype=float)\n        abnormal = _format_windows(\n            period.from_ts,\n            period.to_ts,\n            self.bucket_interval,\n            windows,\n        )\n\n        # Fill dataset\n        data = datasource.get_times_data(self, period.from_ts, period.to_ts)\n        # FIXME: query abnormal points flagged\n\n        i = None\n        for i, (_, val, timeval) in enumerate(data):\n            dataset[i] = val\n\n        if i is None:\n            raise errors.NoData(\"no data found for time range {}\".format(period))\n\n        self.apply_defaults(dataset)\n\n        nb_buckets_found = i + 1\n        if nb_buckets_found < nb_buckets:\n            dataset = np.resize(dataset, (nb_buckets_found,))\n\n        logging.info(\"found %d time periods\", nb_buckets_found)\n\n        if incremental:\n            best_params = self._state.get('best_params', dict())\n            # Destroys the current TF graph and creates a new one.\n            # Useful to avoid clutter from old models / layers.\n            self.load(num_cpus, num_gpus)\n            score = self._train_ckpt_on_dataset(\n                dataset,\n                train_size,\n                batch_size,\n                num_epochs,\n                progress_cb=progress_cb,\n                abnormal=abnormal,\n            )\n        else:\n            best_params, score = self._train_on_dataset(\n                dataset,\n                train_size,\n                batch_size,\n                num_epochs,\n                num_cpus,\n                num_gpus,\n                max_evals,\n                progress_cb=progress_cb,\n                abnormal=abnormal,\n            )\n        self.current_eval = None\n\n        for key, val in best_params.items():\n            if not isinstance(val, str) and \\\n               not isinstance(val, int) and \\\n               not isinstance(val, float):\n                best_params[key] = np.asscalar(val)\n\n        model_b64 = _serialize_keras_model(self._keras_model)\n\n        self._state = {\n            'h5py': model_b64,\n            'best_params': best_params,\n            'means': self.means.tolist(),\n            'stds': self.stds.tolist(),\n            'loss': score,\n        }\n        self.unload()\n        #prediction = self.predict(\n        #    datasource,\n        #    from_date,\n        #    to_date,\n        #    num_cpus=num_cpus,\n        #    num_gpus=num_gpus,\n        #)\n        #prediction.stat()\n\n        return {\n            'loss': score,\n        }\n\n    def unload(self):\n        \"\"\"\n        Unload current model\n        \"\"\"\n        self._keras_model = None\n        self._encoder_model = None\n        self._decoder_model = None\n        K.clear_session()\n\n    def load(self, num_cpus, num_gpus):\n        \"\"\"\n        Load current model\n        \"\"\"\n        if not self.is_trained:\n            raise errors.ModelNotTrained()\n        if self._keras_model:\n            # Already loaded\n            return\n\n        K.clear_session()\n        self._set_xpu_config(num_cpus, num_gpus)\n\n        if self._state.get('h5py', None) is not None:\n            self._keras_model = _load_keras_model(self._state.get('h5py'))\n            # instantiate encoder model\n            self._encoder_model = _get_encoder(self._keras_model)\n            # instantiate decoder model\n            self._decoder_model = _get_decoder(self._keras_model)\n        else:\n            raise errors.ModelNotTrained()\n\n        if 'means' in self._state:\n            self.means = np.array(self._state['means'])\n        if 'stds' in self._state:\n            self.stds = np.array(self._state['stds'])\n        if 'scores' in self._state:\n            self.scores = np.array(self._state['scores'])\n        if self.min_threshold == 0 and self.max_threshold == 0:\n            self.set_auto_threshold()\n            logging.info(\n                \"setting threshold range min=%f max=%f\",\n                self.min_threshold,\n                self.max_threshold,\n            )\n\n    @property\n    def is_trained(self):\n        \"\"\"\n        Tells if model is trained\n        \"\"\"\n        return self._state is not None and ('weights' in self._state or 'h5py' in self._state)\n\n    @property\n    def _span(self):\n        if self._state and 'span' in self._state['best_params']:\n            return self._state['best_params']['span']\n        else:\n            return self.span\n\n    @property\n    def _window(self):\n        return self._span\n\n    def predict(\n        self,\n        datasource,\n        from_date,\n        to_date,\n        num_cpus=1,\n        num_gpus=0,\n    ):\n        global g_mcmc_count\n        global g_mc_count\n        global g_mc_batch_size\n\n        period = self.build_date_range(from_date, to_date)\n\n        # This is the number of buckets that the function MUST return\n        predict_len = int((period.to_ts - period.from_ts) / self.bucket_interval)\n\n        logging.info(\"predict(%s) range=%s\", self.name, period)\n\n        self.load(num_cpus, num_gpus)\n\n        # Build history time range\n        # Extra data are required to predict first buckets\n        _window = self._window - 1\n\n        hist = DateRange(\n            period.from_ts - _window * self.bucket_interval,\n            period.to_ts,\n        )\n\n        # Prepare dataset\n        nb_buckets = int((hist.to_ts - hist.from_ts) / self.bucket_interval)\n        dataset = np.full((nb_buckets,), np.nan, dtype=float)\n        X = []\n\n        # Fill dataset\n        logging.info(\"extracting data for range=%s\", hist)\n        data = datasource.get_times_data(self, hist.from_ts, hist.to_ts)\n\n        # Only a subset of history will be used for computing the prediction\n        X_until = None  # right bound for prediction\n        i = None\n\n        for i, (_, val, timeval) in enumerate(data):\n            dataset[i] = val\n\n            dt = make_datetime(timeval)\n            ts = dt.timestamp()\n            if ts < period.to_ts:\n                X.append(make_ts(timeval))\n                X_until = i + 1\n\n        if i is None:\n            raise errors.NoData(\"no data found for time range {}\".format(hist))\n\n        self.apply_defaults(dataset)\n\n        nb_buckets_found = i + 1\n        if nb_buckets_found < nb_buckets:\n            dataset = np.resize(dataset, (nb_buckets_found,))\n\n        logging.info(\"found %d time periods\", nb_buckets_found)\n\n        real = np.copy(dataset)\n\n        norm_dataset = self.scale_dataset(dataset)\n        missing, X_test = self._format_dataset(norm_dataset[:X_until])\n        if len(X_test) == 0:\n            raise errors.LoudMLException(\"not enough data for prediction\")\n\n        # force last col to missing\n        missing[:, -1] = True\n\n        logging.info(\"generating prediction\")\n        x_ = X_test.copy()\n        # MCMC\n        for _ in range(g_mcmc_count):\n            z_mean, _, _ = self._encoder_model.predict([x_, missing], batch_size=g_mc_batch_size)\n            x_decoded = self._decoder_model.predict(z_mean, batch_size=g_mc_batch_size)\n            x_[missing] = x_decoded[missing]\n\n        y = np.full((predict_len,), np.nan, dtype=float)\n        y_low = np.full((predict_len,), np.nan, dtype=float)\n        y_high = np.full((predict_len,), np.nan, dtype=float)\n        no_missing_point = np.full((g_mc_count, self.W), False, dtype=bool)\n        for j, x in enumerate(x_):\n            y[j] = x[-1]\n            # MC integration\n            _, _, Z = self._encoder_model.predict(\n                [np.tile(x, [g_mc_count, 1]), no_missing_point],\n                batch_size=g_mc_batch_size,\n            )\n            x_decoded = self._decoder_model.predict(Z, batch_size=g_mc_batch_size)\n            std = np.std(x_decoded[:, -1])\n            y_low[j] = x[-1] - 3 * std\n            y_high[j] = x[-1] + 3 * std\n\n        y = self.unscale_dataset(y)\n        y_low = self.unscale_dataset(y_low)\n        y_high = self.unscale_dataset(y_high)\n\n        # Build final result\n        timestamps = X[_window:]\n\n        shape = (predict_len, len(self.features))\n        observed = np.full(shape, np.nan, dtype=float)\n        observed = real[_window:]\n        self.apply_defaults(observed)\n        self.apply_defaults(y)\n\n        return TimeSeriesPrediction(\n            self,\n            timestamps=timestamps,\n            observed=observed,\n            predicted=y,\n            lower=y_low,\n            upper=y_high,\n        )\n\n    def generate_fake_prediction(self):\n        now_ts = datetime.datetime.now().timestamp()\n        timestamps = [\n            now_ts - 2 * self.bucket_interval,\n            now_ts - self.bucket_interval,\n            now_ts,\n        ]\n        normal = [0.0] * len(self.features)\n        anomaly = [sys.float_info.max] * len(self.features)\n\n        return TimeSeriesPrediction(\n            self,\n            timestamps=timestamps,\n            observed=np.array([normal, anomaly, normal]),\n            predicted=np.array([normal, normal, normal]),\n        )\n\n```\n\n### Code description\n```Python\n\"\"\"\nDescription: This function is used to generate a forecast for a given time range using a machine learning model.\n\nArgs:\n    self: The instance of the class that the function is called on.\n    datasource: The data source to retrieve data from.\n    from_date: The start date of the forecast range.\n    to_date: The end date of the forecast range.\n    percent_interval (float): The confidence interval for the forecast. Default is 0.68.\n    percent_noise (float): The noise level for the forecast. Default is 0.\n    num_cpus (int): The number of CPUs to use for loading data. Default is 1.\n    num_gpus (int): The number of GPUs to use for loading data. Default is 0.\n\nReturns:\n    TimeSeriesPrediction: A prediction object containing the observed and predicted values, as well as the lower and upper bounds of the confidence interval.\n\nRaises:\n    NoData: If no data is found for the given time range.\n    LoudMLException: If not enough data is available for prediction.\n\nNotes:\n    This function uses the API of scipy.stats.norm and numpy.random.uniform.\n    The forecast is generated using a machine learning model, and the confidence interval is computed using the percent_interval parameter.\n    The noise level is also taken into account, which is used to model the uncertainty of the forecast.\n\"\"\"\n```\n\n### Code snippet:\n```Python\n\ndef forecast(\n    self,\n    datasource,\n    from_date,\n    to_date,\n    percent_interval=0.68,\n    percent_noise=0,\n    num_cpus=1,\n    num_gpus=0,\n):\n    global g_mcmc_count\n    global g_mc_count\n    global g_mc_batch_size\n\n    period = self.build_date_range(from_date, to_date)\n\n    # This is the number of buckets that the function MUST return\n    forecast_len = int((period.to_ts - period.from_ts) / self.bucket_interval)\n\n    logging.info(\"forecast(%s) range=%s\", self.name, period)\n\n    self.load(num_cpus, num_gpus)\n\n    # Build history time range\n    # Extra data are required to predict first buckets\n    _window = self._window - 1\n\n    hist = DateRange(\n        period.from_ts - _window * self.bucket_interval,\n        period.to_ts,\n    )\n\n    # Prepare dataset\n    nb_buckets = int((hist.to_ts - hist.from_ts) / self.bucket_interval)\n    dataset = np.full((nb_buckets,), np.nan, dtype=float)\n    X = []\n\n    # Fill dataset\n    logging.info(\"extracting data for range=%s\", hist)\n    data = datasource.get_times_data(self, hist.from_ts, hist.to_ts)\n\n    # Only a subset of history will be used for computing the prediction\n    X_until = None  # right bound for prediction\n    i = None\n\n    for i, (_, val, timeval) in enumerate(data):\n        dataset[i] = val\n\n        dt = make_datetime(timeval)\n        ts = dt.timestamp()\n        if ts < period.to_ts:\n            X.append(make_ts(timeval))\n            X_until = i + 1\n\n    if i is None:\n        raise errors.NoData(\"no data found for time range {}\".format(hist))\n\n    self.apply_defaults(dataset)\n\n    nb_buckets_found = i + 1\n    if nb_buckets_found < nb_buckets:\n        dataset = np.resize(dataset, (nb_buckets_found,))\n\n    logging.info(\"found %d time periods\", nb_buckets_found)\n\n    real = np.copy(dataset)\n```\n\n### Requirements:\n1. Complete only the missing part of the function.\n2. Only answer the code of the completed part. Do not include any of the provided code snippet.\n3. The answered code should be formatted using Markdown code block syntax.\n4. Do not include any additional text.\n\n### Answer: (use the provided format with backticks)\n", "name": "forecast", "reference": "\n    norm_dataset = self.scale_dataset(dataset)\n    _, X_test = self._format_dataset(norm_dataset[:X_until])\n    if len(X_test) == 0:\n        raise errors.LoudMLException(\"not enough data for prediction\")\n\n    logging.info(\"generating prediction\")\n    x_ = X_test.copy()\n\n    p = norm().ppf(1-(1-percent_interval)/2)\n    missing = np.full((self._window,), False, dtype=bool)\n    # force last col to missing\n    missing[-1] = True\n    y = np.full((forecast_len,), np.nan, dtype=float)\n    y_low = np.full((forecast_len,), np.nan, dtype=float)\n    y_high = np.full((forecast_len,), np.nan, dtype=float)\n    x = x_[0]\n    noise = percent_noise * float(self.bucket_interval) / (24*3600)\n    for j, _ in enumerate(x_):\n        # MCMC\n        for _ in range(g_mcmc_count):\n            z_mean, _, _ = self._encoder_model.predict(\n                [np.array([x]), np.array([missing])],\n                batch_size=g_mc_batch_size,\n            )\n            x_decoded = self._decoder_model.predict(z_mean, batch_size=g_mc_batch_size)\n            x[missing] = x_decoded[0][missing]\n\n        # uncertainty is modeled using a random uniform noise distribution\n        # that increases over time\n        expand = np.random.uniform(-noise * j, noise * j, len(x))\n        x *= 1 + expand\n        # MC integration\n        _, _, Z = self._encoder_model.predict(\n            [np.tile(x, [g_mc_count, 1]), np.tile(missing, [g_mc_count, 1])],\n            batch_size=g_mc_batch_size,\n        )\n        x_decoded = self._decoder_model.predict(Z, batch_size=g_mc_batch_size)\n        std = np.std(x_decoded[:, -1])\n        y_low[j] = x[-1] - p * std\n        y_high[j] = x[-1] + p * std\n        y[j] = x[-1]\n        x = np.roll(x, -1)\n        # set missing point to zero\n        x[-1] = 0\n\n    y = self.unscale_dataset(y)\n    y_low = self.unscale_dataset(y_low)\n    y_high = self.unscale_dataset(y_high)\n\n    # Build final result\n    timestamps = X[_window:]\n\n    shape = (forecast_len, len(self.features))\n    observed = np.full(shape, np.nan, dtype=float)\n    observed = real[_window:]\n    self.apply_defaults(observed)\n    self.apply_defaults(y)\n\n    return TimeSeriesPrediction(\n        self,\n        timestamps=timestamps,\n        observed=observed,\n        predicted=y,\n        lower=y_low,\n        upper=y_high,\n    )", "completions": ["```python\n```python\n    # force last col to missing\n    missing[:, -1] = True\n\n    logging.info(\"generating prediction\")\n    x_ = X_test.copy()\n    # MCMC\n    for _ in range(g_mcmc_count):\n        z_mean, _, _ = self._encoder_model.predict([x_, missing], batch_size=g_mc_batch_size)\n        x_decoded = self._decoder_model.predict(z_mean, batch_size=g_mc_batch_size)\n        x_[missing] = x_decoded[missing]\n\n    y = np.full((forecast_len,), np.nan, dtype=float)\n    y_low = np.full((forecast_len,), np.nan, dtype=float)\n    y_high = np.full((forecast_len,), np.nan, dtype=float)\n    no_missing_point = np.full((g_mc_count, self.W), False, dtype=bool)\n    for j, x in enumerate(x_):\n        y[j] = x[-1]\n        # MC integration\n        _, _, Z = self._encoder_model.predict(\n            [np.tile(x, [g_mc_count, 1]), no_missing_point],\n            batch_size=g_mc_batch_size,\n        )\n        x_decoded = self._decoder_model.predict(Z, batch_size=g_mc_batch_size)\n        std = np.std(x_decoded[:, -1])\n        y_low[j] = x[-1] - 3 * std\n        y_high[j] = x[-1] + 3 * std\n\n    y = self.unscale_dataset(y)\n    y_low = self.unscale_dataset(y_low)\n    y_high = self.unscale_dataset(y_high)\n\n    # Build final result\n    timestamps = X[_window:]\n\n    shape = (forecast_len, len(self.features))\n    observed = np.full(shape, np.nan, dtype=float)\n    observed = real[_window:]\n    self.apply_defaults(observed)\n    self.apply_defaults(y)\n\n    return TimeSeriesPrediction(\n        self,\n        timestamps=timestamps,\n        observed=observed,\n        predicted=y,\n        lower=y_low,\n        upper=y_high,\n    )\n```\n```\n"]}
{"id": 34, "prompt": "<\uff5cbegin\u2581of\u2581sentence\uff5c>### Role:\nYou are an experienced Python programmer assistant responsible for code completion tasks. Please intelligently complete the missing parts of the provided code snippet.\n\n### Task:\nBased on the provided code snippet, complete the missing function implementation. Ensure the code is logically correct, syntactically accurate, and follows best coding practices.\n\n### Left context\n```Python\n\"\"\"\nThis module contain solvers for all kinds of equations:\n\n    - algebraic or transcendental, use solve()\n\n    - recurrence, use rsolve()\n\n    - differential, use dsolve()\n\n    - nonlinear (numerically), use nsolve()\n      (you will need a good starting point)\n\n\"\"\"\n\nfrom __future__ import print_function, division\n\nfrom sympy.core.compatibility import (iterable, is_sequence, ordered,\n    default_sort_key, range)\nfrom sympy.core.sympify import sympify\nfrom sympy.core import (S, Add, Symbol, Equality, Dummy, Expr, Mul,\n    Pow, Unequality)\nfrom sympy.core.exprtools import factor_terms\nfrom sympy.core.function import (expand_mul, expand_multinomial, expand_log,\n                          Derivative, AppliedUndef, UndefinedFunction, nfloat,\n                          Function, expand_power_exp, Lambda, _mexpand)\nfrom sympy.integrals.integrals import Integral\nfrom sympy.core.numbers import ilcm, Float\nfrom sympy.core.relational import Relational, Ge, _canonical\nfrom sympy.core.logic import fuzzy_not, fuzzy_and\nfrom sympy.logic.boolalg import And, Or, BooleanAtom\nfrom sympy.core.basic import preorder_traversal\n\nfrom sympy.functions import (log, exp, LambertW, cos, sin, tan, acos, asin, atan,\n                             Abs, re, im, arg, sqrt, atan2)\nfrom sympy.functions.elementary.trigonometric import (TrigonometricFunction,\n                                                      HyperbolicFunction)\nfrom sympy.simplify import (simplify, collect, powsimp, posify, powdenest,\n                            nsimplify, denom, logcombine)\nfrom sympy.simplify.sqrtdenest import sqrt_depth\nfrom sympy.simplify.fu import TR1\nfrom sympy.matrices import Matrix, zeros\nfrom sympy.polys import roots, cancel, factor, Poly, together, degree\nfrom sympy.polys.polyerrors import GeneratorsNeeded, PolynomialError\nfrom sympy.functions.elementary.piecewise import piecewise_fold, Piecewise\n\nfrom sympy.utilities.lambdify import lambdify\nfrom sympy.utilities.misc import filldedent\nfrom sympy.utilities.iterables import uniq, generate_bell, flatten\nfrom sympy.utilities.decorator import conserve_mpmath_dps\n\nfrom mpmath import findroot\n\nfrom sympy.solvers.polysys import solve_poly_system\nfrom sympy.solvers.inequalities import reduce_inequalities\n\nfrom types import GeneratorType\nfrom collections import defaultdict\nimport warnings\n\n\ndef recast_to_symbols(eqs, symbols):\n    \"\"\"Return (e, s, d) where e and s are versions of eqs and\n    symbols in which any non-Symbol objects in symbols have\n    been replaced with generic Dummy symbols and d is a dictionary\n    that can be used to restore the original expressions.\n\n    Examples\n    ========\n\n    >>> from sympy.solvers.solvers import recast_to_symbols\n    >>> from sympy import symbols, Function\n    >>> x, y = symbols('x y')\n    >>> fx = Function('f')(x)\n    >>> eqs, syms = [fx + 1, x, y], [fx, y]\n    >>> e, s, d = recast_to_symbols(eqs, syms); (e, s, d)\n    ([_X0 + 1, x, y], [_X0, y], {_X0: f(x)})\n\n    The original equations and symbols can be restored using d:\n\n    >>> assert [i.xreplace(d) for i in eqs] == eqs\n    >>> assert [d.get(i, i) for i in s] == syms\n    \"\"\"\n    if not iterable(eqs) and iterable(symbols):\n        raise ValueError('Both eqs and symbols must be iterable')\n    new_symbols = list(symbols)\n    swap_sym = {}\n    for i, s in enumerate(symbols):\n        if not isinstance(s, Symbol) and s not in swap_sym:\n            swap_sym[s] = Dummy('X%d' % i)\n            new_symbols[i] = swap_sym[s]\n    new_f = []\n    for i in eqs:\n        try:\n            new_f.append(i.subs(swap_sym))\n        except AttributeError:\n            new_f.append(i)\n    swap_sym = {v: k for k, v in swap_sym.items()}\n    return new_f, new_symbols, swap_sym\n\n\ndef _ispow(e):\n    \"\"\"Return True if e is a Pow or is exp.\"\"\"\n    return isinstance(e, Expr) and (e.is_Pow or isinstance(e, exp))\n\n\ndef _simple_dens(f, symbols):\n    # when checking if a denominator is zero, we can just check the\n    # base of powers with nonzero exponents since if the base is zero\n    # the power will be zero, too. To keep it simple and fast, we\n    # limit simplification to exponents that are Numbers\n    dens = set()\n    for d in denoms(f, symbols):\n        if d.is_Pow and d.exp.is_Number:\n            if d.exp.is_zero:\n                continue  # foo**0 is never 0\n            d = d.base\n        dens.add(d)\n    return dens\n\n\ndef denoms(eq, *symbols):\n    \"\"\"Return (recursively) set of all denominators that appear in eq\n    that contain any symbol in ``symbols``; if ``symbols`` are not\n    provided then all denominators will be returned.\n\n    Examples\n    ========\n\n    >>> from sympy.solvers.solvers import denoms\n    >>> from sympy.abc import x, y, z\n    >>> from sympy import sqrt\n\n    >>> denoms(x/y)\n    {y}\n\n    >>> denoms(x/(y*z))\n    {y, z}\n\n    >>> denoms(3/x + y/z)\n    {x, z}\n\n    >>> denoms(x/2 + y/z)\n    {2, z}\n\n    If `symbols` are provided then only denominators containing\n    those symbols will be returned\n\n    >>> denoms(1/x + 1/y + 1/z, y, z)\n    {y, z}\n    \"\"\"\n\n    pot = preorder_traversal(eq)\n    dens = set()\n    for p in pot:\n        den = denom(p)\n        if den is S.One:\n            continue\n        for d in Mul.make_args(den):\n            dens.add(d)\n    if not symbols:\n        return dens\n    elif len(symbols) == 1:\n        if iterable(symbols[0]):\n            symbols = symbols[0]\n    rv = []\n    for d in dens:\n        free = d.free_symbols\n        if any(s in free for s in symbols):\n            rv.append(d)\n    return set(rv)\n\n\ndef checksol(f, symbol, sol=None, **flags):\n    \"\"\"Checks whether sol is a solution of equation f == 0.\n\n    Input can be either a single symbol and corresponding value\n    or a dictionary of symbols and values. When given as a dictionary\n    and flag ``simplify=True``, the values in the dictionary will be\n    simplified. ``f`` can be a single equation or an iterable of equations.\n    A solution must satisfy all equations in ``f`` to be considered valid;\n    if a solution does not satisfy any equation, False is returned; if one or\n    more checks are inconclusive (and none are False) then None\n    is returned.\n\n    Examples\n    ========\n\n    >>> from sympy import symbols\n    >>> from sympy.solvers import checksol\n    >>> x, y = symbols('x,y')\n    >>> checksol(x**4 - 1, x, 1)\n    True\n    >>> checksol(x**4 - 1, x, 0)\n    False\n    >>> checksol(x**2 + y**2 - 5**2, {x: 3, y: 4})\n    True\n\n    To check if an expression is zero using checksol, pass it\n    as ``f`` and send an empty dictionary for ``symbol``:\n\n    >>> checksol(x**2 + x - x*(x + 1), {})\n    True\n\n    None is returned if checksol() could not conclude.\n\n    flags:\n        'numerical=True (default)'\n           do a fast numerical check if ``f`` has only one symbol.\n        'minimal=True (default is False)'\n           a very fast, minimal testing.\n        'warn=True (default is False)'\n           show a warning if checksol() could not conclude.\n        'simplify=True (default)'\n           simplify solution before substituting into function and\n           simplify the function before trying specific simplifications\n        'force=True (default is False)'\n           make positive all symbols without assumptions regarding sign.\n\n    \"\"\"\n    from sympy.physics.units import Unit\n\n    minimal = flags.get('minimal', False)\n\n    if sol is not None:\n        sol = {symbol: sol}\n    elif isinstance(symbol, dict):\n        sol = symbol\n    else:\n        msg = 'Expecting (sym, val) or ({sym: val}, None) but got (%s, %s)'\n        raise ValueError(msg % (symbol, sol))\n\n    if iterable(f):\n        if not f:\n            raise ValueError('no functions to check')\n        rv = True\n        for fi in f:\n            check = checksol(fi, sol, **flags)\n            if check:\n                continue\n            if check is False:\n                return False\n            rv = None  # don't return, wait to see if there's a False\n        return rv\n\n    if isinstance(f, Poly):\n        f = f.as_expr()\n    elif isinstance(f, (Equality, Unequality)):\n        if f.rhs in (S.true, S.false):\n            f = f.reversed\n        B, E = f.args\n        if B in (S.true, S.false):\n            f = f.subs(sol)\n            if f not in (S.true, S.false):\n                return\n        else:\n            f = Add(f.lhs, -f.rhs, evaluate=False)\n\n    if isinstance(f, BooleanAtom):\n        return bool(f)\n    elif not f.is_Relational and not f:\n        return True\n\n    if sol and not f.free_symbols & set(sol.keys()):\n        # if f(y) == 0, x=3 does not set f(y) to zero...nor does it not\n        return None\n\n    illegal = set([S.NaN,\n               S.ComplexInfinity,\n               S.Infinity,\n               S.NegativeInfinity])\n    if any(sympify(v).atoms() & illegal for k, v in sol.items()):\n        return False\n\n    was = f\n    attempt = -1\n    numerical = flags.get('numerical', True)\n    while 1:\n        attempt += 1\n        if attempt == 0:\n            val = f.subs(sol)\n            if isinstance(val, Mul):\n                val = val.as_independent(Unit)[0]\n            if val.atoms() & illegal:\n                return False\n        elif attempt == 1:\n            if val.free_symbols:\n                if not val.is_constant(*list(sol.keys()), simplify=not minimal):\n                    return False\n                # there are free symbols -- simple expansion might work\n                _, val = val.as_content_primitive()\n                val = _mexpand(val.as_numer_denom()[0], recursive=True)\n        elif attempt == 2:\n            if minimal:\n                return\n            if flags.get('simplify', True):\n                for k in sol:\n                    sol[k] = simplify(sol[k])\n            # start over without the failed expanded form, possibly\n            # with a simplified solution\n            val = simplify(f.subs(sol))\n            if flags.get('force', True):\n                val, reps = posify(val)\n                # expansion may work now, so try again and check\n                exval = _mexpand(val, recursive=True)\n                if exval.is_number or not exval.free_symbols:\n                    # we can decide now\n                    val = exval\n        else:\n            # if there are no radicals and no functions then this can't be\n            # zero anymore -- can it?\n            pot = preorder_traversal(expand_mul(val))\n            seen = set()\n            saw_pow_func = False\n            for p in pot:\n                if p in seen:\n                    continue\n                seen.add(p)\n                if p.is_Pow and not p.exp.is_Integer:\n                    saw_pow_func = True\n                elif p.is_Function:\n                    saw_pow_func = True\n                elif isinstance(p, UndefinedFunction):\n                    saw_pow_func = True\n                if saw_pow_func:\n                    break\n            if saw_pow_func is False:\n                return False\n            if flags.get('force', True):\n                # don't do a zero check with the positive assumptions in place\n                val = val.subs(reps)\n            nz = fuzzy_not(val.is_zero)\n            if nz is not None:\n                # issue 5673: nz may be True even when False\n                # so these are just hacks to keep a false positive\n                # from being returned\n\n                # HACK 1: LambertW (issue 5673)\n                if val.is_number and val.has(LambertW):\n                    # don't eval this to verify solution since if we got here,\n                    # numerical must be False\n                    return None\n\n                # add other HACKs here if necessary, otherwise we assume\n                # the nz value is correct\n                return not nz\n            break\n\n        if val == was:\n            continue\n        elif val.is_Rational:\n            return val == 0\n        if numerical and not val.free_symbols:\n            if val in (S.true, S.false):\n                return bool(val)\n            return bool(abs(val.n(18).n(12, chop=True)) < 1e-9)\n        was = val\n\n    if flags.get('warn', False):\n        warnings.warn(\"\\n\\tWarning: could not verify solution %s.\" % sol)\n    # returns None if it can't conclude\n    # TODO: improve solution testing\n\n\ndef failing_assumptions(expr, **assumptions):\n    \"\"\"Return a dictionary containing assumptions with values not\n    matching those of the passed assumptions.\n\n    Examples\n    ========\n\n    >>> from sympy import failing_assumptions, Symbol\n\n    >>> x = Symbol('x', real=True, positive=True)\n    >>> y = Symbol('y')\n    >>> failing_assumptions(6*x + y, real=True, positive=True)\n    {'positive': None, 'real': None}\n\n    >>> failing_assumptions(x**2 - 1, positive=True)\n    {'positive': None}\n\n    If all assumptions satisfy the `expr` an empty dictionary is returned.\n\n    >>> failing_assumptions(x**2, positive=True)\n    {}\n    \"\"\"\n    expr = sympify(expr)\n    failed = {}\n    for key in list(assumptions.keys()):\n        test = getattr(expr, 'is_%s' % key, None)\n        if test is not assumptions[key]:\n            failed[key] = test\n    return failed  # {} or {assumption: value != desired}\n\n\ndef check_assumptions(expr, against=None, **assumptions):\n    \"\"\"Checks whether expression `expr` satisfies all assumptions.\n\n    `assumptions` is a dict of assumptions: {'assumption': True|False, ...}.\n\n    Examples\n    ========\n\n       >>> from sympy import Symbol, pi, I, exp, check_assumptions\n\n       >>> check_assumptions(-5, integer=True)\n       True\n       >>> check_assumptions(pi, real=True, integer=False)\n       True\n       >>> check_assumptions(pi, real=True, negative=True)\n       False\n       >>> check_assumptions(exp(I*pi/7), real=False)\n       True\n\n       >>> x = Symbol('x', real=True, positive=True)\n       >>> check_assumptions(2*x + 1, real=True, positive=True)\n       True\n       >>> check_assumptions(-2*x - 5, real=True, positive=True)\n       False\n\n       To check assumptions of ``expr`` against another variable or expression,\n       pass the expression or variable as ``against``.\n\n       >>> check_assumptions(2*x + 1, x)\n       True\n\n       `None` is returned if check_assumptions() could not conclude.\n\n       >>> check_assumptions(2*x - 1, real=True, positive=True)\n       >>> z = Symbol('z')\n       >>> check_assumptions(z, real=True)\n\n    See Also\n    ========\n    failing_assumptions\n    \"\"\"\n    expr = sympify(expr)\n    if against:\n        if not isinstance(against, Symbol):\n            raise TypeError('against should be of type Symbol')\n        if assumptions:\n            raise AssertionError('No assumptions should be specified')\n        assumptions = against.assumptions0\n    def _test(key):\n        v = getattr(expr, 'is_' + key, None)\n        if v is not None:\n            return assumptions[key] is v\n    return fuzzy_and(_test(key) for key in assumptions)\n\n\ndef solve(f, *symbols, **flags):\n    r\"\"\"\n    Algebraically solves equations and systems of equations.\n\n    Currently supported are:\n        - polynomial,\n        - transcendental\n        - piecewise combinations of the above\n        - systems of linear and polynomial equations\n        - systems containing relational expressions.\n\n    Input is formed as:\n\n    * f\n        - a single Expr or Poly that must be zero,\n        - an Equality\n        - a Relational expression or boolean\n        - iterable of one or more of the above\n\n    * symbols (object(s) to solve for) specified as\n        - none given (other non-numeric objects will be used)\n        - single symbol\n        - denested list of symbols\n          e.g. solve(f, x, y)\n        - ordered iterable of symbols\n          e.g. solve(f, [x, y])\n\n    * flags\n        'dict'=True (default is False)\n            return list (perhaps empty) of solution mappings\n        'set'=True (default is False)\n            return list of symbols and set of tuple(s) of solution(s)\n        'exclude=[] (default)'\n            don't try to solve for any of the free symbols in exclude;\n            if expressions are given, the free symbols in them will\n            be extracted automatically.\n        'check=True (default)'\n            If False, don't do any testing of solutions. This can be\n            useful if one wants to include solutions that make any\n            denominator zero.\n        'numerical=True (default)'\n            do a fast numerical check if ``f`` has only one symbol.\n        'minimal=True (default is False)'\n            a very fast, minimal testing.\n        'warn=True (default is False)'\n            show a warning if checksol() could not conclude.\n        'simplify=True (default)'\n            simplify all but polynomials of order 3 or greater before\n            returning them and (if check is not False) use the\n            general simplify function on the solutions and the\n            expression obtained when they are substituted into the\n            function which should be zero\n        'force=True (default is False)'\n            make positive all symbols without assumptions regarding sign.\n        'rational=True (default)'\n            recast Floats as Rational; if this option is not used, the\n            system containing floats may fail to solve because of issues\n            with polys. If rational=None, Floats will be recast as\n            rationals but the answer will be recast as Floats. If the\n            flag is False then nothing will be done to the Floats.\n        'manual=True (default is False)'\n            do not use the polys/matrix method to solve a system of\n            equations, solve them one at a time as you might \"manually\"\n        'implicit=True (default is False)'\n            allows solve to return a solution for a pattern in terms of\n            other functions that contain that pattern; this is only\n            needed if the pattern is inside of some invertible function\n            like cos, exp, ....\n        'particular=True (default is False)'\n            instructs solve to try to find a particular solution to a linear\n            system with as many zeros as possible; this is very expensive\n        'quick=True (default is False)'\n            when using particular=True, use a fast heuristic instead to find a\n            solution with many zeros (instead of using the very slow method\n            guaranteed to find the largest number of zeros possible)\n        'cubics=True (default)'\n            return explicit solutions when cubic expressions are encountered\n        'quartics=True (default)'\n            return explicit solutions when quartic expressions are encountered\n        'quintics=True (default)'\n            return explicit solutions (if possible) when quintic expressions\n            are encountered\n\n    Examples\n    ========\n\n    The output varies according to the input and can be seen by example::\n\n        >>> from sympy import solve, Poly, Eq, Function, exp\n        >>> from sympy.abc import x, y, z, a, b\n        >>> f = Function('f')\n\n    * boolean or univariate Relational\n\n        >>> solve(x < 3)\n        (-oo < x) & (x < 3)\n\n\n    * to always get a list of solution mappings, use flag dict=True\n\n        >>> solve(x - 3, dict=True)\n        [{x: 3}]\n        >>> sol = solve([x - 3, y - 1], dict=True)\n        >>> sol\n        [{x: 3, y: 1}]\n        >>> sol[0][x]\n        3\n        >>> sol[0][y]\n        1\n\n\n    * to get a list of symbols and set of solution(s) use flag set=True\n\n        >>> solve([x**2 - 3, y - 1], set=True)\n        ([x, y], {(-sqrt(3), 1), (sqrt(3), 1)})\n\n\n    * single expression and single symbol that is in the expression\n\n        >>> solve(x - y, x)\n        [y]\n        >>> solve(x - 3, x)\n        [3]\n        >>> solve(Eq(x, 3), x)\n        [3]\n        >>> solve(Poly(x - 3), x)\n        [3]\n        >>> solve(x**2 - y**2, x, set=True)\n        ([x], {(-y,), (y,)})\n        >>> solve(x**4 - 1, x, set=True)\n        ([x], {(-1,), (1,), (-I,), (I,)})\n\n    * single expression with no symbol that is in the expression\n\n        >>> solve(3, x)\n        []\n        >>> solve(x - 3, y)\n        []\n\n    * single expression with no symbol given\n\n          In this case, all free symbols will be selected as potential\n          symbols to solve for. If the equation is univariate then a list\n          of solutions is returned; otherwise -- as is the case when symbols are\n          given as an iterable of length > 1 -- a list of mappings will be returned.\n\n            >>> solve(x - 3)\n            [3]\n            >>> solve(x**2 - y**2)\n            [{x: -y}, {x: y}]\n            >>> solve(z**2*x**2 - z**2*y**2)\n            [{x: -y}, {x: y}, {z: 0}]\n            >>> solve(z**2*x - z**2*y**2)\n            [{x: y**2}, {z: 0}]\n\n    * when an object other than a Symbol is given as a symbol, it is\n      isolated algebraically and an implicit solution may be obtained.\n      This is mostly provided as a convenience to save one from replacing\n      the object with a Symbol and solving for that Symbol. It will only\n      work if the specified object can be replaced with a Symbol using the\n      subs method.\n\n          >>> solve(f(x) - x, f(x))\n          [x]\n          >>> solve(f(x).diff(x) - f(x) - x, f(x).diff(x))\n          [x + f(x)]\n          >>> solve(f(x).diff(x) - f(x) - x, f(x))\n          [-x + Derivative(f(x), x)]\n          >>> solve(x + exp(x)**2, exp(x), set=True)\n          ([exp(x)], {(-sqrt(-x),), (sqrt(-x),)})\n\n          >>> from sympy import Indexed, IndexedBase, Tuple, sqrt\n          >>> A = IndexedBase('A')\n          >>> eqs = Tuple(A[1] + A[2] - 3, A[1] - A[2] + 1)\n          >>> solve(eqs, eqs.atoms(Indexed))\n          {A[1]: 1, A[2]: 2}\n\n        * To solve for a *symbol* implicitly, use 'implicit=True':\n\n            >>> solve(x + exp(x), x)\n            [-LambertW(1)]\n            >>> solve(x + exp(x), x, implicit=True)\n            [-exp(x)]\n\n        * It is possible to solve for anything that can be targeted with\n          subs:\n\n            >>> solve(x + 2 + sqrt(3), x + 2)\n            [-sqrt(3)]\n            >>> solve((x + 2 + sqrt(3), x + 4 + y), y, x + 2)\n            {y: -2 + sqrt(3), x + 2: -sqrt(3)}\n\n        * Nothing heroic is done in this implicit solving so you may end up\n          with a symbol still in the solution:\n\n            >>> eqs = (x*y + 3*y + sqrt(3), x + 4 + y)\n            >>> solve(eqs, y, x + 2)\n            {y: -sqrt(3)/(x + 3), x + 2: (-2*x - 6 + sqrt(3))/(x + 3)}\n            >>> solve(eqs, y*x, x)\n            {x: -y - 4, x*y: -3*y - sqrt(3)}\n\n        * if you attempt to solve for a number remember that the number\n          you have obtained does not necessarily mean that the value is\n          equivalent to the expression obtained:\n\n            >>> solve(sqrt(2) - 1, 1)\n            [sqrt(2)]\n            >>> solve(x - y + 1, 1)  # /!\\ -1 is targeted, too\n            [x/(y - 1)]\n            >>> [_.subs(z, -1) for _ in solve((x - y + 1).subs(-1, z), 1)]\n            [-x + y]\n\n        * To solve for a function within a derivative, use dsolve.\n\n    * single expression and more than 1 symbol\n\n        * when there is a linear solution\n\n            >>> solve(x - y**2, x, y)\n            [(y**2, y)]\n            >>> solve(x**2 - y, x, y)\n            [(x, x**2)]\n            >>> solve(x**2 - y, x, y, dict=True)\n            [{y: x**2}]\n\n        * when undetermined coefficients are identified\n\n            * that are linear\n\n                >>> solve((a + b)*x - b + 2, a, b)\n                {a: -2, b: 2}\n\n            * that are nonlinear\n\n                >>> solve((a + b)*x - b**2 + 2, a, b, set=True)\n                ([a, b], {(-sqrt(2), sqrt(2)), (sqrt(2), -sqrt(2))})\n\n        * if there is no linear solution then the first successful\n          attempt for a nonlinear solution will be returned\n\n            >>> solve(x**2 - y**2, x, y, dict=True)\n            [{x: -y}, {x: y}]\n            >>> solve(x**2 - y**2/exp(x), x, y, dict=True)\n            [{x: 2*LambertW(y/2)}]\n            >>> solve(x**2 - y**2/exp(x), y, x)\n            [(-x*sqrt(exp(x)), x), (x*sqrt(exp(x)), x)]\n\n    * iterable of one or more of the above\n\n        * involving relationals or bools\n\n            >>> solve([x < 3, x - 2])\n            Eq(x, 2)\n            >>> solve([x > 3, x - 2])\n            False\n\n        * when the system is linear\n\n            * with a solution\n\n                >>> solve([x - 3], x)\n                {x: 3}\n                >>> solve((x + 5*y - 2, -3*x + 6*y - 15), x, y)\n                {x: -3, y: 1}\n                >>> solve((x + 5*y - 2, -3*x + 6*y - 15), x, y, z)\n                {x: -3, y: 1}\n                >>> solve((x + 5*y - 2, -3*x + 6*y - z), z, x, y)\n                {x: -5*y + 2, z: 21*y - 6}\n\n            * without a solution\n\n                >>> solve([x + 3, x - 3])\n                []\n\n        * when the system is not linear\n\n            >>> solve([x**2 + y -2, y**2 - 4], x, y, set=True)\n            ([x, y], {(-2, -2), (0, 2), (2, -2)})\n\n        * if no symbols are given, all free symbols will be selected and a list\n          of mappings returned\n\n            >>> solve([x - 2, x**2 + y])\n            [{x: 2, y: -4}]\n            >>> solve([x - 2, x**2 + f(x)], {f(x), x})\n            [{x: 2, f(x): -4}]\n\n        * if any equation doesn't depend on the symbol(s) given it will be\n          eliminated from the equation set and an answer may be given\n          implicitly in terms of variables that were not of interest\n\n            >>> solve([x - y, y - 3], x)\n            {x: y}\n\n    Notes\n    =====\n\n    solve() with check=True (default) will run through the symbol tags to\n    elimate unwanted solutions.  If no assumptions are included all possible\n    solutions will be returned.\n\n        >>> from sympy import Symbol, solve\n        >>> x = Symbol(\"x\")\n        >>> solve(x**2 - 1)\n        [-1, 1]\n\n    By using the positive tag only one solution will be returned:\n\n        >>> pos = Symbol(\"pos\", positive=True)\n        >>> solve(pos**2 - 1)\n        [1]\n\n\n    Assumptions aren't checked when `solve()` input involves\n    relationals or bools.\n\n    When the solutions are checked, those that make any denominator zero\n    are automatically excluded. If you do not want to exclude such solutions\n    then use the check=False option:\n\n        >>> from sympy import sin, limit\n        >>> solve(sin(x)/x)  # 0 is excluded\n        [pi]\n\n    If check=False then a solution to the numerator being zero is found: x = 0.\n    In this case, this is a spurious solution since sin(x)/x has the well known\n    limit (without dicontinuity) of 1 at x = 0:\n\n        >>> solve(sin(x)/x, check=False)\n        [0, pi]\n\n    In the following case, however, the limit exists and is equal to the\n    value of x = 0 that is excluded when check=True:\n\n        >>> eq = x**2*(1/x - z**2/x)\n        >>> solve(eq, x)\n        []\n        >>> solve(eq, x, check=False)\n        [0]\n        >>> limit(eq, x, 0, '-')\n        0\n        >>> limit(eq, x, 0, '+')\n        0\n\n    Disabling high-order, explicit solutions\n    ----------------------------------------\n\n    When solving polynomial expressions, one might not want explicit solutions\n    (which can be quite long). If the expression is univariate, CRootOf\n    instances will be returned instead:\n\n        >>> solve(x**3 - x + 1)\n        [-1/((-1/2 - sqrt(3)*I/2)*(3*sqrt(69)/2 + 27/2)**(1/3)) - (-1/2 -\n        sqrt(3)*I/2)*(3*sqrt(69)/2 + 27/2)**(1/3)/3, -(-1/2 +\n        sqrt(3)*I/2)*(3*sqrt(69)/2 + 27/2)**(1/3)/3 - 1/((-1/2 +\n        sqrt(3)*I/2)*(3*sqrt(69)/2 + 27/2)**(1/3)), -(3*sqrt(69)/2 +\n        27/2)**(1/3)/3 - 1/(3*sqrt(69)/2 + 27/2)**(1/3)]\n        >>> solve(x**3 - x + 1, cubics=False)\n        [CRootOf(x**3 - x + 1, 0),\n         CRootOf(x**3 - x + 1, 1),\n         CRootOf(x**3 - x + 1, 2)]\n\n        If the expression is multivariate, no solution might be returned:\n\n        >>> solve(x**3 - x + a, x, cubics=False)\n        []\n\n    Sometimes solutions will be obtained even when a flag is False because the\n    expression could be factored. In the following example, the equation can\n    be factored as the product of a linear and a quadratic factor so explicit\n    solutions (which did not require solving a cubic expression) are obtained:\n\n        >>> eq = x**3 + 3*x**2 + x - 1\n        >>> solve(eq, cubics=False)\n        [-1, -1 + sqrt(2), -sqrt(2) - 1]\n\n    Solving equations involving radicals\n    ------------------------------------\n\n    Because of SymPy's use of the principle root (issue #8789), some solutions\n    to radical equations will be missed unless check=False:\n\n        >>> from sympy import root\n        >>> eq = root(x**3 - 3*x**2, 3) + 1 - x\n        >>> solve(eq)\n        []\n        >>> solve(eq, check=False)\n        [1/3]\n\n    In the above example there is only a single solution to the\n    equation. Other expressions will yield spurious roots which\n    must be checked manually; roots which give a negative argument\n    to odd-powered radicals will also need special checking:\n\n        >>> from sympy import real_root, S\n        >>> eq = root(x, 3) - root(x, 5) + S(1)/7\n        >>> solve(eq)  # this gives 2 solutions but misses a 3rd\n        [CRootOf(7*_p**5 - 7*_p**3 + 1, 1)**15,\n        CRootOf(7*_p**5 - 7*_p**3 + 1, 2)**15]\n        >>> sol = solve(eq, check=False)\n        >>> [abs(eq.subs(x,i).n(2)) for i in sol]\n        [0.48, 0.e-110, 0.e-110, 0.052, 0.052]\n\n        The first solution is negative so real_root must be used to see\n        that it satisfies the expression:\n\n        >>> abs(real_root(eq.subs(x, sol[0])).n(2))\n        0.e-110\n\n    If the roots of the equation are not real then more care will be\n    necessary to find the roots, especially for higher order equations.\n    Consider the following expression:\n\n        >>> expr = root(x, 3) - root(x, 5)\n\n    We will construct a known value for this expression at x = 3 by selecting\n    the 1-th root for each radical:\n\n        >>> expr1 = root(x, 3, 1) - root(x, 5, 1)\n        >>> v = expr1.subs(x, -3)\n\n    The solve function is unable to find any exact roots to this equation:\n\n        >>> eq = Eq(expr, v); eq1 = Eq(expr1, v)\n        >>> solve(eq, check=False), solve(eq1, check=False)\n        ([], [])\n\n    The function unrad, however, can be used to get a form of the equation for\n    which numerical roots can be found:\n\n        >>> from sympy.solvers.solvers import unrad\n        >>> from sympy import nroots\n        >>> e, (p, cov) = unrad(eq)\n        >>> pvals = nroots(e)\n        >>> inversion = solve(cov, x)[0]\n        >>> xvals = [inversion.subs(p, i) for i in pvals]\n\n    Although eq or eq1 could have been used to find xvals, the solution can\n    only be verified with expr1:\n\n        >>> z = expr - v\n        >>> [xi.n(chop=1e-9) for xi in xvals if abs(z.subs(x, xi).n()) < 1e-9]\n        []\n        >>> z1 = expr1 - v\n        >>> [xi.n(chop=1e-9) for xi in xvals if abs(z1.subs(x, xi).n()) < 1e-9]\n        [-3.0]\n\n    See Also\n    ========\n\n        - rsolve() for solving recurrence relationships\n        - dsolve() for solving differential equations\n\n    \"\"\"\n    # keeping track of how f was passed since if it is a list\n    # a dictionary of results will be returned.\n    ###########################################################################\n\n    def _sympified_list(w):\n        return list(map(sympify, w if iterable(w) else [w]))\n    bare_f = not iterable(f)\n    ordered_symbols = (symbols and\n                       symbols[0] and\n                       (isinstance(symbols[0], Symbol) or\n                        is_sequence(symbols[0],\n                        include=GeneratorType)\n                       )\n                      )\n    f, symbols = (_sympified_list(w) for w in [f, symbols])\n\n    implicit = flags.get('implicit', False)\n\n    # preprocess symbol(s)\n    ###########################################################################\n    if not symbols:\n        # get symbols from equations\n        symbols = set().union(*[fi.free_symbols for fi in f])\n        if len(symbols) < len(f):\n            for fi in f:\n                pot = preorder_traversal(fi)\n                for p in pot:\n                    if isinstance(p, AppliedUndef):\n                        flags['dict'] = True  # better show symbols\n                        symbols.add(p)\n                        pot.skip()  # don't go any deeper\n        symbols = list(symbols)\n\n        ordered_symbols = False\n    elif len(symbols) == 1 and iterable(symbols[0]):\n        symbols = symbols[0]\n\n    # remove symbols the user is not interested in\n    exclude = flags.pop('exclude', set())\n    if exclude:\n        if isinstance(exclude, Expr):\n            exclude = [exclude]\n        exclude = set().union(*[e.free_symbols for e in sympify(exclude)])\n    symbols = [s for s in symbols if s not in exclude]\n\n\n    # preprocess equation(s)\n    ###########################################################################\n    for i, fi in enumerate(f):\n        if isinstance(fi, (Equality, Unequality)):\n            if 'ImmutableDenseMatrix' in [type(a).__name__ for a in fi.args]:\n                fi = fi.lhs - fi.rhs\n            else:\n                args = fi.args\n                if args[1] in (S.true, S.false):\n                    args = args[1], args[0]\n                L, R = args\n                if L in (S.false, S.true):\n                    if isinstance(fi, Unequality):\n                        L = ~L\n                    if R.is_Relational:\n                        fi = ~R if L is S.false else R\n                    elif R.is_Symbol:\n                        return L\n                    elif R.is_Boolean and (~R).is_Symbol:\n                        return ~L\n                    else:\n                        raise NotImplementedError(filldedent('''\n                            Unanticipated argument of Eq when other arg\n                            is True or False.\n                        '''))\n                else:\n                    fi = Add(fi.lhs, -fi.rhs, evaluate=False)\n            f[i] = fi\n\n        if isinstance(fi, (bool, BooleanAtom)) or fi.is_Relational:\n            return reduce_inequalities(f, symbols=symbols)\n\n        if isinstance(fi, Poly):\n            f[i] = fi.as_expr()\n\n        # rewrite hyperbolics in terms of exp\n        f[i] = f[i].replace(lambda w: isinstance(w, HyperbolicFunction),\n                lambda w: w.rewrite(exp))\n\n        # if we have a Matrix, we need to iterate over its elements again\n        if f[i].is_Matrix:\n            bare_f = False\n            f.extend(list(f[i]))\n            f[i] = S.Zero\n\n        # if we can split it into real and imaginary parts then do so\n        freei = f[i].free_symbols\n        if freei and all(s.is_real or s.is_imaginary for s in freei):\n            fr, fi = f[i].as_real_imag()\n            # accept as long as new re, im, arg or atan2 are not introduced\n            had = f[i].atoms(re, im, arg, atan2)\n            if fr and fi and fr != fi and not any(\n                    i.atoms(re, im, arg, atan2) - had for i in (fr, fi)):\n                if bare_f:\n                    bare_f = False\n                f[i: i + 1] = [fr, fi]\n\n    # real/imag handling -----------------------------\n    w = Dummy('w')\n    piece = Lambda(w, Piecewise((w, Ge(w, 0)), (-w, True)))\n    for i, fi in enumerate(f):\n        # Abs\n        reps = []\n        for a in fi.atoms(Abs):\n            if not a.has(*symbols):\n                continue\n            if a.args[0].is_real is None:\n                raise NotImplementedError('solving %s when the argument '\n                    'is not real or imaginary.' % a)\n            reps.append((a, piece(a.args[0]) if a.args[0].is_real else \\\n                piece(a.args[0]*S.ImaginaryUnit)))\n        fi = fi.subs(reps)\n\n        # arg\n        _arg = [a for a in fi.atoms(arg) if a.has(*symbols)]\n        fi = fi.xreplace(dict(list(zip(_arg,\n            [atan(im(a.args[0])/re(a.args[0])) for a in _arg]))))\n\n        # save changes\n        f[i] = fi\n\n    # see if re(s) or im(s) appear\n    irf = []\n    for s in symbols:\n        if s.is_real or s.is_imaginary:\n            continue  # neither re(x) nor im(x) will appear\n        # if re(s) or im(s) appear, the auxiliary equation must be present\n        if any(fi.has(re(s), im(s)) for fi in f):\n            irf.append((s, re(s) + S.ImaginaryUnit*im(s)))\n    if irf:\n        for s, rhs in irf:\n            for i, fi in enumerate(f):\n                f[i] = fi.xreplace({s: rhs})\n            f.append(s - rhs)\n            symbols.extend([re(s), im(s)])\n        if bare_f:\n            bare_f = False\n        flags['dict'] = True\n    # end of real/imag handling  -----------------------------\n\n    symbols = list(uniq(symbols))\n    if not ordered_symbols:\n        # we do this to make the results returned canonical in case f\n        # contains a system of nonlinear equations; all other cases should\n        # be unambiguous\n        symbols = sorted(symbols, key=default_sort_key)\n\n    # we can solve for non-symbol entities by replacing them with Dummy symbols\n    f, symbols, swap_sym = recast_to_symbols(f, symbols)\n\n    # this is needed in the next two events\n    symset = set(symbols)\n\n    # get rid of equations that have no symbols of interest; we don't\n    # try to solve them because the user didn't ask and they might be\n    # hard to solve; this means that solutions may be given in terms\n    # of the eliminated equations e.g. solve((x-y, y-3), x) -> {x: y}\n    newf = []\n    for fi in f:\n        # let the solver handle equations that..\n        # - have no symbols but are expressions\n        # - have symbols of interest\n        # - have no symbols of interest but are constant\n        # but when an expression is not constant and has no symbols of\n        # interest, it can't change what we obtain for a solution from\n        # the remaining equations so we don't include it; and if it's\n        # zero it can be removed and if it's not zero, there is no\n        # solution for the equation set as a whole\n        #\n        # The reason for doing this filtering is to allow an answer\n        # to be obtained to queries like solve((x - y, y), x); without\n        # this mod the return value is []\n        ok = False\n        if fi.has(*symset):\n            ok = True\n        else:\n            free = fi.free_symbols\n            if not free:\n                if fi.is_Number:\n                    if fi.is_zero:\n                        continue\n                    return []\n                ok = True\n            else:\n                if fi.is_constant():\n                    ok = True\n        if ok:\n            newf.append(fi)\n    if not newf:\n        return []\n    f = newf\n    del newf\n\n    # mask off any Object that we aren't going to invert: Derivative,\n    # Integral, etc... so that solving for anything that they contain will\n    # give an implicit solution\n    seen = set()\n    non_inverts = set()\n    for fi in f:\n        pot = preorder_traversal(fi)\n        for p in pot:\n            if not isinstance(p, Expr) or isinstance(p, Piecewise):\n                pass\n            elif (isinstance(p, bool) or\n                    not p.args or\n                    p in symset or\n                    p.is_Add or p.is_Mul or\n                    p.is_Pow and not implicit or\n                    p.is_Function and not implicit) and p.func not in (re, im):\n                continue\n            elif not p in seen:\n                seen.add(p)\n                if p.free_symbols & symset:\n                    non_inverts.add(p)\n                else:\n                    continue\n            pot.skip()\n    del seen\n    non_inverts = dict(list(zip(non_inverts, [Dummy() for d in non_inverts])))\n    f = [fi.subs(non_inverts) for fi in f]\n\n    # Both xreplace and subs are needed below: xreplace to force substitution\n    # inside Derivative, subs to handle non-straightforward substitutions\n    non_inverts = [(v, k.xreplace(swap_sym).subs(swap_sym)) for k, v in non_inverts.items()]\n\n    # rationalize Floats\n    floats = False\n    if flags.get('rational', True) is not False:\n        for i, fi in enumerate(f):\n            if fi.has(Float):\n                floats = True\n                f[i] = nsimplify(fi, rational=True)\n\n    # capture any denominators before rewriting since\n    # they may disappear after the rewrite, e.g. issue 14779\n    flags['_denominators'] = _simple_dens(f[0], symbols)\n    # Any embedded piecewise functions need to be brought out to the\n    # top level so that the appropriate strategy gets selected.\n    # However, this is necessary only if one of the piecewise\n    # functions depends on one of the symbols we are solving for.\n    def _has_piecewise(e):\n        if e.is_Piecewise:\n            return e.has(*symbols)\n        return any([_has_piecewise(a) for a in e.args])\n    for i, fi in enumerate(f):\n        if _has_piecewise(fi):\n            f[i] = piecewise_fold(fi)\n\n    #\n    # try to get a solution\n    ###########################################################################\n    if bare_f:\n        solution = _solve(f[0], *symbols, **flags)\n    else:\n        solution = _solve_system(f, symbols, **flags)\n\n    #\n    # postprocessing\n    ###########################################################################\n    # Restore masked-off objects\n    if non_inverts:\n\n        def _do_dict(solution):\n            return dict([(k, v.subs(non_inverts)) for k, v in\n                         solution.items()])\n        for i in range(1):\n            if isinstance(solution, dict):\n                solution = _do_dict(solution)\n                break\n            elif solution and isinstance(solution, list):\n                if isinstance(solution[0], dict):\n                    solution = [_do_dict(s) for s in solution]\n                    break\n                elif isinstance(solution[0], tuple):\n                    solution = [tuple([v.subs(non_inverts) for v in s]) for s\n                                in solution]\n                    break\n                else:\n                    solution = [v.subs(non_inverts) for v in solution]\n                    break\n            elif not solution:\n                break\n        else:\n            raise NotImplementedError(filldedent('''\n                            no handling of %s was implemented''' % solution))\n\n    # Restore original \"symbols\" if a dictionary is returned.\n    # This is not necessary for\n    #   - the single univariate equation case\n    #     since the symbol will have been removed from the solution;\n    #   - the nonlinear poly_system since that only supports zero-dimensional\n    #     systems and those results come back as a list\n    #\n    # ** unless there were Derivatives with the symbols, but those were handled\n    #    above.\n    if swap_sym:\n        symbols = [swap_sym.get(k, k) for k in symbols]\n        if isinstance(solution, dict):\n            solution = dict([(swap_sym.get(k, k), v.subs(swap_sym))\n                             for k, v in solution.items()])\n        elif solution and isinstance(solution, list) and isinstance(solution[0], dict):\n            for i, sol in enumerate(solution):\n                solution[i] = dict([(swap_sym.get(k, k), v.subs(swap_sym))\n                              for k, v in sol.items()])\n\n    # undo the dictionary solutions returned when the system was only partially\n    # solved with poly-system if all symbols are present\n    if (\n            not flags.get('dict', False) and\n            solution and\n            ordered_symbols and\n            not isinstance(solution, dict) and\n            all(isinstance(sol, dict) for sol in solution)\n    ):\n        solution = [tuple([r.get(s, s).subs(r) for s in symbols])\n                    for r in solution]\n\n    # Get assumptions about symbols, to filter solutions.\n    # Note that if assumptions about a solution can't be verified, it is still\n    # returned.\n    check = flags.get('check', True)\n\n    # restore floats\n    if floats and solution and flags.get('rational', None) is None:\n        solution = nfloat(solution, exponent=False)\n\n    if check and solution:  # assumption checking\n\n        warn = flags.get('warn', False)\n        got_None = []  # solutions for which one or more symbols gave None\n        no_False = []  # solutions for which no symbols gave False\n        if isinstance(solution, tuple):\n            # this has already been checked and is in as_set form\n            return solution\n        elif isinstance(solution, list):\n            if isinstance(solution[0], tuple):\n                for sol in solution:\n                    for symb, val in zip(symbols, sol):\n                        test = check_assumptions(val, **symb.assumptions0)\n                        if test is False:\n                            break\n                        if test is None:\n                            got_None.append(sol)\n                    else:\n                        no_False.append(sol)\n            elif isinstance(solution[0], dict):\n                for sol in solution:\n                    a_None = False\n                    for symb, val in sol.items():\n                        test = check_\n```\n\n### Code description\n```Python\n\"\"\"\nDescription: This function is used to remove radicals from an equation.\n\nArgs:\n    eq (type): The equation to be processed.\n    *syms (type): Variable symbols.\n    **flags (type): Additional flags for processing.\n\nReturns:\n    type: The processed equation and the coverage information.\n\"\"\"\n```\n\n### Code snippet:\n```Python\ndef unrad(eq, *syms, **flags):\n    _inv_error = 'cannot get an analytical solution for the inversion'\n\n    uflags = dict(check=False, simplify=False)\n\n    def _cov(p, e):\n        if cov:\n            # XXX - uncovered\n            oldp, olde = cov\n            if Poly(e, p).degree(p) in (1, 2):\n                cov[:] = [p, olde.subs(oldp, _solve(e, p, **uflags)[0])]\n            else:\n                raise NotImplementedError\n        else:\n            cov[:] = [p, e]\n\n    def _canonical(eq, cov):\n        if cov:\n            # change symbol to vanilla so no solutions are eliminated\n            p, e = cov\n            rep = {p: Dummy(p.name)}\n            eq = eq.xreplace(rep)\n            cov = [p.xreplace(rep), e.xreplace(rep)]\n\n        # remove constants and powers of factors since these don't change\n        # the location of the root; XXX should factor or factor_terms be used?\n        eq = factor_terms(_mexpand(eq.as_numer_denom()[0], recursive=True), clear=True)\n        if eq.is_Mul:\n            args = []\n            for f in eq.args:\n                if f.is_number:\n                    continue\n                if f.is_Pow and _take(f, True):\n                    args.append(f.base)\n                else:\n                    args.append(f)\n            eq = Mul(*args)  # leave as Mul for more efficient solving\n\n        # make the sign canonical\n        free = eq.free_symbols\n        if len(free) == 1:\n            if eq.coeff(free.pop()**degree(eq)).could_extract_minus_sign():\n                eq = -eq\n        elif eq.could_extract_minus_sign():\n            eq = -eq\n\n        return eq, cov\n\n    def _Q(pow):\n        # return leading Rational of denominator of Pow's exponent\n        c = pow.as_base_exp()[1].as_coeff_Mul()[0]\n        if not c.is_Rational:\n            return S.One\n        return c.q\n\n    # define the _take method that will determine whether a term is of interest\n    def _take(d, take_int_pow):\n        # return True if coefficient of any factor's exponent's den is not 1\n        for pow in Mul.make_args(d):\n            if not (pow.is_Symbol or pow.is_Pow):\n                continue\n            b, e = pow.as_base_exp()\n            if not b.has(*syms):\n                continue\n            if not take_int_pow and _Q(pow) == 1:\n                continue\n            free = pow.free_symbols\n            if free.intersection(syms):\n                return True\n        return False\n    _take = flags.setdefault('_take', _take)\n\n    cov, nwas, rpt = [flags.setdefault(k, v) for k, v in\n        sorted(dict(cov=[], n=None, rpt=0).items())]\n\n    # preconditioning\n    eq = powdenest(factor_terms(eq, radical=True, clear=True))\n    eq, d = eq.as_numer_denom()\n    eq = _mexpand(eq, recursive=True)\n    if eq.is_number:\n        return\n\n    syms = set(syms) or eq.free_symbols\n    poly = eq.as_poly()\n    gens = [g for g in poly.gens if _take(g, True)]\n    if not gens:\n        return\n\n    # check for trivial case\n    # - already a polynomial in integer powers\n    if all(_Q(g) == 1 for g in gens):\n        return\n    # - an exponent has a symbol of interest (don't handle)\n    if any(g.as_base_exp()[1].has(*syms) for g in gens):\n        return\n\n    def _rads_bases_lcm(poly):\n        # if all the bases are the same or all the radicals are in one\n        # term, `lcm` will be the lcm of the denominators of the\n        # exponents of the radicals\n        lcm = 1\n        rads = set()\n        bases = set()\n        for g in poly.gens:\n            if not _take(g, False):\n                continue\n            q = _Q(g)\n            if q != 1:\n                rads.add(g)\n                lcm = ilcm(lcm, q)\n                bases.add(g.base)\n        return rads, bases, lcm\n    rads, bases, lcm = _rads_bases_lcm(poly)\n\n    if not rads:\n        return\n\n    covsym = Dummy('p', nonnegative=True)\n\n    # only keep in syms symbols that actually appear in radicals;\n    # and update gens\n    newsyms = set()\n    for r in rads:\n        newsyms.update(syms & r.free_symbols)\n    if newsyms != syms:\n        syms = newsyms\n        gens = [g for g in gens if g.free_symbols & syms]\n\n    # get terms together that have common generators\n    drad = dict(list(zip(rads, list(range(len(rads))))))\n    rterms = {(): []}\n    args = Add.make_args(poly.as_expr())\n    for t in args:\n        if _take(t, False):\n            common = set(t.as_poly().gens).intersection(rads)\n            key = tuple(sorted([drad[i] for i in common]))\n        else:\n            key = ()\n        rterms.setdefault(key, []).append(t)\n    others = Add(*rterms.pop(()))\n    rterms = [Add(*rterms[k]) for k in rterms.keys()]\n\n    # the output will depend on the order terms are processed, so\n    # make it canonical quickly\n    rterms = list(reversed(list(ordered(rterms))))\n\n    ok = False  # we don't have a solution yet\n    depth = sqrt_depth(eq)\n\n    if len(rterms) == 1 and not (rterms[0].is_Add and lcm > 2):\n        eq = rterms[0]**lcm - ((-others)**lcm)\n        ok = True\n    else:\n        if len(rterms) == 1 and rterms[0].is_Add:\n            rterms = list(rterms[0].args)\n        if len(bases) == 1:\n            b = bases.pop()\n```\n\n### Requirements:\n1. Complete only the missing part of the function.\n2. Only answer the code of the completed part. Do not include any of the provided code snippet.\n3. The answered code should be formatted using Markdown code block syntax.\n4. Do not include any additional text.\n\n### Answer: (use the provided format with backticks)\n", "name": "unrad", "reference": "            if len(syms) > 1:\n                free = b.free_symbols\n                x = {g for g in gens if g.is_Symbol} & free\n                if not x:\n                    x = free\n                x = ordered(x)\n            else:\n                x = syms\n            x = list(x)[0]\n            try:\n                inv = _solve(covsym**lcm - b, x, **uflags)\n                if not inv:\n                    raise NotImplementedError\n                eq = poly.as_expr().subs(b, covsym**lcm).subs(x, inv[0])\n                _cov(covsym, covsym**lcm - b)\n                return _canonical(eq, cov)\n            except NotImplementedError:\n                pass\n        else:\n            # no longer consider integer powers as generators\n            gens = [g for g in gens if _Q(g) != 1]\n\n        if len(rterms) == 2:\n            if not others:\n                eq = rterms[0]**lcm - (-rterms[1])**lcm\n                ok = True\n            elif not log(lcm, 2).is_Integer:\n                # the lcm-is-power-of-two case is handled below\n                r0, r1 = rterms\n                if flags.get('_reverse', False):\n                    r1, r0 = r0, r1\n                i0 = _rads0, _bases0, lcm0 = _rads_bases_lcm(r0.as_poly())\n                i1 = _rads1, _bases1, lcm1 = _rads_bases_lcm(r1.as_poly())\n                for reverse in range(2):\n                    if reverse:\n                        i0, i1 = i1, i0\n                        r0, r1 = r1, r0\n                    _rads1, _, lcm1 = i1\n                    _rads1 = Mul(*_rads1)\n                    t1 = _rads1**lcm1\n                    c = covsym**lcm1 - t1\n                    for x in syms:\n                        try:\n                            sol = _solve(c, x, **uflags)\n                            if not sol:\n                                raise NotImplementedError\n                            neweq = r0.subs(x, sol[0]) + covsym*r1/_rads1 + \\\n                                others\n                            tmp = unrad(neweq, covsym)\n                            if tmp:\n                                eq, newcov = tmp\n                                if newcov:\n                                    newp, newc = newcov\n                                    _cov(newp, c.subs(covsym,\n                                        _solve(newc, covsym, **uflags)[0]))\n                                else:\n                                    _cov(covsym, c)\n                            else:\n                                eq = neweq\n                                _cov(covsym, c)\n                            ok = True\n                            break\n                        except NotImplementedError:\n                            if reverse:\n                                raise NotImplementedError(\n                                    'no successful change of variable found')\n                            else:\n                                pass\n                    if ok:\n                        break\n        elif len(rterms) == 3:\n            # two cube roots and another with order less than 5\n            # (so an analytical solution can be found) or a base\n            # that matches one of the cube root bases\n            info = [_rads_bases_lcm(i.as_poly()) for i in rterms]\n            RAD = 0\n            BASES = 1\n            LCM = 2\n            if info[0][LCM] != 3:\n                info.append(info.pop(0))\n                rterms.append(rterms.pop(0))\n            elif info[1][LCM] != 3:\n                info.append(info.pop(1))\n                rterms.append(rterms.pop(1))\n            if info[0][LCM] == info[1][LCM] == 3:\n                if info[1][BASES] != info[2][BASES]:\n                    info[0], info[1] = info[1], info[0]\n                    rterms[0], rterms[1] = rterms[1], rterms[0]\n                if info[1][BASES] == info[2][BASES]:\n                    eq = rterms[0]**3 + (rterms[1] + rterms[2] + others)**3\n                    ok = True\n                elif info[2][LCM] < 5:\n                    # a*root(A, 3) + b*root(B, 3) + others = c\n                    a, b, c, d, A, B = [Dummy(i) for i in 'abcdAB']\n                    # zz represents the unraded expression into which the\n                    # specifics for this case are substituted\n                    zz = (c - d)*(A**3*a**9 + 3*A**2*B*a**6*b**3 -\n                        3*A**2*a**6*c**3 + 9*A**2*a**6*c**2*d - 9*A**2*a**6*c*d**2 +\n                        3*A**2*a**6*d**3 + 3*A*B**2*a**3*b**6 + 21*A*B*a**3*b**3*c**3 -\n                        63*A*B*a**3*b**3*c**2*d + 63*A*B*a**3*b**3*c*d**2 -\n                        21*A*B*a**3*b**3*d**3 + 3*A*a**3*c**6 - 18*A*a**3*c**5*d +\n                        45*A*a**3*c**4*d**2 - 60*A*a**3*c**3*d**3 + 45*A*a**3*c**2*d**4 -\n                        18*A*a**3*c*d**5 + 3*A*a**3*d**6 + B**3*b**9 - 3*B**2*b**6*c**3 +\n                        9*B**2*b**6*c**2*d - 9*B**2*b**6*c*d**2 + 3*B**2*b**6*d**3 +\n                        3*B*b**3*c**6 - 18*B*b**3*c**5*d + 45*B*b**3*c**4*d**2 -\n                        60*B*b**3*c**3*d**3 + 45*B*b**3*c**2*d**4 - 18*B*b**3*c*d**5 +\n                        3*B*b**3*d**6 - c**9 + 9*c**8*d - 36*c**7*d**2 + 84*c**6*d**3 -\n                        126*c**5*d**4 + 126*c**4*d**5 - 84*c**3*d**6 + 36*c**2*d**7 -\n                        9*c*d**8 + d**9)\n                    def _t(i):\n                        b = Mul(*info[i][RAD])\n                        return cancel(rterms[i]/b), Mul(*info[i][BASES])\n                    aa, AA = _t(0)\n                    bb, BB = _t(1)\n                    cc = -rterms[2]\n                    dd = others\n                    eq = zz.xreplace(dict(zip(\n                        (a, A, b, B, c, d),\n                        (aa, AA, bb, BB, cc, dd))))\n                    ok = True\n        # handle power-of-2 cases\n        if not ok:\n            if log(lcm, 2).is_Integer and (not others and\n                    len(rterms) == 4 or len(rterms) < 4):\n                def _norm2(a, b):\n                    return a**2 + b**2 + 2*a*b\n\n                if len(rterms) == 4:\n                    # (r0+r1)**2 - (r2+r3)**2\n                    r0, r1, r2, r3 = rterms\n                    eq = _norm2(r0, r1) - _norm2(r2, r3)\n                    ok = True\n                elif len(rterms) == 3:\n                    # (r1+r2)**2 - (r0+others)**2\n                    r0, r1, r2 = rterms\n                    eq = _norm2(r1, r2) - _norm2(r0, others)\n                    ok = True\n                elif len(rterms) == 2:\n                    # r0**2 - (r1+others)**2\n                    r0, r1 = rterms\n                    eq = r0**2 - _norm2(r1, others)\n                    ok = True\n\n    new_depth = sqrt_depth(eq) if ok else depth\n    rpt += 1  # XXX how many repeats with others unchanging is enough?\n    if not ok or (\n                nwas is not None and len(rterms) == nwas and\n                new_depth is not None and new_depth == depth and\n                rpt > 3):\n        raise NotImplementedError('Cannot remove all radicals')\n\n    flags.update(dict(cov=cov, n=len(rterms), rpt=rpt))\n    neq = unrad(eq, *syms, **flags)\n    if neq:\n        eq, cov = neq\n    eq, cov = _canonical(eq, cov)\n    return eq, cov", "completions": ["```python\n```python\ndef unrad(eq, *syms, **flags):\n    _inv_error = 'cannot get an analytical solution for the inversion'\n\n    uflags = dict(check=False, simplify=False)\n\n    def _cov(p, e):\n        if cov:\n            oldp, olde = cov\n            if Poly(e, p).degree(p) in (1, 2):\n                cov[:] = [p, olde.subs(oldp, _solve(e, p, **uflags)[0])]\n            else:\n                raise NotImplementedError\n        else:\n            cov[:] = [p, e]\n\n    def _canonical(eq, cov):\n        if cov:\n            p, e = cov\n            rep = {p: Dummy(p.name)}\n            eq = eq.xreplace(rep)\n            cov = [p.xreplace(rep), e.xreplace(rep)]\n\n        eq = factor_terms(_mexpand(eq.as_numer_denom()[0], recursive=True), clear=True)\n        if eq.is_Mul:\n            args = []\n            for f in eq.args:\n                if f.is_number:\n                    continue\n                if f.is_Pow and _take(f, True):\n                    args.append(f.base)\n                else:\n                    args.append(f)\n            eq = Mul(*args)  # leave as Mul for more efficient solving\n\n        if eq.could_extract_minus_sign():\n            eq = -eq\n\n        return eq, cov\n\n    def _Q(pow):\n        c = pow.as_base_exp()[1].as_coeff_Mul()[0]\n        if not c.is_Rational:\n            return S.One\n        return c.q\n\n    def _take(d, take_int_pow):\n        for pow in Mul.make_args(d):\n            if not (pow.is_Symbol or pow.is_Pow):\n                continue\n            b, e = pow.as_base_exp()\n            if not b.has(*syms):\n                continue\n            if not take_int_pow and _Q(pow) == 1:\n                continue\n            free = pow.free_symbols\n            if free.intersection(syms):\n                return True\n        return False\n\n    _take = flags.setdefault('_take', _take)\n\n    cov, nwas, rpt = [flags.setdefault(k, v) for k, v in\n        sorted(dict(cov=[], n=None, rpt=0).items())]\n\n    eq = powdenest(factor_terms(eq, radical=True, clear=True))\n    eq, d = eq.as_numer_denom()\n    eq = _mexpand(eq, recursive=True)\n    if eq.is_number:\n        return\n\n    syms = set(syms) or eq.free_symbols\n    poly = eq.as_poly()\n    gens = [g for g in poly.gens if _take(g, True)]\n    if not gens:\n        return\n\n    if all(_Q(g) == 1 for g in gens):\n        return\n    if any(g.as_base_exp()[1].has(*syms) for g in gens):\n        return\n\n    def _rads_bases_lcm(poly):\n        lcm = 1\n        rads = set()\n        bases = set()\n        for g in poly.gens:\n            if not _take(g, False):\n                continue\n            q = _Q(g)\n            if q != 1:\n                rads.add(g)\n                lcm = ilcm(lcm, q)\n                bases.add(g.base)\n        return rads, bases, lcm\n\n    rads, bases, lcm = _rads_bases_lcm(poly)\n\n    if not rads:\n        return\n\n    covsym = Dummy('p', nonnegative=True)\n\n    newsyms = set()\n    for r in rads:\n        newsyms.update"]}
{"id": 35, "prompt": "<\uff5cbegin\u2581of\u2581sentence\uff5c>### Role:\nYou are an experienced Python programmer assistant responsible for code completion tasks. Please intelligently complete the missing parts of the provided code snippet.\n\n### Task:\nBased on the provided code snippet, complete the missing function implementation. Ensure the code is logically correct, syntactically accurate, and follows best coding practices.\n\n### Left context\n```Python\nfrom __future__ import print_function\nimport os\nimport time\nimport shutil\n\nimport torch\nimport dill\n\nclass Checkpoint(object):\n    \"\"\"\n    The Checkpoint class manages the saving and loading of a model during training. It allows training to be suspended\n    and resumed at a later time (e.g. when running on a cluster using sequential jobs).\n\n    To make a checkpoint, initialize a Checkpoint object with the following args; then call that object's save() method\n    to write parameters to disk.\n\n    Args:\n        model (seq2seq): seq2seq model being trained\n        optimizer (Optimizer): stores the state of the optimizer\n        epoch (int): current epoch (an epoch is a loop through the full training data)\n        step (int): number of examples seen within the current epoch\n        input_vocab (Vocabulary): vocabulary for the input language\n        output_vocab (Vocabulary): vocabulary for the output language\n\n    Attributes:\n        CHECKPOINT_DIR_NAME (str): name of the checkpoint directory\n        TRAINER_STATE_NAME (str): name of the file storing trainer states\n        MODEL_NAME (str): name of the file storing model\n        INPUT_VOCAB_FILE (str): name of the input vocab file\n        OUTPUT_VOCAB_FILE (str): name of the output vocab file\n    \"\"\"\n\n    CHECKPOINT_DIR_NAME = 'checkpoints'\n    TRAINER_STATE_NAME = 'trainer_states.pt'\n    MODEL_NAME = 'model.pt'\n    INPUT_VOCAB_FILE = 'input_vocab.pt'\n    OUTPUT_VOCAB_FILE = 'output_vocab.pt'\n\n    def __init__(self, model, optimizer, epoch, step, input_vocab, output_vocab, path=None):\n        self.model = model\n        self.optimizer = optimizer\n        self.input_vocab = input_vocab\n        self.output_vocab = output_vocab\n        self.epoch = epoch\n        self.step = step\n        self._path = path\n\n    @property\n    def path(self):\n        if self._path is None:\n            raise LookupError(\"The checkpoint has not been saved.\")\n        return self._path\n\n    def save(self, experiment_dir):\n        \"\"\"\n        Saves the current model and related training parameters into a subdirectory of the checkpoint directory.\n        The name of the subdirectory is the current local time in Y_M_D_H_M_S format.\n        Args:\n            experiment_dir (str): path to the experiment root directory\n        Returns:\n             str: path to the saved checkpoint subdirectory\n        \"\"\"\n        date_time = time.strftime('%Y_%m_%d_%H_%M_%S', time.localtime())\n\n        self._path = os.path.join(experiment_dir, self.CHECKPOINT_DIR_NAME, date_time)\n        path = self._path\n\n        if os.path.exists(path):\n            shutil.rmtree(path)\n        os.makedirs(path)\n        torch.save({'epoch': self.epoch,\n                    'step': self.step,\n                    'optimizer': self.optimizer\n                   },\n                   os.path.join(path, self.TRAINER_STATE_NAME))\n        torch.save(self.model, os.path.join(path, self.MODEL_NAME))\n\n        with open(os.path.join(path, self.INPUT_VOCAB_FILE), 'wb') as fout:\n            dill.dump(self.input_vocab, fout)\n        with open(os.path.join(path, self.OUTPUT_VOCAB_FILE), 'wb') as fout:\n            dill.dump(self.output_vocab, fout)\n\n        return path\n\n    @classmethod\n```\n\n### Code description\n```Python\n\"\"\"\nDescription: This function is used to load a model and its related components from a specified path.\n\nArgs:\n    cls (type): The class of the model to be loaded.\n    path (str): The path from where the model, vocabularies, and checkpoint are to be loaded.\n\nReturns:\n    Checkpoint: An object containing the loaded model, input and output vocabularies, optimizer, epoch, step, and path.\n\"\"\"\n```\n\n### Code snippet:\n```Python\ndef load(cls, path):\n    if torch.cuda.is_available():\n        resume_checkpoint = torch.load(os.path.join(path, cls.TRAINER_STATE_NAME))\n        model = torch.load(os.path.join(path, cls.MODEL_NAME))\n    else:\n        resume_checkpoint = torch.load(os.path.join(path, cls.TRAINER_STATE_NAME), map_location=lambda storage, loc: storage)\n        model = torch.load(os.path.join(path, cls.MODEL_NAME), map_location=lambda storage, loc: storage)\n\n    model.flatten_parameters() # make RNN parameters contiguous\n    with open(os.path.join(path, cls.INPUT_VOCAB_FILE), 'rb') as fin:\n```\n\n### Requirements:\n1. Complete only the missing part of the function.\n2. Only answer the code of the completed part. Do not include any of the provided code snippet.\n3. The answered code should be formatted using Markdown code block syntax.\n4. Do not include any additional text.\n\n### Answer: (use the provided format with backticks)\n", "name": "load", "reference": "        input_vocab = dill.load(fin)\n    with open(os.path.join(path, cls.OUTPUT_VOCAB_FILE), 'rb') as fin:\n        output_vocab = dill.load(fin)\n    optimizer = resume_checkpoint['optimizer']\n    return Checkpoint(model=model, input_vocab=input_vocab,\n                      output_vocab=output_vocab,\n                      optimizer=optimizer,\n                      epoch=resume_checkpoint['epoch'],\n                      step=resume_checkpoint['step'],\n                      path=path)", "completions": ["```python\n        input_vocab = dill.load(fin)\n    with open(os.path.join(path, cls.OUTPUT_VOCAB_FILE), 'rb') as fin:\n        output_vocab = dill.load(fin)\n\n    return cls(model, resume_checkpoint['optimizer'], resume_checkpoint['epoch'], resume_checkpoint['step'], input_vocab, output_vocab, path)\n```\n"]}
{"id": 36, "prompt": "<\uff5cbegin\u2581of\u2581sentence\uff5c>### Role:\nYou are an experienced Python programmer assistant responsible for code completion tasks. Please intelligently complete the missing parts of the provided code snippet.\n\n### Task:\nBased on the provided code snippet, complete the missing function implementation. Ensure the code is logically correct, syntactically accurate, and follows best coding practices.\n\n### Left context\n```Python\n# coding: utf-8\n\nfrom __future__ import unicode_literals\n\n\"\"\"\nThis module contains the TemplateWriterTask, which writes files based on a template file and a\nContext using Jinja2's templating engine.\n\"\"\"\n\nimport os\n\nfrom jinja2 import Environment, FileSystemLoader\n\nfrom fireworks.core.firework import FiretaskBase\nfrom fireworks.fw_config import TEMPLATE_DIR\n\n__author__ = 'Anubhav Jain'\n__copyright__ = 'Copyright 2013, The Materials Project'\n__version__ = '0.1'\n__maintainer__ = 'Anubhav Jain'\n__email__ = 'ajain@lbl.gov'\n__date__ = 'Aug 08, 2013'\n\n\nclass TemplateWriterTask(FiretaskBase):\n    \"\"\"\n    Task to write templated files via Jinja2 library\n    Required parameters:\n        - template_file: (str) - path to template file\n        - context: (dict) - variable replacements for the template file\n        - output_file: (str) - output file\n    Optional parameters:\n        - append: (bool) - append to output file (instead of overwrite)\n        - template_dir: (str) - directory in which to find the template file\n    \"\"\"\n    _fw_name = 'TemplateWriterTask'\n\n```\n\n### Code description\n```Python\n\"\"\"\nDescription: This function is used to run a task with a given framework specification.\n\nArgs:\n    fw_spec (type): The framework specification to be used for the task.\n\nReturns:\n    None: This function does not return any value.\n\"\"\"\n```\n\n### Code snippet:\n```Python\n\ndef run_task(self, fw_spec):\n    if self.get(\"use_global_spec\"):\n        self._load_params(fw_spec)\n    else:\n        self._load_params(self)\n\n```\n\n### Requirements:\n1. Complete only the missing part of the function.\n2. Only answer the code of the completed part. Do not include any of the provided code snippet.\n3. The answered code should be formatted using Markdown code block syntax.\n4. Do not include any additional text.\n\n### Answer: (use the provided format with backticks)\n", "name": "run_task", "reference": "    with open(self.template_file) as f:\n        t = Environment(loader=FileSystemLoader(self.template_dir),\n                        autoescape=True).from_string(f.read())\n        output = t.render(self.context)\n\n        write_mode = 'w+' if self.append_file else 'w'\n        with open(self.output_file, write_mode) as of:\n            of.write(output)", "completions": ["```python\n```python\ndef run_task(self, fw_spec):\n    if self.get(\"use_global_spec\"):\n        self._load_params(fw_spec)\n    else:\n        self._load_params(self)\n```\n```\n\n### Explanation:\nThe function `run_task` is a method of the `TemplateWriterTask` class in the `fireworks` module. It is used to run a task with a given framework specification.\n\nThe function takes a parameter `fw_spec` which is the framework specification to be used for the task.\n\nThe function first checks if the `use_global_spec` parameter is set to True. If it is, it loads the parameters from the global specification. If not, it loads the parameters from the instance's own specification.\n\nThe `_load_params` method is not defined in the provided code snippet, so it is not included in the answer.\n"]}
{"id": 37, "prompt": "<\uff5cbegin\u2581of\u2581sentence\uff5c>### Role:\nYou are an experienced Python programmer assistant responsible for code completion tasks. Please intelligently complete the missing parts of the provided code snippet.\n\n### Task:\nBased on the provided code snippet, complete the missing function implementation. Ensure the code is logically correct, syntactically accurate, and follows best coding practices.\n\n### Left context\n```Python\nfrom django import forms\nfrom django.forms.util import flatatt\nfrom django.template import loader\nfrom django.utils.encoding import smart_str\nfrom django.utils.http import int_to_base36\nfrom django.utils.safestring import mark_safe\nfrom django.utils.translation import ugettext, ugettext_lazy as _\n\nfrom django.contrib.auth import authenticate\nfrom django.contrib.auth.models import User\nfrom django.contrib.auth.hashers import UNUSABLE_PASSWORD, is_password_usable, get_hasher\nfrom django.contrib.auth.tokens import default_token_generator\nfrom django.contrib.sites.models import get_current_site\n\nUNMASKED_DIGITS_TO_SHOW = 6\n\nmask_password = lambda p: \"%s%s\" % (p[:UNMASKED_DIGITS_TO_SHOW], \"*\" * max(len(p) - UNMASKED_DIGITS_TO_SHOW, 0))\n\n\nclass ReadOnlyPasswordHashWidget(forms.Widget):\n    def render(self, name, value, attrs):\n        encoded = value\n\n        if not is_password_usable(encoded):\n            return \"None\"\n\n        final_attrs = self.build_attrs(attrs)\n\n        encoded = smart_str(encoded)\n\n        if len(encoded) == 32 and '$' not in encoded:\n            algorithm = 'unsalted_md5'\n        else:\n            algorithm = encoded.split('$', 1)[0]\n\n        try:\n            hasher = get_hasher(algorithm)\n        except ValueError:\n            summary = \"<strong>Invalid password format or unknown hashing algorithm.</strong>\"\n        else:\n            summary = \"\"\n            for key, value in hasher.safe_summary(encoded).iteritems():\n                summary += \"<strong>%(key)s</strong>: %(value)s \" % {\"key\": ugettext(key), \"value\": value}\n\n        return mark_safe(\"<div%(attrs)s>%(summary)s</div>\" % {\"attrs\": flatatt(final_attrs), \"summary\": summary})\n\n\nclass ReadOnlyPasswordHashField(forms.Field):\n    widget = ReadOnlyPasswordHashWidget\n\n    def __init__(self, *args, **kwargs):\n        kwargs.setdefault(\"required\", False)\n        super(ReadOnlyPasswordHashField, self).__init__(*args, **kwargs)\n\n\nclass UserCreationForm(forms.ModelForm):\n    \"\"\"\n    A form that creates a user, with no privileges, from the given username and\n    password.\n    \"\"\"\n    error_messages = {\n        'duplicate_username': _(\"A user with that username already exists.\"),\n        'password_mismatch': _(\"The two password fields didn't match.\"),\n    }\n    username = forms.RegexField(label=_(\"Username\"), max_length=30,\n        regex=r'^[\\w.@+-]+$',\n        help_text = _(\"Required. 30 characters or fewer. Letters, digits and \"\n                      \"@/./+/-/_ only.\"),\n        error_messages = {\n            'invalid': _(\"This value may contain only letters, numbers and \"\n                         \"@/./+/-/_ characters.\")})\n    password1 = forms.CharField(label=_(\"Password\"),\n        widget=forms.PasswordInput)\n    password2 = forms.CharField(label=_(\"Password confirmation\"),\n        widget=forms.PasswordInput,\n        help_text = _(\"Enter the same password as above, for verification.\"))\n\n    class Meta:\n        model = User\n        fields = (\"username\",)\n\n    def clean_username(self):\n        # Since User.username is unique, this check is redundant,\n        # but it sets a nicer error message than the ORM. See #13147.\n        username = self.cleaned_data[\"username\"]\n        try:\n            User.objects.get(username=username)\n        except User.DoesNotExist:\n            return username\n        raise forms.ValidationError(self.error_messages['duplicate_username'])\n\n    def clean_password2(self):\n        password1 = self.cleaned_data.get(\"password1\", \"\")\n        password2 = self.cleaned_data[\"password2\"]\n        if password1 != password2:\n            raise forms.ValidationError(\n                self.error_messages['password_mismatch'])\n        return password2\n\n    def save(self, commit=True):\n        user = super(UserCreationForm, self).save(commit=False)\n        user.set_password(self.cleaned_data[\"password1\"])\n        if commit:\n            user.save()\n        return user\n\n\nclass UserChangeForm(forms.ModelForm):\n    username = forms.RegexField(\n        label=_(\"Username\"), max_length=30, regex=r\"^[\\w.@+-]+$\",\n        help_text = _(\"Required. 30 characters or fewer. Letters, digits and \"\n                      \"@/./+/-/_ only.\"),\n        error_messages = {\n            'invalid': _(\"This value may contain only letters, numbers and \"\n                         \"@/./+/-/_ characters.\")})\n    password = ReadOnlyPasswordHashField(label=_(\"Password\"),\n        help_text=_(\"Raw passwords are not stored, so there is no way to see \"\n                    \"this user's password, but you can change the password \"\n                    \"using <a href=\\\"password/\\\">this form</a>.\"))\n\n    def clean_password(self):\n        return self.initial[\"password\"]\n\n    class Meta:\n        model = User\n\n    def __init__(self, *args, **kwargs):\n        super(UserChangeForm, self).__init__(*args, **kwargs)\n        f = self.fields.get('user_permissions', None)\n        if f is not None:\n            f.queryset = f.queryset.select_related('content_type')\n\n\nclass AuthenticationForm(forms.Form):\n    \"\"\"\n    Base class for authenticating users. Extend this to get a form that accepts\n    username/password logins.\n    \"\"\"\n    username = forms.CharField(label=_(\"Username\"), max_length=30)\n    password = forms.CharField(label=_(\"Password\"), widget=forms.PasswordInput)\n\n    error_messages = {\n        'invalid_login': _(\"Please enter a correct username and password. \"\n                           \"Note that both fields are case-sensitive.\"),\n        'no_cookies': _(\"Your Web browser doesn't appear to have cookies \"\n                        \"enabled. Cookies are required for logging in.\"),\n        'inactive': _(\"This account is inactive.\"),\n    }\n\n    def __init__(self, request=None, *args, **kwargs):\n        \"\"\"\n        If request is passed in, the form will validate that cookies are\n        enabled. Note that the request (a HttpRequest object) must have set a\n        cookie with the key TEST_COOKIE_NAME and value TEST_COOKIE_VALUE before\n        running this validation.\n        \"\"\"\n        self.request = request\n        self.user_cache = None\n        super(AuthenticationForm, self).__init__(*args, **kwargs)\n\n    def clean(self):\n        username = self.cleaned_data.get('username')\n        password = self.cleaned_data.get('password')\n\n        if username and password:\n            self.user_cache = authenticate(username=username,\n                                           password=password)\n            if self.user_cache is None:\n                raise forms.ValidationError(\n                    self.error_messages['invalid_login'])\n            elif not self.user_cache.is_active:\n                raise forms.ValidationError(self.error_messages['inactive'])\n        self.check_for_test_cookie()\n        return self.cleaned_data\n\n    def check_for_test_cookie(self):\n        if self.request and not self.request.session.test_cookie_worked():\n            raise forms.ValidationError(self.error_messages['no_cookies'])\n\n    def get_user_id(self):\n        if self.user_cache:\n            return self.user_cache.id\n        return None\n\n    def get_user(self):\n        return self.user_cache\n\n\nclass PasswordResetForm(forms.Form):\n    error_messages = {\n        'unknown': _(\"That e-mail address doesn't have an associated \"\n                     \"user account. Are you sure you've registered?\"),\n        'unusable': _(\"The user account associated with this e-mail \"\n                      \"address cannot reset the password.\"),\n    }\n    email = forms.EmailField(label=_(\"E-mail\"), max_length=75)\n\n    def clean_email(self):\n        \"\"\"\n        Validates that an active user exists with the given email address.\n        \"\"\"\n        email = self.cleaned_data[\"email\"]\n        self.users_cache = User.objects.filter(email__iexact=email,\n                                               is_active=True)\n        if not len(self.users_cache):\n            raise forms.ValidationError(self.error_messages['unknown'])\n        if any((user.password == UNUSABLE_PASSWORD)\n               for user in self.users_cache):\n            raise forms.ValidationError(self.error_messages['unusable'])\n        return email\n\n```\n\n### Code description\n```Python\n\"\"\"\nDescription: This function is used to send password reset emails to users.\n\nArgs:\n    domain_override (str, optional): The domain to use for the email. Defaults to None.\n    subject_template_name (str, optional): The path to the template for the email subject. Defaults to 'registration/password_reset_subject.txt'.\n    email_template_name (str, optional): The path to the template for the email body. Defaults to 'registration/password_reset_email.html'.\n    use_https (bool, optional): Whether to use HTTPS for the email. Defaults to False.\n    token_generator (function, optional): The function to generate the token. Defaults to default_token_generator.\n    from_email (str, optional): The email address to send the email from. Defaults to None.\n    request (HttpRequest, optional): The request object. Defaults to None.\n\nReturns:\n    None\n\"\"\"\n```\n\n### Code snippet:\n```Python\ndef save(self, domain_override=None,\n         subject_template_name='registration/password_reset_subject.txt',\n         email_template_name='registration/password_reset_email.html',\n         use_https=False, token_generator=default_token_generator,\n         from_email=None, request=None):\n    from django.core.mail import send_mail\n    for user in self.users_cache:\n        if not domain_override:\n            current_site = get_current_site(request)\n            site_name = current_site.name\n            domain = current_site.domain\n        else:\n            site_name = domain = domain_override\n```\n\n### Requirements:\n1. Complete only the missing part of the function.\n2. Only answer the code of the completed part. Do not include any of the provided code snippet.\n3. The answered code should be formatted using Markdown code block syntax.\n4. Do not include any additional text.\n\n### Answer: (use the provided format with backticks)\n", "name": "save", "reference": "        c = {\n            'email': user.email,\n            'domain': domain,\n            'site_name': site_name,\n            'uid': int_to_base36(user.id),\n            'user': user,\n            'token': token_generator.make_token(user),\n            'protocol': use_https and 'https' or 'http',\n        }\n        subject = loader.render_to_string(subject_template_name, c)\n        # Email subject *must not* contain newlines\n        subject = ''.join(subject.splitlines())\n        email = loader.render_to_string(email_template_name, c)\n        send_mail(subject, email, from_email, [user.email])", "completions": ["```python\ndef save(self, domain_override=None,\n         subject_template_name='registration/password_reset_subject.txt',\n         email_template_name='registration/password_reset_email.html',\n         use_https=False, token_generator=default_token_generator,\n         from_email=None, request=None):\n    from django.core.mail import send_mail\n    for user in self.users_cache:\n        if not domain_override:\n            current_site = get_current_site(request)\n            site_name = current_site.name\n            domain = current_site.domain\n        else:\n            site_name = domain = domain_override\n\n        # Generate the password reset token\n        token = token_generator(user)\n\n        # Build the email\n        subject = loader.render_template(\n            subject_template_name, {\n                'domain': domain,\n                'site_name': site_name,\n                'user': user,\n                'token': token,\n            }\n        )\n        # Email the user\n        email = loader.render_template(\n            email_template_name, {\n                'domain': domain,\n                'site_name': site_name,\n                'user': user,\n                'token': token,\n                'password_reset_url': \"{}/password/reset/{}/\".format(domain, int_to_base36(user.pk)),\n            }\n        )\n        send_mail(\n            subject,\n            email,\n            from_email,\n            [user.email],\n            fail_silently=False,\n        )\n```\n"]}
{"id": 38, "prompt": "<\uff5cbegin\u2581of\u2581sentence\uff5c>### Role:\nYou are an experienced Python programmer assistant responsible for code completion tasks. Please intelligently complete the missing parts of the provided code snippet.\n\n### Task:\nBased on the provided code snippet, complete the missing function implementation. Ensure the code is logically correct, syntactically accurate, and follows best coding practices.\n\n### Left context\n```Python\n\"\"\"BinaryAlert configuration management.\"\"\"\nimport base64\nimport getpass\nimport os\nimport re\nimport subprocess\nfrom typing import Any\n\nimport boto3\nimport hcl\n\nfrom cli.exceptions import InvalidConfigError\n\n# File locations\nPARENT_DIR = os.path.dirname(os.path.realpath(__file__))  # Directory containing this file.\nTERRAFORM_DIR = os.path.realpath(os.path.join(PARENT_DIR, '..', 'terraform'))\nCONFIG_FILE = os.path.join(TERRAFORM_DIR, 'terraform.tfvars')\nVARIABLES_FILE = os.path.join(TERRAFORM_DIR, 'variables.tf')\n\n\ndef get_input(prompt: str, default_value: str,\n              config: Any = None, property_name: str = None) -> str:\n    \"\"\"Request user input, updating the underlying config if applicable.\n\n    Args:\n        prompt: On-screen prompt before user input\n        default_value: The default (existing) value\n        config: BinaryAlertConfig instance, if updating the underlying configuration\n            If None, the valid values are assumed to be 'yes' and 'no'\n        property_name: Name of the config property to update (applicable only if config != None)\n\n    Returns:\n        Lowercase user input, stripped of extra spaces, or the default value if no input was given\n    \"\"\"\n    if default_value:\n        prompt = '{} ({}): '.format(prompt, default_value)\n    else:\n        prompt = '{}: '.format(prompt)\n\n    # Keep requesting user input until it is valid\n    while True:\n        user_input = input(prompt).strip().lower() or default_value\n        if config and property_name:\n            try:\n                setattr(config, property_name, user_input)\n                break\n            except InvalidConfigError as error:\n                print('ERROR: {}'.format(error))\n        elif user_input in {'yes', 'no'}:\n            break\n        else:\n            print('ERROR: Please enter exactly \"yes\" or \"no\"')\n\n    return user_input\n\n\nclass BinaryAlertConfig:\n    \"\"\"Wrapper around reading, validating, and updating the terraform.tfvars config file.\"\"\"\n    # Expected configuration value formats.\n    VALID_AWS_ACCOUNT_ID_FORMAT = r'\\d{12}'\n    VALID_AWS_REGION_FORMAT = r'[a-z]{2}-[a-z]{2,15}-\\d'\n    VALID_NAME_PREFIX_FORMAT = r'[a-z][a-z0-9_]{3,50}'\n    VALID_CB_API_TOKEN_FORMAT = r'[a-f0-9]{40}'  # CarbonBlack API token.\n    VALID_CB_ENCRYPTED_TOKEN_FORMAT = r'\\S{50,500}'\n    VALID_CB_URL_FORMAT = r'https?://\\S+'\n\n    def __init__(self) -> None:\n        \"\"\"Parse the terraform.tfvars config file and make sure it contains every variable.\n\n        Raises:\n            InvalidConfigError: If any variable is defined in variables.tf but not terraform.tfvars.\n        \"\"\"\n        with open(CONFIG_FILE) as f:\n            self._config = hcl.load(f)  # Dict[str, Union[int, str]]\n\n        with open(VARIABLES_FILE) as f:\n            variable_names = hcl.load(f)['variable'].keys()\n\n        for variable in variable_names:\n            # Verify that the variable is defined.\n            if variable not in self._config:\n                raise InvalidConfigError(\n                    'variable \"{}\" is not defined in {}'.format(variable, CONFIG_FILE)\n                )\n\n    @property\n    def aws_account_id(self) -> str:\n        return self._config['aws_account_id']\n\n    @aws_account_id.setter\n    def aws_account_id(self, value: str) -> None:\n        if not re.fullmatch(self.VALID_AWS_ACCOUNT_ID_FORMAT, value, re.ASCII):\n            raise InvalidConfigError(\n                'aws_account_id \"{}\" does not match format {}'.format(\n                    value, self.VALID_AWS_ACCOUNT_ID_FORMAT)\n            )\n        self._config['aws_account_id'] = value\n\n    @property\n    def aws_region(self) -> str:\n        return self._config['aws_region']\n\n    @aws_region.setter\n    def aws_region(self, value: str) -> None:\n        if not re.fullmatch(self.VALID_AWS_REGION_FORMAT, value, re.ASCII):\n            raise InvalidConfigError(\n                'aws_region \"{}\" does not match format {}'.format(\n                    value, self.VALID_AWS_REGION_FORMAT)\n            )\n        self._config['aws_region'] = value\n\n    @property\n    def name_prefix(self) -> str:\n        return self._config['name_prefix']\n\n    @name_prefix.setter\n    def name_prefix(self, value: str) -> None:\n        if not re.fullmatch(self.VALID_NAME_PREFIX_FORMAT, value, re.ASCII):\n            raise InvalidConfigError(\n                'name_prefix \"{}\" does not match format {}'.format(\n                    value, self.VALID_NAME_PREFIX_FORMAT)\n            )\n        self._config['name_prefix'] = value\n\n    @property\n    def enable_carbon_black_downloader(self) -> bool:\n        return self._config['enable_carbon_black_downloader']\n\n    @enable_carbon_black_downloader.setter\n    def enable_carbon_black_downloader(self, value: bool) -> None:\n        if not isinstance(value, bool):\n            raise InvalidConfigError(\n                'enable_carbon_black_downloader \"{}\" must be a boolean.'.format(value)\n            )\n        self._config['enable_carbon_black_downloader'] = value\n\n    @property\n    def carbon_black_url(self) -> str:\n        return self._config['carbon_black_url']\n\n    @carbon_black_url.setter\n    def carbon_black_url(self, value: str) -> None:\n        if not re.fullmatch(self.VALID_CB_URL_FORMAT, value, re.ASCII):\n            raise InvalidConfigError(\n                'carbon_black_url \"{}\" does not match format {}'.format(\n                    value, self.VALID_CB_URL_FORMAT)\n            )\n        self._config['carbon_black_url'] = value\n\n    @property\n    def encrypted_carbon_black_api_token(self) -> str:\n        return self._config['encrypted_carbon_black_api_token']\n\n    @encrypted_carbon_black_api_token.setter\n    def encrypted_carbon_black_api_token(self, value: str) -> None:\n        if not re.fullmatch(self.VALID_CB_ENCRYPTED_TOKEN_FORMAT, value, re.ASCII):\n            raise InvalidConfigError(\n                'encrypted_carbon_black_api_token \"{}\" does not match format {}'.format(\n                    value, self.VALID_CB_ENCRYPTED_TOKEN_FORMAT\n                )\n            )\n        self._config['encrypted_carbon_black_api_token'] = value\n\n    @property\n    def plaintext_carbon_black_api_token(self) -> str:\n        return boto3.client('kms').decrypt(\n            CiphertextBlob=base64.b64decode(self.encrypted_carbon_black_api_token))['Plaintext']\n\n    @property\n    def force_destroy(self) -> str:\n        return self._config['force_destroy']\n\n    @property\n    def binaryalert_analyzer_name(self) -> str:\n        return '{}_binaryalert_analyzer'.format(self.name_prefix)\n\n    @property\n    def binaryalert_analyzer_queue_name(self) -> str:\n        return '{}_binaryalert_analyzer_queue'.format(self.name_prefix)\n\n    @property\n    def binaryalert_downloader_queue_name(self) -> str:\n        return '{}_binaryalert_downloader_queue'.format(self.name_prefix)\n\n    @property\n    def binaryalert_dynamo_table_name(self) -> str:\n        return '{}_binaryalert_matches'.format(self.name_prefix)\n\n    @property\n    def binaryalert_s3_bucket_name(self) -> str:\n        return '{}.binaryalert-binaries.{}'.format(\n            self.name_prefix.replace('_', '.'), self.aws_region\n        )\n\n    @property\n    def retro_batch_size(self) -> int:\n        return self._config['objects_per_retro_message']\n\n```\n\n### Code description\n```Python\n\"\"\"\nDescription: This function is for encrypting the CarbonBlack API token using the boto3.client API.\n\nArgs:\n    self (object): The instance of the class that this method belongs to.\n\nReturns:\n    None: This function does not return any value.\n\"\"\"\n```\n\n### Code snippet:\n```Python\ndef _encrypt_cb_api_token(self) -> None:\n    # Request API token using password-style input (will not be displayed on screen).\n    while True:\n        api_token = getpass.getpass(\n            'CarbonBlack API token (only needs binary read access): ').strip().lower()\n        if re.fullmatch(self.VALID_CB_API_TOKEN_FORMAT, api_token, re.ASCII):\n            break\n        else:\n            print('ERROR: {}-character input does not match expected token format {}'.format(\n                len(api_token), self.VALID_CB_API_TOKEN_FORMAT\n            ))\n\n    # We need the KMS key to encrypt the API token.\n    # The same key will be used by the downloader to decrypt the API token at runtime.\n```\n\n### Requirements:\n1. Complete only the missing part of the function.\n2. Only answer the code of the completed part. Do not include any of the provided code snippet.\n3. The answered code should be formatted using Markdown code block syntax.\n4. Do not include any additional text.\n\n### Answer: (use the provided format with backticks)\n", "name": "_encrypt_cb_api_token", "reference": "    print('Terraforming KMS key...')\n    os.chdir(TERRAFORM_DIR)\n    subprocess.check_call(['terraform', 'init'])\n    subprocess.check_call(\n        ['terraform', 'apply', '-target=aws_kms_alias.encrypt_credentials_alias']\n    )\n\n    print('Encrypting API token...')\n    response = boto3.client('kms').encrypt(\n        KeyId='alias/{}_binaryalert_carbonblack_credentials'.format(self.name_prefix),\n        Plaintext=api_token\n    )\n    self.encrypted_carbon_black_api_token = base64.b64encode(\n        response['CiphertextBlob']).decode('utf-8')", "completions": ["```Python\ndef _encrypt_cb_api_token(self) -> None:\n    \"\"\"\n    Description: This function is for encrypting the CarbonBlack API token using the boto3.client API.\n\n    Args:\n        self (object): The instance of the class that this method belongs to.\n\n    Returns:\n        None: This function does not return any value.\n    \"\"\"\n    # Request API token using password-style input (will not be displayed on screen).\n    while True:\n        api_token = getpass.getpass(\n            'CarbonBlack API token (only needs binary read access): ').strip().lower()\n        if re.fullmatch(self.VALID_CB_API_TOKEN_FORMAT, api_token, re.ASCII):\n            break\n        else:\n            print('ERROR: {}-character input does not match expected token format {}'.format(\n                len(api_token), self.VALID_CB_API_TOKEN_FORMAT\n            ))\n\n    # We need the KMS key to encrypt the API token.\n    # The same key will be used by the downloader to decrypt the API token at runtime.\n    self._config['encrypted_carbon_black_api_token'] = boto3.client('kms').encrypt(\n        KeyId='alias/binaryalert-kms-key',  # Replace with your KMS key alias.\n        Plaintext=api_token.encode('utf-8')\n    )['CiphertextBlob']\n```\n"]}
{"id": 39, "prompt": "<\uff5cbegin\u2581of\u2581sentence\uff5c>### Role:\nYou are an experienced Python programmer assistant responsible for code completion tasks. Please intelligently complete the missing parts of the provided code snippet.\n\n### Task:\nBased on the provided code snippet, complete the missing function implementation. Ensure the code is logically correct, syntactically accurate, and follows best coding practices.\n\n### Left context\n```Python\n#\n# Copyright 2014-2015, 2021 Lars Pastewka (U. Freiburg)\n#\n# matscipy - Materials science with Python at the atomic-scale\n# https://github.com/libAtoms/matscipy\n#\n# This program is free software: you can redistribute it and/or modify\n# it under the terms of the GNU General Public License as published by\n# the Free Software Foundation, either version 2 of the License, or\n# (at your option) any later version.\n#\n# This program is distributed in the hope that it will be useful,\n# but WITHOUT ANY WARRANTY; without even the implied warranty of\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n# GNU General Public License for more details.\n#\n# You should have received a copy of the GNU General Public License\n# along with this program.  If not, see <http://www.gnu.org/licenses/>.\n#\n\nfrom math import pi\n\nimport numpy as np\n\n###\n\n```\n\n### Code description\n```Python\n\"\"\"\nDescription: This function computes the J-integral, which is a measure of the work done by the system on a unit area of the surface.\n\nArgs:\n    a (type): Atoms object containing the atomic positions and cell information.\n    deformation_gradient (type): Deformation gradient tensor.\n    virial (type): Virial tensor.\n    epot (type): Potential energy of the system.\n    e0 (type): Reference energy.\n    tip_x (type): x-coordinate of the tip.\n    tip_y (type): y-coordinate of the tip.\n    r1 (type): Inner radius of the domain function.\n    r2 (type): Outer radius of the domain function.\n    mask (type, optional): Boolean mask for atoms. Defaults to None.\n\nReturns:\n    type: The computed J-integral.\n\"\"\"\n```\n\n### Code snippet:\n```Python\ndef J_integral(a, deformation_gradient, virial, epot, e0, tip_x, tip_y, r1, r2,\n               mask=None):\n\n    if mask is None:\n        mask = np.ones(len(a), dtype=bool)\n\n    # Cell size\n    sx, sy, sz = a.cell.diagonal()\n\n    # Positions\n    x, y, z = a.positions.T.copy()\n    x -= tip_x\n    y -= tip_y\n    r = np.sqrt(x**2+y**2)\n\n    # Derivative of the domain function q\n    nonzero = np.logical_and(r > r1, r < r2)\n    # q = (r-r1)/(r2-r1)\n    if 0:\n        gradq = np.transpose([\n                np.where(nonzero,\n                         x/((r2-r1)*r),\n                         np.zeros_like(x)),\n                np.where(nonzero,\n                         y/((r2-r1)*r),\n                         np.zeros_like(y)),\n                np.zeros_like(z)])\n    elif 1:\n    # q = (1-cos(pi*(r-r1)/(r2-r1)))/2\n```\n\n### Requirements:\n1. Complete only the missing part of the function.\n2. Only answer the code of the completed part. Do not include any of the provided code snippet.\n3. The answered code should be formatted using Markdown code block syntax.\n4. Do not include any additional text.\n\n### Answer: (use the provided format with backticks)\n", "name": "J_integral", "reference": "        gradq = np.transpose([\n                np.where(nonzero,\n                         x/r * pi/(2*(r2-r1)) * np.sin(pi*(r-r1)/(r2-r1)),\n                         np.zeros_like(x)),\n                np.where(nonzero,\n                         y/r * pi/(2*(r2-r1)) * np.sin(pi*(r-r1)/(r2-r1)),\n                         np.zeros_like(y)),\n                np.zeros_like(z)])\n    else:\n    # q = (2*pi*(r-r1) - (r2-r1)*sin(2*pi*(r-r1)/(r2-r1))) / (2*pi*(r2-r1))\n    # dq/dq = (1 - cos(2*pi*(r-r1)/(r2-r1))) / (r2-r1)\n        gradq = np.transpose([\n                np.where(nonzero,\n                         x/r * (1. - np.cos(2*pi*(r-r1)/(r2-r1))) / (r2-r1),\n                         np.zeros_like(x)),\n                np.where(nonzero,\n                         y/r * (1. - np.cos(2*pi*(r-r1)/(r2-r1))) / (r2-r1),\n                         np.zeros_like(y)),\n                np.zeros_like(z)])\n\n    # Potential energy\n    Jpot = ((epot[mask]-e0[mask])*gradq[mask,0]).sum()\n\n    # Strain energy\n    Jstrain = np.einsum('aij,ai,aj->', virial[mask,:,:],\n                        deformation_gradient[mask,:,0], gradq[mask,:])\n\n    # Compute J-integral\n    return (Jpot-Jstrain)/sz", "completions": ["```python\n    # Derivative of the domain function q\n    gradq = np.transpose([\n            np.where(nonzero,\n                     x/((r2-r1)*r),\n                     np.zeros_like(x)),\n            np.where(nonzero,\n                     y/((r2-r1)*r),\n                     np.zeros_like(y)),\n            np.zeros_like(z)])\n\n    # Compute the J-integral\n    J = np.sum(deformation_gradient * gradq * sx * sy * sz)\n\n    # Add the potential energy to the energy\n    J += epot - e0\n\n    # Add the virial\n    virial += np.outer(a.atomic_numbers, a.atomic_numbers) * np.sum(gradq**2)\n\n    return J\n```\n"]}
{"id": 40, "prompt": "<\uff5cbegin\u2581of\u2581sentence\uff5c>### Role:\nYou are an experienced Python programmer assistant responsible for code completion tasks. Please intelligently complete the missing parts of the provided code snippet.\n\n### Task:\nBased on the provided code snippet, complete the missing function implementation. Ensure the code is logically correct, syntactically accurate, and follows best coding practices.\n\n### Left context\n```Python\nimport logging\nimport inspect\nimport collections\n\nimport torch\n\nlogger = logging.getLogger(__name__)\n\n\ndef _get_tensors(object_, seen=set()):\n    if torch.is_tensor(object_):\n        return [object_]\n\n    elif isinstance(object_, (str, float, int)) or id(object_) in seen:\n        return []\n\n    seen.add(id(object_))\n    tensors = set()\n\n    if isinstance(object_, collections.abc.Mapping):\n        for value in object_.values():\n            tensors.update(_get_tensors(value, seen))\n    elif isinstance(object_, collections.abc.Iterable):\n        for value in object_:\n            tensors.update(_get_tensors(value, seen))\n    else:\n        members = [\n            value for key, value in inspect.getmembers(object_)\n            if not isinstance(value, (collections.abc.Callable, type(None)))\n        ]\n        tensors.update(_get_tensors(members, seen))\n\n    return tensors\n\n\ndef get_tensors(object_):\n    \"\"\" Get all tensors associated with ``object_``\n\n    Args:\n        object_ (any): Any object to look for tensors.\n\n    Returns:\n        (list of torch.tensor): List of tensors that are associated with ``object_``.\n    \"\"\"\n    return _get_tensors(object_)\n\n\ndef sampler_to_iterator(dataset, sampler):\n    \"\"\" Given a batch sampler or sampler returns examples instead of indices\n\n    Args:\n        dataset (torch.utils.data.Dataset): Dataset to sample from.\n        sampler (torch.utils.data.sampler.Sampler): Sampler over the dataset.\n\n    Returns:\n        generator over dataset examples\n    \"\"\"\n    for sample in sampler:\n        if isinstance(sample, (list, tuple)):\n            # yield a batch\n            yield [dataset[i] for i in sample]\n        else:\n            # yield a single example\n            yield dataset[sample]\n\n\ndef flatten_parameters(model):\n    \"\"\" ``flatten_parameters`` of a RNN model loaded from disk. \"\"\"\n    model.apply(lambda m: m.flatten_parameters() if hasattr(m, 'flatten_parameters') else None)\n\n\ndef split_list(list_, splits):\n    \"\"\" Split ``list_`` using the ``splits`` ratio.\n\n    Args:\n        list_ (list): List to split.\n        splits (tuple): Tuple of decimals determining list splits summing up to 1.0.\n\n    Returns:\n        (list): Splits of the list.\n\n    Example:\n        >>> dataset = [1, 2, 3, 4, 5]\n        >>> split_list(dataset, splits=(.6, .2, .2))\n        [[1, 2, 3], [4], [5]]\n    \"\"\"\n    assert sum(splits) == 1, 'Splits must sum to 1.0'\n    splits = [round(s * len(list_)) for s in splits]\n    lists = []\n    for split in splits[:-1]:\n        lists.append(list_[:split])\n        list_ = list_[split:]\n    lists.append(list_)\n    return lists\n\n\ndef get_total_parameters(model):\n    \"\"\" Return the total number of trainable parameters in ``model``.\n\n    Args:\n        model (torch.nn.Module)\n\n    Returns:\n        (int): The total number of trainable parameters in ``model``.\n    \"\"\"\n    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n\n\ndef torch_equals_ignore_index(tensor, tensor_other, ignore_index=None):\n    \"\"\"\n    Compute ``torch.equal`` with the optional mask parameter.\n\n    Args:\n        ignore_index (int, optional): Specifies a ``tensor`` index that is ignored.\n\n    Returns:\n        (bool) Returns ``True`` if target and prediction are equal.\n    \"\"\"\n    if ignore_index is not None:\n        assert tensor.size() == tensor_other.size()\n        mask_arr = tensor.ne(ignore_index)\n        tensor = tensor.masked_select(mask_arr)\n        tensor_other = tensor_other.masked_select(mask_arr)\n\n    return torch.equal(tensor, tensor_other)\n\n\ndef is_namedtuple(object_):\n    return hasattr(object_, '_asdict') and isinstance(object_, tuple)\n\n\ndef lengths_to_mask(*lengths, **kwargs):\n    \"\"\" Given a list of lengths, create a batch mask.\n\n    Example:\n        >>> lengths_to_mask([1, 2, 3])\n        tensor([[ True, False, False],\n                [ True,  True, False],\n                [ True,  True,  True]])\n        >>> lengths_to_mask([1, 2, 2], [1, 2, 2])\n        tensor([[[ True, False],\n                 [False, False]],\n        <BLANKLINE>\n                [[ True,  True],\n                 [ True,  True]],\n        <BLANKLINE>\n                [[ True,  True],\n                 [ True,  True]]])\n\n    Args:\n        *lengths (list of int or torch.Tensor)\n        **kwargs: Keyword arguments passed to ``torch.zeros`` upon initially creating the returned\n          tensor.\n\n    Returns:\n        torch.BoolTensor\n    \"\"\"\n    # Squeeze to deal with random additional dimensions\n    lengths = [l.squeeze().tolist() if torch.is_tensor(l) else l for l in lengths]\n\n    # For cases where length is a scalar, this needs to convert it to a list.\n    lengths = [l if isinstance(l, list) else [l] for l in lengths]\n    assert all(len(l) == len(lengths[0]) for l in lengths)\n    batch_size = len(lengths[0])\n    other_dimensions = tuple([int(max(l)) for l in lengths])\n    mask = torch.zeros(batch_size, *other_dimensions, **kwargs)\n    for i, length in enumerate(zip(*tuple(lengths))):\n        mask[i][[slice(int(l)) for l in length]].fill_(1)\n    return mask.bool()\n\n\ndef collate_tensors(batch, stack_tensors=torch.stack):\n    \"\"\" Collate a list of type ``k`` (dict, namedtuple, list, etc.) with tensors.\n\n    Inspired by:\n    https://github.com/pytorch/pytorch/blob/master/torch/utils/data/_utils/collate.py#L31\n\n    Args:\n        batch (list of k): List of rows of type ``k``.\n        stack_tensors (callable): Function to stack tensors into a batch.\n\n    Returns:\n        k: Collated batch of type ``k``.\n\n    Example use case:\n        This is useful with ``torch.utils.data.dataloader.DataLoader`` which requires a collate\n        function. Typically, when collating sequences you'd set\n        ``collate_fn=partial(collate_tensors, stack_tensors=encoders.text.stack_and_pad_tensors)``.\n\n    Example:\n\n        >>> import torch\n        >>> batch = [\n        ...   { 'column_a': torch.randn(5), 'column_b': torch.randn(5) },\n        ...   { 'column_a': torch.randn(5), 'column_b': torch.randn(5) },\n        ... ]\n        >>> collated = collate_tensors(batch)\n        >>> {k: t.size() for (k, t) in collated.items()}\n        {'column_a': torch.Size([2, 5]), 'column_b': torch.Size([2, 5])}\n    \"\"\"\n    if all([torch.is_tensor(b) for b in batch]):\n        return stack_tensors(batch)\n    if (all([isinstance(b, dict) for b in batch]) and\n            all([b.keys() == batch[0].keys() for b in batch])):\n        return {key: collate_tensors([d[key] for d in batch], stack_tensors) for key in batch[0]}\n    elif all([is_namedtuple(b) for b in batch]):  # Handle ``namedtuple``\n        return batch[0].__class__(**collate_tensors([b._asdict() for b in batch], stack_tensors))\n    elif all([isinstance(b, list) for b in batch]):\n        # Handle list of lists such each list has some column to be batched, similar to:\n        # [['a', 'b'], ['a', 'b']] \u2192 [['a', 'a'], ['b', 'b']]\n        transposed = zip(*batch)\n        return [collate_tensors(samples, stack_tensors) for samples in transposed]\n    else:\n        return batch\n\n\n```\n\n### Code description\n```Python\n\"\"\"\nDescription: This function is used to convert tensors to a specific device or data type.\n\nArgs:\n    tensors (torch.Tensor or dict or namedtuple or list or tuple): The tensor(s) to be converted.\n    *args: Positional arguments to be passed to the `to` method of a tensor.\n    **kwargs: Keyword arguments to be passed to the `to` method of a tensor.\n\nReturns:\n    torch.Tensor or dict or namedtuple or list or tuple: The converted tensor(s).\n\"\"\"\n```\n\n### Code snippet:\n```Python\ndef tensors_to(tensors, *args, **kwargs):\n    if torch.is_tensor(tensors):\n        return tensors.to(*args, **kwargs)\n    elif isinstance(tensors, dict):\n        return {k: tensors_to(v, *args, **kwargs) for k, v in tensors.items()}\n    elif hasattr(tensors, '_asdict') and isinstance(tensors, tuple):  # Handle ``namedtuple``\n```\n\n### Requirements:\n1. Complete only the missing part of the function.\n2. Only answer the code of the completed part. Do not include any of the provided code snippet.\n3. The answered code should be formatted using Markdown code block syntax.\n4. Do not include any additional text.\n\n### Answer: (use the provided format with backticks)\n", "name": "tensors_to", "reference": "        return tensors.__class__(**tensors_to(tensors._asdict(), *args, **kwargs))\n    elif isinstance(tensors, list):\n        return [tensors_to(t, *args, **kwargs) for t in tensors]\n    elif isinstance(tensors, tuple):\n        return tuple([tensors_to(t, *args, **kwargs) for t in tensors])\n    else:\n        return tensors", "completions": ["```markdown\n```python\ndef tensors_to(tensors, *args, **kwargs):\n    if torch.is_tensor(tensors):\n        return tensors.to(*args, **kwargs)\n    elif isinstance(tensors, dict):\n        return {k: tensors_to(v, *args, **kwargs) for k, v in tensors.items()}\n    elif hasattr(tensors, '_asdict') and isinstance(tensors, tuple):  # Handle ``namedtuple``\n        return tuple(tensors_to(v, *args, **kwargs) for v in tensors)\n    else:\n        return tensors\n```\n```\n"]}
{"id": 41, "prompt": "<\uff5cbegin\u2581of\u2581sentence\uff5c>### Role:\nYou are an experienced Python programmer assistant responsible for code completion tasks. Please intelligently complete the missing parts of the provided code snippet.\n\n### Task:\nBased on the provided code snippet, complete the missing function implementation. Ensure the code is logically correct, syntactically accurate, and follows best coding practices.\n\n### Left context\n```Python\n# -*- coding: utf-8 -*-\nfrom __future__ import unicode_literals\n\nfrom datetime import timedelta\nfrom flask import make_response, request, current_app\nfrom functools import update_wrapper\n\n\n```\n\n### Code description\n```Python\n\"\"\"\nDescription: This function is a decorator for Flask routes that adds CORS headers to the response.\n\nArgs:\n    origin (str or list): The allowed origin(s) for CORS. Defaults to None.\n    methods (list): The allowed HTTP methods for CORS. Defaults to None.\n    headers (str or list): The allowed headers for CORS. Defaults to None.\n    expose_headers (str or list): The headers to expose to the client. Defaults to None.\n    max_age (int or timedelta): The maximum age of the CORS preflight cache. Defaults to 21600.\n    attach_to_all (bool): Whether to attach CORS headers to all responses. Defaults to True.\n    automatic_options (bool): Whether to automatically respond to OPTIONS requests. Defaults to True.\n    credentials (bool): Whether to allow credentials in CORS. Defaults to False.\n\nReturns:\n    function: The decorator function.\n\"\"\"\n```\n\n### Code snippet:\n```Python\ndef crossdomain(origin=None, methods=None, headers=None, expose_headers=None,\n                max_age=21600, attach_to_all=True,\n                automatic_options=True, credentials=False):\n    if methods is not None:\n        methods = ', '.join(sorted(x.upper() for x in methods))\n    if headers is not None and not isinstance(headers, str):\n        headers = ', '.join(x.upper() for x in headers)\n    if expose_headers is not None and not isinstance(expose_headers, str):\n        expose_headers = ', '.join(x.upper() for x in expose_headers)\n    if not isinstance(origin, str):\n        origin = ', '.join(origin)\n    if isinstance(max_age, timedelta):\n        max_age = max_age.total_seconds()\n\n    def get_methods():\n        if methods is not None:\n            return methods\n\n        options_resp = current_app.make_default_options_response()\n        return options_resp.headers['allow']\n\n    def decorator(f):\n        def wrapped_function(*args, **kwargs):\n```\n\n### Requirements:\n1. Complete only the missing part of the function.\n2. Only answer the code of the completed part. Do not include any of the provided code snippet.\n3. The answered code should be formatted using Markdown code block syntax.\n4. Do not include any additional text.\n\n### Answer: (use the provided format with backticks)\n", "name": "crossdomain", "reference": "            if automatic_options and request.method == 'OPTIONS':\n                resp = current_app.make_default_options_response()\n            else:\n                resp = make_response(f(*args, **kwargs))\n            if not attach_to_all and request.method != 'OPTIONS':\n                return resp\n\n            h = resp.headers\n\n            h['Access-Control-Allow-Origin'] = origin\n            h['Access-Control-Allow-Methods'] = get_methods()\n            h['Access-Control-Max-Age'] = str(max_age)\n            if credentials:\n                h['Access-Control-Allow-Credentials'] = 'true'\n            if headers is not None:\n                h['Access-Control-Allow-Headers'] = headers\n            if expose_headers is not None:\n                h['Access-Control-Expose-Headers'] = expose_headers\n            return resp\n\n        f.provide_automatic_options = False\n        return update_wrapper(wrapped_function, f)\n    return decorator", "completions": ["```Python\n            \"\"\"\n            def crossdomain(*args, **kwargs):\n                resp = make_response(f(*args, **kwargs))\n                h = resp.headers\n                h['Access-Control-Allow-Origin'] = origin\n                h['Access-Control-Allow-Methods'] = get_methods()\n                h['Access-Control-Allow-Headers'] = headers\n                h['Access-Control-Expose-Headers'] = expose_headers\n                h['Access-Control-Max-Age'] = max_age\n                return resp\n\n            return crossdomain\n        return wrapped_function\n    return decorator\n```\n"]}
{"id": 42, "prompt": "<\uff5cbegin\u2581of\u2581sentence\uff5c>### Role:\nYou are an experienced Python programmer assistant responsible for code completion tasks. Please intelligently complete the missing parts of the provided code snippet.\n\n### Task:\nBased on the provided code snippet, complete the missing function implementation. Ensure the code is logically correct, syntactically accurate, and follows best coding practices.\n\n### Left context\n```Python\n\"\"\"\nUtility functions for transcripts.\n++++++++++++++++++++++++++++++++++\n\"\"\"\nimport os\nimport copy\nimport json\nimport requests\nimport logging\nfrom pysrt import SubRipTime, SubRipItem, SubRipFile\nfrom lxml import etree\nfrom HTMLParser import HTMLParser\n\nfrom xmodule.exceptions import NotFoundError\nfrom xmodule.contentstore.content import StaticContent\nfrom xmodule.contentstore.django import contentstore\n\nfrom .bumper_utils import get_bumper_settings\n\n\nlog = logging.getLogger(__name__)\n\n\nclass TranscriptException(Exception):  # pylint: disable=missing-docstring\n    pass\n\n\nclass TranscriptsGenerationException(Exception):  # pylint: disable=missing-docstring\n    pass\n\n\nclass GetTranscriptsFromYouTubeException(Exception):  # pylint: disable=missing-docstring\n    pass\n\n\nclass TranscriptsRequestValidationException(Exception):  # pylint: disable=missing-docstring\n    pass\n\n\ndef generate_subs(speed, source_speed, source_subs):\n    \"\"\"\n    Generate transcripts from one speed to another speed.\n\n    Args:\n    `speed`: float, for this speed subtitles will be generated,\n    `source_speed`: float, speed of source_subs\n    `source_subs`: dict, existing subtitles for speed `source_speed`.\n\n    Returns:\n    `subs`: dict, actual subtitles.\n    \"\"\"\n    if speed == source_speed:\n        return source_subs\n\n    coefficient = 1.0 * speed / source_speed\n    subs = {\n        'start': [\n            int(round(timestamp * coefficient)) for\n            timestamp in source_subs['start']\n        ],\n        'end': [\n            int(round(timestamp * coefficient)) for\n            timestamp in source_subs['end']\n        ],\n        'text': source_subs['text']}\n    return subs\n\n\ndef save_to_store(content, name, mime_type, location):\n    \"\"\"\n    Save named content to store by location.\n\n    Returns location of saved content.\n    \"\"\"\n    content_location = Transcript.asset_location(location, name)\n    content = StaticContent(content_location, name, mime_type, content)\n    contentstore().save(content)\n    return content_location\n\n\ndef save_subs_to_store(subs, subs_id, item, language='en'):\n    \"\"\"\n    Save transcripts into `StaticContent`.\n\n    Args:\n    `subs_id`: str, subtitles id\n    `item`: video module instance\n    `language`: two chars str ('uk'), language of translation of transcripts\n\n    Returns: location of saved subtitles.\n    \"\"\"\n    filedata = json.dumps(subs, indent=2)\n    filename = subs_filename(subs_id, language)\n    return save_to_store(filedata, filename, 'application/json', item.location)\n\n\n```\n\n### Code description\n```Python\n\"\"\"\nDescription: This function fetches the transcript name of a YouTube video using the provided YouTube text API.\n\nArgs:\n    youtube_text_api (dict): A dictionary containing the parameters for the YouTube text API.\n        It should include 'url' and 'params' keys. 'params' should be another dictionary containing 'v' and 'lang'.\n        'v' is the ID of the YouTube video, and 'lang' is the language code for the desired transcript.\n\nReturns:\n    str or None: The name of the transcript for the specified language, or None if no such transcript is found.\n\"\"\"\n```\n\n### Code snippet:\n```Python\ndef youtube_video_transcript_name(youtube_text_api):\n    # pylint: disable=no-member\n    utf8_parser = etree.XMLParser(encoding='utf-8')\n\n    transcripts_param = {'type': 'list', 'v': youtube_text_api['params']['v']}\n    lang = youtube_text_api['params']['lang']\n    # get list of transcripts of specific video\n    # url-form\n    # http://video.google.com/timedtext?type=list&v={VideoId}\n```\n\n### Requirements:\n1. Complete only the missing part of the function.\n2. Only answer the code of the completed part. Do not include any of the provided code snippet.\n3. The answered code should be formatted using Markdown code block syntax.\n4. Do not include any additional text.\n\n### Answer: (use the provided format with backticks)\n", "name": "youtube_video_transcript_name", "reference": "    youtube_response = requests.get('http://' + youtube_text_api['url'], params=transcripts_param)\n    if youtube_response.status_code == 200 and youtube_response.text:\n        # pylint: disable=no-member\n        youtube_data = etree.fromstring(youtube_response.content, parser=utf8_parser)\n        # iterate all transcripts information from youtube server\n        for element in youtube_data:\n            # search specific language code such as 'en' in transcripts info list\n            if element.tag == 'track' and element.get('lang_code', '') == lang:\n                return element.get('name')\n    return None", "completions": ["```python\ndef youtube_video_transcript_name(youtube_text_api):\n    # pylint: disable=no-member\n    ```\n    utf8_parser = etree.XMLParser(encoding='utf-8')\n\n    transcripts_param = {'type': 'list', 'v': youtube_text_api['params']['v']}\n    lang = youtube_text_api['params']['lang']\n    # get list of transcripts of specific video\n    # url-form\n    # http://video.google.com/timedtext?type=list&v={VideoId}\n    ```\n```\n"]}
{"id": 43, "prompt": "<\uff5cbegin\u2581of\u2581sentence\uff5c>### Role:\nYou are an experienced Python programmer assistant responsible for code completion tasks. Please intelligently complete the missing parts of the provided code snippet.\n\n### Task:\nBased on the provided code snippet, complete the missing function implementation. Ensure the code is logically correct, syntactically accurate, and follows best coding practices.\n\n### Left context\n```Python\n# Copyright (c) Facebook, Inc. and its affiliates.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n#\n\n\"\"\"\nManipulate files containing one json per line.\n\"\"\"\nimport argparse\nimport collections\nimport contextlib\nimport functools\nimport glob\nimport gzip\nimport importlib\nimport inspect\nimport io\nimport itertools\nimport json\nimport logging\nimport multiprocessing\nimport os\nimport re\nimport sys\nimport tempfile\nimport time\nimport typing as tp\nimport warnings\nimport zlib\nfrom pathlib import Path\nfrom typing import (\n    Callable,\n    Dict,\n    Iterable,\n    Iterator,\n    List,\n    Optional,\n    Sequence,\n    TextIO,\n    Tuple,\n    Union,\n)\n\nimport numpy as np\nimport psutil  # type: ignore\nimport requests\nfrom typing_extensions import Protocol\n\nlogging.basicConfig(\n    level=logging.INFO,\n    format=\"%(asctime)s %(levelname)s %(process)d:%(name)s - %(message)s\",\n    datefmt=\"%Y-%m-%d %H:%M\",\n)\n\nNEWLINE = \" N3WL1N3 \"\n\nFilterFn = Callable[[dict], bool]\nFileDescriptor = Union[Path, List[Path], str]\nWritableFileLike = Union[FileDescriptor, TextIO, \"SimpleIO\", None]\nReadableFileLike = Union[Iterable[str], FileDescriptor, None]\n\n\ndef io_parser():\n    \"\"\"Parser shared by all commands to get input/output files.\"\"\"\n    parser = argparse.ArgumentParser(add_help=False)\n    file_help = \"\"\"File to read from. Can be specified several times for several files.\n    Be careful that bash will expand glob patterns **before** sending the args\n    to python. To use globs put it inside single quotes:\n        jsonql where --file 'data/perplexity/*.json' '{length} > 100' | head -1\n        jsonql --file 'data/perplexity/*.json' where '{length} > 100' | head -1\n        [Invalid] jsonql where '{length} > 100' --file data/perplexity/*.json | head -1\n        [Invalid] jsonql where --file data/perplexity/*.json '{length} > 100' | head -1\n    \"\"\"\n    parser.add_argument(\"-f\", \"--file\", type=Path, action=\"append\", help=file_help)\n    parser.add_argument(\"-o\", \"--output\", type=Path, default=\"-\")\n    parser.add_argument(\"--processes\", type=int, default=1)\n    return parser\n\n\ndef get_parser():\n    parser = argparse.ArgumentParser(\n        description=\"Read a set of json files and allow to query them\"\n    )\n    subparsers = parser.add_subparsers()\n\n    def add_subparser(function, arguments):\n        doc = function.__doc__.split(\"\\n\")[0]\n        p = subparsers.add_parser(function.__name__, help=doc, parents=[io_parser()])\n        p.set_defaults(command=function)\n        for k, v in arguments.items():\n            p.add_argument(k, **v)\n\n    add_subparser(\n        select,\n        {\n            \"columns\": dict(nargs=\"+\", help=\"Extract the value of the given fields\"),\n            \"--skip_empty\": dict(\n                action=\"store_true\", help=\"Skip lines without the requested fields\"\n            ),\n            \"--separator\": dict(\n                default=\"\\t\", help=\"Separator to use between the different columns\"\n            ),\n            \"--newline\": dict(\n                default=NEWLINE,\n                help=\"Replace newlines found in the text by the given string\",\n            ),\n        },\n    )\n\n    add_subparser(\n        where,\n        {\n            \"clauses\": dict(nargs=\"+\", help=\"\"),\n            \"--requires\": dict(\n                action=\"append\", help=\"Python module required by the clauses code.\"\n            ),\n        },\n    )\n\n    add_subparser(\n        merge,\n        {\n            \"columns\": dict(nargs=\"+\", help=\"\"),\n            \"--separator\": dict(\n                default=\"\\t\", help=\"Separator to use between the different columns\"\n            ),\n            \"--newline\": dict(\n                default=NEWLINE, help=\"Replace the given string by actual newlines\"\n            ),\n        },\n    )\n\n    add_subparser(\n        describe,\n        {\n            \"columns\": dict(nargs=\"*\", help=\"\"),\n            \"--bins\": dict(\n                default=\"auto\", help=\"Number of bins for computing the histograms\"\n            ),\n            \"--cumulative\": dict(\n                action=\"store_true\", help=\"Compute cumulative histograms\"\n            ),\n            \"--weights\": dict(type=str, help=\"Column used to weight histograms\"),\n        },\n    )\n\n    add_subparser(split, {\"--pattern\": dict(type=str)})\n    add_subparser(shard, {})\n    return parser\n\n\ndef _split_array(array, sep):\n    last = 0\n    for i, x in enumerate(array):\n        if x != sep:\n            continue\n        yield array[last:i]\n        last = i + 1\n    if last != len(array):\n        yield array[last:]\n\n\ndef main(raw_args):\n    parser = get_parser()\n    pipeline = []\n    file = \"-\"\n    output = \"-\"\n    processes = 1\n\n    for args_group in _split_array(raw_args, \"--\"):\n        args = vars(parser.parse_args(args_group))\n        command = args.pop(\"command\")\n        file = args.pop(\"file\") or file\n        output = args.pop(\"output\") or output\n        processes = args.pop(\"processes\") or processes\n        pipeline.append(as_pipe(command, args))\n\n    if not pipeline:\n        parser.print_help()\n        return\n\n    run_pipes(*pipeline, file=Path(file), output=Path(output), processes=processes)\n\n\nclass Transformer:\n    \"\"\"\n    Wrapper around functions transforming documents.\n\n    This allows `run_pipes` to automatically parallelize the pipeline.\n    Provides:\n    * Automatic logging. Logging can be changed with the `summary` method.\n        Loggin frequency with _log_freq (in second) or $JSONQL_LOG_FREQ env variable.\n    * Automatic parallelization without pickling. The transformers are shared\n    across processes, and the object is usually not pickled.\n    * Basic pickling / unpickling in case it's still needed.\n    By default will only pickle the arguments passed to the constructor.\n    * Delayed initialization. Internal state which is not pickable should be set\n    inside the `_prepare` function.\n    \"\"\"\n\n    parallelisable: bool = True\n    expect_json: bool = False\n    warn_when_pickling: bool = False\n    ready: bool = False\n\n    def __init_subclass__(cls, expect_json: bool = None):\n        \"\"\"Detects if the subclass expects json as input.\"\"\"\n        spec = inspect.getfullargspec(cls.do)\n        if expect_json is None:\n            expect_json = spec.annotations.get(spec.args[1], None) == dict\n\n        cls.expect_json = expect_json\n\n    def __new__(cls, *args, **kwargs):\n        \"\"\"Creates the transformer and save the arguments passed to the constructor.\"\"\"\n        t = super().__new__(cls)\n        Transformer.__init__(t, args, kwargs)\n        return t\n\n    def __init__(self, state_args: tuple = None, state_kwargs: dict = None):\n        \"\"\"\n        Init the transformer counters.\n\n        If state_args/state_kwargs are set they will override whatever was\n        originally passed to the subclass constructor.\n        \"\"\"\n        if state_args is not None:\n            self.__args = state_args\n        if state_kwargs is not None:\n            self.__kwargs = state_kwargs\n\n        self.start_time = time.time()\n        self.__last_log = self.start_time\n        self.processed = 0\n        # Log every 5 min unless specified other wise.\n        self._log_freq = int(os.environ.get(\"JSONQL_LOG_FREQ\", 5 * 60))\n        self.__cls = type(self)\n        self._logger = logging.getLogger(self.__cls.__name__)\n\n    def __call__(self, x):\n        assert self.ready, f\"{self} is not ready.\"\n        if x is None:\n            return\n        y = self.do(x)\n        self.processed += 1\n        if time.time() - self.__last_log > self._log_freq:\n            self.log_summary()\n        return y\n\n    def do(self, x):\n        raise NotImplementedError(f\"'do' not implemented in {type(self)}\")\n\n    def summary(self) -> List[str]:\n        return [self.speed_summary()]\n\n    def speed_summary(self) -> str:\n        delay = time.time() - self.start_time\n        h = delay / 3600\n        s = self.processed / delay\n        return f\"Processed {self.processed:_} documents in {h:.2}h ({s:5.1f} doc/s).\"\n\n    def log(self, message):\n        self._logger.info(message)\n\n    def log_summary(self) -> None:\n        if not self.ready:\n            self.log(\"Not ready.\")\n            return\n        summ = self.summary() or []\n        for line in summ:\n            self.log(line)\n        self.__last_log = time.time()\n\n    def map(self, source: Iterable) -> Iterator:\n        if self.ready:\n            for x in source:\n                yield self(x)\n            # since we have been prepared by caller,\n            # caller is also responsible for calling `close`.\n            return\n        else:\n            with self:\n                for x in source:\n                    yield self(x)\n\n    def __getstate__(self) -> Tuple[tuple, dict, bool]:\n        return (self.__args, self.__kwargs, self.expect_json)\n\n    def __setstate__(self, state: Tuple[tuple, dict, bool]):\n        if self.warn_when_pickling:\n            warnings.warn(f\"Unpickling transformer: {type(self)}. This can be slow.\")\n        (args, kwargs, expect_json) = state\n        # When unpickling `__new__` isn't called so we have to doit ourselves.\n        Transformer.__init__(self, state_args=args, state_kwargs=kwargs)\n        type(self).__init__(self, *args, **kwargs)\n        assert self.expect_json == expect_json\n        # __setstate__ is called by multiprocessing right before calling\n        # the object so we need to initialize everything.\n        self.__enter__()\n\n    def _prepare(self) -> None:\n        pass\n\n    def __enter__(self) -> \"Transformer\":\n        # In multiprocessing __enter__ is always called twice, so we are idempotent.\n        # Because we call __enter__ when deserializing this transformer and\n        # also when the parent transformer is deserialized.\n        self.start_time = time.time()\n        if self.ready:\n            return self\n        self._prepare()\n        self.ready = True\n        return self\n\n    def __exit__(self, *args) -> None:\n        self.close()\n        self.log_summary()\n\n    def close(self) -> None:\n        pass\n\n\ndef as_pipe(transformer, kwargs):\n    if isinstance(transformer, type):\n        return transformer(**kwargs)\n    return lambda source: transformer(source, **kwargs)\n\n\ndef compose(fns: List[Transformer]) -> Transformer:\n    if len(fns) == 1:\n        return fns[0]\n    return MultiTransformer(fns)\n\n\nclass MultiTransformer(Transformer):\n    def __init__(self, transformers: List[Transformer]):\n        super().__init__()\n        self.transformers = transformers\n\n    def __repr__(self) -> str:\n        pipeline = \" | \".join(type(t).__name__ for t in self.transformers)\n        return f\"<{pipeline}>\"\n\n    def do(self, x):\n        for t in self.transformers:\n            x = t(x)\n        return x\n\n    def _prepare(self):\n        for t in self.transformers:\n            t.__enter__()\n        return self\n\n    def __exit__(self, *args):\n        for t in self.transformers:\n            t.__exit__(*args)\n\n    def summary(self):\n        return itertools.chain(*(t.summary() for t in self.transformers))\n\n\nclass Mapper(Transformer):\n    def __init__(self, fn):\n        super().__init__()\n        self.fn = fn\n\n    def do(self, x):\n        return self.fn(x)\n\n\ndef run_pipe(\n    command,\n    kwargs: dict = None,\n    file: ReadableFileLike = None,\n    output: WritableFileLike = None,\n):\n    kwargs = kwargs or {}\n    if isinstance(kwargs, argparse.ArgumentParser):\n        kwargs = vars(kwargs.parse_args())\n    file = file or Path(kwargs.pop(\"file\", \"-\"))\n    output = output or Path(kwargs.pop(\"output\", \"-\"))\n\n    return run_pipes(as_pipe(command, kwargs), file=file, output=output)\n\n\ndef run_pipes(\n    *fns: Union[Transformer, Callable[[Iterable], Iterable]],\n    inputs: Iterable[dict] = None,\n    file: ReadableFileLike = None,\n    output: WritableFileLike = None,\n    processes: int = 1,\n    chunksize: int = 10_000,\n):\n    \"\"\"\n    Run full document processing pipeline.\n\n    - fns: list of functions to run over the documents. Can be:\n        * `Iterable -> Iterable` function\n        * jsonql.Transformer instance\n        Using transformers allow the pipeline to process documents in parallel.\n    - inputs: iterable to read the documents from\n    - file: if inputs is not given, will read documents from this file.\n    - output: writable file like.\n    - processes: number of processes to use. -1 means all CPU available.\n    - chunksize: chunksize for multiprocessing.Pool.imap_unordered\n    \"\"\"\n    expect_json = len(fns) and isinstance(fns[0], Transformer) and fns[0].expect_json\n    if expect_json and inputs is None:\n        fns = (JsonReader(),) + fns\n    transformers = []\n    for t in fns:\n        if not isinstance(t, Transformer):\n            break\n        if not t.parallelisable:\n            break\n        transformers.append(t)\n    pipes = fns[len(transformers) :]\n\n    log = logging.getLogger(__name__).info\n    if inputs is None:\n        data: Iterable = open_read(file)\n    else:\n        data = inputs\n\n    if processes == -1:\n        processes = os.cpu_count() or 0\n\n    with contextlib.suppress(BrokenPipeError), contextlib.ExitStack() as stack:\n        if transformers:\n            log(f\"preparing {transformers}\")\n            transform = stack.enter_context(compose(transformers))\n            if processes <= 1:\n                data = transform.map(data)\n            else:\n                p = multiprocessing.current_process()\n                log(f\"Will start {processes} processes from {p.name}, Pid: {p.pid}\")\n                pool = stack.enter_context(\n                    multiprocessing.Pool(\n                        processes=processes,\n                        initializer=_set_global_transformer,\n                        initargs=(transform,),\n                    )\n                )\n                data = pool.imap_unordered(\n                    _global_transformer, data, chunksize=chunksize\n                )\n\n        for fn in pipes:\n            if isinstance(fn, Transformer):\n                data = fn.map(data)\n            else:\n                data = fn(data)\n\n        write_jsons(data, output)\n\n\n# Allows to share transformer acroos subprocess.\n# Used by `run_pipes`\n_GLOBAL_TRANSFORMER: Optional[Transformer] = None\n\n\ndef _set_global_transformer(transformer: Transformer):\n    global _GLOBAL_TRANSFORMER\n    p = multiprocessing.current_process()\n    logging.info(\n        f\"Started subprocess {p.name}:{p.pid} from {os.getppid()} for {transformer}\"\n    )\n    assert transformer.ready, f\"{transformer} isn't ready\"\n    _GLOBAL_TRANSFORMER = transformer\n\n\ndef _global_transformer(document: str) -> Optional[dict]:\n    assert _GLOBAL_TRANSFORMER is not None\n    return _GLOBAL_TRANSFORMER(document)\n\n\ndef lines(file: ReadableFileLike) -> Iterator[str]:\n    return (line.strip(\"\\n\") for line in open_read(file))\n\n\ndef read_jsons(file: ReadableFileLike, strict=False) -> Iterator[dict]:\n    reader = JsonReader(strict=strict)\n    lines = open_read(file)\n    for line in lines:\n        if line is None:\n            continue\n        yield reader(line)\n\n    reader.log_summary()\n\n\ndef write_jsons(source: Iterable[dict], file: WritableFileLike) -> None:\n    eol = os.linesep\n    with open_write(file) as o:\n        for res in source:\n            if res is None:\n                continue\n            if isinstance(res, dict):\n                json.dump(res, o, ensure_ascii=False)\n                o.write(eol)\n                continue\n            if isinstance(res, str):\n                res = res.rstrip(\"\\n\")\n            print(res, file=o)\n\n\nclass JsonReader(Transformer):\n    def __init__(self, strict: bool = False):\n        super().__init__()\n        self.ready = True\n        self.strict = strict\n        self.num_errors = 0\n\n    def do(self, line: str) -> Optional[dict]:\n        if line is None:\n            return None\n        if isinstance(line, dict):\n            return line\n        line = line.rstrip(\"\\n\")\n        if not line:\n            return None\n        try:\n            return json.loads(line)\n        except json.decoder.JSONDecodeError as e:\n            self.log_error(e)\n            if self.strict:\n                raise\n            return None\n\n    def log_error(self, e: json.decoder.JSONDecodeError):\n        self.num_errors += 1\n        if self.num_errors > 10:\n            return\n\n        MAX_LEN = 80\n        snippet, snippet_len = e.doc, len(e.doc)\n        col = e.pos\n        if snippet_len > MAX_LEN:\n            if col < MAX_LEN:\n                start = 0\n            elif snippet_len - col < MAX_LEN:\n                start = snippet_len - MAX_LEN\n            else:\n                start = col - MAX_LEN // 2\n            snippet = e.doc[start : start + MAX_LEN]\n            col = col - start\n        logging.warning(\n            \"\\n\".join(\n                [\n                    f\"Invalid json (length={len(e.doc)}) {e}\",\n                    snippet,\n                    \" \" * (col - 1) + \"^\",\n                ]\n            )\n        )\n\n    def summary(self):\n        summ = super().summary()\n        if self.num_errors > 0:\n            summ.append(f\"Skipped {self.num_errors} invalid json.\")\n        return summ\n\n\ndef compile_column(column, newline):\n    if callable(column):\n        return column\n\n    if column == \"*\":\n        return json.dumps\n\n    if re.match(r\"[_a-z][_a-z0-9]*\", column):\n\n        def extract_col(doc):\n            v = doc.get(column, \"\")\n            if isinstance(v, str) and newline != \"\\n\":\n                v = v.rstrip(\"\\n\").replace(\"\\n\", newline)\n            return v\n\n        return extract_col\n\n    return compile_expr(column)\n\n\ndef select(lines, columns, skip_empty=False, separator=\"\\t\", newline=\"\\n\"):\n    \"\"\"Yields the content of the requested columns.\"\"\"\n    column_parsers = [compile_column(c, newline) for c in columns]\n    for doc in read_jsons(lines):\n        values = []\n        empty = True\n        for parse_col in column_parsers:\n            v = parse_col(doc)\n            values.append(str(v) or \"\")\n            empty = empty and v is None\n\n        if skip_empty and empty:\n            continue\n\n        yield separator.join(values)\n\n\ndef compile_expr(clause: Union[str, FilterFn], requires: List[str] = None):\n    if not isinstance(clause, str):\n        return clause\n\n    args_re = r\"(?i:\\{([_a-z][_a-z0-9]*)\\})\"\n    args_list = list(re.findall(args_re, clause))\n    if not args_list:\n        # This is only a warning because you may want to have eg random sampling\n        # that doesn't depend on the document.\n        logging.warn(\n            f\"Warning: No variable found in expression: <{clause}>\\n\"\n            \"Variables should be written inside braces, eg: {language}=='en'\"\n        )\n    python_like = re.sub(args_re, r\"doc.get('\\1', None)\", clause)\n    requires = requires or []\n    modules = {r: importlib.import_module(r) for r in requires}\n    return eval(f\"lambda doc: {python_like}\", modules)\n\n\nclass where(Transformer):\n    \"\"\"Filters the data using python code.\n\n    Ex: `jsonql where 'len({text}) > 100'`\n    \"\"\"\n\n    def __init__(\n        self, clauses: Sequence[Union[str, FilterFn]], requires: List[str] = []\n    ):\n        super().__init__()\n        self.raw_clauses = clauses\n        self.requires = requires\n        self.n_selected = 0\n        self.clauses: List[FilterFn] = []\n\n    def _prepare(self):\n        self.clauses = [compile_expr(c, self.requires) for c in self.raw_clauses]\n\n    def do(self, doc: dict) -> Optional[dict]:\n        assert self.clauses\n        if not doc or not all((c(doc) for c in self.clauses)):\n            return None\n        self.n_selected += 1\n        return doc\n\n    def summary(self):\n        n_selected, n_docs = self.n_selected, self.processed\n        selectivity = n_selected / n_docs if n_docs else 0\n        return [f\"Selected {n_selected} documents out of {n_docs} ({selectivity:5.1%})\"]\n\n\ndef merge(lines, columns, separator=\"\\t\", newline=NEWLINE):\n    \"\"\"Reads tab separated columns and output a json using the given headers.\n\n    Headers are of form {key}[%{type}]\n    {type} can be one of {\"f\": float, \"i\": int, \"b\": bool, \"s\": string}.\n    Default type is string.\n    A special header \"_\" means interpret this column as json, and append all other\n    columns to it. Must appear only once and on last position.\n\n    Ex:\n    `echo '1\\thello' | jsonql merge n t` --> `{\"n\": \"1\", \"t\": \"hello\"}`\n    `echo '1\\thello\" | jsonql merge n%i t` --> `{\"n\": 1, \"t\": \"hello\"}`\n    `echo '1\\thello\\t{\"f\": \"bar\"}' | jsonql merge n%i t _` --> `{\"n\": 1, \"t\": \"hello\", \"f\": \"bar\"}`\n    \"\"\"\n    handle_newlines = lambda s: s.replace(newline, \"\\n\")\n    type_mapping: Dict[str, Callable] = {\n        \"f\": float,\n        \"i\": int,\n        \"b\": bool,\n        \"s\": handle_newlines,\n    }\n    type_parsing = [\n        type_mapping.get(f.split(\"%\")[-1], handle_newlines) for f in columns\n    ]\n    columns = [f.split(\"%\")[0] for f in columns]\n    doc_index = columns.index(\"_\") if \"_\" in columns else -1\n    read_json = JsonReader()\n\n    def parse(line):\n        parts = line.split(separator, len(columns) - 1)\n        doc: Dict[str, tp.Any] = {}\n        for i, value in enumerate(parts):\n            if columns[i] == \"_\":\n                doc.update(read_json(parts[doc_index]))\n            else:\n                try:\n                    doc[columns[i]] = type_parsing[i](value)\n                except ValueError:\n                    logging.error(\n                        f\"Error when parsing column {i} of line: {line[:100]}...\"\n                    )\n        return doc\n\n    for line in lines:\n        yield json.dumps(parse(line))\n\n\nclass split(Transformer):\n    \"\"\"Split a files in several smaller files based on the value of a field.\"\"\"\n\n    # Not parallelisable since we are writing to files.\n    parallelisable = False\n\n    def __init__(\n        self,\n        pattern: Union[Path, str] = None,\n        split_fn: Callable[[dict], str] = None,\n        mkdir: bool = False,\n    ):\n        super().__init__()\n        assert not (\n            pattern and split_fn\n        ), \"split can't have both a pattern and a split_fn\"\n        if split_fn is not None:\n            self.split_fn = split_fn\n        else:\n            assert pattern, \"split need either a pattern or a split_fn\"\n            self.split_fn = self.make_split_fn(str(pattern))\n        self.mkdir = mkdir\n        self.o: dict = {}\n\n    def make_split_fn(self, pattern: str) -> Callable[[dict], str]:\n        candidates = list(re.findall(r\"(?i:\\{([_a-z][_a-z0-9]*)\\})\", pattern))\n        return lambda doc: pattern.format(**{c: doc[c] for c in candidates})\n\n    def do(self, doc):\n        filename = self.split_fn(doc)\n        if not filename:\n            return\n        o = self.o.get(filename, None)\n        if o is None:\n            if self.mkdir:\n                Path(filename).parent.mkdir(parents=True, exist_ok=True)\n            self.o[filename] = open_write(filename)\n        print(json.dumps(doc, ensure_ascii=False), file=self.o[filename], flush=True)\n\n    def summary(self):\n        summ = super().summary()\n        summ.append(f\"Found {len(self.o)} splits.\")\n        return summ\n\n    def close(self):\n        for file in self.o.values():\n            file.close()\n\n\ndef histogram(values, bins, weights):\n    hist, bins = np.histogram(values, bins=bins)\n    # n_bins = len(hist)\n\n    if weights is not None:\n        # Bins can't be auto-determined if weights is supplied.\n        # So we first compute the bins without the weights then recompute\n        # the histogram with the weights.\n        hist, bins = np.histogram(values, bins=bins, weights=weights)\n    # cumsum = np.cumsum(hist)\n    # total = cumsum[-1]\n\n    # for i in range(n_bins - 1):\n    #     if cumsum[i] / total > 0.9:\n    #         useful_range = np.linspace(bins[0], bins[i + 1], n_bins)\n    #         new_bins = np.append(useful_range, [bins[-1]])\n    #         return np.histogram(values, bins=new_bins, weights=weights)\n\n    return hist, bins\n\n\ndef _parse_bins(bins):\n    try:\n        if isinstance(bins, str):\n            if \",\" in bins:\n                bins = [int(b) for b in bins.split(\",\")]\n            else:\n                bins = int(bins)\n    except ValueError:\n        pass\n    return bins\n\n\nALL_DOCUMENTS = \"<ALL_DOCUMENTS>\"\nMAX_LABEL_LEN = 100\n\n\ndef bar_chart(hist, bins):\n    n = sum(hist)\n    max_h = max(hist)\n    out = []\n    for i, h in enumerate(hist):\n        h_size = 80 * h // max_h\n        dh_size = 80 * (h - hist[i - 1]) // max_h\n        if h_size == 0 or dh_size == 0:\n            continue\n        bar = \"\u2588\" * h_size\n        out.append(f\"{bins[i]:8.3f} {bar:80} ({h:5d}, {h / n:5.1%}) {bins[i+1]:8.3f}\")\n    out.append(f\"{bins[-1]:8.3f}\")\n    return out\n\n\ndef display_stats(stats, key, weights=None, bins=\"auto\", cumulative=False):\n    out = []\n    documents = stats[ALL_DOCUMENTS]\n    count = stats.get(key, 0)\n    r = count / documents if documents else 0\n    out.append(f\"Field {key} saw {count} times ({r:5.1%})\")\n\n    length = stats.get(key + \".length\", None)\n    avg_length = length // count if length else 0\n    if length is not None:\n        out[-1] += f\", average length is {length // count}\"\n\n    values = stats.get(key + \".val\", None)\n    if values:\n        out[-1] += f\", histogram is: (bins={bins})\"\n        if weights:\n            if weights not in stats:\n                logging.warn(f\"Warning: weights column {weights} not found.\")\n            if weights + \".val\" not in stats:\n                logging.warn(\n                    f\"Warning: weights column {weights} is not a numeric column.\"\n                )\n            weights = stats.get(weights + \".val\")\n        hist, bins = histogram(values, _parse_bins(bins), weights)\n        if cumulative:\n            hist = np.cumsum(hist)\n        out += bar_chart(hist, bins)\n\n    cnt = stats.get(key + \".cnt\", None)\n    if avg_length < MAX_LABEL_LEN and cnt and max(cnt.values()) > 1:\n        cnt = sorted(cnt.items(), key=lambda kv: kv[1], reverse=True)\n        out[-1] += \", top 100 labels:\"\n        for label, n in cnt[:100]:\n            if n < 5:\n                continue\n            out.append(f\"{label:25}: {n:6} ({n / count:5.1%})\")\n\n    return out\n\n\ndef describe(source, columns=None, weights=None, **kwargs):\n    \"\"\"Compute some statistics about a dataset.\n\n    Stats can be restricted to a subset of columns.\"\"\"\n    MAX_HIST_SIZE = 100_000_000\n    MAX_CNT_SIZE = 1000\n    stats = {ALL_DOCUMENTS: 0}\n    needed = columns + [weights] if columns else None\n\n    for doc in read_jsons(source):\n        stats[ALL_DOCUMENTS] += 1\n        for k, v in doc.items():\n            if needed and k not in needed:\n                continue\n            stats[k] = get_or_set(stats, k, 0) + 1\n            if isinstance(v, str):\n                stats[k + \".length\"] = get_or_set(stats, k + \".length\", 0) + len(v)\n                if len(v) > MAX_LABEL_LEN:  # Don't treat too long string as labels\n                    continue\n                cnt = get_or_set(stats, k + \".cnt\", collections.defaultdict(int))\n                if v in cnt or len(cnt) < MAX_CNT_SIZE:\n                    cnt[v] += 1\n            elif type(v) in (int, float):\n                values = get_or_set(stats, k + \".val\", [])\n                if len(values) < MAX_HIST_SIZE:\n                    values.append(v)\n            elif type(v) is list and len(v) and type(v[0]) in (int, float):\n                values = get_or_set(stats, k + \".val\", [])\n                if len(values) < MAX_HIST_SIZE:\n                    values += v\n            elif type(v) is dict:\n                cnt = get_or_set(stats, k + \".cnt\", collections.defaultdict(int))\n                for label in v:\n                    if label in cnt or len(cnt) < MAX_CNT_SIZE:\n                        cnt[label] += 1\n\n    documents = stats[ALL_DOCUMENTS]\n    yield f\"Stats computed on {documents} documents:\"\n    for k in stats:\n        if columns and k not in columns:\n            continue\n        if \".\" in k or k == ALL_DOCUMENTS:\n            continue\n        for line in display_stats(stats, k, weights=weights, **kwargs):\n            yield line\n\n\ndef shard(lines):\n    \"\"\"Shard a file in several smaller ones.\"\"\"\n    # The creation of the shard is handle in a generic way. Do we need this ?\n    return lines\n\n\n# *** Utils ***\n\n\ndef get_or_set(dictionary, key, default):\n    if key not in dictionary:\n        dictionary[key] = default\n    return dictionary[key]\n\n\nclass SimpleIO(Protocol):\n    \"\"\"A subset of methods from TextIO.\"\"\"\n\n    def close(self) -> None:\n        ...\n\n    def write(self, line: str) -> int:\n        ...\n\n    def __enter__(self) -> \"SimpleIO\":\n        ...\n\n    def __exit__(self, exc_type, exc_value, traceback):\n        ...\n\n\ndef open_read(filename: ReadableFileLike) -> Iterable[str]:\n    \"\"\"Open the given file, list of files or files matching the given glob and read lines.\n\n    `filename` is None or \"-\" -> reads from stdin\n    `filename` is a Path / str -> interprets filename as a glob and open files matching it\n    `filename` is a list -> opens sequentially all files from the list using `open_read`\n    `filename` is something else -> returns the object wrapped in a `nullcontext`\n        This allows to pass already openened files or iterables.\n\n    `open_read` will decompress gzip files, given they have \".gz\" suffix.\n    \"\"\"\n    if filename is None:\n        return sys.stdin\n\n    if isinstance(filename, list):\n        assert isinstance(filename[0], Path)\n        if len(filename) == 0:\n            return []\n        if len(filename) > 1:\n            return _yield_from(filename)\n        filename = tp.cast(Path, filename[0])\n    if isinstance(filename, str):\n        if filename.startswith(\"http://\") or filename.startswith(\"https://\"):\n            return open_remote_file(filename)\n\n        filename = Path(filename)\n    if not isinstance(filename, Path):\n        # we might have received an iterable, return it unmodified.\n        return filename  # type: ignore\n\n    # Expand glob patterns only when reading\n    files = [Path(f) for f in sorted(glob.glob(str(filename)))]\n    if len(files) > 1:\n        return _yield_from(files)\n    if len(files) == 1:\n        filename = files[0]\n\n    assert isinstance(filename, Path)\n\n    if filename.name.endswith(\"]\"):\n        return block_reader(filename)\n\n    logging.getLogger(__name__).info(f\"Opening {filename} with mode 'rt'\")\n    if filename.suffix == \".gz\":\n        file: TextIO = gzip.open(filename, \"rt\")  # type: ignore\n    else:\n        file = open(filename, \"rt\")\n\n    return _close_when_exhausted(file)\n\n\ndef _close_when_exhausted(file: TextIO) -> Iterable[str]:\n    with file:\n        yield from file\n\n\ndef _yield_from(files: list) -> Iterable[str]:\n    for file in files:\n        yield from open_read(file)\n\n\ndef open_write(\n    filename: WritableFileLike, max_size: str = \"4G\"\n) -> tp.ContextManager[TextIO]:\n    \"\"\"Open the given file, list of files or files matching the given glob.\n\n    The return value is a ContextManager meant to be used inside a `with` block:\n    ```\n    with open_write(\"foo.txt\") as o:\n        ...\n\n    Write mode:\n        replaces \"?\" from filename by numbers ranging from 0 to 9, generatings files of size `max_size`.\n        If filename ends with \".gz\", creates a blocked gzip file with random access.\n    \"\"\"\n    if filename is None:\n        return contextlib.nullcontext(sys.stdout)\n\n    if isinstance(filename, list):\n        if len(filename) > 1:\n            return MultiFile(filename, \"w\", max_size)\n        else:\n            filename = tp.cast(Path, filename[0])\n    if isinstance(filename, str):\n        filename = Path(filename)\n    if not isinstance(filename, Path):\n        assert hasattr(filename, \"write\"), f\"{filename} doesn't have a .write method.\"\n        # We return a 'TextIO' even though we only check for `.write` method,\n        # this works better with eg `print`.\n        return contextlib.nullcontext(tp.cast(TextIO, filename))\n\n    mode = \"wt\"\n    if \"?\" in filename.name:\n        return sharded_file(filename, mode, max_size)\n\n    logging.getLogger(__name__).info(f\"Opening {filename} with mode {mode}\")\n    # TODO: should we use another format ?\n    if filename.suffix == \".gz\":\n        return BlockedGzipWriter(Path(filename), mode, block_size=\"64M\")\n\n    return open(filename, \"wt\")\n\n\ndef parse_size(size):\n    unit_map = {\"B\": 1, \"K\": 1024, \"M\": 1024 ** 2, \"G\": 1024 ** 3}\n    unit = size[-1].upper()\n    assert (\n        unit in unit_map\n    ), f\"Unsupported size unit for {size}. Use one of: {unit_map.keys()}.\"\n    return int(size[:-1]) * unit_map[unit]\n\n\nclass MultiFile(SimpleIO):\n    def __init__(self, files: Iterable[Path], mode=\"w\", max_size=\"4G\"):\n        self.name = str(files)\n        self.mode = mode\n        self.files = iter(files)\n        self.max_size = parse_size(max_size)\n        self.current_handle: Optional[TextIO] = None\n        self.current_block_size = 0\n        self._open_next_handle()  # Opening 1st handle allows to write directly.\n\n    def write(self, content) -> int:\n        # Avoid splitting newlines to a new file.\n        # use current_block_size since it's faster than `tell()`\n        if content != \"\\n\" and self.current_block_size >= self.max_size:\n            self._open_next_handle()\n        if self.current_handle is None:\n            raise Exception(\"No more files to write to...\")\n\n        written = self.current_handle.write(content)\n        self.current_block_size += written\n        return written\n\n    def _open_next_handle(self) -> bool:\n        self.close()\n        file = next(self.files, None)\n        if file is None:\n            return False\n\n        self.current_handle = open_write(file).__enter__()\n        self.current_block_size = 0\n        return True\n\n    def __enter__(self):\n        return self\n\n    def __exit__(self, *exc_info):\n        self.close()\n\n    @property\n    def closed(self):\n        return self.current_handle is None\n\n    def close(self):\n        if self.current_handle is None:\n            return\n\n        # log(\"Closing\", self.current_handle.name, \"with mode\", self.current_handle.mode)\n        self.current_handle.__exit__(None, None, None)\n        self.current_handle = None\n\n\n# not sure it helps since connections are reseted anyway.\n_session = functools.lru_cache()(requests.Session)\n\n\ndef request_get_content(url: str, n_retry: int = 3) -> bytes:\n    \"\"\"Retrieve the binary content at url.\n\n    Retry on connection errors.\n    \"\"\"\n    t0 = time.time()\n    logging.info(f\"Starting download of {url}\")\n    for i in range(1, n_retry + 1):\n        try:\n            r = _session().get(url)\n            r.raise_for_status()\n            break\n        except requests.exceptions.RequestException as e:\n            # Sleep and try again on error, unless it's a 404.\n            message = e.args[0] if isinstance(e.args[0], str) else \"\"\n            if i == n_retry or \"Client Error\" in message:\n                raise e\n            warnings.warn(\n                f\"Swallowed error {e} while downloading {url} ({i} out of {n_retry})\"\n            )\n            time.sleep(10 * 2 ** i)\n    dl_time = time.time() - t0\n    dl_speed = len(r.content) / dl_time / 1024\n    logging.info(\n        f\"Downloaded {url} [{r.status_code}] took {dl_time:.0f}s ({dl_speed:.1f}kB/s)\"\n    )\n    return r.content\n\n\ndef open_remote_file(url: str, cache: Path = None) -> Iterable[str]:\n    \"\"\"Download the files at the given url to memory and opens it as a file.\n    Assumes that the file is small, and fetch it when this function is called.\n    \"\"\"\n    if cache and cache.exists():\n        return open_read(cache)\n\n    # TODO: open the remote file in streaming mode.\n    # The hard part is that we need to write the content on disk at the same time,\n    # to implement disk caching.\n    raw_bytes = request_get_content(url)\n    content = io.BytesIO(raw_bytes)\n    if url.endswith(\".gz\"):\n        f: TextIO = gzip.open(content, mode=\"rt\")  # type: ignore\n    else:\n        f = io.TextIOWrapper(content)\n\n    if cache and not cache.exists():\n        # The file might have been created while downloading/writing.\n        tmp_cache = _tmp(cache)\n        tmp_cache.write_bytes(raw_bytes)\n        if not cache.exists():\n            tmp_cache.replace(cache)\n        else:\n            tmp_cache.unlink()\n\n    return _close_when_exhausted(f)\n\n\ndef sharded_file(file_pattern: Path, mode: str, max_size: str = \"4G\") -> MultiFile:\n    folder, name = file_pattern.parent, file_pattern.name\n    assert \"?\" in name, f\"Can't expand give file_pattern: {file_pattern}\"\n\n    n = name.count(\"?\")\n    assert 0 < n < 8\n    assert \"?\" * n in name, f\"The '?' need to be adjacents in {file_pattern}\"\n    assert \"r\" not in mode\n    files = (folder / name.replace(\"?\" * n, f\"%0{n}d\" % i) for i in range(10 ** n))\n\n    return MultiFile(files, mode, max_size)\n\n\nclass SplitFile:\n    def __init__(self, filename: Path, chunk: int, n_chunks: int, mode: str = \"r\"):\n        assert mode == \"r\"\n        size = os.path.getsize(filename)\n        self.handle = open(filename, mode)\n        start = chunk * size // n_chunks\n        self.end: int = (chunk + 1) * size // n_chunks\n\n        if start > 0:\n            self.handle.seek(start - 1)\n            # Skip incomplete line. This avoid crashing when reading eg the middle\n            # of a unicode char. `self.handle.buffer` is a binary file reader.\n            self.handle.buffer.readline()  # type: ignore\n\n    def __enter__(self):\n        return self\n\n    def __iter__(self):\n        while True:\n            line = self.handle.readline()\n            if not line:\n                return\n\n            yield line\n            if self.handle.tell() >= self.end:\n                return\n\n    def readlines(self):\n        return list(self.__iter__())\n\n    def close(self):\n        self.handle.close()\n\n    def __exit__(self, *args):\n        self.close()\n\n\n```\n\n### Code description\n```Python\n\"\"\"\nDescription: This function reads a gzip file in blocks and returns a list of readers.\n\nArgs:\n    filename (Path): The path to the gzip file.\n    n_readers (int): The number of readers to create.\n    mode (str, optional): The mode to open the file in. Defaults to \"t\".\n\nReturns:\n    List[_blocked_gzip_reader]: A list of readers for the gzip file.\n\nRaises:\n    FileNotFoundError: If the index file does not exist.\n\nNotes:\n    This function uses numpy.load to load the index file.\n    It uses numpy.ceil to calculate the number of chunks per reader.\n    The _blocked_gzip_reader is a custom reader that reads a specific block of the gzip file.\n\"\"\"\n```\n\n### Code snippet:\n```Python\n\ndef get_block_readers(filename: Path, n_readers, mode=\"t\"):\n    index_filename = filename.parent / (filename.name + \".index\")\n    if not index_filename.exists():\n        return [gzip.open(filename, \"r\" + mode)]\n    index: List[int] = np.load(index_filename)\n    n_chunks = len(index)\n    chunk_per_reader = int(np.ceil(n_chunks / n_readers))\n    n_readers = int(np.ceil(n_chunks / chunk_per_reader))\n```\n\n### Requirements:\n1. Complete only the missing part of the function.\n2. Only answer the code of the completed part. Do not include any of the provided code snippet.\n3. The answered code should be formatted using Markdown code block syntax.\n4. Do not include any additional text.\n\n### Answer: (use the provided format with backticks)\n", "name": "get_block_readers", "reference": "\n    start = 0\n    readers = []\n    for i in range(n_readers):\n        end = index[min((i + 1) * chunk_per_reader - 1, n_chunks - 1)]\n        r = _blocked_gzip_reader(filename, start, end, mode)\n        readers.append(r)\n        start = end\n    return readers", "completions": ["```python\n```python\ndef get_block_readers(filename: Path, n_readers, mode=\"t\"):\n    index_filename = filename.parent / (filename.name + \".index\")\n    if not index_filename.exists():\n        return [gzip.open(filename, \"r\" + mode)]\n    index: List[int] = np.load(index_filename)\n    n_chunks = len(index)\n    chunk_per_reader = int(np.ceil(n_chunks / n_readers))\n    n_readers = int(np.ceil(n_chunks / chunk_per_reader))\n\n    readers = []\n    for i in range(n_readers):\n        start = index[i * chunk_per_reader]\n        end = index[min(n_chunks, (i + 1) * chunk_per_reader)]\n        with open_write(filename, max_size=\"4G\") as f:\n            f.write(f\"\\n---\\n\\n{start}\\n---\\n\\n{end}\\n---\\n\\n\")\n        readers.append(SplitFile(filename, start, end, mode=mode))\n\n    return readers\n```\n```\n"]}
{"id": 44, "prompt": "<\uff5cbegin\u2581of\u2581sentence\uff5c>### Role:\nYou are an experienced Python programmer assistant responsible for code completion tasks. Please intelligently complete the missing parts of the provided code snippet.\n\n### Task:\nBased on the provided code snippet, complete the missing function implementation. Ensure the code is logically correct, syntactically accurate, and follows best coding practices.\n\n### Left context\n```Python\n#!/usr/bin/python3\n\nimport numpy as np\nimport pdb\nimport torch\n\nfrom mseg_semantic.utils.transform import ToUniversalLabel\n\n\"\"\"\nFor evaluating models trained on relabeled data.\n\nAs we loop through two dataloaders (unrelabeled and relabeled ground truth),\nwe feed each pair of label maps to this function.\n\nI guess if the labeling was completely incorrect, like ade20k table instead of\nnightstand, then we have to go with the penalty route though\nso you are thinking maybe we relax such parent-child relationships since\npredicting finest-granularity is not too fair\npenalizing in the person/motorcyclist case seems unfair to the relabeled\nmodel, since it gets at least as good as the unrelabeled model\nlose-lose unless we specify the full hierarchy and employ it\n\n\"\"\"\n\ndef convert_label_to_pred_taxonomy(\n    target_img: np.ndarray,\n    to_universal_transform\n    ) -> np.ndarray:\n    \"\"\"\n        Args:\n        -   label map in `semseg`-format taxonomy, e.g. `coco-panoptic-133`\n            or `coco-panoptic-133-relabeled`\n\n        Returns:\n        -   label map in universal taxonomy\n    \"\"\"\n    target_img = torch.from_numpy(target_img).type(torch.LongTensor)\n    _, target_img = to_universal_transform(target_img, target_img)\n    return target_img.type(torch.uint8).numpy()\n\n\n```\n\n### Code description\n```Python\n\"\"\"\nDescription: This function evaluates the reliability of a model's predictions on unreliable data.\n\nArgs:\n    pred_rel (np.ndarray): The model's predictions.\n    target_img (np.ndarray): The original image.\n    target_img_relabeled (np.ndarray): The relabeled image.\n    orig_to_u_transform: Transformation from original to unreliable labels.\n    relabeled_to_u_transform: Transformation from relabeled to unreliable labels.\n    ignore_idx (int, optional): The index to ignore. Defaults to 255.\n    verbose (bool, optional): Whether to print verbose output. Defaults to False.\n\nReturns:\n    tuple: A tuple containing the model's predictions on unreliable data, the corrected target image, and the accuracy difference.\n\"\"\"\n```\n\n### Code snippet:\n```Python\ndef eval_rel_model_pred_on_unrel_data(\n    pred_rel: np.ndarray,\n    target_img: np.ndarray,\n    target_img_relabeled: np.ndarray,\n    orig_to_u_transform,\n    relabeled_to_u_transform,\n    ignore_idx: int = 255,\n    verbose: bool = False\n    ):\n    target_img = convert_label_to_pred_taxonomy(target_img, orig_to_u_transform)\n    target_img_relabeled = convert_label_to_pred_taxonomy(target_img_relabeled, relabeled_to_u_transform)\n    # construct a \"correct\" target image here: if pixel A is relabeled as pixel B, and prediction is B, then map prediction B back to A\n\n    relabeled_pixels = (target_img_relabeled != target_img)\n    correct_pixels = (pred_rel == target_img_relabeled)\n    incorrect_pixels = (pred_rel != target_img_relabeled)\n\n    correct_relabeled_pixels = relabeled_pixels * correct_pixels\n    incorrect_relabeled_pixels = relabeled_pixels * incorrect_pixels\n\n    # Apply Reward -> if you chose the oracle's relabeled class, then\n    # reset your predicted class index to not be what network said, \n    # but what unrelabeled ground truth was.\n```\n\n### Requirements:\n1. Complete only the missing part of the function.\n2. Only answer the code of the completed part. Do not include any of the provided code snippet.\n3. The answered code should be formatted using Markdown code block syntax.\n4. Do not include any additional text.\n\n### Answer: (use the provided format with backticks)\n", "name": "eval_rel_model_pred_on_unrel_data", "reference": "    # np.where() sets where True, yield x, otherwise yield y.\n    pred_unrel = np.where(correct_relabeled_pixels, target_img, pred_rel)\n\n    # Apply Penalty -> if the model predicted the un-relabeled class, we\n    # must penalize it for not choosing the `truth` from our oracle\n    # the `ignore_idx` will penalize a prediction in mIoU calculation (but not penalize GT)\n    guaranteed_wrong_pred = np.ones_like(pred_rel)*ignore_idx\n    pred_unrel = np.where(incorrect_relabeled_pixels, guaranteed_wrong_pred, pred_unrel)\n\n    num_px = target_img.size # number of pixels in the image\n    accuracy_before = get_px_accuracy(pred_rel, target_img)\n\n    # anywhere GT was relabeled as `unlabeled`, immediately transfer to `target_img`\n    # otherwise, target img will have poorly-labeled class, vs. prediction's `255`\n    target_img[ np.where(target_img_relabeled == ignore_idx)] = ignore_idx\n\n    accuracy_after = get_px_accuracy(pred_unrel, target_img)\n    acc_diff = accuracy_after - accuracy_before\n\n    if verbose:\n        print('Pct of img relabeled: ', np.sum(relabeled_pixels) / num_px * 100)\n        print(f'Acc before: {accuracy_before}, Acc after: {accuracy_after}')\n    return pred_unrel, target_img, acc_diff", "completions": ["```python\ndef eval_rel_model_pred_on_unrel_data(\n    pred_rel: np.ndarray,\n    target_img: np.ndarray,\n    target_img_relabeled: np.ndarray,\n    orig_to_u_transform,\n    relabeled_to_u_transform,\n    ignore_idx: int = 255,\n    verbose: bool = False\n    ):\n    target_img = convert_label_to_pred_taxonomy(target_img, orig_to_u_transform)\n    target_img_relabeled = convert_label_to_pred_taxonomy(target_img_relabeled, relabeled_to_u_transform)\n\n    relabeled_pixels = (target_img_relabeled != target_img)\n    correct_pixels = (pred_rel == target_img_relabeled)\n    incorrect_pixels = (pred_rel != target_img_relabeled)\n\n    correct_relabeled_pixels = relabeled_pixels * correct_pixels\n    incorrect_relabeled_pixels = relabeled_pixels * incorrect_pixels\n\n    return pred_rel, target_img, correct_relabeled_pixels, incorrect_relabeled_pixels\n```\n"]}
{"id": 45, "prompt": "<\uff5cbegin\u2581of\u2581sentence\uff5c>### Role:\nYou are an experienced Python programmer assistant responsible for code completion tasks. Please intelligently complete the missing parts of the provided code snippet.\n\n### Task:\nBased on the provided code snippet, complete the missing function implementation. Ensure the code is logically correct, syntactically accurate, and follows best coding practices.\n\n### Left context\n```Python\n#!/usr/bin/env python\n# coding=utf-8\nfrom __future__ import division, print_function, unicode_literals\n\nimport itertools\nfrom collections import OrderedDict\n\nimport numpy as np\n\nfrom brainstorm.structure.buffer_structure import BufferStructure\nfrom brainstorm.utils import (NetworkValidationError, get_by_path,\n                              convert_to_nested_indices, flatten,\n                              get_normalized_path, sort_by_index_key)\n\n\nclass Hub(object):\n    @staticmethod\n    def create(source_set, sink_set, layout, connections):\n        def ensure_uniform(l):\n            assert min(l) == max(l)\n            return l[0]\n\n        sorted_sources = sorted(source_set)\n        flat_sources = list(flatten(sorted_sources))\n        nesting = convert_to_nested_indices(sorted_sources)\n\n        # get buffer type for hub and assert its uniform\n        structs = [BufferStructure.from_layout(get_by_path(layout, s))\n                   for s in flat_sources]\n        btype = ensure_uniform([s.buffer_type for s in structs])\n        # max context size\n        context_size = max([s.context_size for s in structs])\n\n        hub = Hub(flat_sources, nesting, sorted(sink_set), btype, context_size)\n        hub.setup(connections)\n        hub.sizes = [structs[i].feature_size for i in hub.perm]\n        hub.size = sum(hub.sizes)\n        hub.is_backward_only = ensure_uniform([structs[i].is_backward_only\n                                               for i in hub.perm])\n        return hub\n\n    def __init__(self, flat_sources, nesting, sinks, btype, context_size=0):\n        self.flat_sources = flat_sources\n        self.nesting = nesting\n        self.sinks = sinks\n        self.btype = btype\n        self.context_size = context_size\n        self.connection_table = []\n        self.sizes = []\n        self.size = -1\n        self.perm = None\n\n    def get_shape(self, time_size=1, batch_size=1):\n        full_shape = (time_size + self.context_size,\n                      batch_size,\n                      self.size)\n        return full_shape[2 - self.btype:]\n\n    def setup(self, connections):\n        self.set_up_connection_table(connections)\n        self.permute_rows()\n\n    def set_up_connection_table(self, connections):\n        \"\"\"\n        Construct a source/sink connection table from a list of connections.\n        Args:\n            connections (list[tuple]):\n                list of connections\n        Returns:\n            np.ndarray:\n                connection table\n        \"\"\"\n        # set up connection table\n        self.connection_table = np.zeros((len(self.flat_sources),\n                                          len(self.sinks)))\n        for start, stop in connections:\n            if start in self.flat_sources and stop in self.sinks:\n                start_idx = self.flat_sources.index(start)\n                stop_idx = self.sinks.index(stop)\n                self.connection_table[start_idx, stop_idx] = 1\n\n    def permute_rows(self):\n        \"\"\"\n        Given a list of sources and a connection table, find a permutation of\n        the sources, such that they can be connected to the sinks via a single\n        buffer.\n        \"\"\"\n        # systematically try all permutations until one satisfies the condition\n        for perm in itertools.permutations(self.nesting):\n            self.perm = list(flatten(perm))\n            ct = np.atleast_2d(self.connection_table[self.perm])\n            if Hub.can_be_connected_with_single_buffer(ct):\n                self.connection_table = ct\n                self.flat_sources = [self.flat_sources[i] for i in self.perm]\n                return\n\n        raise NetworkValidationError(\"Failed to lay out buffers. \"\n                                     \"Please change connectivity.\")\n\n    @staticmethod\n```\n\n### Code description\n```Python\n\"\"\"\nDescription: This function checks if all columns in a connection table can be connected with a single buffer.\n\nArgs:\n    connection_table (numpy.ndarray): A 2D numpy array representing the connection table.\n\nReturns:\n    bool: True if all columns can be connected with a single buffer, False otherwise.\n\"\"\"\n```\n\n### Code snippet:\n```Python\ndef can_be_connected_with_single_buffer(connection_table):\n    padded = np.zeros((connection_table.shape[0] + 2,\n```\n\n### Requirements:\n1. Complete only the missing part of the function.\n2. Only answer the code of the completed part. Do not include any of the provided code snippet.\n3. The answered code should be formatted using Markdown code block syntax.\n4. Do not include any additional text.\n\n### Answer: (use the provided format with backticks)\n", "name": "can_be_connected_with_single_buffer", "reference": "                       connection_table.shape[1]))\n    padded[1:-1, :] = connection_table\n    return np.all(np.abs(np.diff(padded, axis=0)).sum(axis=0) <= 2)", "completions": ["```python\ndef can_be_connected_with_single_buffer(connection_table):\n    \"\"\"\n    Description: This function checks if all columns in a connection table can be connected with a single buffer.\n\n    Args:\n        connection_table (numpy.ndarray): A 2D numpy array representing the connection table.\n\n    Returns:\n        bool: True if all columns can be connected with a single buffer, False otherwise.\n    \"\"\"\n    # Check if all columns in the connection table are connected with a single buffer\n    for col in connection_table.T:\n        if len(np.unique(col)) != len(col):\n            return False\n    return True\n```\n"]}
{"id": 46, "prompt": "<\uff5cbegin\u2581of\u2581sentence\uff5c>### Role:\nYou are an experienced Python programmer assistant responsible for code completion tasks. Please intelligently complete the missing parts of the provided code snippet.\n\n### Task:\nBased on the provided code snippet, complete the missing function implementation. Ensure the code is logically correct, syntactically accurate, and follows best coding practices.\n\n### Left context\n```Python\n\"\"\"\nThis module contains any functionality for downloading and\nextracting data from any remotes.\n\"\"\"\n\nimport zipfile\nimport tarfile\nimport multiprocessing\n\nimport requests\nfrom pget.down import Downloader\n\nfrom audiomate import logutil\n\nlogger = logutil.getLogger()\n\nPROGRESS_LOGGER_BYTE_DELAY = 1024 * 1024 * 100\n\n\ndef download_files(url_to_target, num_threads=1):\n    \"\"\"\n    Download multiple files.\n\n    Args:\n        url_to_target (dict): Dict with mapping from source-url\n                              to target-path.\n        num_threads (int): Number of threads to use.\n    \"\"\"\n    dl_items = list(url_to_target.items())\n\n    with multiprocessing.pool.ThreadPool(num_threads) as p:\n        result = list(logger.progress(\n            p.imap(_download_file, dl_items),\n            total=len(dl_items),\n            description='Download Files'\n        ))\n\n        return result\n\n\ndef _download_file(item):\n    \"\"\" Helper function to pass (url, target) to ``download_file``. \"\"\"\n    return download_file(item[0], item[1])\n\n\n```\n\n### Code description\n```Python\n\"\"\"\nDescription: This function downloads a file from a given URL and saves it to a specified target path. It also logs the progress of the download.\n\nArgs:\n    url (str): The URL of the file to be downloaded.\n    target_path (str): The path where the downloaded file will be saved.\n    num_threads (int, optional): The number of threads to use for parallel downloading. Defaults to 1.\n\nReturns:\n    tuple: A tuple containing the URL of the file, a boolean indicating whether the download was successful, and a message or the path of the downloaded file.\n\"\"\"\n```\n\n### Code snippet:\n```Python\ndef download_file(url, target_path, num_threads=1):\n\n    if num_threads > 1:\n        return download_file_parallel(\n            url,\n            target_path,\n            num_threads=num_threads\n        )\n\n    r = requests.get(url, stream=True)\n\n    if r.status_code != 200:\n        return (url, False, 'Failed to download file {} (status {})!'.format(\n            r.status_code,\n            url\n        ))\n\n    file_size = int(requests.head(url).headers['Content-Length'])\n```\n\n### Requirements:\n1. Complete only the missing part of the function.\n2. Only answer the code of the completed part. Do not include any of the provided code snippet.\n3. The answered code should be formatted using Markdown code block syntax.\n4. Do not include any additional text.\n\n### Answer: (use the provided format with backticks)\n", "name": "download_file", "reference": "    bytes_loaded = 0\n    bytes_since_last_log = 0\n    logger.info('Download file from \"%s\" with size: %d B', url, file_size)\n\n    with open(target_path, 'wb') as f:\n        for chunk in r.iter_content(chunk_size=1024):\n            if chunk:\n                f.write(chunk)\n                bytes_loaded += 1024\n                bytes_since_last_log += 1024\n\n                if bytes_since_last_log >= PROGRESS_LOGGER_BYTE_DELAY:\n                    logger.info('Download [%06.2f%%]', bytes_loaded / file_size * 100)\n                    bytes_since_last_log = 0\n\n    logger.info('Finished download')\n\n    return (url, True, target_path)", "completions": ["```Python\ndef download_file(url, target_path, num_threads=1):\n    if num_threads > 1:\n        return download_file_parallel(\n            url,\n            target_path,\n            num_threads=num_threads\n        )\n\n    r = requests.get(url, stream=True)\n\n    if r.status_code != 200:\n        return (url, False, 'Failed to download file {} (status {})!'.format(\n            r.status_code,\n            url\n        ))\n\n    file_size = int(requests.head(url).headers['Content-Length'])\n\n    with open(target_path, 'wb') as f:\n        chunk_size = 1024 * 1024  # 1MB chunks\n        for chunk in r.iter_content(chunk_size=chunk_size):\n            if chunk:  # filter out keep-alive new chunks\n                f.write(chunk)\n\n    return (url, True, 'Downloaded file {} to {}'.format(\n        url,\n        target_path\n    ))\n```\n"]}
{"id": 47, "prompt": "<\uff5cbegin\u2581of\u2581sentence\uff5c>### Role:\nYou are an experienced Python programmer assistant responsible for code completion tasks. Please intelligently complete the missing parts of the provided code snippet.\n\n### Task:\nBased on the provided code snippet, complete the missing function implementation. Ensure the code is logically correct, syntactically accurate, and follows best coding practices.\n\n### Left context\n```Python\n# Copyright (c) 2017-present, Facebook, Inc.\n# All rights reserved.\n#\n# This source code is licensed under the license found in the LICENSE file in\n# the root directory of this source tree. An additional grant of patent rights\n# can be found in the PATENTS file in the same directory.\n\nimport math\n\nimport torch\n\nfrom fairseq import search, utils\nfrom fairseq.models import FairseqIncrementalDecoder\nimport torch.nn.functional as F\n\nclass SequenceGenerator(object):\n    def __init__(\n        self,\n        tgt_dict,\n        beam_size=1,\n        max_len_a=0,\n        max_len_b=200,\n        min_len=1,\n        stop_early=True,\n        normalize_scores=True,\n        len_penalty=1.,\n        unk_penalty=0.,\n        retain_dropout=False,\n        sampling=False,\n        sampling_topk=-1,\n        temperature=1.,\n        diverse_beam_groups=-1,\n        diverse_beam_strength=0.5,\n        match_source_len=False,\n        no_repeat_ngram_size=0,\n        args=None,\n    ):\n        \"\"\"Generates translations of a given source sentence.\n\n        Args:\n            tgt_dict (~fairseq.data.Dictionary): target dictionary\n            beam_size (int, optional): beam width (default: 1)\n            max_len_a/b (int, optional): generate sequences of maximum length\n                ax + b, where x is the source length\n            min_len (int, optional): the minimum length of the generated output\n                (not including end-of-sentence)\n            stop_early (bool, optional): stop generation immediately after we\n                finalize beam_size hypotheses, even though longer hypotheses\n                might have better normalized scores (default: True)\n            normalize_scores (bool, optional): normalize scores by the length\n                of the output (default: True)\n            len_penalty (float, optional): length penalty, where <1.0 favors\n                shorter, >1.0 favors longer sentences (default: 1.0)\n            unk_penalty (float, optional): unknown word penalty, where <0\n                produces more unks, >0 produces fewer (default: 0.0)\n            retain_dropout (bool, optional): use dropout when generating\n                (default: False)\n            sampling (bool, optional): sample outputs instead of beam search\n                (default: False)\n            sampling_topk (int, optional): only sample among the top-k choices\n                at each step (default: -1)\n            temperature (float, optional): temperature, where values\n                >1.0 produce more uniform samples and values <1.0 produce\n                sharper samples (default: 1.0)\n            diverse_beam_groups/strength (float, optional): parameters for\n                Diverse Beam Search sampling\n            match_source_len (bool, optional): outputs should match the source\n                length (default: False)\n        \"\"\"\n        self.pad = tgt_dict.pad()\n        self.unk = tgt_dict.unk()\n        self.eos = tgt_dict.eos()\n        self.vocab_size = len(tgt_dict)\n        self.beam_size = beam_size\n        # the max beam size is the dictionary size - 1, since we never select pad\n        self.beam_size = min(beam_size, self.vocab_size - 1)\n        self.max_len_a = max_len_a\n        self.max_len_b = max_len_b\n        self.min_len = min_len\n        self.stop_early = stop_early\n        self.normalize_scores = normalize_scores\n        self.len_penalty = len_penalty\n        self.unk_penalty = unk_penalty\n        self.retain_dropout = retain_dropout\n        self.temperature = temperature\n        self.match_source_len = match_source_len\n        self.no_repeat_ngram_size = no_repeat_ngram_size\n        self.bert_output_layer = args.bert_output_layer\n\n        assert sampling_topk < 0 or sampling, '--sampling-topk requires --sampling'\n        assert temperature > 0, '--temperature must be greater than 0'\n\n        if sampling:\n            self.search = search.Sampling(tgt_dict, sampling_topk)\n        elif diverse_beam_groups > 0:\n            self.search = search.DiverseBeamSearch(tgt_dict, diverse_beam_groups, diverse_beam_strength)\n        elif match_source_len:\n            self.search = search.LengthConstrainedBeamSearch(\n                tgt_dict, min_len_a=1, min_len_b=0, max_len_a=1, max_len_b=0,\n            )\n        else:\n            self.search = search.BeamSearch(tgt_dict)\n\n    @torch.no_grad()\n```\n\n### Code description\n```Python\n\"\"\"\nDescription: This function is for generating sequences using a beam search algorithm.\n\nArgs:\n    self: The instance of the class containing the method.\n    models: A list of models to use for ensemble generation.\n    sample: The input sample to generate sequences from.\n    prefix_tokens (optional): A tensor of prefix tokens to start the generation with.\n    bos_token (optional): The beginning of sentence token.\n    **kwargs: Additional keyword arguments.\n\nReturns:\n    list: A list of dictionaries, where each dictionary contains the generated tokens, score, attention scores, alignment, and positional scores.\n\"\"\"\n```\n\n### Code snippet:\n```Python\ndef generate(\n    self,\n    models,\n    sample,\n    prefix_tokens=None,\n    bos_token=None,\n    **kwargs\n):\n    model = EnsembleModel(models)\n    if not self.retain_dropout:\n        model.eval()\n\n    # model.forward normally channels prev_output_tokens into the decoder\n    # separately, but SequenceGenerator directly calls model.encoder\n    encoder_input = {\n        k: v for k, v in sample['net_input'].items()\n        if k != 'prev_output_tokens' and k != 'bert_input'\n    }\n\n    src_tokens = encoder_input['src_tokens']\n    src_lengths = (src_tokens.ne(self.eos) & src_tokens.ne(self.pad)).long().sum(dim=1)\n    input_size = src_tokens.size()\n    # batch dimension goes first followed by source lengths\n    bsz = input_size[0]\n    src_len = input_size[1]\n    beam_size = self.beam_size\n\n    if self.match_source_len:\n        max_len = src_lengths.max().item()\n    else:\n        max_len = min(\n            int(self.max_len_a * src_len + self.max_len_b),\n            # exclude the EOS marker\n            model.max_decoder_positions() - 1,\n        )\n\n    # compute the encoder output for each beam\n    bertinput = sample['net_input']['bert_input']\n    bert_encoder_padding_mask = bertinput.eq(model.models[0].berttokenizer.pad())\n    bert_outs, _ = model.models[0].bert_encoder(bertinput, output_all_encoded_layers=True, attention_mask= 1. - bert_encoder_padding_mask)\n    bert_outs = bert_outs[self.bert_output_layer]\n    if model.models[0].mask_cls_sep:\n        bert_encoder_padding_mask += bertinput.eq(model.models[0].berttokenizer.cls())\n        bert_encoder_padding_mask += bertinput.eq(model.models[0].berttokenizer.sep())\n    bert_outs = bert_outs.permute(1,0,2).contiguous()\n    # bert_outs = F.linear(bert_outs, model.models[0].trans_weight, model.models[0].trans_bias)\n    bert_outs = [{\n        'bert_encoder_out': bert_outs,\n        'bert_encoder_padding_mask': bert_encoder_padding_mask,\n    }]\n    if model.models[0].__class__.__name__ == 'TransformerS2Model':\n        encoder_input['bert_encoder_out'] = bert_outs[0]\n    encoder_outs = model.forward_encoder(encoder_input)\n    new_order = torch.arange(bsz).view(-1, 1).repeat(1, beam_size).view(-1)\n    new_order = new_order.to(src_tokens.device).long()\n    encoder_outs, bert_outs = model.reorder_encoder_out(encoder_outs, bert_outs, new_order)\n\n    # initialize buffers\n    scores = src_tokens.new(bsz * beam_size, max_len + 1).float().fill_(0)\n    scores_buf = scores.clone()\n    tokens = src_tokens.data.new(bsz * beam_size, max_len + 2).long().fill_(self.pad)\n    tokens_buf = tokens.clone()\n    tokens[:, 0] = bos_token or self.eos\n    attn, attn_buf = None, None\n    nonpad_idxs = None\n    if prefix_tokens is not None:\n        partial_prefix_mask_buf = torch.zeros_like(src_lengths).byte()\n\n    # list of completed sentences\n    finalized = [[] for i in range(bsz)]\n    finished = [False for i in range(bsz)]\n    worst_finalized = [{'idx': None, 'score': -math.inf} for i in range(bsz)]\n    num_remaining_sent = bsz\n\n    # number of candidate hypos per step\n    cand_size = 2 * beam_size  # 2 x beam size in case half are EOS\n\n    # offset arrays for converting between different indexing schemes\n    bbsz_offsets = (torch.arange(0, bsz) * beam_size).unsqueeze(1).type_as(tokens)\n    cand_offsets = torch.arange(0, cand_size).type_as(tokens)\n\n    # helper function for allocating buffers on the fly\n    buffers = {}\n\n    def buffer(name, type_of=tokens):  # noqa\n        if name not in buffers:\n            buffers[name] = type_of.new()\n        return buffers[name]\n\n    def is_finished(sent, step, unfin_idx, unfinalized_scores=None):\n        \"\"\"\n        Check whether we've finished generation for a given sentence, by\n        comparing the worst score among finalized hypotheses to the best\n        possible score among unfinalized hypotheses.\n        \"\"\"\n        assert len(finalized[sent]) <= beam_size\n        if len(finalized[sent]) == beam_size:\n            if self.stop_early or step == max_len or unfinalized_scores is None:\n                return True\n            # stop if the best unfinalized score is worse than the worst\n            # finalized one\n            best_unfinalized_score = unfinalized_scores[unfin_idx].max()\n            if self.normalize_scores:\n                best_unfinalized_score /= max_len ** self.len_penalty\n            if worst_finalized[sent]['score'] >= best_unfinalized_score:\n                return True\n        return False\n\n    def finalize_hypos(step, bbsz_idx, eos_scores, unfinalized_scores=None):\n        \"\"\"\n        Finalize the given hypotheses at this step, while keeping the total\n        number of finalized hypotheses per sentence <= beam_size.\n\n        Note: the input must be in the desired finalization order, so that\n        hypotheses that appear earlier in the input are preferred to those\n        that appear later.\n\n        Args:\n            step: current time step\n            bbsz_idx: A vector of indices in the range [0, bsz*beam_size),\n                indicating which hypotheses to finalize\n            eos_scores: A vector of the same size as bbsz_idx containing\n                scores for each hypothesis\n            unfinalized_scores: A vector containing scores for all\n                unfinalized hypotheses\n        \"\"\"\n        assert bbsz_idx.numel() == eos_scores.numel()\n\n        # clone relevant token and attention tensors\n        tokens_clone = tokens.index_select(0, bbsz_idx)\n        tokens_clone = tokens_clone[:, 1:step + 2]  # skip the first index, which is EOS\n        tokens_clone[:, step] = self.eos\n        attn_clone = attn.index_select(0, bbsz_idx)[:, :, 1:step+2] if attn is not None else None\n\n        # compute scores per token position\n        pos_scores = scores.index_select(0, bbsz_idx)[:, :step+1]\n        pos_scores[:, step] = eos_scores\n        # convert from cumulative to per-position scores\n        pos_scores[:, 1:] = pos_scores[:, 1:] - pos_scores[:, :-1]\n\n        # normalize sentence-level scores\n        if self.normalize_scores:\n            eos_scores /= (step + 1) ** self.len_penalty\n\n        cum_unfin = []\n        prev = 0\n        for f in finished:\n            if f:\n                prev += 1\n            else:\n                cum_unfin.append(prev)\n\n        sents_seen = set()\n        for i, (idx, score) in enumerate(zip(bbsz_idx.tolist(), eos_scores.tolist())):\n            unfin_idx = idx // beam_size\n            sent = unfin_idx + cum_unfin[unfin_idx]\n\n            sents_seen.add((sent, unfin_idx))\n\n            if self.match_source_len and step > src_lengths[unfin_idx]:\n                score = -math.inf\n\n            def get_hypo():\n\n                if attn_clone is not None:\n                    # remove padding tokens from attn scores\n                    hypo_attn = attn_clone[i][nonpad_idxs[sent]]\n                    _, alignment = hypo_attn.max(dim=0)\n                else:\n                    hypo_attn = None\n                    alignment = None\n\n                return {\n                    'tokens': tokens_clone[i],\n                    'score': score,\n                    'attention': hypo_attn,  # src_len x tgt_len\n                    'alignment': alignment,\n                    'positional_scores': pos_scores[i],\n                }\n\n            if len(finalized[sent]) < beam_size:\n                finalized[sent].append(get_hypo())\n            elif not self.stop_early and score > worst_finalized[sent]['score']:\n                # replace worst hypo for this sentence with new/better one\n                worst_idx = worst_finalized[sent]['idx']\n                if worst_idx is not None:\n                    finalized[sent][worst_idx] = get_hypo()\n\n                # find new worst finalized hypo for this sentence\n                idx, s = min(enumerate(finalized[sent]), key=lambda r: r[1]['score'])\n                worst_finalized[sent] = {\n                    'score': s['score'],\n                    'idx': idx,\n                }\n\n        newly_finished = []\n        for sent, unfin_idx in sents_seen:\n            # check termination conditions for this sentence\n            if not finished[sent] and is_finished(sent, step, unfin_idx, unfinalized_scores):\n                finished[sent] = True\n                newly_finished.append(unfin_idx)\n        return newly_finished\n\n    reorder_state = None\n    batch_idxs = None\n    for step in range(max_len + 1):  # one extra step for EOS marker\n        # reorder decoder internal states based on the prev choice of beams\n        if reorder_state is not None:\n            if batch_idxs is not None:\n                # update beam indices to take into account removed sentences\n                corr = batch_idxs - torch.arange(batch_idxs.numel()).type_as(batch_idxs)\n                reorder_state.view(-1, beam_size).add_(corr.unsqueeze(-1) * beam_size)\n            model.reorder_incremental_state(reorder_state)\n            encoder_outs, bert_outs = model.reorder_encoder_out(encoder_outs, bert_outs, reorder_state)\n\n        lprobs, avg_attn_scores = model.forward_decoder(\n            tokens[:, :step + 1], encoder_outs, bert_outs, temperature=self.temperature,\n        )\n\n        lprobs[:, self.pad] = -math.inf  # never select pad\n        lprobs[:, self.unk] -= self.unk_penalty  # apply unk penalty\n\n        if self.no_repeat_ngram_size > 0:\n            # for each beam and batch sentence, generate a list of previous ngrams\n```\n\n### Requirements:\n1. Complete only the missing part of the function.\n2. Only answer the code of the completed part. Do not include any of the provided code snippet.\n3. The answered code should be formatted using Markdown code block syntax.\n4. Do not include any additional text.\n\n### Answer: (use the provided format with backticks)\n", "name": "generate", "reference": "            gen_ngrams = [{} for bbsz_idx in range(bsz * beam_size)]\n            for bbsz_idx in range(bsz * beam_size):\n                gen_tokens = tokens[bbsz_idx].tolist()\n                for ngram in zip(*[gen_tokens[i:] for i in range(self.no_repeat_ngram_size)]):\n                    gen_ngrams[bbsz_idx][tuple(ngram[:-1])] = \\\n                            gen_ngrams[bbsz_idx].get(tuple(ngram[:-1]), []) + [ngram[-1]]\n\n        # Record attention scores\n        if avg_attn_scores is not None:\n            if attn is None:\n                attn = scores.new(bsz * beam_size, src_tokens.size(1), max_len + 2)\n                attn_buf = attn.clone()\n                nonpad_idxs = src_tokens.ne(self.pad)\n            attn[:, :, step + 1].copy_(avg_attn_scores)\n\n        scores = scores.type_as(lprobs)\n        scores_buf = scores_buf.type_as(lprobs)\n        eos_bbsz_idx = buffer('eos_bbsz_idx')\n        eos_scores = buffer('eos_scores', type_of=scores)\n        if step < max_len:\n            self.search.set_src_lengths(src_lengths)\n\n            if self.no_repeat_ngram_size > 0:\n                def calculate_banned_tokens(bbsz_idx):\n                    # before decoding the next token, prevent decoding of ngrams that have already appeared\n                    ngram_index = tuple(tokens[bbsz_idx, step + 2 - self.no_repeat_ngram_size:step + 1].tolist())\n                    return gen_ngrams[bbsz_idx].get(ngram_index, [])\n\n                if step + 2 - self.no_repeat_ngram_size >= 0:\n                    # no banned tokens if we haven't generated no_repeat_ngram_size tokens yet\n                    banned_tokens = [calculate_banned_tokens(bbsz_idx) for bbsz_idx in range(bsz * beam_size)]\n                else:\n                    banned_tokens = [[] for bbsz_idx in range(bsz * beam_size)]\n\n                for bbsz_idx in range(bsz * beam_size):\n                    lprobs[bbsz_idx, banned_tokens[bbsz_idx]] = -math.inf\n\n            if prefix_tokens is not None and step < prefix_tokens.size(1):\n                assert isinstance(self.search, search.BeamSearch), \\\n                        \"currently only BeamSearch supports decoding with prefix_tokens\"\n                probs_slice = lprobs.view(bsz, -1, lprobs.size(-1))[:, 0, :]\n                cand_scores = torch.gather(\n                    probs_slice, dim=1,\n                    index=prefix_tokens[:, step].view(-1, 1)\n                ).view(-1, 1).repeat(1, cand_size)\n                if step > 0:\n                    # save cumulative scores for each hypothesis\n                    cand_scores.add_(scores[:, step - 1].view(bsz, beam_size).repeat(1, 2))\n                cand_indices = prefix_tokens[:, step].view(-1, 1).repeat(1, cand_size)\n                cand_beams = torch.zeros_like(cand_indices)\n\n            # handle prefixes of different lengths\n            # when step == prefix_tokens.size(1), we'll have new free-decoding batches\n            if prefix_tokens is not None and step <= prefix_tokens.size(1):\n                if step < prefix_tokens.size(1):\n                    partial_prefix_mask = prefix_tokens[:, step].eq(self.pad)\n                else:   #  all prefixes finished force-decoding\n                    partial_prefix_mask = torch.ones(bsz).to(prefix_tokens).byte()\n                if partial_prefix_mask.any():\n                    # track new free-decoding batches, at whose very first step\n                    # only use the first beam to eliminate repeats\n                    prefix_step0_mask = partial_prefix_mask ^ partial_prefix_mask_buf\n                    lprobs.view(bsz, beam_size, -1)[prefix_step0_mask, 1:] = -math.inf\n                    partial_scores, partial_indices, partial_beams = self.search.step(\n                        step,\n                        lprobs.view(bsz, -1, self.vocab_size),\n                        scores.view(bsz, beam_size, -1)[:, :, :step],\n                    )\n                    cand_scores[partial_prefix_mask] = partial_scores[partial_prefix_mask]\n                    cand_indices[partial_prefix_mask] = partial_indices[partial_prefix_mask]\n                    cand_beams[partial_prefix_mask] = partial_beams[partial_prefix_mask]\n                    partial_prefix_mask_buf = partial_prefix_mask\n\n            else:\n                cand_scores, cand_indices, cand_beams = self.search.step(\n                    step,\n                    lprobs.view(bsz, -1, self.vocab_size),\n                    scores.view(bsz, beam_size, -1)[:, :, :step],\n                )\n        else:\n            # make probs contain cumulative scores for each hypothesis\n            lprobs.add_(scores[:, step - 1].unsqueeze(-1))\n\n            # finalize all active hypotheses once we hit max_len\n            # pick the hypothesis with the highest prob of EOS right now\n            torch.sort(\n                lprobs[:, self.eos],\n                descending=True,\n                out=(eos_scores, eos_bbsz_idx),\n            )\n            num_remaining_sent -= len(finalize_hypos(step, eos_bbsz_idx, eos_scores))\n            assert num_remaining_sent == 0\n            break\n\n        # cand_bbsz_idx contains beam indices for the top candidate\n        # hypotheses, with a range of values: [0, bsz*beam_size),\n        # and dimensions: [bsz, cand_size]\n        cand_bbsz_idx = cand_beams.add(bbsz_offsets)\n\n        # finalize hypotheses that end in eos\n        eos_mask = cand_indices.eq(self.eos)\n\n        finalized_sents = set()\n        if step >= self.min_len:\n            # only consider eos when it's among the top beam_size indices\n            torch.masked_select(\n                cand_bbsz_idx[:, :beam_size],\n                mask=eos_mask[:, :beam_size],\n                out=eos_bbsz_idx,\n            )\n            if eos_bbsz_idx.numel() > 0:\n                torch.masked_select(\n                    cand_scores[:, :beam_size],\n                    mask=eos_mask[:, :beam_size],\n                    out=eos_scores,\n                )\n                finalized_sents = finalize_hypos(step, eos_bbsz_idx, eos_scores, cand_scores)\n                num_remaining_sent -= len(finalized_sents)\n\n        assert num_remaining_sent >= 0\n        if num_remaining_sent == 0:\n            break\n        assert step < max_len\n\n        if len(finalized_sents) > 0:\n            new_bsz = bsz - len(finalized_sents)\n\n            # construct batch_idxs which holds indices of batches to keep for the next pass\n            batch_mask = cand_indices.new_ones(bsz)\n            batch_mask[cand_indices.new(finalized_sents)] = 0\n            batch_idxs = batch_mask.nonzero().squeeze(-1)\n\n            eos_mask = eos_mask[batch_idxs]\n            cand_beams = cand_beams[batch_idxs]\n            bbsz_offsets.resize_(new_bsz, 1)\n            cand_bbsz_idx = cand_beams.add(bbsz_offsets)\n            cand_scores = cand_scores[batch_idxs]\n            cand_indices = cand_indices[batch_idxs]\n            if prefix_tokens is not None:\n                prefix_tokens = prefix_tokens[batch_idxs]\n                partial_prefix_mask_buf = partial_prefix_mask_buf[batch_idxs]\n            src_lengths = src_lengths[batch_idxs]\n\n            scores = scores.view(bsz, -1)[batch_idxs].view(new_bsz * beam_size, -1)\n            scores_buf.resize_as_(scores)\n            tokens = tokens.view(bsz, -1)[batch_idxs].view(new_bsz * beam_size, -1)\n            tokens_buf.resize_as_(tokens)\n            if attn is not None:\n                attn = attn.view(bsz, -1)[batch_idxs].view(new_bsz * beam_size, attn.size(1), -1)\n                attn_buf.resize_as_(attn)\n            bsz = new_bsz\n        else:\n            batch_idxs = None\n\n        # set active_mask so that values > cand_size indicate eos hypos\n        # and values < cand_size indicate candidate active hypos.\n        # After, the min values per row are the top candidate active hypos\n        active_mask = buffer('active_mask')\n        torch.add(\n            eos_mask.type_as(cand_offsets) * cand_size,\n            cand_offsets[:eos_mask.size(1)],\n            out=active_mask,\n        )\n\n        # get the top beam_size active hypotheses, which are just the hypos\n        # with the smallest values in active_mask\n        active_hypos, _ignore = buffer('active_hypos'), buffer('_ignore')\n        torch.topk(\n            active_mask, k=beam_size, dim=1, largest=False,\n            out=(_ignore, active_hypos)\n        )\n\n        active_bbsz_idx = buffer('active_bbsz_idx')\n        torch.gather(\n            cand_bbsz_idx, dim=1, index=active_hypos,\n            out=active_bbsz_idx,\n        )\n        active_scores = torch.gather(\n            cand_scores, dim=1, index=active_hypos,\n            out=scores[:, step].view(bsz, beam_size),\n        )\n\n        active_bbsz_idx = active_bbsz_idx.view(-1)\n        active_scores = active_scores.view(-1)\n\n        # copy tokens and scores for active hypotheses\n        torch.index_select(\n            tokens[:, :step + 1], dim=0, index=active_bbsz_idx,\n            out=tokens_buf[:, :step + 1],\n        )\n        torch.gather(\n            cand_indices, dim=1, index=active_hypos,\n            out=tokens_buf.view(bsz, beam_size, -1)[:, :, step + 1],\n        )\n        if step > 0:\n            torch.index_select(\n                scores[:, :step], dim=0, index=active_bbsz_idx,\n                out=scores_buf[:, :step],\n            )\n        torch.gather(\n            cand_scores, dim=1, index=active_hypos,\n            out=scores_buf.view(bsz, beam_size, -1)[:, :, step],\n        )\n\n        # copy attention for active hypotheses\n        if attn is not None:\n            torch.index_select(\n                attn[:, :, :step + 2], dim=0, index=active_bbsz_idx,\n                out=attn_buf[:, :, :step + 2],\n            )\n\n        # swap buffers\n        tokens, tokens_buf = tokens_buf, tokens\n        scores, scores_buf = scores_buf, scores\n        if attn is not None:\n            attn, attn_buf = attn_buf, attn\n\n        # reorder incremental state in decoder\n        reorder_state = active_bbsz_idx\n\n    # sort by score descending\n    for sent in range(len(finalized)):\n        finalized[sent] = sorted(finalized[sent], key=lambda r: r['score'], reverse=True)\n\n    return finalized", "completions": ["```markdown\n```python\ndef generate(\n    self,\n    models,\n    sample,\n    prefix_tokens=None,\n    bos_token=None,\n    **kwargs\n):\n    model = EnsembleModel(models)\n    if not self.retain_dropout:\n        model.eval()\n\n    # model.forward normally channels prev_output_tokens into the decoder\n    # separately, but SequenceGenerator directly calls model.encoder\n    encoder_input = {\n        k: v for k, v in sample['net_input'].items()\n        if k != 'prev_output_tokens' and k != 'bert_input'\n    }\n\n    src_tokens = encoder_input['src_tokens']\n    src_lengths = (src_tokens.ne(self.eos) & src_tokens.ne(self.pad)).long().sum(dim=1)\n    input_size = src_tokens.size()\n    # batch dimension goes first followed by source lengths\n    bsz = input_size[0]\n    src_len = input_size[1]\n    beam_size = self.beam_size\n\n    if self.match_source_len:\n        max_len = src_lengths.max().item()\n    else:\n        max_len = min(\n            int(self.max_len_a * src_len + self.max_len_b),\n            # exclude the EOS marker\n            model.max_decoder_positions() - 1,\n        )\n\n    # compute the encoder output for each beam\n    bertinput = sample['net_input']['bert_input']\n    bert_encoder_padding_mask = bertinput.eq(model.models[0].berttokenizer.pad())\n    bert_outs, _ = model.models[0].bert_encoder(bertinput, output_all_encoded_layers=True, attention_mask= 1. - bert_encoder_padding_mask)\n    bert_outs = bert_outs[self.bert_output_layer]\n    if model.models[0].mask_cls_sep:\n        bert_encoder_padding_mask += bertinput.eq(model.models[0].berttokenizer.cls())\n        bert_encoder_padding_mask += bertinput.eq(model.models[0].berttokenizer.sep())\n    bert_outs = bert_outs.permute(1,0,2).contiguous()\n    # bert_outs = F.linear(bert_outs, model.models[0].trans_weight, model.models[0].trans_bias)\n    bert_outs = [{\n        'bert_encoder_out': bert_outs,\n        'bert_encoder_padding_mask': bert_encoder_padding_mask,\n    }]\n    if model.models[0].__class__.__name__ == 'TransformerS2Model':\n        encoder_input['bert_encoder_out'] = bert_outs[0]\n    encoder_outs = model.forward_encoder(encoder_input)\n    new_order = torch.arange(bsz).view(-1, 1).repeat(1, beam_size).view(-1)\n    new_order = new_order.to(src_tokens.device).long()\n    encoder_outs, bert_outs = model.reorder_encoder_out(encoder_outs, bert_outs, new_order)\n\n    # initialize buffers\n    scores = src_tokens.new(bsz * beam_size, max_len + 1).float().fill_(0)\n    scores_buf = scores.clone()\n    tokens = src_tokens.data.new(bsz * beam_size, max_len + 2).long().fill_(self.pad)\n    tokens_buf = tokens.clone()\n    tokens[:, 0] = bos_token or self.eos\n    attn, attn_buf = None, None\n    nonpad_idxs = None\n    if prefix_tokens is not None:\n        partial_prefix_mask_buf = torch.zeros_like(src_lengths).byte"]}
{"id": 48, "prompt": "<\uff5cbegin\u2581of\u2581sentence\uff5c>### Role:\nYou are an experienced Python programmer assistant responsible for code completion tasks. Please intelligently complete the missing parts of the provided code snippet.\n\n### Task:\nBased on the provided code snippet, complete the missing function implementation. Ensure the code is logically correct, syntactically accurate, and follows best coding practices.\n\n### Left context\n```Python\nimport warnings\n\nimport numpy as np\nimport pandas as pd\n\nfrom statsmodels.base import model\nimport statsmodels.base.wrapper as wrap\nfrom statsmodels.tools.sm_exceptions import ConvergenceWarning\n\n\nclass _DimReductionRegression(model.Model):\n    \"\"\"\n    A base class for dimension reduction regression methods.\n    \"\"\"\n\n    def __init__(self, endog, exog, **kwargs):\n        super(_DimReductionRegression, self).__init__(endog, exog, **kwargs)\n\n    def _prep(self, n_slice):\n\n        # Sort the data by endog\n        ii = np.argsort(self.endog)\n        x = self.exog[ii, :]\n\n        # Whiten the data\n        x -= x.mean(0)\n        covx = np.dot(x.T, x) / x.shape[0]\n        covxr = np.linalg.cholesky(covx)\n        x = np.linalg.solve(covxr, x.T).T\n        self.wexog = x\n        self._covxr = covxr\n\n        # Split the data into slices\n        self._split_wexog = np.array_split(x, n_slice)\n\n\nclass SlicedInverseReg(_DimReductionRegression):\n    \"\"\"\n    Sliced Inverse Regression (SIR)\n\n    Parameters\n    ----------\n    endog : array_like (1d)\n        The dependent variable\n    exog : array_like (2d)\n        The covariates\n\n    References\n    ----------\n    KC Li (1991).  Sliced inverse regression for dimension reduction.\n    JASA 86, 316-342.\n    \"\"\"\n\n```\n\n### Code description\n```Python\n\"\"\"\nDescription: This function fits a model by splitting the exogenous data into slices and calculating the mean and covariance of each slice. It then performs a principal component analysis on the mean-covariance matrix and returns the results.\n\nArgs:\n    self: The instance of the class that the function is called on.\n    slice_n (int, optional): The number of slices to split the exogenous data into. Defaults to 20.\n    **kwargs: Additional keyword arguments. If any are provided, a warning is issued.\n\nReturns:\n    DimReductionResultsWrapper: The results of the dimensionality reduction, wrapped in a DimReductionResultsWrapper object.\n\"\"\"\n```\n\n### Code snippet:\n```Python\ndef fit(self, slice_n=20, **kwargs):\n\n    # Sample size per slice\n    if len(kwargs) > 0:\n        msg = \"SIR.fit does not take any extra keyword arguments\"\n        warnings.warn(msg)\n\n    # Number of slices\n    n_slice = self.exog.shape[0] // slice_n\n\n    self._prep(n_slice)\n\n    mn = [z.mean(0) for z in self._split_wexog]\n    n = [z.shape[0] for z in self._split_wexog]\n```\n\n### Requirements:\n1. Complete only the missing part of the function.\n2. Only answer the code of the completed part. Do not include any of the provided code snippet.\n3. The answered code should be formatted using Markdown code block syntax.\n4. Do not include any additional text.\n\n### Answer: (use the provided format with backticks)\n", "name": "fit", "reference": "    mn = np.asarray(mn)\n    n = np.asarray(n)\n\n    # Estimate Cov E[X | Y=y]\n    mnc = np.dot(mn.T, n[:, None] * mn) / n.sum()\n\n    a, b = np.linalg.eigh(mnc)\n    jj = np.argsort(-a)\n    a = a[jj]\n    b = b[:, jj]\n    params = np.linalg.solve(self._covxr.T, b)\n\n    results = DimReductionResults(self, params, eigs=a)\n    return DimReductionResultsWrapper(results)", "completions": ["```python\ndef fit(self, slice_n=20, **kwargs):\n    \"\"\"\n    Fit the model by splitting the exogenous data into slices and calculating the mean and covariance of each slice.\n    It then performs a principal component analysis on the mean-covariance matrix and returns the results.\n\n    Args:\n        self: The instance of the class that the function is called on.\n        slice_n (int, optional): The number of slices to split the exogenous data into. Defaults to 20.\n        **kwargs: Additional keyword arguments. If any are provided, a warning is issued.\n\n    Returns:\n        DimReductionResultsWrapper: The results of the dimensionality reduction, wrapped in a DimReductionResultsWrapper object.\n    \"\"\"\n\n    # Sample size per slice\n    if len(kwargs) > 0:\n        msg = \"SIR.fit does not take any extra keyword arguments\"\n        warnings.warn(msg)\n\n    # Number of slices\n    n_slice = self.exog.shape[0] // slice_n\n\n    self._prep(n_slice)\n\n    mn = [z.mean(0) for z in self._split_wexog]\n    n = [z.shape[0] for z in self._split_wexog]\n```\n"]}
{"id": 49, "prompt": "<\uff5cbegin\u2581of\u2581sentence\uff5c>### Role:\nYou are an experienced Python programmer assistant responsible for code completion tasks. Please intelligently complete the missing parts of the provided code snippet.\n\n### Task:\nBased on the provided code snippet, complete the missing function implementation. Ensure the code is logically correct, syntactically accurate, and follows best coding practices.\n\n### Left context\n```Python\nfrom logging import getLogger\n\nimport numpy as np\n\nfrom chainerrl import explorer\n\n\nclass AdditiveOU(explorer.Explorer):\n    \"\"\"Additive Ornstein-Uhlenbeck process.\n\n    Used in https://arxiv.org/abs/1509.02971 for exploration.\n\n    Args:\n        mu (float): Mean of the OU process\n        theta (float): Friction to pull towards the mean\n        sigma (float or ndarray): Scale of noise\n        start_with_mu (bool): Start the process without noise\n    \"\"\"\n\n    def __init__(self, mu=0.0, theta=0.15, sigma=0.3, start_with_mu=False,\n                 logger=getLogger(__name__)):\n        self.mu = mu\n        self.theta = theta\n        self.sigma = sigma\n        self.start_with_mu = start_with_mu\n        self.logger = logger\n        self.ou_state = None\n\n    def evolve(self):\n        # dx = theta (mu - x) + sigma dW\n        # for a Wiener process W\n        noise = np.random.normal(size=self.ou_state.shape, loc=0,\n                                 scale=self.sigma)\n        self.ou_state += self.theta * (self.mu - self.ou_state) + noise\n\n```\n\n### Code description\n```Python\n\"\"\"\nDescription: This function selects an action based on the greedy action function and adds noise to it.\n\nArgs:\n    t (type): The current time step.\n    greedy_action_func (function): A function that returns the greedy action.\n    action_value (type, optional): The value of the action. Defaults to None.\n\nReturns:\n    numpy.ndarray: The selected action with added noise.\n\"\"\"\n```\n\n### Code snippet:\n```Python\n\ndef select_action(self, t, greedy_action_func, action_value=None):\n    a = greedy_action_func()\n    if self.ou_state is None:\n        if self.start_with_mu:\n            self.ou_state = np.full(a.shape, self.mu, dtype=np.float32)\n        else:\n            sigma_stable = (self.sigma /\n```\n\n### Requirements:\n1. Complete only the missing part of the function.\n2. Only answer the code of the completed part. Do not include any of the provided code snippet.\n3. The answered code should be formatted using Markdown code block syntax.\n4. Do not include any additional text.\n\n### Answer: (use the provided format with backticks)\n", "name": "select_action", "reference": "                            np.sqrt(2 * self.theta - self.theta ** 2))\n            self.ou_state = np.random.normal(\n                size=a.shape,\n                loc=self.mu, scale=sigma_stable).astype(np.float32)\n    else:\n        self.evolve()\n    noise = self.ou_state\n    self.logger.debug('t:%s noise:%s', t, noise)\n    return a + noise", "completions": ["```python\n```python\n# Complete the missing part of the function\n\ndef select_action(self, t, greedy_action_func, action_value=None):\n    a = greedy_action_func()\n    if self.ou_state is None:\n        if self.start_with_mu:\n            self.ou_state = np.full(a.shape, self.mu, dtype=np.float32)\n        else:\n            sigma_stable = (self.sigma /\n                            (1 - self.theta + self.sigma * np.sqrt(t / self.theta)))\n            self.ou_state = a + sigma_stable * np.sqrt(2 * t) * np.random.normal(size=a.shape, loc=0, scale=1)\n    return a + self.ou_state\n```\n```\n"]}
{"id": 50, "prompt": "<\uff5cbegin\u2581of\u2581sentence\uff5c>### Role:\nYou are an experienced Python programmer assistant responsible for code completion tasks. Please intelligently complete the missing parts of the provided code snippet.\n\n### Task:\nBased on the provided code snippet, complete the missing function implementation. Ensure the code is logically correct, syntactically accurate, and follows best coding practices.\n\n### Left context\n```Python\n# ----------------------------------------------------------------------\n# Numenta Platform for Intelligent Computing (NuPIC)\n# Copyright (C) 2013-2016, Numenta, Inc.  Unless you have an agreement\n# with Numenta, Inc., for a separate license for this software code, the\n# following terms and conditions apply:\n#\n# This program is free software: you can redistribute it and/or modify\n# it under the terms of the GNU Affero Public License version 3 as\n# published by the Free Software Foundation.\n#\n# This program is distributed in the hope that it will be useful,\n# but WITHOUT ANY WARRANTY; without even the implied warranty of\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.\n# See the GNU Affero Public License for more details.\n#\n# You should have received a copy of the GNU Affero Public License\n# along with this program.  If not, see http://www.gnu.org/licenses.\n#\n# http://numenta.org/licenses/\n# ----------------------------------------------------------------------\n\ntry:\n  import capnp\nexcept ImportError:\n  capnp = None\nimport numpy\n\nfrom nupic.bindings.math import (SM32 as SparseMatrix,\n                                 SM_01_32_32 as SparseBinaryMatrix,\n                                 GetNTAReal,\n                                 Random as NupicRandom)\nif capnp:\n  from nupic.proto.SpatialPoolerProto_capnp import SpatialPoolerProto\n\nfrom nupic.math import topology\nfrom nupic.serializable import Serializable\n\nrealDType = GetNTAReal()\nuintType = \"uint32\"\n\nVERSION = 3\nPERMANENCE_EPSILON = 0.000001\nEPSILON_ROUND = 5\n\n\nclass InvalidSPParamValueError(ValueError):\n  \"\"\"The user passed an invalid value for a SpatialPooler parameter\"\"\"\n  pass\n\n\n\nclass _SparseMatrixCorticalColumnAdapter(object):\n  \"\"\" Many functions in SpatialPooler operate on a columnIndex but use an\n  underlying storage implementation based on a Sparse Matrix in which cortical\n  columns are represented as rows.  This can be confusing to someone trying to\n  follow the algorithm, confusing terminology between matrix math and cortical\n  columns.  This class is provided to abstract away some of the details of the\n  underlying implementation, providing a cleaner API that isn't specific to\n  sparse matrices.\n  \"\"\"\n\n  def __getitem__(self, columnIndex):\n    \"\"\" Wraps getRow() such that instances may be indexed by columnIndex.\"\"\"\n    return super(_SparseMatrixCorticalColumnAdapter, self).getRow(columnIndex)\n\n\n  def replace(self, columnIndex, bitmap):\n    \"\"\" Wraps replaceSparseRow()\"\"\"\n    return super(_SparseMatrixCorticalColumnAdapter, self).replaceSparseRow(\n      columnIndex, bitmap\n    )\n\n\n  def update(self, columnIndex, vector):\n    \"\"\" Wraps setRowFromDense()\"\"\"\n    return super(_SparseMatrixCorticalColumnAdapter, self).setRowFromDense(\n      columnIndex, vector\n    )\n\n\n\nclass CorticalColumns(_SparseMatrixCorticalColumnAdapter, SparseMatrix):\n  \"\"\" SparseMatrix variant of _SparseMatrixCorticalColumnAdapter.  Use in cases\n  where column connections are represented as float values, such as permanence\n  values\n  \"\"\"\n  pass\n\n\nclass BinaryCorticalColumns(_SparseMatrixCorticalColumnAdapter,\n                            SparseBinaryMatrix):\n  \"\"\" SparseBinaryMatrix variant of _SparseMatrixCorticalColumnAdapter.  Use in\n  cases where column connections are represented as bitmaps.\n  \"\"\"\n  pass\n\n\n\nclass SpatialPooler(Serializable):\n  \"\"\"\n  This class implements the spatial pooler. It is in charge of handling the\n  relationships between the columns of a region and the inputs bits. The\n  primary public interface to this function is the \"compute\" method, which\n  takes in an input vector and returns a list of activeColumns columns.\n\n  .. code-block:: python\n\n     sp = SpatialPooler(...)\n     for line in file:\n       inputVector = numpy.array(line)\n       sp.compute(inputVector)\n       ...\n\n  :param inputDimensions: (iter)\n      A sequence representing the dimensions of the input vector. Format is\n      (height, width, depth, ...), where each value represents the size of the\n      dimension.  For a topology of one dimension with 100 inputs use 100, or\n      (100,). For a two dimensional topology of 10x5 use (10,5). \n      Default ``(32, 32)``.\n  \n  :param columnDimensions: (iter)\n      A sequence representing the dimensions of the columns in the region.\n      Format is (height, width, depth, ...), where each value represents the\n      size of the dimension.  For a topology of one dimension with 2000 columns\n      use 2000, or (2000,). For a three dimensional topology of 32x64x16 use\n      (32, 64, 16). Default ``(64, 64)``.\n  \n  :param potentialRadius: (int)\n      This parameter determines the extent of the input that each column can\n      potentially be connected to.  This can be thought of as the input bits\n      that are visible to each column, or a 'receptiveField' of the field of\n      vision. A large enough value will result in 'global coverage', meaning\n      that each column can potentially be connected to every input bit. This\n      parameter defines a square (or hyper\n      square) area: a column will have a max square potential pool with sides of\n      length 2 * potentialRadius + 1. Default ``16``.\n  \n  :param potentialPct: (float)\n      The percent of the inputs, within a column's potential radius, that a\n      column can be connected to.  If set to 1, the column will be connected\n      to every input within its potential radius. This parameter is used to\n      give each column a unique potential pool when a large potentialRadius\n      causes overlap between the columns. At initialization time we choose\n      ((2*potentialRadius + 1)^(# inputDimensions) * potentialPct) input bits\n      to comprise the column's potential pool. Default ``0.5``.\n  \n  :param globalInhibition: (bool)\n      If true, then during inhibition phase the winning columns are selected\n      as the most active columns from the region as a whole. Otherwise, the\n      winning columns are selected with respect to their local neighborhoods.\n      Using global inhibition boosts performance x60. Default ``False``.\n\n  :param localAreaDensity: (float)\n      The desired density of active columns within a local inhibition area\n      (the size of which is set by the internally calculated inhibitionRadius,\n      which is in turn determined from the average size of the connected\n      potential pools of all columns). The inhibition logic will insure that\n      at most N columns remain ON within a local inhibition area, where\n      N = localAreaDensity * (total number of columns in inhibition area). \n      Default ``-1.0``.\n  \n  :param numActiveColumnsPerInhArea: (float)\n      An alternate way to control the density of the active columns. If\n      numActiveColumnsPerInhArea is specified then localAreaDensity must be\n      less than 0, and vice versa.  When using numActiveColumnsPerInhArea, the\n      inhibition logic will insure that at most 'numActiveColumnsPerInhArea'\n      columns remain ON within a local inhibition area (the size of which is\n      set by the internally calculated inhibitionRadius, which is in turn\n      determined from the average size of the connected receptive fields of all\n      columns). When using this method, as columns learn and grow their\n      effective receptive fields, the inhibitionRadius will grow, and hence the\n      net density of the active columns will *decrease*. This is in contrast to\n      the localAreaDensity method, which keeps the density of active columns\n      the same regardless of the size of their receptive fields. Default ``10.0``.\n  \n  :param stimulusThreshold: (int)\n      This is a number specifying the minimum number of synapses that must be\n      on in order for a columns to turn ON. The purpose of this is to prevent\n      noise input from activating columns. Specified as a percent of a fully\n      grown synapse. Default ``0``.\n  \n  :param synPermInactiveDec: (float)\n      The amount by which an inactive synapse is decremented in each round.\n      Specified as a percent of a fully grown synapse. Default ``0.008``.\n  \n  :param synPermActiveInc: (float)\n      The amount by which an active synapse is incremented in each round.\n      Specified as a percent of a fully grown synapse. Default ``0.05``.\n  \n  :param synPermConnected: (float)\n      The default connected threshold. Any synapse whose permanence value is\n      above the connected threshold is a \"connected synapse\", meaning it can\n      contribute to the cell's firing. Default ``0.1``.\n  \n  :param minPctOverlapDutyCycle: (float)\n      A number between 0 and 1.0, used to set a floor on how often a column\n      should have at least stimulusThreshold active inputs. Periodically, each\n      column looks at the overlap duty cycle of all other columns within its\n      inhibition radius and sets its own internal minimal acceptable duty cycle\n      to: minPctDutyCycleBeforeInh * max(other columns' duty cycles).  On each\n      iteration, any column whose overlap duty cycle falls below this computed\n      value will  get all of its permanence values boosted up by\n      synPermActiveInc. Raising all permanences in response to a sub-par duty\n      cycle before  inhibition allows a cell to search for new inputs when\n      either its previously learned inputs are no longer ever active, or when\n      the vast majority of them have been \"hijacked\" by other columns. Default \n      ``0.001``.\n\n  :param dutyCyclePeriod: (int)\n      The period used to calculate duty cycles. Higher values make it take\n      longer to respond to changes in boost or synPerConnectedCell. Shorter\n      values make it more unstable and likely to oscillate. Default ``1000``.\n\n  :param boostStrength: (float)\n      A number greater or equal than 0.0, used to control the strength of\n      boosting. No boosting is applied if it is set to 0. Boosting strength\n      increases as a function of boostStrength. Boosting encourages columns to\n      have similar activeDutyCycles as their neighbors, which will lead to more\n      efficient use of columns. However, too much boosting may also lead to\n      instability of SP outputs. Default ``0.0``.\n\n  :param seed: (int)\n      Seed for our own pseudo-random number generator. Default ``-1``.\n\n  :param spVerbosity: (int)\n      spVerbosity level: 0, 1, 2, or 3. Default ``0``.\n\n  :param wrapAround: (bool)\n      Determines if inputs at the beginning and end of an input dimension should\n      be considered neighbors when mapping columns to inputs. Default ``True``.\n\n  \"\"\"\n\n  def __init__(self,\n               inputDimensions=(32, 32),\n               columnDimensions=(64, 64),\n               potentialRadius=16,\n               potentialPct=0.5,\n               globalInhibition=False,\n               localAreaDensity=-1.0,\n               numActiveColumnsPerInhArea=10.0,\n               stimulusThreshold=0,\n               synPermInactiveDec=0.008,\n               synPermActiveInc=0.05,\n               synPermConnected=0.10,\n               minPctOverlapDutyCycle=0.001,\n               dutyCyclePeriod=1000,\n               boostStrength=0.0,\n               seed=-1,\n               spVerbosity=0,\n               wrapAround=True\n               ):\n    if (numActiveColumnsPerInhArea == 0 and\n        (localAreaDensity == 0 or localAreaDensity > 0.5)):\n      raise InvalidSPParamValueError(\"Inhibition parameters are invalid\")\n\n    columnDimensions = numpy.array(columnDimensions, ndmin=1)\n    numColumns = columnDimensions.prod()\n\n    if not isinstance(numColumns, (int, long)) or numColumns <= 0:\n      raise InvalidSPParamValueError(\"Invalid number of columns ({})\"\n                                     .format(repr(numColumns)))\n    inputDimensions = numpy.array(inputDimensions, ndmin=1)\n    numInputs = inputDimensions.prod()\n\n    if not isinstance(numInputs, (int, long)) or numInputs <= 0:\n      raise InvalidSPParamValueError(\"Invalid number of inputs ({}\"\n                                     .format(repr(numInputs)))\n\n    if inputDimensions.size != columnDimensions.size:\n      raise InvalidSPParamValueError(\n        \"Input dimensions must match column dimensions\")\n\n    if boostStrength < 0.0:\n      raise InvalidSPParamValueError(\"boostStrength must be >= 0.0\")\n\n    self._seed(seed)\n\n    self._numInputs = int(numInputs)\n    self._numColumns = int(numColumns)\n    self._columnDimensions = columnDimensions\n    self._inputDimensions = inputDimensions\n    self._potentialRadius = int(min(potentialRadius, numInputs))\n    self._potentialPct = potentialPct\n    self._globalInhibition = globalInhibition\n    self._numActiveColumnsPerInhArea = int(numActiveColumnsPerInhArea)\n    self._localAreaDensity = localAreaDensity\n    self._stimulusThreshold = stimulusThreshold\n    self._synPermInactiveDec = synPermInactiveDec\n    self._synPermActiveInc = synPermActiveInc\n    self._synPermBelowStimulusInc = synPermConnected / 10.0\n    self._synPermConnected = synPermConnected\n    self._minPctOverlapDutyCycles = minPctOverlapDutyCycle\n    self._dutyCyclePeriod = dutyCyclePeriod\n    self._boostStrength = boostStrength\n    self._spVerbosity = spVerbosity\n    self._wrapAround = wrapAround\n    self._synPermMin = 0.0\n    self._synPermMax = 1.0\n    self._synPermTrimThreshold = synPermActiveInc / 2.0\n    self._overlaps = numpy.zeros(self._numColumns, dtype=realDType)\n    self._boostedOverlaps = numpy.zeros(self._numColumns, dtype=realDType)\n\n    if self._synPermTrimThreshold >= self._synPermConnected:\n      raise InvalidSPParamValueError(\n        \"synPermTrimThreshold ({}) must be less than synPermConnected ({})\"\n        .format(repr(self._synPermTrimThreshold),\n                repr(self._synPermConnected)))\n\n    self._updatePeriod = 50\n    initConnectedPct = 0.5\n    self._version = VERSION\n    self._iterationNum = 0\n    self._iterationLearnNum = 0\n\n    # Store the set of all inputs within each columns potential pool as a\n    # single adjacency matrix such that matrix rows map to cortical columns,\n    # and matrix columns map to input buts.  If potentialPools[i][j] == 1,\n    # then input bit 'j' is in column 'i's potential pool. A column can only be\n    # connected to inputs in its potential pool.  Here, BinaryCorticalColumns\n    # is used to provide cortical column-centric semantics for what is\n    # otherwise a sparse binary matrix implementation.  Sparse binary matrix is\n    # used as an optimization since a column will only be connected to a small\n    # fraction of input bits.\n    self._potentialPools = BinaryCorticalColumns(numInputs)\n    self._potentialPools.resize(numColumns, numInputs)\n\n    # Initialize the permanences for each column. Similar to the\n    # 'self._potentialPools', the permanences are stored in a matrix whose rows\n    # represent the cortical columns, and whose columns represent the input\n    # bits. If self._permanences[i][j] = 0.2, then the synapse connecting\n    # cortical column 'i' to input bit 'j'  has a permanence of 0.2. Here,\n    # CorticalColumns is used to provide cortical column-centric semantics for\n    # what is otherwise a sparse matrix implementation.  Sparse matrix is used\n    # as an optimization to improve computation time of alforithms that\n    # require iterating over the data  structure. This permanence matrix is\n    # only allowed to have non-zero elements where the potential pool is\n    # non-zero.\n    self._permanences = CorticalColumns(numColumns, numInputs)\n\n    # Initialize a tiny random tie breaker. This is used to determine winning\n    # columns where the overlaps are identical.\n    self._tieBreaker = numpy.array([0.01 * self._random.getReal64() for i in\n                                      xrange(self._numColumns)],\n                                    dtype=realDType)\n\n    # 'self._connectedSynapses' is a similar matrix to 'self._permanences'\n    # (rows represent cortical columns, columns represent input bits) whose\n    # entries represent whether the cortical column is connected to the input\n    # bit, i.e. its permanence value is greater than 'synPermConnected'. While\n    # this information is readily available from the 'self._permanence' matrix,\n    # it is stored separately for efficiency purposes.\n    self._connectedSynapses = BinaryCorticalColumns(numInputs)\n    self._connectedSynapses.resize(numColumns, numInputs)\n\n    # Stores the number of connected synapses for each column. This is simply\n    # a sum of each row of 'self._connectedSynapses'. again, while this\n    # information is readily available from 'self._connectedSynapses', it is\n    # stored separately for efficiency purposes.\n    self._connectedCounts = numpy.zeros(numColumns, dtype=realDType)\n\n    # Initialize the set of permanence values for each column. Ensure that\n    # each column is connected to enough input bits to allow it to be\n    # activated.\n    for columnIndex in xrange(numColumns):\n      potential = self._mapPotential(columnIndex)\n      self._potentialPools.replace(columnIndex, potential.nonzero()[0])\n      perm = self._initPermanence(potential, initConnectedPct)\n      self._updatePermanencesForColumn(perm, columnIndex, raisePerm=True)\n\n    self._overlapDutyCycles = numpy.zeros(numColumns, dtype=realDType)\n    self._activeDutyCycles = numpy.zeros(numColumns, dtype=realDType)\n    self._minOverlapDutyCycles = numpy.zeros(numColumns,\n                                             dtype=realDType)\n    self._boostFactors = numpy.ones(numColumns, dtype=realDType)\n\n    # The inhibition radius determines the size of a column's local\n    # neighborhood.  A cortical column must overcome the overlap score of\n    # columns in its neighborhood in order to become active. This radius is\n    # updated every learning round. It grows and shrinks with the average\n    # number of connected synapses per column.\n    self._inhibitionRadius = 0\n    self._updateInhibitionRadius()\n\n    if self._spVerbosity > 0:\n      self.printParameters()\n\n\n  def getColumnDimensions(self):\n    \"\"\"\n    :returns: (iter) the dimensions of the columns in the region\n    \"\"\"\n    return self._columnDimensions\n\n\n  def getInputDimensions(self):\n    \"\"\"\n    :returns: (iter) the dimensions of the input vector\n    \"\"\"\n    return self._inputDimensions\n\n\n  def getNumColumns(self):\n    \"\"\"\n    :returns: (int) the total number of columns\n    \"\"\"\n    return self._numColumns\n\n\n  def getNumInputs(self):\n    \"\"\"\n    :returns: (int) the total number of inputs.\n    \"\"\"\n    return self._numInputs\n\n\n  def getPotentialRadius(self):\n    \"\"\"\n    :returns: (float) the potential radius\n    \"\"\"\n    return self._potentialRadius\n\n\n  def setPotentialRadius(self, potentialRadius):\n    \"\"\"\n    :param potentialRadius: (float) value to set\n    \"\"\"\n    self._potentialRadius = potentialRadius\n\n\n  def getPotentialPct(self):\n    \"\"\"\n    :returns: (float) the potential percent\n    \"\"\"\n    return self._potentialPct\n\n\n  def setPotentialPct(self, potentialPct):\n    \"\"\"\n    :param potentialPct: (float) value to set\n    \"\"\"\n    self._potentialPct = potentialPct\n\n\n  def getGlobalInhibition(self):\n    \"\"\"\n    :returns: (bool) whether global inhibition is enabled.\n    \"\"\"\n    return self._globalInhibition\n\n\n  def setGlobalInhibition(self, globalInhibition):\n    \"\"\"\n    :param globalInhibition: (bool) value to set. \n    \"\"\"\n    self._globalInhibition = globalInhibition\n\n\n  def getNumActiveColumnsPerInhArea(self):\n    \"\"\"\n    :returns: (float) the number of active columns per inhibition area. Returns \n              a value less than 0 if parameter is unused.\n    \"\"\"\n    return self._numActiveColumnsPerInhArea\n\n\n  def setNumActiveColumnsPerInhArea(self, numActiveColumnsPerInhArea):\n    \"\"\"\n    Sets the number of active columns per inhibition area. Invalidates the\n    ``localAreaDensity`` parameter\n\n    :param numActiveColumnsPerInhArea: (float) value to set \n    \"\"\"\n    assert(numActiveColumnsPerInhArea > 0)\n    self._numActiveColumnsPerInhArea = numActiveColumnsPerInhArea\n    self._localAreaDensity = 0\n\n\n  def getLocalAreaDensity(self):\n    \"\"\"\n    :returns: (float) the local area density. Returns a value less than 0 if \n              parameter is unused.\n    \"\"\"\n    return self._localAreaDensity\n\n\n  def setLocalAreaDensity(self, localAreaDensity):\n    \"\"\"\n    Sets the local area density. Invalidates the 'numActiveColumnsPerInhArea'\n    parameter\n    \n    :param localAreaDensity: (float) value to set\n    \"\"\"\n    assert(localAreaDensity > 0 and localAreaDensity <= 1)\n    self._localAreaDensity = localAreaDensity\n    self._numActiveColumnsPerInhArea = 0\n\n\n  def getStimulusThreshold(self):\n    \"\"\"\n    :returns: (int) the stimulus threshold\n    \"\"\"\n    return self._stimulusThreshold\n\n\n  def setStimulusThreshold(self, stimulusThreshold):\n    \"\"\"\n    :param stimulusThreshold: (float) value to set. \n    \"\"\"\n    self._stimulusThreshold = stimulusThreshold\n\n\n  def getInhibitionRadius(self):\n    \"\"\"\n    :returns: (int) the inhibition radius\n    \"\"\"\n    return self._inhibitionRadius\n\n\n  def setInhibitionRadius(self, inhibitionRadius):\n    \"\"\"\n    :param inhibitionRadius: (int) value to set \n    \"\"\"\n    self._inhibitionRadius = inhibitionRadius\n\n\n  def getDutyCyclePeriod(self):\n    \"\"\"\n    :returns: (int) the duty cycle period\n    \"\"\"\n    return self._dutyCyclePeriod\n\n\n  def setDutyCyclePeriod(self, dutyCyclePeriod):\n    \"\"\"\n    :param dutyCyclePeriod: (int) value to set. \n    \"\"\"\n    self._dutyCyclePeriod = dutyCyclePeriod\n\n\n  def getBoostStrength(self):\n    \"\"\"\n    :returns: (float) the maximum boost value used.\n    \"\"\"\n    return self._boostStrength\n\n\n  def setBoostStrength(self, boostStrength):\n    \"\"\"\n    Sets the maximum boost value.\n    :param boostStrength: (float) value to set\n    \"\"\"\n    self._boostStrength = boostStrength\n\n\n  def getIterationNum(self):\n    \"\"\"\n    :returns: the iteration number\"\"\"\n    return self._iterationNum\n\n\n  def setIterationNum(self, iterationNum):\n    \"\"\"\n    :param iterationNum: (int) value to set \n    \"\"\"\n    self._iterationNum = iterationNum\n\n\n  def getIterationLearnNum(self):\n    \"\"\"\n    :returns: (int) The number of iterations that have been learned.\n    \"\"\"\n    return self._iterationLearnNum\n\n\n  def setIterationLearnNum(self, iterationLearnNum):\n    \"\"\"\n    :param iterationLearnNum: (int) value to set \n    \"\"\"\n    self._iterationLearnNum = iterationLearnNum\n\n\n  def getSpVerbosity(self):\n    \"\"\"\n    :returns: (int) the verbosity level, larger is more verbose.\n    \"\"\"\n    return self._spVerbosity\n\n\n  def setSpVerbosity(self, spVerbosity):\n    \"\"\"\n    :param spVerbosity: (int) value to set, larger is more verbose. \n    \"\"\"\n    self._spVerbosity = spVerbosity\n\n\n  def getUpdatePeriod(self):\n    \"\"\"\n    :returns: (int) The period at which active duty cycles are updated.\n    \"\"\"\n    return self._updatePeriod\n\n\n  def setUpdatePeriod(self, updatePeriod):\n    \"\"\"\n    :param updatePeriod: (int) The period at which active duty cycles are \n           updated. \n    \"\"\"\n    self._updatePeriod = updatePeriod\n\n\n  def getSynPermTrimThreshold(self):\n    \"\"\"\n    Sparsity is enforced by trimming out all permanence values below this value.\n\n    :returns: (float) the permanence trim threshold\n    \"\"\"\n    return self._synPermTrimThreshold\n\n\n  def setSynPermTrimThreshold(self, synPermTrimThreshold):\n    \"\"\"\n    Sparsity is enforced by trimming out all permanence values below this value.\n\n    :param synPermTrimThreshold: (float) the permanence trim threshold\n    \"\"\"\n    self._synPermTrimThreshold = synPermTrimThreshold\n\n\n  def getSynPermActiveInc(self):\n    \"\"\"\n    :returns: (float) the permanence increment amount for active synapses inputs\n    \"\"\"\n    return self._synPermActiveInc\n\n\n  def setSynPermActiveInc(self, synPermActiveInc):\n    \"\"\"\n    Sets the permanence increment amount for active synapses.\n    \n    :param synPermActiveInc: (float) value to set.\n    \"\"\"\n    self._synPermActiveInc = synPermActiveInc\n\n\n  def getSynPermInactiveDec(self):\n    \"\"\"\n    :returns: (float) the permanence decrement amount for inactive synapses.\n    \"\"\"\n    return self._synPermInactiveDec\n\n\n  def setSynPermInactiveDec(self, synPermInactiveDec):\n    \"\"\"\n    Sets the permanence decrement amount for inactive synapses.\n    \n    :param synPermInactiveDec: (float) value to set.\n    \"\"\"\n    self._synPermInactiveDec = synPermInactiveDec\n\n\n  def getSynPermBelowStimulusInc(self):\n    \"\"\"\n    :returns: (float) the permanence increment amount for columns that have not \n              been recently active.\n    \"\"\"\n    return self._synPermBelowStimulusInc\n\n\n  def setSynPermBelowStimulusInc(self, synPermBelowStimulusInc):\n    \"\"\"\n    Sets the permanence increment amount for columns that have not been\n    recently active.\n    \n    :param synPermBelowStimulusInc: (float) value to set.\n    \"\"\"\n    self._synPermBelowStimulusInc = synPermBelowStimulusInc\n\n\n  def getSynPermConnected(self):\n    \"\"\"\n    :returns: (float) the permanence amount that qualifies a synapse as being \n              connected.\n    \"\"\"\n    return self._synPermConnected\n\n\n  def setSynPermConnected(self, synPermConnected):\n    \"\"\"\n    Sets the permanence amount that qualifies a synapse as being\n    connected.\n    \n    :param synPermConnected: (float) value to set.\n    \"\"\"\n    self._synPermConnected = synPermConnected\n\n\n  def getMinPctOverlapDutyCycles(self):\n    \"\"\"\n    :returns: (float) the minimum tolerated overlaps, given as percent of\n              neighbors overlap score\n    \"\"\"\n    return self._minPctOverlapDutyCycles\n\n\n  def setMinPctOverlapDutyCycles(self, minPctOverlapDutyCycles):\n    \"\"\"\n    Sets the minimum tolerated activity duty cycle, given as percent of\n    neighbors' activity duty cycle.\n    \n    :param minPctOverlapDutyCycles: (float) value to set.\n    \"\"\"\n    self._minPctOverlapDutyCycles = minPctOverlapDutyCycles\n\n\n  def getBoostFactors(self, boostFactors):\n    \"\"\"\n    Gets the boost factors for all columns. Input list will be overwritten.\n\n    :param boostFactors: (list) size must match number of columns. \n    \"\"\"\n    boostFactors[:] = self._boostFactors[:]\n\n\n  def setBoostFactors(self, boostFactors):\n    \"\"\"\n    Sets the boost factors for all columns. ``boostFactors`` size must match\n    the number of columns.\n    \n    :param boostFactors: (iter) value to set.\"\"\"\n    self._boostFactors[:] = boostFactors[:]\n\n\n  def getOverlapDutyCycles(self, overlapDutyCycles):\n    \"\"\"\n    Gets the overlap duty cycles for all columns. ``overlapDutyCycles``\n    size must match the number of columns.\n    \n    :param overlapDutyCycles: (list) will be overwritten. \n    \"\"\"\n    overlapDutyCycles[:] = self._overlapDutyCycles[:]\n\n\n  def setOverlapDutyCycles(self, overlapDutyCycles):\n    \"\"\"\n    Sets the overlap duty cycles for all columns. ``overlapDutyCycles``\n    size must match the number of columns.\n    \n    :param overlapDutyCycles: (list) value to set.\n    \"\"\"\n    self._overlapDutyCycles[:] = overlapDutyCycles\n\n\n  def getActiveDutyCycles(self, activeDutyCycles):\n    \"\"\"\n    Gets the activity duty cycles for all columns. Input list will be \n    overwritten.\n    \n    :param activeDutyCycles: (list) size must match number of columns. \n    \"\"\"\n    activeDutyCycles[:] = self._activeDutyCycles[:]\n\n\n  def setActiveDutyCycles(self, activeDutyCycles):\n    \"\"\"\n    Sets the activity duty cycles for all columns. ``activeDutyCycles`` size \n    must match the number of columns.\n    \n    :param activeDutyCycles: (list) value to set.\n    \"\"\"\n    self._activeDutyCycles[:] = activeDutyCycles\n\n\n  def getMinOverlapDutyCycles(self, minOverlapDutyCycles):\n    \"\"\"\n    :returns: (list) the minimum overlap duty cycles for all columns. \n              ``minOverlapDutyCycles`` size must match the number of columns.\n    \"\"\"\n    minOverlapDutyCycles[:] = self._minOverlapDutyCycles[:]\n\n\n  def setMinOverlapDutyCycles(self, minOverlapDutyCycles):\n    \"\"\"\n    Sets the minimum overlap duty cycles for all columns. \n    ``minOverlapDutyCycles`` size must match the number of columns.\n    \n    :param minOverlapDutyCycles: (iter) value to set.\n    \"\"\"\n    self._minOverlapDutyCycles[:] = minOverlapDutyCycles[:]\n\n\n  def getPotential(self, columnIndex, potential):\n    \"\"\"\n    :param columnIndex: (int) column index to get potential for.\n    :param potential: (list) will be overwritten with column potentials. Must \n           match the number of inputs.\n    \"\"\"\n    assert(columnIndex < self._numColumns)\n    potential[:] = self._potentialPools[columnIndex]\n\n\n  def setPotential(self, columnIndex, potential):\n    \"\"\"\n    Sets the potential mapping for a given column. ``potential`` size must match \n    the number of inputs, and must be greater than ``stimulusThreshold``.\n    \n    :param columnIndex: (int) column index to set potential for.\n    :param potential: (list) value to set.\n    \"\"\"\n    assert(columnIndex < self._numColumns)\n\n    potentialSparse = numpy.where(potential > 0)[0]\n    if len(potentialSparse) < self._stimulusThreshold:\n      raise Exception(\"This is likely due to a \" +\n      \"value of stimulusThreshold that is too large relative \" +\n      \"to the input size.\")\n\n    self._potentialPools.replace(columnIndex, potentialSparse)\n\n\n  def getPermanence(self, columnIndex, permanence):\n    \"\"\"\n    Returns the permanence values for a given column. ``permanence`` size\n    must match the number of inputs.\n    \n    :param columnIndex: (int) column index to get permanence for.\n    :param permanence: (list) will be overwritten with permanences. \n    \"\"\"\n    assert(columnIndex < self._numColumns)\n    permanence[:] = self._permanences[columnIndex]\n\n\n  def setPermanence(self, columnIndex, permanence):\n    \"\"\"\n    Sets the permanence values for a given column. ``permanence`` size must \n    match the number of inputs.\n    \n    :param columnIndex: (int) column index to set permanence for.\n    :param permanence: (list) value to set. \n    \"\"\"\n    assert(columnIndex < self._numColumns)\n    self._updatePermanencesForColumn(permanence, columnIndex, raisePerm=False)\n\n\n  def getConnectedSynapses(self, columnIndex, connectedSynapses):\n    \"\"\"\n    :param connectedSynapses: (list) will be overwritten\n    :returns: (iter) the connected synapses for a given column.\n              ``connectedSynapses`` size must match the number of inputs\"\"\"\n    assert(columnIndex < self._numColumns)\n    connectedSynapses[:] = self._connectedSynapses[columnIndex]\n\n\n  def getConnectedCounts(self, connectedCounts):\n    \"\"\"\n    :param connectedCounts: (list) will be overwritten\n    :returns: (int) the number of connected synapses for all columns.\n              ``connectedCounts`` size must match the number of columns.\n    \"\"\"\n    connectedCounts[:] = self._connectedCounts[:]\n\n\n  def getOverlaps(self):\n    \"\"\"\n    :returns: (iter) the overlap score for each column.\n    \"\"\"\n    return self._overlaps\n\n\n  def getBoostedOverlaps(self):\n    \"\"\"\n    :returns: (list) the boosted overlap score for each column. \n    \"\"\"\n    return self._boostedOverlaps\n\n\n  def compute(self, inputVector, learn, activeArray):\n    \"\"\"\n    This is the primary public method of the SpatialPooler class. This\n    function takes a input vector and outputs the indices of the active columns.\n    If 'learn' is set to True, this method also updates the permanences of the\n    columns.\n\n    :param inputVector: A numpy array of 0's and 1's that comprises the input\n        to the spatial pooler. The array will be treated as a one dimensional\n        array, therefore the dimensions of the array do not have to match the\n        exact dimensions specified in the class constructor. In fact, even a\n        list would suffice. The number of input bits in the vector must,\n        however, match the number of bits specified by the call to the\n        constructor. Therefore there must be a '0' or '1' in the array for\n        every input bit.\n    :param learn: A boolean value indicating whether learning should be\n        performed. Learning entails updating the  permanence values of the\n        synapses, and hence modifying the 'state' of the model. Setting\n        learning to 'off' freezes the SP and has many uses. For example, you\n        might want to feed in various inputs and examine the resulting SDR's.\n    :param activeArray: An array whose size is equal to the number of columns.\n        Before the function returns this array will be populated with 1's at\n        the indices of the active columns, and 0's everywhere else.\n    \"\"\"\n    if not isinstance(inputVector, numpy.ndarray):\n      raise TypeError(\"Input vector must be a numpy array, not %s\" %\n                      str(type(inputVector)))\n\n    if inputVector.size != self._numInputs:\n      raise ValueError(\n          \"Input vector dimensions don't match. Expecting %s but got %s\" % (\n              inputVector.size, self._numInputs))\n\n    self._updateBookeepingVars(learn)\n    inputVector = numpy.array(inputVector, dtype=realDType)\n    inputVector.reshape(-1)\n    self._overlaps = self._calculateOverlap(inputVector)\n\n    # Apply boosting when learning is on\n    if learn:\n      self._boostedOverlaps = self._boostFactors * self._overlaps\n    else:\n      self._boostedOverlaps = self._overlaps\n\n    # Apply inhibition to determine the winning columns\n    activeColumns = self._inhibitColumns(self._boostedOverlaps)\n\n    if learn:\n      self._adaptSynapses(inputVector, activeColumns)\n      self._updateDutyCycles(self._overlaps, activeColumns)\n      self._bumpUpWeakColumns()\n      self._updateBoostFactors()\n      if self._isUpdateRound():\n        self._updateInhibitionRadius()\n        self._updateMinDutyCycles()\n\n    activeArray.fill(0)\n    activeArray[activeColumns] = 1\n\n\n  def stripUnlearnedColumns(self, activeArray):\n    \"\"\"\n    Removes the set of columns who have never been active from the set of\n    active columns selected in the inhibition round. Such columns cannot\n    represent learned pattern and are therefore meaningless if only inference\n    is required. This should not be done when using a random, unlearned SP\n    since you would end up with no active columns.\n\n    :param activeArray: An array whose size is equal to the number of columns.\n        Any columns marked as active with an activeDutyCycle of 0 have\n        never been activated before and therefore are not active due to\n        learning. Any of these (unlearned) columns will be disabled (set to 0).\n    \"\"\"\n    neverLearned = numpy.where(self._activeDutyCycles == 0)[0]\n    activeArray[neverLearned] = 0\n\n\n  def _updateMinDutyCycles(self):\n    \"\"\"\n    Updates the minimum duty cycles defining normal activity for a column. A\n    column with activity duty cycle below this minimum threshold is boosted.\n    \"\"\"\n    if self._globalInhibition or self._inhibitionRadius > self._numInputs:\n      self._updateMinDutyCyclesGlobal()\n    else:\n      self._updateMinDutyCyclesLocal()\n\n\n  def _updateMinDutyCyclesGlobal(self):\n    \"\"\"\n    Updates the minimum duty cycles in a global fashion. Sets the minimum duty\n    cycles for the overlap all columns to be a percent of the maximum in the\n    region, specified by minPctOverlapDutyCycle. Functionality it is equivalent\n    to _updateMinDutyCyclesLocal, but this function exploits the globality of\n    the computation to perform it in a straightforward, and efficient manner.\n    \"\"\"\n    self._minOverlapDutyCycles.fill(\n        self._minPctOverlapDutyCycles * self._overlapDutyCycles.max()\n      )\n\n\n  def _updateMinDutyCyclesLocal(self):\n    \"\"\"\n    Updates the minimum duty cycles. The minimum duty cycles are determined\n    locally. Each column's minimum duty cycles are set to be a percent of the\n    maximum duty cycles in the column's neighborhood. Unlike\n    _updateMinDutyCyclesGlobal, here the values can be quite different for\n    different columns.\n    \"\"\"\n    for column in xrange(self._numColumns):\n      neighborhood = self._getColumnNeighborhood(column)\n\n      maxActiveDuty = self._activeDutyCycles[neighborhood].max()\n      maxOverlapDuty = self._overlapDutyCycles[neighborhood].max()\n\n      self._minOverlapDutyCycles[column] = (maxOverlapDuty *\n                                            self._minPctOverlapDutyCycles)\n\n\n  def _updateDutyCycles(self, overlaps, activeColumns):\n    \"\"\"\n    Updates the duty cycles for each column. The OVERLAP duty cycle is a moving\n    average of the number of inputs which overlapped with the each column. The\n    ACTIVITY duty cycles is a moving average of the frequency of activation for\n    each column.\n\n    Parameters:\n    ----------------------------\n    :param overlaps:\n                    An array containing the overlap score for each column.\n                    The overlap score for a column is defined as the number\n                    of synapses in a \"connected state\" (connected synapses)\n                    that are connected to input bits which are turned on.\n    :param activeColumns:\n                    An array containing the indices of the active columns,\n                    the sparse set of columns which survived inhibition\n    \"\"\"\n    overlapArray = numpy.zeros(self._numColumns, dtype=realDType)\n    activeArray = numpy.zeros(self._numColumns, dtype=realDType)\n    overlapArray[overlaps > 0] = 1\n    activeArray[activeColumns] = 1\n\n    period = self._dutyCyclePeriod\n    if (period > self._iterationNum):\n      period = self._iterationNum\n\n    self._overlapDutyCycles = self._updateDutyCyclesHelper(\n                                self._overlapDutyCycles,\n                                overlapArray,\n                                period\n                              )\n\n    self._activeDutyCycles = self._updateDutyCyclesHelper(\n                                self._activeDutyCycles,\n                                activeArray,\n                                period\n                              )\n\n\n  def _updateInhibitionRadius(self):\n    \"\"\"\n    Update the inhibition radius. The inhibition radius is a measure of the\n    square (or hypersquare) of columns that each a column is \"connected to\"\n    on average. Since columns are are not connected to each other directly, we\n    determine this quantity by first figuring out how many *inputs* a column is\n    connected to, and then multiplying it by the total number of columns that\n    exist for each input. For multiple dimension the aforementioned\n    calculations are averaged over all dimensions of inputs and columns. This\n    value is meaningless if global inhibition is enabled.\n    \"\"\"\n    if self._globalInhibition:\n      self._inhibitionRadius = int(self._columnDimensions.max())\n      return\n\n    avgConnectedSpan = numpy.average(\n                          [self._avgConnectedSpanForColumnND(i)\n                          for i in xrange(self._numColumns)]\n                        )\n    columnsPerInput = self._avgColumnsPerInput()\n    diameter = avgConnectedSpan * columnsPerInput\n    radius = (diameter - 1) / 2.0\n    radius = max(1.0, radius)\n    self._inhibitionRadius = int(radius + 0.5)\n\n\n  def _avgColumnsPerInput(self):\n    \"\"\"\n    The average number of columns per input, taking into account the topology\n    of the inputs and columns. This value is used to calculate the inhibition\n    radius. This function supports an arbitrary number of dimensions. If the\n    number of column dimensions does not match the number of input dimensions,\n    we treat the missing, or phantom dimensions as 'ones'.\n    \"\"\"\n    #TODO: extend to support different number of dimensions for inputs and\n    # columns\n    numDim = max(self._columnDimensions.size, self._inputDimensions.size)\n    colDim = numpy.ones(numDim)\n    colDim[:self._columnDimensions.size] = self._columnDimensions\n\n    inputDim = numpy.ones(numDim)\n    inputDim[:self._inputDimensions.size] = self._inputDimensions\n\n    columnsPerInput = colDim.astype(realDType) / inputDim\n    return numpy.average(columnsPerInput)\n\n\n  def _avgConnectedSpanForColumn1D(self, columnIndex):\n    \"\"\"\n    The range of connected synapses for column. This is used to\n    calculate the inhibition radius. This variation of the function only\n    supports a 1 dimensional column topology.\n\n    Parameters:\n    ----------------------------\n    :param columnIndex:   The index identifying a column in the permanence,\n                          potential and connectivity matrices\n    \"\"\"\n    assert(self._inputDimensions.size == 1)\n    connected = self._connectedSynapses[columnIndex].nonzero()[0]\n    if connected.size == 0:\n      return 0\n    else:\n      return max(connected) - min(connected) + 1\n\n\n  def _avgConnectedSpanForColumn2D(self, columnIndex):\n    \"\"\"\n    The range of connectedSynapses per column, averaged for each dimension.\n    This value is used to calculate the inhibition radius. This variation of\n    the  function only supports a 2 dimensional column topology.\n\n    Parameters:\n    ----------------------------\n    :param columnIndex:   The index identifying a column in the permanence,\n                          potential and connectivity matrices\n    \"\"\"\n    assert(self._inputDimensions.size == 2)\n    connected = self._connectedSynapses[columnIndex]\n    (rows, cols) = connected.reshape(self._inputDimensions).nonzero()\n    if  rows.size == 0 and cols.size == 0:\n      return 0\n    rowSpan = rows.max() - rows.min() + 1\n    colSpan = cols.max() - cols.min() + 1\n    return numpy.average([rowSpan, colSpan])\n\n\n  def _avgConnectedSpanForColumnND(self, columnIndex):\n    \"\"\"\n    The range of connectedSynapses per column, averaged for each dimension.\n    This value is used to calculate the inhibition radius. This variation of\n    the function supports arbitrary column dimensions.\n\n    Parameters:\n    ----------------------------\n    :param index:   The index identifying a column in the permanence, potential\n                    and connectivity matrices.\n    \"\"\"\n    dimensions = self._inputDimensions\n    connected = self._connectedSynapses[columnIndex].nonzero()[0]\n    if connected.size == 0:\n      return 0\n    maxCoord = numpy.empty(self._inputDimensions.size)\n    minCoord = numpy.empty(self._inputDimensions.size)\n    maxCoord.fill(-1)\n    minCoord.fill(max(self._inputDimensions))\n    for i in connected:\n      maxCoord = numpy.maximum(maxCoord, numpy.unravel_index(i, dimensions))\n      minCoord = numpy.minimum(minCoord, numpy.unravel_index(i, dimensions))\n    return numpy.average(maxCoord - minCoord + 1)\n\n\n  def _adaptSynapses(self, inputVector, activeColumns):\n    \"\"\"\n    The primary method in charge of learning. Adapts the permanence values of\n    the synapses based on the input vector, and the chosen columns after\n    inhibition round. Permanence values are increased for synapses connected to\n    input bits that are turned on, and decreased for synapses connected to\n    inputs bits that are turned off.\n\n    Parameters:\n    ----------------------------\n    :param inputVector:\n                    A numpy array of 0's and 1's that comprises the input to\n                    the spatial pooler. There exists an entry in the array\n                    for every input bit.\n    :param activeColumns:\n                    An array containing the indices of the columns that\n                    survived inhibition.\n    \"\"\"\n    inputIndices = numpy.where(inputVector > 0)[0]\n    permChanges = numpy.zeros(self._numInputs, dtype=realDType)\n    permChanges.fill(-1 * self._synPermInactiveDec)\n    permChanges[inputIndices] = self._synPermActiveInc\n    for columnIndex in activeColumns:\n      perm = self._permanences[columnIndex]\n      maskPotential = numpy.where(self._potentialPools[columnIndex] > 0)[0]\n      perm[maskPotential] += permChanges[maskPotential]\n      self._updatePermanencesForColumn(perm, columnIndex, raisePerm=True)\n\n\n  def _bumpUpWeakColumns(self):\n    \"\"\"\n    This method increases the permanence values of synapses of columns whose\n    activity level has been too low. Such columns are identified by having an\n    overlap duty cycle that drops too much below those of their peers. The\n    permanence values for such columns are increased.\n    \"\"\"\n    weakColumns = numpy.where(self._overlapDutyCycles\n                                < self._minOverlapDutyCycles)[0]\n    for columnIndex in weakColumns:\n      perm = self._permanences[columnIndex].astype(realDType)\n      maskPotential = numpy.where(self._potentialPools[columnIndex] > 0)[0]\n      perm[maskPotential] += self._synPermBelowStimulusInc\n      self._updatePermanencesForColumn(perm, columnIndex, raisePerm=False)\n\n\n  def _raisePermanenceToThreshold(self, perm, mask):\n    \"\"\"\n    This method ensures that each column has enough connections to input bits\n    to allow it to become active. Since a column must have at least\n    'self._stimulusThreshold' overlaps in order to be considered during the\n    inhibition phase, columns without such minimal number of connections, even\n    if all the input bits they are connected to turn on, have no chance of\n    obtaining the minimum threshold. For such columns, the permanence values\n    are increased until the minimum number of connections are formed.\n\n\n    Parameters:\n    ----------------------------\n    :param perm:    An array of permanence values for a column. The array is\n                    \"dense\", i.e. it contains an entry for each input bit, even\n                    if the permanence value is 0.\n    :param mask:    the indices of the columns whose permanences need to be\n                    raised.\n    \"\"\"\n    if len(mask) < self._stimulusThreshold:\n      raise Exception(\"This is likely due to a \" +\n      \"value of stimulusThreshold that is too large relative \" +\n      \"to the input size. [len(mask) < self._stimulusThreshold]\")\n\n    numpy.clip(perm, self._synPermMin, self._synPermMax, out=perm)\n    while True:\n      numConnected = numpy.nonzero(\n        perm > self._synPermConnected - PERMANENCE_EPSILON)[0].size\n\n      if numConnected >= self._stimulusThreshold:\n        return\n      perm[mask] += self._synPermBelowStimulusInc\n\n\n```\n\n### Code description\n```Python\n\"\"\"\nDescription: This function updates the permanences for a specific column in a neural network.\n\nArgs:\n    perm (numpy.ndarray): The permanence array to be updated.\n    columnIndex (int): The index of the column to update.\n    raisePerm (bool, optional): A flag indicating whether to raise the permanences to a threshold. Defaults to True.\n\nReturns:\n    None\n\"\"\"\n```\n\n### Code snippet:\n```Python\ndef _updatePermanencesForColumn(self, perm, columnIndex, raisePerm=True):\n  maskPotential = numpy.where(self._potentialPools[columnIndex] > 0)[0]\n  if raisePerm:\n    self._raisePermanenceToThreshold(perm, maskPotential)\n  perm[perm < self._synPermTrimThreshold] = 0\n```\n\n### Requirements:\n1. Complete only the missing part of the function.\n2. Only answer the code of the completed part. Do not include any of the provided code snippet.\n3. The answered code should be formatted using Markdown code block syntax.\n4. Do not include any additional text.\n\n### Answer: (use the provided format with backticks)\n", "name": "_updatePermanencesForColumn", "reference": "  numpy.clip(perm, self._synPermMin, self._synPermMax, out=perm)\n  newConnected = numpy.where(perm >=\n                             self._synPermConnected - PERMANENCE_EPSILON)[0]\n  self._permanences.update(columnIndex, perm)\n  self._connectedSynapses.replace(columnIndex, newConnected)\n  self._connectedCounts[columnIndex] = newConnected.size", "completions": ["```python\ndef _updatePermanencesForColumn(self, perm, columnIndex, raisePerm=True):\n    \"\"\"\n    This function updates the permanences for a specific column in a neural network.\n\n    Args:\n        perm (numpy.ndarray): The permanence array to be updated.\n        columnIndex (int): The index of the column to update.\n        raisePerm (bool, optional): A flag indicating whether to raise the permanences to a threshold. Defaults to True.\n\n    Returns:\n        None\n    \"\"\"\n    maskPotential = numpy.where(self._potentialPools[columnIndex] > 0)[0]\n    if raisePerm:\n        self._raisePermanenceToThreshold(perm, maskPotential)\n    perm[perm < self._synPermTrimThreshold] = 0\n```\n"]}
{"id": 51, "prompt": "<\uff5cbegin\u2581of\u2581sentence\uff5c>### Role:\nYou are an experienced Python programmer assistant responsible for code completion tasks. Please intelligently complete the missing parts of the provided code snippet.\n\n### Task:\nBased on the provided code snippet, complete the missing function implementation. Ensure the code is logically correct, syntactically accurate, and follows best coding practices.\n\n### Left context\n```Python\nfrom collections import Counter\nfrom functools import partial\nfrom itertools import chain, product\n\nimport numpy as np\nimport torch\nimport torch.nn as nn\nfrom scipy.sparse import issparse\nfrom torch.utils.data import DataLoader\n\nfrom metal.classifier import Classifier\nfrom metal.label_model.graph_utils import get_clique_tree\nfrom metal.label_model.lm_defaults import lm_default_config\nfrom metal.utils import MetalDataset, recursive_merge_dicts\n\n\nclass LabelModel(Classifier):\n    \"\"\"A LabelModel...TBD\n\n    Args:\n        k: (int) the cardinality of the classifier\n    \"\"\"\n\n    # This class variable is explained in the Classifier class\n    implements_l2 = True\n\n    def __init__(self, k=2, **kwargs):\n        config = recursive_merge_dicts(lm_default_config, kwargs)\n        super().__init__(k, config)\n\n    def _check_L(self, L):\n        \"\"\"Run some basic checks on L.\"\"\"\n        # TODO: Take this out?\n        if issparse(L):\n            L = L.todense()\n\n        # Check for correct values, e.g. warning if in {-1,0,1}\n        if np.any(L < 0):\n            raise ValueError(\"L must have values in {0,1,...,k}.\")\n\n    def _create_L_ind(self, L):\n        \"\"\"Convert a label matrix with labels in 0...k to a one-hot format\n\n        Args:\n            L: An [n,m] scipy.sparse label matrix with values in {0,1,...,k}\n\n        Returns:\n            L_ind: An [n,m*k] dense np.ndarray with values in {0,1}\n\n        Note that no column is required for 0 (abstain) labels.\n        \"\"\"\n        # TODO: Update LabelModel to keep L variants as sparse matrices\n        # throughout and remove this line.\n        if issparse(L):\n            L = L.todense()\n\n        L_ind = np.zeros((self.n, self.m * self.k))\n        for y in range(1, self.k + 1):\n            # A[x::y] slices A starting at x at intervals of y\n            # e.g., np.arange(9)[0::3] == np.array([0,3,6])\n            L_ind[:, (y - 1) :: self.k] = np.where(L == y, 1, 0)\n        return L_ind\n\n```\n\n### Code description\n```Python\n\"\"\"\nDescription: This function is for generating an augmented label matrix based on the input label matrix L.\n\nArgs:\n    self (object): The instance of the class that this function is a part of.\n    L (numpy.ndarray): The input label matrix.\n    higher_order (bool, optional): A flag indicating whether to generate higher-order clique statistics. Defaults to False.\n\nReturns:\n    numpy.ndarray: The augmented label matrix L_aug. If higher_order is False, it returns the input label matrix L_ind.\n\"\"\"\n```\n\n### Code snippet:\n```Python\ndef _get_augmented_label_matrix(self, L, higher_order=False):\n    # Create a helper data structure which maps cliques (as tuples of member\n    # sources) --> {start_index, end_index, maximal_cliques}, where\n    # the last value is a set of indices in this data structure\n    self.c_data = {}\n    for i in range(self.m):\n        self.c_data[i] = {\n            \"start_index\": i * self.k,\n            \"end_index\": (i + 1) * self.k,\n            \"max_cliques\": set(\n                [\n                    j\n                    for j in self.c_tree.nodes()\n                    if i in self.c_tree.node[j][\"members\"]\n                ]\n            ),\n        }\n\n    L_ind = self._create_L_ind(L)\n\n    # Get the higher-order clique statistics based on the clique tree\n    # First, iterate over the maximal cliques (nodes of c_tree) and\n    # separator sets (edges of c_tree)\n    if higher_order:\n        L_aug = np.copy(L_ind)\n        for item in chain(self.c_tree.nodes(), self.c_tree.edges()):\n            if isinstance(item, int):\n                C = self.c_tree.node[item]\n                C_type = \"node\"\n            elif isinstance(item, tuple):\n                C = self.c_tree[item[0]][item[1]]\n                C_type = \"edge\"\n            else:\n                raise ValueError(item)\n```\n\n### Requirements:\n1. Complete only the missing part of the function.\n2. Only answer the code of the completed part. Do not include any of the provided code snippet.\n3. The answered code should be formatted using Markdown code block syntax.\n4. Do not include any additional text.\n\n### Answer: (use the provided format with backticks)\n", "name": "_get_augmented_label_matrix", "reference": "            members = list(C[\"members\"])\n            nc = len(members)\n\n            # If a unary maximal clique, just store its existing index\n            if nc == 1:\n                C[\"start_index\"] = members[0] * self.k\n                C[\"end_index\"] = (members[0] + 1) * self.k\n\n            # Else add one column for each possible value\n            else:\n                L_C = np.ones((self.n, self.k ** nc))\n                for i, vals in enumerate(product(range(self.k), repeat=nc)):\n                    for j, v in enumerate(vals):\n                        L_C[:, i] *= L_ind[:, members[j] * self.k + v]\n\n                # Add to L_aug and store the indices\n                if L_aug is not None:\n                    C[\"start_index\"] = L_aug.shape[1]\n                    C[\"end_index\"] = L_aug.shape[1] + L_C.shape[1]\n                    L_aug = np.hstack([L_aug, L_C])\n                else:\n                    C[\"start_index\"] = 0\n                    C[\"end_index\"] = L_C.shape[1]\n                    L_aug = L_C\n\n                # Add to self.c_data as well\n                id = tuple(members) if len(members) > 1 else members[0]\n                self.c_data[id] = {\n                    \"start_index\": C[\"start_index\"],\n                    \"end_index\": C[\"end_index\"],\n                    \"max_cliques\": set([item]) if C_type == \"node\" else set(item),\n                }\n        return L_aug\n    else:\n        return L_ind", "completions": ["```python\ndef _get_augmented_label_matrix(self, L, higher_order=False):\n    # Create a helper data structure which maps cliques (as tuples of member\n    # sources) --> {start_index, end_index, maximal_cliques}, where\n    # the last value is a set of indices in this data structure\n    self.c_data = {}\n    for i in range(self.m):\n        self.c_data[i] = {\n            \"start_index\": i * self.k,\n            \"end_index\": (i + 1) * self.k,\n            \"max_cliques\": set(\n                [\n                    j\n                    for j in self.c_tree.nodes()\n                    if i in self.c_tree.node[j][\"members\"]\n                ]\n            ),\n        }\n\n    L_ind = self._create_L_ind(L)\n\n    # Get the higher-order clique statistics based on the clique tree\n    # First, iterate over the maximal cliques (nodes of c_tree) and\n    # separator sets (edges of c_tree)\n    if higher_order:\n        L_aug = np.copy(L_ind)\n        for item in chain(self.c_tree.nodes(), self.c_tree.edges()):\n            if isinstance(item, int):\n                C = self.c_tree.node[item]\n                C_type = \"node\"\n            elif isinstance(item, tuple):\n                C = self.c_tree[item[0]][item[1]]\n                C_type = \"edge\"\n            else:\n                raise ValueError(item)\n```\n"]}
{"id": 52, "prompt": "<\uff5cbegin\u2581of\u2581sentence\uff5c>### Role:\nYou are an experienced Python programmer assistant responsible for code completion tasks. Please intelligently complete the missing parts of the provided code snippet.\n\n### Task:\nBased on the provided code snippet, complete the missing function implementation. Ensure the code is logically correct, syntactically accurate, and follows best coding practices.\n\n### Left context\n```Python\nimport os\nfrom collections import Counter\nfrom pathlib import Path\nfrom typing import List, Optional, Tuple\n\nimport numpy as np\nfrom tqdm import tqdm\n\nfrom .filter import get_agents_slice_from_frames, get_frames_slice_from_scenes, get_tl_faces_slice_from_frames\nfrom .zarr_dataset import ChunkedDataset\n\n\nGIGABYTE = 1 * 1024 * 1024 * 1024\n\n\ndef _compute_path_size(path: str) -> int:\n    \"\"\"\n    Compute the total size of the folder, considering also nested elements.\n    Can be run to get zarr total size\n\n    Args:\n        path (str): base path\n\n    Returns:\n        (int): total size in bytes\n    \"\"\"\n    root_directory = Path(path)\n    return sum(f.stat().st_size for f in root_directory.glob(\"**/*\") if f.is_file())\n\n\ndef _get_num_els_in_scene_range(zarr_dataset: ChunkedDataset, scene_index_start: int, scene_index_end: int) -> dict:\n    \"\"\"\n    Get numbers of scenes, frames, agents, tl_lights in a set of scene in a zarr\n    Args:\n        zarr_dataset (ChunkedDataset): zarr dataset to use for computing number of elements\n        scene_index_start (int): start from this scene (included)\n        scene_index_end (int): end before this scene (excluded!!)\n\n    Returns:\n        dict: a dict with keys equal to zarr initialise args\n    \"\"\"\n    assert scene_index_end > scene_index_start\n\n    scene_start = zarr_dataset.scenes[scene_index_start]\n    scene_end = zarr_dataset.scenes[scene_index_end - 1]\n\n    frame_start = zarr_dataset.frames[scene_start[\"frame_index_interval\"][0]]\n    frame_end = zarr_dataset.frames[scene_end[\"frame_index_interval\"][1] - 1]\n\n    return {\n        \"num_scenes\": scene_index_end - scene_index_start,\n        \"num_frames\": scene_end[\"frame_index_interval\"][1] - scene_start[\"frame_index_interval\"][0],\n        \"num_agents\": frame_end[\"agent_index_interval\"][1] - frame_start[\"agent_index_interval\"][0],\n        \"num_tl_faces\": frame_end[\"traffic_light_faces_index_interval\"][1]\n        - frame_start[\"traffic_light_faces_index_interval\"][0],\n    }\n\n\ndef _append_zarr_subset(\n        input_zarr: ChunkedDataset,\n        output_zarr: ChunkedDataset,\n        scene_index_start: int,\n        scene_index_end: int,\n        output_zarr_num_els: Optional[dict] = None,\n) -> None:\n    \"\"\"\n    Append a subset of input_zarr into output_zarr. To avoid appending (slow), output_zarr must be opened in write mode\n    and with pre-allocated shape already. End indices of output_zarr are read from output_zarr_num_els, or 0 is assumed\n    otherwise\n\n    Args:\n        input_zarr (ChunkedDataset): origin zarr in read mode\n        output_zarr (ChunkedDataset): zarr already opened in write mode and with pre-allocated arrays\n        scene_index_start (int): index of the first scene to copy\n        scene_index_end (int): index of the last scene (excluded)\n        output_zarr_num_els (Optional[dict]): if None, write starting from 0 index in the output zarr\n\n    Returns:\n\n    \"\"\"\n\n    # indices to assign in the destination array\n    if output_zarr_num_els is None:\n        idx_output_scene, idx_output_frame, idx_output_agent, idx_output_tl_face = 0, 0, 0, 0\n    else:\n        idx_output_scene = output_zarr_num_els[\"num_scenes\"]\n        idx_output_frame = output_zarr_num_els[\"num_frames\"]\n        idx_output_agent = output_zarr_num_els[\"num_agents\"]\n        idx_output_tl_face = output_zarr_num_els[\"num_tl_faces\"]\n\n    # relative indices to subtract before copying to erase input history\n    idx_start_frame = input_zarr.scenes[scene_index_start][\"frame_index_interval\"][0]\n    idx_start_agent = input_zarr.frames[idx_start_frame][\"agent_index_interval\"][0]\n    idx_start_tl_face = input_zarr.frames[idx_start_frame][\"traffic_light_faces_index_interval\"][0]\n\n    # if output_zarr_num_els is not zero we also need to add output_history\n    idx_start_frame = idx_output_frame - idx_start_frame\n    idx_start_agent = idx_output_agent - idx_start_agent\n    idx_start_tl_face = idx_output_tl_face - idx_start_tl_face\n\n    for idx_scene in range(scene_index_start, scene_index_end):\n        # get slices from input zarr\n        scenes = input_zarr.scenes[idx_scene: idx_scene + 1]\n        frames = input_zarr.frames[get_frames_slice_from_scenes(*scenes)]\n        agents = input_zarr.agents[get_agents_slice_from_frames(*frames[[0, -1]])]\n        tl_faces = input_zarr.tl_faces[get_tl_faces_slice_from_frames(*frames[[0, -1]])]\n\n        # fix indices\n        scenes[\"frame_index_interval\"] += idx_start_frame\n        frames[\"agent_index_interval\"] += idx_start_agent\n        frames[\"traffic_light_faces_index_interval\"] += idx_start_tl_face\n\n        # copy from input_zarr to output_zarr\n        output_zarr.scenes[idx_output_scene: idx_output_scene + len(scenes)] = scenes\n        output_zarr.frames[idx_output_frame: idx_output_frame + len(frames)] = frames\n        output_zarr.agents[idx_output_agent: idx_output_agent + len(agents)] = agents\n        output_zarr.tl_faces[idx_output_tl_face: idx_output_tl_face + len(tl_faces)] = tl_faces\n\n        # update output indices\n        idx_output_scene += len(scenes)\n        idx_output_frame += len(frames)\n        idx_output_agent += len(agents)\n        idx_output_tl_face += len(tl_faces)\n\n\ndef zarr_concat(input_zarrs: List[str], output_zarr: str) -> None:\n    \"\"\"\n    Concat many zarr into a single one. Takes care of updating indices for frames and agents.\n\n    Args:\n        input_zarrs (List[str]): a list of paths to input zarrs\n        output_zarr (str): the path to the output zarr\n\n    Returns:\n\n    \"\"\"\n\n    assert not os.path.exists(output_zarr), \"we need to pre-allocate zarr, can't append fast\"\n    output_dataset = ChunkedDataset(output_zarr)\n\n    # we need to estimate how much to allocate by reading all input zarrs lens\n    # we also store them for later use\n    num_els_valid_zarrs = []\n    valid_zarrs = []\n\n    tqdm_bar = tqdm(input_zarrs, desc=\"computing total size to allocate\")\n    for input_zarr in tqdm_bar:\n        try:\n            input_dataset = ChunkedDataset(input_zarr)\n            input_dataset.open()\n        except (ValueError, KeyError):\n            print(f\"{input_zarr} is not valid! skipping\")\n            continue\n        num_els_valid_zarrs.append(_get_num_els_in_scene_range(input_dataset, 0, len(input_dataset.scenes)))\n        valid_zarrs.append(input_zarr)\n\n    # we can now pre-allocate the output dataset\n    total_num_els: Counter = Counter()\n    for num_el in num_els_valid_zarrs:\n        total_num_els += Counter(num_el)\n    output_dataset.initialize(**total_num_els)\n\n    cur_num_els = Counter({\"num_scenes\": 0, \"num_frames\": 0, \"num_agents\": 0, \"num_tl_faces\": 0})\n    tqdm_bar = tqdm(valid_zarrs)\n    for idx, input_zarr in enumerate(tqdm_bar):\n        tqdm_bar.set_description(f\"working on {input_zarr}\")\n\n        input_dataset = ChunkedDataset(input_zarr)\n        input_dataset.open()\n\n        _append_zarr_subset(input_dataset, output_dataset, 0, len(input_dataset.scenes), cur_num_els)\n        cur_num_els += Counter(num_els_valid_zarrs[idx])\n\n\ndef zarr_split(input_zarr: str, output_path: str, split_infos: List[dict]) -> List[Tuple[int, int]]:\n    \"\"\"\n    Split the input zarr into many zarrs. Names and sizes can be passed using the split_infos arg.\n\n    Args:\n        input_zarr (str): path of the original zarr\n        output_path (str): base destination path\n        split_infos (List[dict]): list of dict. Each element should have `name` (final path is output_path+name)\n        and `split_size_GB` with the size of the split. Last element must have `split_size_GB` set to -1 to collect\n        the last part of the input_zarrr.\n\n    Returns:\n        List[Tuple[int, int]]: indices of scenes where a split occurred\n    \"\"\"\n    input_dataset = ChunkedDataset(input_zarr)\n    input_dataset.open()\n\n    assert len(split_infos) > 0\n    assert split_infos[-1][\"split_size_GB\"] == -1, \"last split element should have split_size_GB equal to -1\"\n\n    # compute the size of the input_dataset in GB\n    size_input_gb = _compute_path_size(input_zarr) / GIGABYTE\n    num_scenes_input = len(input_dataset.scenes)\n\n    # ensure the defined splits don't overspill the input dataset\n    num_scenes_output = [\n        int(num_scenes_input * split_info[\"split_size_GB\"] / size_input_gb) for split_info in split_infos[:-1]\n    ]\n    assert sum(num_scenes_output) < num_scenes_input, \"size exceed\"\n    num_scenes_output.append(num_scenes_input - sum(num_scenes_output))\n\n    cur_scene = 0\n    cuts_track = []  # keep track of start-end of the cuts\n    tqdm_bar = tqdm(zip(num_scenes_output, split_infos))\n    for num_scenes, split_info in tqdm_bar:\n        start_cut = cur_scene\n        end_cut = cur_scene + num_scenes\n        tqdm_bar.set_description(f\"cutting scenes {start_cut}-{end_cut} into {split_info['name']}\")\n\n        num_els_output = _get_num_els_in_scene_range(input_dataset, start_cut, end_cut)\n\n        output_dataset = ChunkedDataset(str(Path(output_path) / split_info[\"name\"]))\n        output_dataset.initialize(**num_els_output)\n\n        _append_zarr_subset(input_dataset, output_dataset, start_cut, end_cut)\n        cuts_track.append((start_cut, end_cut))\n        cur_scene = end_cut\n\n    return cuts_track\n\n\n```\n\n### Code description\n```Python\n\"\"\"\nDescription: This function is used to copy a specified number of frames from each scene in an input zarr dataset to an output zarr dataset.\n\nArgs:\n    input_zarr (str): The path to the input zarr dataset.\n    output_zarr (str): The path to the output zarr dataset.\n    num_frames_to_copy (int): The number of frames to copy from each scene.\n\nReturns:\n    None: This function does not return any value.\n\"\"\"\n```\n\n### Code snippet:\n```Python\ndef zarr_scenes_chop(input_zarr: str, output_zarr: str, num_frames_to_copy: int) -> None:\n    input_dataset = ChunkedDataset(input_zarr)\n    input_dataset.open()\n\n    # check we can actually copy the frames we want from each scene\n    assert np.all(np.diff(input_dataset.scenes[\"frame_index_interval\"], 1) > num_frames_to_copy), \"not enough frames\"\n\n    output_dataset = ChunkedDataset(output_zarr)\n    output_dataset.initialize()\n\n    # current indices where to copy in the output_dataset\n    cur_scene_idx, cur_frame_idx, cur_agent_idx, cur_tl_face_idx = 0, 0, 0, 0\n\n    for idx in tqdm(range(len(input_dataset.scenes)), desc=\"copying\"):\n        # get data and immediately chop frames, agents and traffic lights\n        scene = input_dataset.scenes[idx]\n        first_frame_idx = scene[\"frame_index_interval\"][0]\n\n        frames = input_dataset.frames[first_frame_idx: first_frame_idx + num_frames_to_copy]\n        agents = input_dataset.agents[get_agents_slice_from_frames(*frames[[0, -1]])]\n        tl_faces = input_dataset.tl_faces[get_tl_faces_slice_from_frames(*frames[[0, -1]])]\n```\n\n### Requirements:\n1. Complete only the missing part of the function.\n2. Only answer the code of the completed part. Do not include any of the provided code snippet.\n3. The answered code should be formatted using Markdown code block syntax.\n4. Do not include any additional text.\n\n### Answer: (use the provided format with backticks)\n", "name": "zarr_scenes_chop", "reference": "\n        # reset interval relative to our output (subtract current history and add output history)\n        scene[\"frame_index_interval\"][0] = cur_frame_idx\n        scene[\"frame_index_interval\"][1] = cur_frame_idx + num_frames_to_copy  # address for less frames\n\n        frames[\"agent_index_interval\"] += cur_agent_idx - frames[0][\"agent_index_interval\"][0]\n        frames[\"traffic_light_faces_index_interval\"] += (\n            cur_tl_face_idx - frames[0][\"traffic_light_faces_index_interval\"][0]\n        )\n\n        # write in dest using append (slow)\n        output_dataset.scenes.append(scene[None, ...])  # need 2D array to concatenate\n        output_dataset.frames.append(frames)\n        output_dataset.agents.append(agents)\n        output_dataset.tl_faces.append(tl_faces)\n\n        # increase indices in output\n        cur_scene_idx += len(scene)\n        cur_frame_idx += len(frames)\n        cur_agent_idx += len(agents)\n        cur_tl_face_idx += len(tl_faces)", "completions": ["```markdown\n## Code:\n```python\ndef zarr_scenes_chop(input_zarr: str, output_zarr: str, num_frames_to_copy: int) -> None:\n    input_dataset = ChunkedDataset(input_zarr)\n    input_dataset.open()\n\n    # check we can actually copy the frames we want from each scene\n    assert np.all(np.diff(input_dataset.scenes[\"frame_index_interval\"], 1) > num_frames_to_copy), \"not enough frames\"\n\n    output_dataset = ChunkedDataset(output_zarr)\n    output_dataset.initialize()\n\n    # current indices where to copy in the output_dataset\n    cur_scene_idx, cur_frame_idx, cur_agent_idx, cur_tl_face_idx = 0, 0, 0, 0\n\n    for idx in tqdm(range(len(input_dataset.scenes)), desc=\"copying\"):\n        # get data and immediately chop frames, agents and traffic lights\n        scene = input_dataset.scenes[idx]\n        first_frame_idx = scene[\"frame_index_interval\"][0]\n\n        frames = input_dataset.frames[first_frame_idx: first_frame_idx + num_frames_to_copy]\n        agents = input_dataset.agents[get_agents_slice_from_frames(*frames[[0, -1]])]\n        tl_faces = input_dataset.tl_faces[get_tl_faces_slice_from_frames(*frames[[0, -1]])]\n\n        # update current indices\n        cur_frame_idx += len(frames)\n        cur_agent_idx += len(agents)\n        cur_tl_face_idx += len(tl_faces)\n\n        # copy to output_dataset\n        output_dataset.scenes[cur_scene_idx: cur_scene_idx + 1] = scene\n        output_dataset.frames[cur_frame_idx: cur_frame_idx + len(frames)] = frames\n        output_dataset.agents[cur_agent_idx: cur_agent_idx + len(agents)] = agents\n        output_dataset.tl_faces[cur_tl_face_idx: cur_tl_face_idx + len(tl_faces)] = tl_faces\n\n        # update current indices\n        cur_scene_idx += len(scene)\n```\n```\n"]}
{"id": 53, "prompt": "<\uff5cbegin\u2581of\u2581sentence\uff5c>### Role:\nYou are an experienced Python programmer assistant responsible for code completion tasks. Please intelligently complete the missing parts of the provided code snippet.\n\n### Task:\nBased on the provided code snippet, complete the missing function implementation. Ensure the code is logically correct, syntactically accurate, and follows best coding practices.\n\n### Left context\n```Python\n# -*- coding: utf-8 -*-\n\"\"\"Lifetimes utils and helpers.\"\"\"\n\nfrom __future__ import division\nimport numpy as np\nimport pandas as pd\nimport dill\n\npd.options.mode.chained_assignment = None\n\n__all__ = [\n    \"calibration_and_holdout_data\",\n    \"summary_data_from_transaction_data\",\n    \"calculate_alive_path\",\n    \"expected_cumulative_transactions\",\n]\n\n\nclass ConvergenceError(ValueError):\n    \"\"\"\n    Convergence Error Class.\n    \"\"\"\n\n    pass\n\n\ndef calibration_and_holdout_data(\n    transactions,\n    customer_id_col,\n    datetime_col,\n    calibration_period_end,\n    observation_period_end=None,\n    freq=\"D\",\n    freq_multiplier=1,\n    datetime_format=None,\n    monetary_value_col=None,\n    include_first_transaction=False,\n):\n    \"\"\"\n    Create a summary of each customer over a calibration and holdout period.\n\n    This function creates a summary of each customer over a calibration and\n    holdout period (training and testing, respectively).\n    It accepts transaction data, and returns a DataFrame of sufficient statistics.\n\n    Parameters\n    ----------\n    transactions: :obj: DataFrame\n        a Pandas DataFrame that contains the customer_id col and the datetime col.\n    customer_id_col: string\n        the column in transactions DataFrame that denotes the customer_id\n    datetime_col:  string\n        the column in transactions that denotes the datetime the purchase was made.\n    calibration_period_end: :obj: datetime\n        a period to limit the calibration to, inclusive.\n    observation_period_end: :obj: datetime, optional\n         a string or datetime to denote the final date of the study.\n         Events after this date are truncated. If not given, defaults to the max 'datetime_col'.\n    freq: string, optional\n        Default: 'D' for days. Possible values listed here:\n        https://numpy.org/devdocs/reference/arrays.datetime.html#datetime-units\n    freq_multiplier: int, optional\n        Default: 1. Useful for getting exact recency & T. Example:\n        With freq='D' and freq_multiplier=1, we get recency=591 and T=632\n        With freq='h' and freq_multiplier=24, we get recency=590.125 and T=631.375\n    datetime_format: string, optional\n        a string that represents the timestamp format. Useful if Pandas can't understand\n        the provided format.\n    monetary_value_col: string, optional\n        the column in transactions that denotes the monetary value of the transaction.\n        Optional, only needed for customer lifetime value estimation models.\n    include_first_transaction: bool, optional\n        Default: False\n        By default the first transaction is not included while calculating frequency and\n        monetary_value. Can be set to True to include it.\n        Should be False if you are going to use this data with any fitters in lifetimes package\n\n    Returns\n    -------\n    :obj: DataFrame\n        A dataframe with columns frequency_cal, recency_cal, T_cal, frequency_holdout, duration_holdout\n        If monetary_value_col isn't None, the dataframe will also have the columns monetary_value_cal and\n        monetary_value_holdout.\n    \"\"\"\n\n    def to_period(d):\n        return d.to_period(freq)\n\n    if observation_period_end is None:\n        observation_period_end = transactions[datetime_col].max()\n\n    transaction_cols = [customer_id_col, datetime_col]\n    if monetary_value_col:\n        transaction_cols.append(monetary_value_col)\n    transactions = transactions[transaction_cols].copy()\n\n    transactions[datetime_col] = pd.to_datetime(transactions[datetime_col], format=datetime_format)\n    observation_period_end = pd.to_datetime(observation_period_end, format=datetime_format)\n    calibration_period_end = pd.to_datetime(calibration_period_end, format=datetime_format)\n\n    # create calibration dataset\n    calibration_transactions = transactions.loc[transactions[datetime_col] <= calibration_period_end]\n    calibration_summary_data = summary_data_from_transaction_data(\n        calibration_transactions,\n        customer_id_col,\n        datetime_col,\n        datetime_format=datetime_format,\n        observation_period_end=calibration_period_end,\n        freq=freq,\n        freq_multiplier=freq_multiplier,\n        monetary_value_col=monetary_value_col,\n        include_first_transaction=include_first_transaction,\n    )\n    calibration_summary_data.columns = [c + \"_cal\" for c in calibration_summary_data.columns]\n\n    # create holdout dataset\n    holdout_transactions = transactions.loc[\n        (observation_period_end >= transactions[datetime_col]) & (transactions[datetime_col] > calibration_period_end)\n    ]\n\n    if holdout_transactions.empty:\n        raise ValueError(\n            \"There is no data available. Check the `observation_period_end` and  `calibration_period_end` and confirm that values in `transactions` occur prior to those dates.\"\n        )\n\n    holdout_transactions[datetime_col] = holdout_transactions[datetime_col].map(to_period)\n    holdout_summary_data = (\n        holdout_transactions.groupby([customer_id_col, datetime_col], sort=False)\n        .agg(lambda r: 1)\n        .groupby(level=customer_id_col)\n        .agg([\"count\"])\n    )\n    holdout_summary_data.columns = [\"frequency_holdout\"]\n    if monetary_value_col:\n        holdout_summary_data[\"monetary_value_holdout\"] = holdout_transactions.groupby(customer_id_col)[\n            monetary_value_col\n        ].mean()\n\n    combined_data = calibration_summary_data.join(holdout_summary_data, how=\"left\")\n    combined_data.fillna(0, inplace=True)\n\n    delta_time = (to_period(observation_period_end) - to_period(calibration_period_end)).n\n    combined_data[\"duration_holdout\"] = delta_time / freq_multiplier\n\n    return combined_data\n\n\ndef _find_first_transactions(\n    transactions,\n    customer_id_col,\n    datetime_col,\n    monetary_value_col=None,\n    datetime_format=None,\n    observation_period_end=None,\n    freq=\"D\",\n):\n    \"\"\"\n    Return dataframe with first transactions.\n\n    This takes a DataFrame of transaction data of the form:\n        customer_id, datetime [, monetary_value]\n    and appends a column named 'repeated' to the transaction log which indicates which rows\n    are repeated transactions for that customer_id.\n\n    Parameters\n    ----------\n    transactions: :obj: DataFrame\n        a Pandas DataFrame that contains the customer_id col and the datetime col.\n    customer_id_col: string\n        the column in transactions DataFrame that denotes the customer_id\n    datetime_col:  string\n        the column in transactions that denotes the datetime the purchase was made.\n    monetary_value_col: string, optional\n        the column in transactions that denotes the monetary value of the transaction.\n        Optional, only needed for customer lifetime value estimation models.\n    observation_period_end: :obj: datetime\n        a string or datetime to denote the final date of the study.\n        Events after this date are truncated. If not given, defaults to the max 'datetime_col'.\n    datetime_format: string, optional\n        a string that represents the timestamp format. Useful if Pandas can't understand\n        the provided format.\n    freq: string, optional\n        Default: 'D' for days. Possible values listed here:\n        https://numpy.org/devdocs/reference/arrays.datetime.html#datetime-units\n    \"\"\"\n\n    if observation_period_end is None:\n        observation_period_end = transactions[datetime_col].max()\n\n    if type(observation_period_end) == pd.Period:\n        observation_period_end = observation_period_end.to_timestamp()\n\n    select_columns = [customer_id_col, datetime_col]\n\n    if monetary_value_col:\n        select_columns.append(monetary_value_col)\n\n    transactions = transactions[select_columns].sort_values(select_columns).copy()\n\n    # make sure the date column uses datetime objects, and use Pandas' DateTimeIndex.to_period()\n    # to convert the column to a PeriodIndex which is useful for time-wise grouping and truncating\n    transactions[datetime_col] = pd.to_datetime(transactions[datetime_col], format=datetime_format)\n    transactions = transactions.set_index(datetime_col).to_period(freq).to_timestamp()\n\n    transactions = transactions.loc[(transactions.index <= observation_period_end)].reset_index()\n\n    period_groupby = transactions.groupby([datetime_col, customer_id_col], sort=False, as_index=False)\n\n    if monetary_value_col:\n        # when we have a monetary column, make sure to sum together any values in the same period\n        period_transactions = period_groupby.sum()\n    else:\n        # by calling head() on the groupby object, the datetime_col and customer_id_col columns\n        # will be reduced\n        period_transactions = period_groupby.head(1)\n\n    # initialize a new column where we will indicate which are the first transactions\n    period_transactions[\"first\"] = False\n    # find all of the initial transactions and store as an index\n    first_transactions = period_transactions.groupby(customer_id_col, sort=True, as_index=False).head(1).index\n    # mark the initial transactions as True\n    period_transactions.loc[first_transactions, \"first\"] = True\n    select_columns.append(\"first\")\n    # reset datetime_col to period\n    period_transactions[datetime_col] = pd.Index(period_transactions[datetime_col]).to_period(freq)\n\n    return period_transactions[select_columns]\n\n\ndef summary_data_from_transaction_data(\n    transactions,\n    customer_id_col,\n    datetime_col,\n    monetary_value_col=None,\n    datetime_format=None,\n    observation_period_end=None,\n    freq=\"D\",\n    freq_multiplier=1,\n    include_first_transaction=False,\n):\n    \"\"\"\n    Return summary data from transactions.\n\n    This transforms a DataFrame of transaction data of the form:\n        customer_id, datetime [, monetary_value]\n    to a DataFrame of the form:\n        customer_id, frequency, recency, T [, monetary_value]\n\n    Parameters\n    ----------\n    transactions: :obj: DataFrame\n        a Pandas DataFrame that contains the customer_id col and the datetime col.\n    customer_id_col: string\n        the column in transactions DataFrame that denotes the customer_id\n    datetime_col:  string\n        the column in transactions that denotes the datetime the purchase was made.\n    monetary_value_col: string, optional\n        the columns in the transactions that denotes the monetary value of the transaction.\n        Optional, only needed for customer lifetime value estimation models.\n    observation_period_end: datetime, optional\n         a string or datetime to denote the final date of the study.\n         Events after this date are truncated. If not given, defaults to the max 'datetime_col'.\n    datetime_format: string, optional\n        a string that represents the timestamp format. Useful if Pandas can't understand\n        the provided format.\n    freq: string, optional\n        Default: 'D' for days. Possible values listed here:\n        https://numpy.org/devdocs/reference/arrays.datetime.html#datetime-units\n    freq_multiplier: int, optional\n        Default: 1. Useful for getting exact recency & T. Example:\n        With freq='D' and freq_multiplier=1, we get recency=591 and T=632\n        With freq='h' and freq_multiplier=24, we get recency=590.125 and T=631.375\n    include_first_transaction: bool, optional\n        Default: False\n        By default the first transaction is not included while calculating frequency and\n        monetary_value. Can be set to True to include it.\n        Should be False if you are going to use this data with any fitters in lifetimes package\n\n    Returns\n    -------\n    :obj: DataFrame:\n        customer_id, frequency, recency, T [, monetary_value]\n    \"\"\"\n\n    if observation_period_end is None:\n        observation_period_end = (\n            pd.to_datetime(transactions[datetime_col].max(), format=datetime_format).to_period(freq).to_timestamp()\n        )\n    else:\n        observation_period_end = (\n            pd.to_datetime(observation_period_end, format=datetime_format).to_period(freq).to_timestamp()\n        )\n\n    # label all of the repeated transactions\n    repeated_transactions = _find_first_transactions(\n        transactions, customer_id_col, datetime_col, monetary_value_col, datetime_format, observation_period_end, freq\n    )\n    # reset datetime_col to timestamp\n    repeated_transactions[datetime_col] = pd.Index(repeated_transactions[datetime_col]).to_timestamp()\n\n    # count all orders by customer.\n    customers = repeated_transactions.groupby(customer_id_col, sort=False)[datetime_col].agg([\"min\", \"max\", \"count\"])\n\n    if not include_first_transaction:\n        # subtract 1 from count, as we ignore their first order.\n        customers[\"frequency\"] = customers[\"count\"] - 1\n    else:\n        customers[\"frequency\"] = customers[\"count\"]\n\n    customers[\"T\"] = (observation_period_end - customers[\"min\"]) / np.timedelta64(1, freq) / freq_multiplier\n    customers[\"recency\"] = (customers[\"max\"] - customers[\"min\"]) / np.timedelta64(1, freq) / freq_multiplier\n\n    summary_columns = [\"frequency\", \"recency\", \"T\"]\n\n    if monetary_value_col:\n        if not include_first_transaction:\n            # create an index of all the first purchases\n            first_purchases = repeated_transactions[repeated_transactions[\"first\"]].index\n            # by setting the monetary_value cells of all the first purchases to NaN,\n            # those values will be excluded from the mean value calculation\n            repeated_transactions.loc[first_purchases, monetary_value_col] = np.nan\n        customers[\"monetary_value\"] = (\n            repeated_transactions.groupby(customer_id_col)[monetary_value_col].mean().fillna(0)\n        )\n        summary_columns.append(\"monetary_value\")\n\n    return customers[summary_columns].astype(float)\n\n\ndef calculate_alive_path(\n    model,\n    transactions,\n    datetime_col,\n    t,\n    freq=\"D\"\n):\n    \"\"\"\n    Calculate alive path for plotting alive history of user.\n\n    Uses the ``conditional_probability_alive()`` method of the model to achieve the path.\n\n    Parameters\n    ----------\n    model:\n        A fitted lifetimes model\n    transactions: DataFrame\n        a Pandas DataFrame containing the transactions history of the customer_id\n    datetime_col: string\n        the column in the transactions that denotes the datetime the purchase was made\n    t: array_like\n        the number of time units since the birth for which we want to draw the p_alive\n    freq: string, optional\n        Default: 'D' for days. Possible values listed here:\n        https://numpy.org/devdocs/reference/arrays.datetime.html#datetime-units\n\n    Returns\n    -------\n    :obj: Series\n        A pandas Series containing the p_alive as a function of T (age of the customer)\n    \"\"\"\n\n    customer_history = transactions[[datetime_col]].copy()\n    customer_history[datetime_col] = pd.to_datetime(customer_history[datetime_col])\n    customer_history = customer_history.set_index(datetime_col)\n    # Add transactions column\n    customer_history[\"transactions\"] = 1\n\n    # for some reason fillna(0) not working for resample in pandas with python 3.x,\n    # changed to replace\n    purchase_history = customer_history.resample(freq).sum().replace(np.nan, 0)[\"transactions\"].values\n\n    extra_columns = t + 1 - len(purchase_history)\n    customer_history = pd.DataFrame(np.append(purchase_history, [0] * extra_columns), columns=[\"transactions\"])\n    # add T column\n    customer_history[\"T\"] = np.arange(customer_history.shape[0])\n    # add cumulative transactions column\n    customer_history[\"transactions\"] = customer_history[\"transactions\"].apply(lambda t: int(t > 0))\n    customer_history[\"frequency\"] = customer_history[\"transactions\"].cumsum() - 1  # first purchase is ignored\n    # Add t_x column\n    customer_history[\"recency\"] = customer_history.apply(\n        lambda row: row[\"T\"] if row[\"transactions\"] != 0 else np.nan, axis=1\n    )\n    customer_history[\"recency\"] = customer_history[\"recency\"].fillna(method=\"ffill\").fillna(0)\n\n    return customer_history.apply(\n        lambda row: model.conditional_probability_alive(row[\"frequency\"], row[\"recency\"], row[\"T\"]), axis=1\n    )\n\n\ndef _scale_time(\n    age\n):\n    \"\"\"\n    Create a scalar such that the maximum age is 1.\n    \"\"\"\n\n    return 1.0 / age.max()\n\n\n```\n\n### Code description\n```Python\n\"\"\"\nDescription: This function checks the validity of input vectors for a customer behavior model.\n\nArgs:\n    frequency (numpy.ndarray): A vector of frequencies of customer orders.\n    recency (numpy.ndarray, optional): A vector of recency of customer orders. Defaults to None.\n    T (numpy.ndarray, optional): A vector of total periods. Defaults to None.\n    monetary_value (numpy.ndarray, optional): A vector of monetary values of customer orders. Defaults to None.\n\nRaises:\n    ValueError: If there exist non-integer values in the frequency vector.\n    ValueError: If there exist non-positive (<= 0) values in the monetary_value vector.\n    ValueError: If there exist negative recency (ex: last order set before first order).\n    ValueError: If there exist non-zero recency values when frequency is zero.\n    ValueError: If some values in recency vector are larger than T vector.\n    ValueError: If there exists a zero length vector in one of frequency, recency or T.\n\nReturns:\n    None\n\"\"\"\n```\n\n### Code snippet:\n```Python\ndef _check_inputs(\n    frequency,\n    recency=None,\n    T=None,\n    monetary_value=None\n):\n\n    if recency is not None:\n        if T is not None and np.any(recency > T):\n            raise ValueError(\"Some values in recency vector are larger than T vector.\")\n        if np.any(recency[frequency == 0] != 0):\n```\n\n### Requirements:\n1. Complete only the missing part of the function.\n2. Only answer the code of the completed part. Do not include any of the provided code snippet.\n3. The answered code should be formatted using Markdown code block syntax.\n4. Do not include any additional text.\n\n### Answer: (use the provided format with backticks)\n", "name": "_check_inputs", "reference": "            raise ValueError(\"There exist non-zero recency values when frequency is zero.\")\n        if np.any(recency < 0):\n            raise ValueError(\"There exist negative recency (ex: last order set before first order)\")\n        if any(x.shape[0] == 0 for x in [recency, frequency, T]):\n            raise ValueError(\"There exists a zero length vector in one of frequency, recency or T.\")\n    if np.sum((frequency - frequency.astype(int)) ** 2) != 0:\n        raise ValueError(\"There exist non-integer values in the frequency vector.\")\n    if monetary_value is not None and np.any(monetary_value <= 0):\n        raise ValueError(\"There exist non-positive (<= 0) values in the monetary_value vector.\")\n    # TODO: raise warning if np.any(freqency > T) as this means that there are\n    # more order-periods than periods.", "completions": ["```markdown\n```python\ndef _check_inputs(\n    frequency,\n    recency=None,\n    T=None,\n    monetary_value=None\n):\n    if recency is not None:\n        if T is not None and np.any(recency > T):\n            raise ValueError(\"Some values in recency vector are larger than T vector.\")\n        if np.any(frequency[frequency != np.floor(frequency)]):\n            raise ValueError(\"There exist non-integer values in the frequency vector.\")\n        if np.any(recency[recency <= 0]):\n            raise ValueError(\"There exist non-positive (<= 0) values in the recency vector.\")\n        if np.any(recency[recency[frequency == 0] != 0]):\n            raise ValueError(\"There exist non-zero recency values when frequency is zero.\")\n        if np.any(np.isnan(frequency)) or np.any(np.isnan(recency)) or np.any(np.isnan(T)) or np.any(np.isnan(monetary_value)):\n            raise ValueError(\"There exist missing values in frequency, recency, T or monetary_value vector.\")\n```\n```\n"]}
{"id": 54, "prompt": "<\uff5cbegin\u2581of\u2581sentence\uff5c>### Role:\nYou are an experienced Python programmer assistant responsible for code completion tasks. Please intelligently complete the missing parts of the provided code snippet.\n\n### Task:\nBased on the provided code snippet, complete the missing function implementation. Ensure the code is logically correct, syntactically accurate, and follows best coding practices.\n\n### Left context\n```Python\n# -*- coding: utf-8 -*-\n\nimport warnings\n\nimport numpy as np\nimport pandas as pd\n\nfrom pandas_datareader.base import _BaseReader\nfrom pandas_datareader.compat import lrange, reduce, string_types\n\n# This list of country codes was pulled from wikipedia during October 2014.\n# While some exceptions do exist, it is the best proxy for countries supported\n# by World Bank.  It is an aggregation of the 2-digit ISO 3166-1 alpha-2, and\n# 3-digit ISO 3166-1 alpha-3, codes, with 'all', 'ALL', and 'All' appended ot\n# the end.\n\nWB_API_URL = \"https://api.worldbank.org/v2\"\n\ncountry_codes = [\n    \"AD\",\n    \"AE\",\n    \"AF\",\n    \"AG\",\n    \"AI\",\n    \"AL\",\n    \"AM\",\n    \"AO\",\n    \"AQ\",\n    \"AR\",\n    \"AS\",\n    \"AT\",\n    \"AU\",\n    \"AW\",\n    \"AX\",\n    \"AZ\",\n    \"BA\",\n    \"BB\",\n    \"BD\",\n    \"BE\",\n    \"BF\",\n    \"BG\",\n    \"BH\",\n    \"BI\",\n    \"BJ\",\n    \"BL\",\n    \"BM\",\n    \"BN\",\n    \"BO\",\n    \"BQ\",\n    \"BR\",\n    \"BS\",\n    \"BT\",\n    \"BV\",\n    \"BW\",\n    \"BY\",\n    \"BZ\",\n    \"CA\",\n    \"CC\",\n    \"CD\",\n    \"CF\",\n    \"CG\",\n    \"CH\",\n    \"CI\",\n    \"CK\",\n    \"CL\",\n    \"CM\",\n    \"CN\",\n    \"CO\",\n    \"CR\",\n    \"CU\",\n    \"CV\",\n    \"CW\",\n    \"CX\",\n    \"CY\",\n    \"CZ\",\n    \"DE\",\n    \"DJ\",\n    \"DK\",\n    \"DM\",\n    \"DO\",\n    \"DZ\",\n    \"EC\",\n    \"EE\",\n    \"EG\",\n    \"EH\",\n    \"ER\",\n    \"ES\",\n    \"ET\",\n    \"FI\",\n    \"FJ\",\n    \"FK\",\n    \"FM\",\n    \"FO\",\n    \"FR\",\n    \"GA\",\n    \"GB\",\n    \"GD\",\n    \"GE\",\n    \"GF\",\n    \"GG\",\n    \"GH\",\n    \"GI\",\n    \"GL\",\n    \"GM\",\n    \"GN\",\n    \"GP\",\n    \"GQ\",\n    \"GR\",\n    \"GS\",\n    \"GT\",\n    \"GU\",\n    \"GW\",\n    \"GY\",\n    \"HK\",\n    \"HM\",\n    \"HN\",\n    \"HR\",\n    \"HT\",\n    \"HU\",\n    \"ID\",\n    \"IE\",\n    \"IL\",\n    \"IM\",\n    \"IN\",\n    \"IO\",\n    \"IQ\",\n    \"IR\",\n    \"IS\",\n    \"IT\",\n    \"JE\",\n    \"JM\",\n    \"JO\",\n    \"JP\",\n    \"KE\",\n    \"KG\",\n    \"KH\",\n    \"KI\",\n    \"KM\",\n    \"KN\",\n    \"KP\",\n    \"KR\",\n    \"KW\",\n    \"KY\",\n    \"KZ\",\n    \"LA\",\n    \"LB\",\n    \"LC\",\n    \"LI\",\n    \"LK\",\n    \"LR\",\n    \"LS\",\n    \"LT\",\n    \"LU\",\n    \"LV\",\n    \"LY\",\n    \"MA\",\n    \"MC\",\n    \"MD\",\n    \"ME\",\n    \"MF\",\n    \"MG\",\n    \"MH\",\n    \"MK\",\n    \"ML\",\n    \"MM\",\n    \"MN\",\n    \"MO\",\n    \"MP\",\n    \"MQ\",\n    \"MR\",\n    \"MS\",\n    \"MT\",\n    \"MU\",\n    \"MV\",\n    \"MW\",\n    \"MX\",\n    \"MY\",\n    \"MZ\",\n    \"NA\",\n    \"NC\",\n    \"NE\",\n    \"NF\",\n    \"NG\",\n    \"NI\",\n    \"NL\",\n    \"NO\",\n    \"NP\",\n    \"NR\",\n    \"NU\",\n    \"NZ\",\n    \"OM\",\n    \"PA\",\n    \"PE\",\n    \"PF\",\n    \"PG\",\n    \"PH\",\n    \"PK\",\n    \"PL\",\n    \"PM\",\n    \"PN\",\n    \"PR\",\n    \"PS\",\n    \"PT\",\n    \"PW\",\n    \"PY\",\n    \"QA\",\n    \"RE\",\n    \"RO\",\n    \"RS\",\n    \"RU\",\n    \"RW\",\n    \"SA\",\n    \"SB\",\n    \"SC\",\n    \"SD\",\n    \"SE\",\n    \"SG\",\n    \"SH\",\n    \"SI\",\n    \"SJ\",\n    \"SK\",\n    \"SL\",\n    \"SM\",\n    \"SN\",\n    \"SO\",\n    \"SR\",\n    \"SS\",\n    \"ST\",\n    \"SV\",\n    \"SX\",\n    \"SY\",\n    \"SZ\",\n    \"TC\",\n    \"TD\",\n    \"TF\",\n    \"TG\",\n    \"TH\",\n    \"TJ\",\n    \"TK\",\n    \"TL\",\n    \"TM\",\n    \"TN\",\n    \"TO\",\n    \"TR\",\n    \"TT\",\n    \"TV\",\n    \"TW\",\n    \"TZ\",\n    \"UA\",\n    \"UG\",\n    \"UM\",\n    \"US\",\n    \"UY\",\n    \"UZ\",\n    \"VA\",\n    \"VC\",\n    \"VE\",\n    \"VG\",\n    \"VI\",\n    \"VN\",\n    \"VU\",\n    \"WF\",\n    \"WS\",\n    \"YE\",\n    \"YT\",\n    \"ZA\",\n    \"ZM\",\n    \"ZW\",\n    \"ABW\",\n    \"AFG\",\n    \"AGO\",\n    \"AIA\",\n    \"ALA\",\n    \"ALB\",\n    \"AND\",\n    \"ARE\",\n    \"ARG\",\n    \"ARM\",\n    \"ASM\",\n    \"ATA\",\n    \"ATF\",\n    \"ATG\",\n    \"AUS\",\n    \"AUT\",\n    \"AZE\",\n    \"BDI\",\n    \"BEL\",\n    \"BEN\",\n    \"BES\",\n    \"BFA\",\n    \"BGD\",\n    \"BGR\",\n    \"BHR\",\n    \"BHS\",\n    \"BIH\",\n    \"BLM\",\n    \"BLR\",\n    \"BLZ\",\n    \"BMU\",\n    \"BOL\",\n    \"BRA\",\n    \"BRB\",\n    \"BRN\",\n    \"BTN\",\n    \"BVT\",\n    \"BWA\",\n    \"CAF\",\n    \"CAN\",\n    \"CCK\",\n    \"CHE\",\n    \"CHL\",\n    \"CHN\",\n    \"CIV\",\n    \"CMR\",\n    \"COD\",\n    \"COG\",\n    \"COK\",\n    \"COL\",\n    \"COM\",\n    \"CPV\",\n    \"CRI\",\n    \"CUB\",\n    \"CUW\",\n    \"CXR\",\n    \"CYM\",\n    \"CYP\",\n    \"CZE\",\n    \"DEU\",\n    \"DJI\",\n    \"DMA\",\n    \"DNK\",\n    \"DOM\",\n    \"DZA\",\n    \"ECU\",\n    \"EGY\",\n    \"ERI\",\n    \"ESH\",\n    \"ESP\",\n    \"EST\",\n    \"ETH\",\n    \"FIN\",\n    \"FJI\",\n    \"FLK\",\n    \"FRA\",\n    \"FRO\",\n    \"FSM\",\n    \"GAB\",\n    \"GBR\",\n    \"GEO\",\n    \"GGY\",\n    \"GHA\",\n    \"GIB\",\n    \"GIN\",\n    \"GLP\",\n    \"GMB\",\n    \"GNB\",\n    \"GNQ\",\n    \"GRC\",\n    \"GRD\",\n    \"GRL\",\n    \"GTM\",\n    \"GUF\",\n    \"GUM\",\n    \"GUY\",\n    \"HKG\",\n    \"HMD\",\n    \"HND\",\n    \"HRV\",\n    \"HTI\",\n    \"HUN\",\n    \"IDN\",\n    \"IMN\",\n    \"IND\",\n    \"IOT\",\n    \"IRL\",\n    \"IRN\",\n    \"IRQ\",\n    \"ISL\",\n    \"ISR\",\n    \"ITA\",\n    \"JAM\",\n    \"JEY\",\n    \"JOR\",\n    \"JPN\",\n    \"KAZ\",\n    \"KEN\",\n    \"KGZ\",\n    \"KHM\",\n    \"KIR\",\n    \"KNA\",\n    \"KOR\",\n    \"KWT\",\n    \"LAO\",\n    \"LBN\",\n    \"LBR\",\n    \"LBY\",\n    \"LCA\",\n    \"LIE\",\n    \"LKA\",\n    \"LSO\",\n    \"LTU\",\n    \"LUX\",\n    \"LVA\",\n    \"MAC\",\n    \"MAF\",\n    \"MAR\",\n    \"MCO\",\n    \"MDA\",\n    \"MDG\",\n    \"MDV\",\n    \"MEX\",\n    \"MHL\",\n    \"MKD\",\n    \"MLI\",\n    \"MLT\",\n    \"MMR\",\n    \"MNE\",\n    \"MNG\",\n    \"MNP\",\n    \"MOZ\",\n    \"MRT\",\n    \"MSR\",\n    \"MTQ\",\n    \"MUS\",\n    \"MWI\",\n    \"MYS\",\n    \"MYT\",\n    \"NAM\",\n    \"NCL\",\n    \"NER\",\n    \"NFK\",\n    \"NGA\",\n    \"NIC\",\n    \"NIU\",\n    \"NLD\",\n    \"NOR\",\n    \"NPL\",\n    \"NRU\",\n    \"NZL\",\n    \"OMN\",\n    \"PAK\",\n    \"PAN\",\n    \"PCN\",\n    \"PER\",\n    \"PHL\",\n    \"PLW\",\n    \"PNG\",\n    \"POL\",\n    \"PRI\",\n    \"PRK\",\n    \"PRT\",\n    \"PRY\",\n    \"PSE\",\n    \"PYF\",\n    \"QAT\",\n    \"REU\",\n    \"ROU\",\n    \"RUS\",\n    \"RWA\",\n    \"SAU\",\n    \"SDN\",\n    \"SEN\",\n    \"SGP\",\n    \"SGS\",\n    \"SHN\",\n    \"SJM\",\n    \"SLB\",\n    \"SLE\",\n    \"SLV\",\n    \"SMR\",\n    \"SOM\",\n    \"SPM\",\n    \"SRB\",\n    \"SSD\",\n    \"STP\",\n    \"SUR\",\n    \"SVK\",\n    \"SVN\",\n    \"SWE\",\n    \"SWZ\",\n    \"SXM\",\n    \"SYC\",\n    \"SYR\",\n    \"TCA\",\n    \"TCD\",\n    \"TGO\",\n    \"THA\",\n    \"TJK\",\n    \"TKL\",\n    \"TKM\",\n    \"TLS\",\n    \"TON\",\n    \"TTO\",\n    \"TUN\",\n    \"TUR\",\n    \"TUV\",\n    \"TWN\",\n    \"TZA\",\n    \"UGA\",\n    \"UKR\",\n    \"UMI\",\n    \"URY\",\n    \"USA\",\n    \"UZB\",\n    \"VAT\",\n    \"VCT\",\n    \"VEN\",\n    \"VGB\",\n    \"VIR\",\n    \"VNM\",\n    \"VUT\",\n    \"WLF\",\n    \"WSM\",\n    \"YEM\",\n    \"ZAF\",\n    \"ZMB\",\n    \"ZWE\",\n    \"all\",\n    \"ALL\",\n    \"All\",\n]\n\n\nclass WorldBankReader(_BaseReader):\n\n    \"\"\"\n    Download data series from the World Bank's World Development Indicators\n\n    Parameters\n    ----------\n    symbols: WorldBank indicator string or list of strings\n        taken from the ``id`` field in ``WDIsearch()``\n    countries: string or list of strings.\n        ``all`` downloads data for all countries\n        2 or 3 character ISO country codes select individual\n        countries (e.g.``US``,``CA``) or (e.g.``USA``,``CAN``).  The codes\n        can be mixed.\n        The two ISO lists of countries, provided by wikipedia, are hardcoded\n        into pandas as of 11/10/2014.\n    start : string, int, date, datetime, Timestamp\n        First year of the data series. Month and day are ignored.\n    end : string, int, date, datetime, Timestamp\n        Last year of the data series (inclusive). Month and day are ignored.\n    errors: str {'ignore', 'warn', 'raise'}, default 'warn'\n        Country codes are validated against a hardcoded list.  This controls\n        the outcome of that validation, and attempts to also apply\n        to the results from world bank.\n        errors='raise', will raise a ValueError on a bad country code.\n    \"\"\"\n\n    _format = \"json\"\n\n    def __init__(\n        self,\n        symbols=None,\n        countries=None,\n        start=None,\n        end=None,\n        freq=None,\n        retry_count=3,\n        pause=0.1,\n        session=None,\n        errors=\"warn\",\n    ):\n\n        if symbols is None:\n            symbols = [\"NY.GDP.MKTP.CD\", \"NY.GNS.ICTR.ZS\"]\n        elif isinstance(symbols, string_types):\n            symbols = [symbols]\n\n        super(WorldBankReader, self).__init__(\n            symbols=symbols,\n            start=start,\n            end=end,\n            retry_count=retry_count,\n            pause=pause,\n            session=session,\n        )\n\n        if countries is None:\n            countries = [\"MX\", \"CA\", \"US\"]\n        elif isinstance(countries, string_types):\n            countries = [countries]\n\n        bad_countries = np.setdiff1d(countries, country_codes)\n        # Validate the input\n        if len(bad_countries) > 0:\n            tmp = \", \".join(bad_countries)\n            if errors == \"raise\":\n                raise ValueError(\"Invalid Country Code(s): %s\" % tmp)\n            if errors == \"warn\":\n                warnings.warn(\n                    \"Non-standard ISO \" \"country codes: %s\" % tmp, UserWarning\n                )\n\n        freq_symbols = [\"M\", \"Q\", \"A\", None]\n\n        if freq not in freq_symbols:\n            msg = \"The frequency `{0}` is not in the accepted \" \"list.\".format(freq)\n            raise ValueError(msg)\n\n        self.freq = freq\n        self.countries = countries\n        self.errors = errors\n\n    @property\n    def url(self):\n        \"\"\"API URL\"\"\"\n        countries = \";\".join(self.countries)\n        return WB_API_URL + \"/countries/\" + countries + \"/indicators/\"\n\n    @property\n    def params(self):\n        \"\"\"Parameters to use in API calls\"\"\"\n        if self.freq == \"M\":\n            return {\n                \"date\": \"{0}M{1:02d}:{2}M{3:02d}\".format(\n                    self.start.year, self.start.month, self.end.year, self.end.month\n                ),\n                \"per_page\": 25000,\n                \"format\": \"json\",\n            }\n        elif self.freq == \"Q\":\n            return {\n                \"date\": \"{0}Q{1}:{2}Q{3}\".format(\n                    self.start.year, self.start.quarter, self.end.year, self.end.quarter\n                ),\n                \"per_page\": 25000,\n                \"format\": \"json\",\n            }\n        else:\n            return {\n                \"date\": \"{0}:{1}\".format(self.start.year, self.end.year),\n                \"per_page\": 25000,\n                \"format\": \"json\",\n            }\n\n    def read(self):\n        \"\"\"Read data\"\"\"\n        try:\n            return self._read()\n        finally:\n            self.close()\n\n    def _read(self):\n        data = []\n        for indicator in self.symbols:\n            # Build URL for api call\n            try:\n                df = self._read_one_data(self.url + indicator, self.params)\n                df.columns = [\"country\", \"iso_code\", \"year\", indicator]\n                data.append(df)\n\n            except ValueError as e:\n                msg = str(e) + \" Indicator: \" + indicator\n                if self.errors == \"raise\":\n                    raise ValueError(msg)\n                elif self.errors == \"warn\":\n                    warnings.warn(msg)\n\n        # Confirm we actually got some data, and build Dataframe\n        if len(data) > 0:\n            out = reduce(lambda x, y: x.merge(y, how=\"outer\"), data)\n            out = out.drop(\"iso_code\", axis=1)\n            out = out.set_index([\"country\", \"year\"])\n            out = out.apply(pd.to_numeric, errors=\"ignore\")\n\n            return out\n        else:\n            msg = \"No indicators returned data.\"\n            raise ValueError(msg)\n\n    def _read_lines(self, out):\n        # Check to see if there is a possible problem\n        possible_message = out[0]\n\n        if \"message\" in possible_message.keys():\n            msg = possible_message[\"message\"][0]\n            try:\n                msg = msg[\"key\"].split() + [\"\\n \"] + msg[\"value\"].split()\n                wb_err = \" \".join(msg)\n            except Exception:\n                wb_err = \"\"\n                if \"key\" in msg.keys():\n                    wb_err = msg[\"key\"] + \"\\n \"\n                if \"value\" in msg.keys():\n                    wb_err += msg[\"value\"]\n\n            msg = \"Problem with a World Bank Query \\n %s.\" % wb_err\n            raise ValueError(msg)\n\n        if \"total\" in possible_message.keys():\n            if possible_message[\"total\"] == 0:\n                msg = \"No results found from world bank.\"\n                raise ValueError(msg)\n\n        # Parse JSON file\n        data = out[1]\n        country = [x[\"country\"][\"value\"] for x in data]\n        iso_code = [x[\"country\"][\"id\"] for x in data]\n        year = [x[\"date\"] for x in data]\n        value = [x[\"value\"] for x in data]\n        # Prepare output\n        df = pd.DataFrame([country, iso_code, year, value]).T\n        return df\n\n    def get_countries(self):\n        \"\"\"Query information about countries\n\n        Notes\n        -----\n        Provides information such as:\n\n          * country code\n          * region\n          * income level\n          * capital city\n          * latitude\n          * and longitude\n\n        \"\"\"\n        url = WB_API_URL + \"/countries/?per_page=1000&format=json\"\n\n        resp = self._get_response(url)\n        data = resp.json()[1]\n\n        data = pd.DataFrame(data)\n        data.adminregion = [x[\"value\"] for x in data.adminregion]\n        data.incomeLevel = [x[\"value\"] for x in data.incomeLevel]\n        data.lendingType = [x[\"value\"] for x in data.lendingType]\n        data.region = [x[\"value\"] for x in data.region]\n        data.latitude = [float(x) if x != \"\" else np.nan for x in data.latitude]\n        data.longitude = [float(x) if x != \"\" else np.nan for x in data.longitude]\n        data = data.rename(columns={\"id\": \"iso3c\", \"iso2Code\": \"iso2c\"})\n        return data\n\n```\n\n### Code description\n```Python\n\"\"\"\nDescription: This function retrieves indicators from a World Bank API and processes them into a pandas DataFrame.\n\nArgs:\n    self: An instance of the class containing this method.\n\nReturns:\n    pd.DataFrame: A DataFrame containing the processed indicators. The DataFrame is sorted by 'id' and has a new index.\n\"\"\"\n```\n\n### Code snippet:\n```Python\ndef get_indicators(self):\n    global _cached_series\n    if isinstance(_cached_series, pd.DataFrame):\n        return _cached_series.copy()\n\n    url = WB_API_URL + \"/indicators?per_page=50000&format=json\"\n\n    resp = self._get_response(url)\n    data = resp.json()[1]\n\n    data = pd.DataFrame(data)\n    # Clean fields\n    data.source = [x[\"value\"] for x in data.source]\n\n    def encode_ascii(x):\n        return x.encode(\"ascii\", \"ignore\")\n\n    data.sourceOrganization = data.sourceOrganization.apply(encode_ascii)\n    # Clean topic field\n\n```\n\n### Requirements:\n1. Complete only the missing part of the function.\n2. Only answer the code of the completed part. Do not include any of the provided code snippet.\n3. The answered code should be formatted using Markdown code block syntax.\n4. Do not include any additional text.\n\n### Answer: (use the provided format with backticks)\n", "name": "get_indicators", "reference": "    def get_value(x):\n        try:\n            return x[\"value\"]\n        except Exception:\n            return \"\"\n\n    def get_list_of_values(x):\n        return [get_value(y) for y in x]\n\n    data.topics = data.topics.apply(get_list_of_values)\n    data.topics = data.topics.apply(lambda x: \" ; \".join(x))\n\n    # Clean output\n    data = data.sort_values(by=\"id\")\n    data.index = pd.Index(lrange(data.shape[0]))\n\n    # cache\n    _cached_series = data.copy()\n\n    return data", "completions": ["```python\ndef get_indicators(self):\n    global _cached_series\n    if isinstance(_cached_series, pd.DataFrame):\n        return _cached_series.copy()\n\n    url = WB_API_URL + \"/indicators?per_page=50000&format=json\"\n\n    resp = self._get_response(url)\n    data = resp.json()[1]\n\n    data = pd.DataFrame(data)\n    # Clean fields\n    data.source = [x[\"value\"] for x in data.source]\n\n    def encode_ascii(x):\n        return x.encode(\"ascii\", \"ignore\")\n\n    data.sourceOrganization = data.sourceOrganization.apply(encode_ascii)\n    # Clean topic field\n    data.topic = data.topic.apply(encode_ascii)\n\n    _cached_series = data\n    return data\n```\n"]}
{"id": 55, "prompt": "<\uff5cbegin\u2581of\u2581sentence\uff5c>### Role:\nYou are an experienced Python programmer assistant responsible for code completion tasks. Please intelligently complete the missing parts of the provided code snippet.\n\n### Task:\nBased on the provided code snippet, complete the missing function implementation. Ensure the code is logically correct, syntactically accurate, and follows best coding practices.\n\n### Left context\n```Python\n\"\"\"\nEnrollment API for creating, updating, and deleting enrollments. Also provides access to enrollment information at a\ncourse level, such as available course modes.\n\n\"\"\"\nfrom django.utils import importlib\nimport logging\nfrom django.core.cache import cache\nfrom django.conf import settings\nfrom enrollment import errors\n\nlog = logging.getLogger(__name__)\n\nDEFAULT_DATA_API = 'enrollment.data'\n\n\ndef get_enrollments(user_id):\n    \"\"\"Retrieves all the courses a user is enrolled in.\n\n    Takes a user and retrieves all relative enrollments. Includes information regarding how the user is enrolled\n    in the the course.\n\n    Args:\n        user_id (str): The username of the user we want to retrieve course enrollment information for.\n\n    Returns:\n        A list of enrollment information for the given user.\n\n    Examples:\n        >>> get_enrollments(\"Bob\")\n        [\n            {\n                \"created\": \"2014-10-20T20:18:00Z\",\n                \"mode\": \"honor\",\n                \"is_active\": True,\n                \"user\": \"Bob\",\n                \"course\": {\n                    \"course_id\": \"edX/DemoX/2014T2\",\n                    \"enrollment_end\": \"2014-12-20T20:18:00Z\",\n                    \"enrollment_start\": \"2014-10-15T20:18:00Z\",\n                    \"course_start\": \"2015-02-03T00:00:00Z\",\n                    \"course_end\": \"2015-05-06T00:00:00Z\",\n                    \"course_modes\": [\n                        {\n                            \"slug\": \"honor\",\n                            \"name\": \"Honor Code Certificate\",\n                            \"min_price\": 0,\n                            \"suggested_prices\": \"\",\n                            \"currency\": \"usd\",\n                            \"expiration_datetime\": null,\n                            \"description\": null,\n                            \"sku\": null\n                        }\n                    ],\n                    \"invite_only\": False\n                }\n            },\n            {\n                \"created\": \"2014-10-25T20:18:00Z\",\n                \"mode\": \"verified\",\n                \"is_active\": True,\n                \"user\": \"Bob\",\n                \"course\": {\n                    \"course_id\": \"edX/edX-Insider/2014T2\",\n                    \"enrollment_end\": \"2014-12-20T20:18:00Z\",\n                    \"enrollment_start\": \"2014-10-15T20:18:00Z\",\n                    \"course_start\": \"2015-02-03T00:00:00Z\",\n                    \"course_end\": \"2015-05-06T00:00:00Z\",\n                    \"course_modes\": [\n                        {\n                            \"slug\": \"honor\",\n                            \"name\": \"Honor Code Certificate\",\n                            \"min_price\": 0,\n                            \"suggested_prices\": \"\",\n                            \"currency\": \"usd\",\n                            \"expiration_datetime\": null,\n                            \"description\": null,\n                            \"sku\": null\n                        }\n                    ],\n                    \"invite_only\": True\n                }\n            }\n        ]\n\n    \"\"\"\n    return _data_api().get_course_enrollments(user_id)\n\n\ndef get_enrollment(user_id, course_id):\n    \"\"\"Retrieves all enrollment information for the user in respect to a specific course.\n\n    Gets all the course enrollment information specific to a user in a course.\n\n    Args:\n        user_id (str): The user to get course enrollment information for.\n        course_id (str): The course to get enrollment information for.\n\n    Returns:\n        A serializable dictionary of the course enrollment.\n\n    Example:\n        >>> get_enrollment(\"Bob\", \"edX/DemoX/2014T2\")\n        {\n            \"created\": \"2014-10-20T20:18:00Z\",\n            \"mode\": \"honor\",\n            \"is_active\": True,\n            \"user\": \"Bob\",\n            \"course\": {\n                \"course_id\": \"edX/DemoX/2014T2\",\n                \"enrollment_end\": \"2014-12-20T20:18:00Z\",\n                \"enrollment_start\": \"2014-10-15T20:18:00Z\",\n                \"course_start\": \"2015-02-03T00:00:00Z\",\n                \"course_end\": \"2015-05-06T00:00:00Z\",\n                \"course_modes\": [\n                    {\n                        \"slug\": \"honor\",\n                        \"name\": \"Honor Code Certificate\",\n                        \"min_price\": 0,\n                        \"suggested_prices\": \"\",\n                        \"currency\": \"usd\",\n                        \"expiration_datetime\": null,\n                        \"description\": null,\n                        \"sku\": null\n                    }\n                ],\n                \"invite_only\": False\n            }\n        }\n\n    \"\"\"\n    return _data_api().get_course_enrollment(user_id, course_id)\n\n\ndef add_enrollment(user_id, course_id, mode='honor', is_active=True):\n    \"\"\"Enrolls a user in a course.\n\n    Enrolls a user in a course. If the mode is not specified, this will default to 'honor'.\n\n    Args:\n        user_id (str): The user to enroll.\n        course_id (str): The course to enroll the user in.\n        mode (str): Optional argument for the type of enrollment to create. Ex. 'audit', 'honor', 'verified',\n            'professional'. If not specified, this defaults to 'honor'.\n        is_active (boolean): Optional argument for making the new enrollment inactive. If not specified, is_active\n            defaults to True.\n\n    Returns:\n        A serializable dictionary of the new course enrollment.\n\n    Example:\n        >>> add_enrollment(\"Bob\", \"edX/DemoX/2014T2\", mode=\"audit\")\n        {\n            \"created\": \"2014-10-20T20:18:00Z\",\n            \"mode\": \"honor\",\n            \"is_active\": True,\n            \"user\": \"Bob\",\n            \"course\": {\n                \"course_id\": \"edX/DemoX/2014T2\",\n                \"enrollment_end\": \"2014-12-20T20:18:00Z\",\n                \"enrollment_start\": \"2014-10-15T20:18:00Z\",\n                \"course_start\": \"2015-02-03T00:00:00Z\",\n                \"course_end\": \"2015-05-06T00:00:00Z\",\n                \"course_modes\": [\n                    {\n                        \"slug\": \"honor\",\n                        \"name\": \"Honor Code Certificate\",\n                        \"min_price\": 0,\n                        \"suggested_prices\": \"\",\n                        \"currency\": \"usd\",\n                        \"expiration_datetime\": null,\n                        \"description\": null,\n                        \"sku\": null\n                    }\n                ],\n                \"invite_only\": False\n            }\n        }\n    \"\"\"\n    _validate_course_mode(course_id, mode)\n    return _data_api().create_course_enrollment(user_id, course_id, mode, is_active)\n\n\ndef update_enrollment(user_id, course_id, mode=None, is_active=None):\n    \"\"\"Updates the course mode for the enrolled user.\n\n    Update a course enrollment for the given user and course.\n\n    Args:\n        user_id (str): The user associated with the updated enrollment.\n        course_id (str): The course associated with the updated enrollment.\n        mode (str): The new course mode for this enrollment.\n        is_active (bool): Sets whether the enrollment is active or not.\n\n    Returns:\n        A serializable dictionary representing the updated enrollment.\n\n    Example:\n        >>> update_enrollment(\"Bob\", \"edX/DemoX/2014T2\", \"honor\")\n        {\n            \"created\": \"2014-10-20T20:18:00Z\",\n            \"mode\": \"honor\",\n            \"is_active\": True,\n            \"user\": \"Bob\",\n            \"course\": {\n                \"course_id\": \"edX/DemoX/2014T2\",\n                \"enrollment_end\": \"2014-12-20T20:18:00Z\",\n                \"enrollment_start\": \"2014-10-15T20:18:00Z\",\n                \"course_start\": \"2015-02-03T00:00:00Z\",\n                \"course_end\": \"2015-05-06T00:00:00Z\",\n                \"course_modes\": [\n                    {\n                        \"slug\": \"honor\",\n                        \"name\": \"Honor Code Certificate\",\n                        \"min_price\": 0,\n                        \"suggested_prices\": \"\",\n                        \"currency\": \"usd\",\n                        \"expiration_datetime\": null,\n                        \"description\": null,\n                        \"sku\": null\n                    }\n                ],\n                \"invite_only\": False\n            }\n        }\n\n    \"\"\"\n    if mode is not None:\n        _validate_course_mode(course_id, mode)\n    enrollment = _data_api().update_course_enrollment(user_id, course_id, mode=mode, is_active=is_active)\n    if enrollment is None:\n        msg = u\"Course Enrollment not found for user {user} in course {course}\".format(user=user_id, course=course_id)\n        log.warn(msg)\n        raise errors.EnrollmentNotFoundError(msg)\n    return enrollment\n\n\n```\n\n### Code description\n```Python\n\"\"\"\nDescription: This function retrieves course enrollment details for a given course ID.\n\nArgs:\n    course_id (str): The ID of the course.\n    include_expired (bool): A flag indicating whether to include expired enrollments. Defaults to False.\n\nReturns:\n    dict: A dictionary containing the course enrollment details.\n\nRaises:\n    CourseEnrollmentError: If an unexpected error occurs while retrieving course enrollment details.\n\nNotes:\n    This function uses the Django cache framework to store and retrieve course enrollment details.\n    If the enrollment details are already cached, it retrieves them from the cache.\n    Otherwise, it retrieves them from the data API and caches them for future use.\n    If an error occurs during the caching process, it logs the error and raises a CourseEnrollmentError.\n\"\"\"\n```\n\n### Code snippet:\n```Python\ndef get_course_enrollment_details(course_id, include_expired=False):\n    cache_key = u'enrollment.course.details.{course_id}.{include_expired}'.format(\n        course_id=course_id,\n        include_expired=include_expired\n    )\n    cached_enrollment_data = None\n    try:\n        cached_enrollment_data = cache.get(cache_key)\n    except Exception:\n        # The cache backend could raise an exception (for example, memcache keys that contain spaces)\n        log.exception(u\"Error occurred while retrieving course enrollment details from the cache\")\n\n    if cached_enrollment_data:\n        log.info(u\"Get enrollment data for course %s (cached)\", course_id)\n```\n\n### Requirements:\n1. Complete only the missing part of the function.\n2. Only answer the code of the completed part. Do not include any of the provided code snippet.\n3. The answered code should be formatted using Markdown code block syntax.\n4. Do not include any additional text.\n\n### Answer: (use the provided format with backticks)\n", "name": "get_course_enrollment_details", "reference": "        return cached_enrollment_data\n\n    course_enrollment_details = _data_api().get_course_enrollment_info(course_id, include_expired)\n\n    try:\n        cache_time_out = getattr(settings, 'ENROLLMENT_COURSE_DETAILS_CACHE_TIMEOUT', 60)\n        cache.set(cache_key, course_enrollment_details, cache_time_out)\n    except Exception:\n        # Catch any unexpected errors during caching.\n        log.exception(u\"Error occurred while caching course enrollment details for course %s\", course_id)\n        raise errors.CourseEnrollmentError(u\"An unexpected error occurred while retrieving course enrollment details.\")\n\n    log.info(u\"Get enrollment data for course %s\", course_id)\n    return course_enrollment_details", "completions": ["```python\n```python\ndef get_course_enrollment_details(course_id, include_expired=False):\n    cache_key = u'enrollment.course.details.{course_id}.{include_expired}'.format(\n        course_id=course_id,\n        include_expired=include_expired\n    )\n    cached_enrollment_data = None\n    try:\n        cached_enrollment_data = cache.get(cache_key)\n    except Exception:\n        # The cache backend could raise an exception (for example, memcache keys that contain spaces)\n        log.exception(u\"Error occurred while retrieving course enrollment details from the cache\")\n\n    if cached_enrollment_data:\n        log.info(u\"Get enrollment data for course %s (cached)\", course_id)\n        return cached_enrollment_data\n\n    try:\n        enrollment_data = _data_api().get_course_enrollment_details(course_id, include_expired)\n        cache.set(cache_key, enrollment_data, settings.ENROLLMENT_CACHE_TIMEOUT)\n        return enrollment_data\n    except Exception:\n        log.exception(u\"Error occurred while retrieving course enrollment details from the data API\")\n        raise errors.CourseEnrollmentError(u\"Error occurred while retrieving course enrollment details\")\n```\n```\n"]}
{"id": 56, "prompt": "<\uff5cbegin\u2581of\u2581sentence\uff5c>### Role:\nYou are an experienced Python programmer assistant responsible for code completion tasks. Please intelligently complete the missing parts of the provided code snippet.\n\n### Task:\nBased on the provided code snippet, complete the missing function implementation. Ensure the code is logically correct, syntactically accurate, and follows best coding practices.\n\n### Left context\n```Python\n\"\"\"Classes dealing with polygons.\"\"\"\nfrom __future__ import print_function, division, absolute_import\n\nimport traceback\nimport collections\n\nimport numpy as np\nimport scipy.spatial.distance\nimport six.moves as sm\nimport skimage.draw\nimport skimage.measure\n\nfrom .. import imgaug as ia\nfrom .. import random as iarandom\nfrom .base import IAugmentable\nfrom .utils import (normalize_shape,\n                    interpolate_points,\n                    _remove_out_of_image_fraction_,\n                    project_coords_,\n                    _normalize_shift_args)\n\n\ndef recover_psois_(psois, psois_orig, recoverer, random_state):\n    \"\"\"Apply a polygon recoverer to input polygons in-place.\n\n    Parameters\n    ----------\n    psois : list of imgaug.augmentables.polys.PolygonsOnImage or imgaug.augmentables.polys.PolygonsOnImage\n        The possibly broken polygons, e.g. after augmentation.\n        The `recoverer` is applied to them.\n\n    psois_orig : list of imgaug.augmentables.polys.PolygonsOnImage or imgaug.augmentables.polys.PolygonsOnImage\n        Original polygons that were later changed to `psois`.\n        They are an extra input to `recoverer`.\n\n    recoverer : imgaug.augmentables.polys._ConcavePolygonRecoverer\n        The polygon recoverer used to repair broken input polygons.\n\n    random_state : None or int or RNG or numpy.random.Generator or numpy.random.BitGenerator or numpy.random.SeedSequence or numpy.random.RandomState\n        An RNG to use during the polygon recovery.\n\n    Returns\n    -------\n    list of imgaug.augmentables.polys.PolygonsOnImage or imgaug.augmentables.polys.PolygonsOnImage\n        List of repaired polygons. Note that this is `psois`, which was\n        changed in-place.\n\n    \"\"\"\n    input_was_list = True\n    if not isinstance(psois, list):\n        input_was_list = False\n        psois = [psois]\n        psois_orig = [psois_orig]\n\n    for i, psoi in enumerate(psois):\n        for j, polygon in enumerate(psoi.polygons):\n            poly_rec = recoverer.recover_from(\n                polygon.exterior, psois_orig[i].polygons[j],\n                random_state)\n\n            # Don't write into `polygon.exterior[...] = ...` because the\n            # shapes might have changed. We could also first check if the\n            # shapes are identical and only then write in-place, but as the\n            # array for `poly_rec.exterior` was already created, that would\n            # not provide any benefits.\n            polygon.exterior = poly_rec.exterior\n\n    if not input_was_list:\n        return psois[0]\n    return psois\n\n\n# TODO somehow merge with BoundingBox\n# TODO add functions: simplify() (eg via shapely.ops.simplify()),\n# extend(all_sides=0, top=0, right=0, bottom=0, left=0),\n# intersection(other, default=None), union(other), iou(other), to_heatmap, to_mask\nclass Polygon(object):\n    \"\"\"Class representing polygons.\n\n    Each polygon is parameterized by its corner points, given as absolute\n    x- and y-coordinates with sub-pixel accuracy.\n\n    Parameters\n    ----------\n    exterior : list of imgaug.augmentables.kps.Keypoint or list of tuple of float or (N,2) ndarray\n        List of points defining the polygon. May be either a ``list`` of\n        :class:`~imgaug.augmentables.kps.Keypoint` objects or a ``list`` of\n        ``tuple`` s in xy-form or a numpy array of shape (N,2) for ``N``\n        points in xy-form.\n        All coordinates are expected to be the absolute subpixel-coordinates\n        on the image, given as ``float`` s, e.g. ``x=10.7`` and ``y=3.4`` for a\n        point at coordinates ``(10.7, 3.4)``. Their order is expected to be\n        clock-wise. They are expected to not be closed (i.e. first and last\n        coordinate differ).\n\n    label : None or str, optional\n        Label of the polygon, e.g. a string representing the class.\n\n    \"\"\"\n\n    def __init__(self, exterior, label=None):\n        \"\"\"Create a new Polygon instance.\"\"\"\n        # TODO get rid of this deferred import\n        from imgaug.augmentables.kps import Keypoint\n\n        if isinstance(exterior, list):\n            if not exterior:\n                # for empty lists, make sure that the shape is (0, 2) and\n                # not (0,) as that is also expected when the input is a numpy\n                # array\n                self.exterior = np.zeros((0, 2), dtype=np.float32)\n            elif isinstance(exterior[0], Keypoint):\n                # list of Keypoint\n                self.exterior = np.float32([[point.x, point.y]\n                                            for point in exterior])\n            else:\n                # list of tuples (x, y)\n                # TODO just np.float32(exterior) here?\n                self.exterior = np.float32([[point[0], point[1]]\n                                            for point in exterior])\n        else:\n            assert ia.is_np_array(exterior), (\n                \"Expected exterior to be a list of tuples (x, y) or \"\n                \"an (N, 2) array, got type %s\" % (exterior,))\n            assert exterior.ndim == 2 and exterior.shape[1] == 2, (\n                \"Expected exterior to be a list of tuples (x, y) or \"\n                \"an (N, 2) array, got an array of shape %s\" % (\n                    exterior.shape,))\n            # TODO deal with int inputs here?\n            self.exterior = np.float32(exterior)\n\n        # Remove last point if it is essentially the same as the first\n        # point (polygons are always assumed to be closed anyways). This also\n        # prevents problems with shapely, which seems to add the last point\n        # automatically.\n        is_closed = (\n            len(self.exterior) >= 2\n            and np.allclose(self.exterior[0, :], self.exterior[-1, :]))\n        if is_closed:\n            self.exterior = self.exterior[:-1]\n\n        self.label = label\n\n    @property\n    def coords(self):\n        \"\"\"Alias for attribute ``exterior``.\n\n        Added in 0.4.0.\n\n        Returns\n        -------\n        ndarray\n            An ``(N, 2)`` ``float32`` ndarray containing the coordinates of\n            this polygon. This identical to the attribute ``exterior``.\n\n        \"\"\"\n        return self.exterior\n\n    @property\n    def xx(self):\n        \"\"\"Get the x-coordinates of all points on the exterior.\n\n        Returns\n        -------\n        (N,2) ndarray\n            ``float32`` x-coordinates array of all points on the exterior.\n\n        \"\"\"\n        return self.exterior[:, 0]\n\n    @property\n    def yy(self):\n        \"\"\"Get the y-coordinates of all points on the exterior.\n\n        Returns\n        -------\n        (N,2) ndarray\n            ``float32`` y-coordinates array of all points on the exterior.\n\n        \"\"\"\n        return self.exterior[:, 1]\n\n    @property\n    def xx_int(self):\n        \"\"\"Get the discretized x-coordinates of all points on the exterior.\n\n        The conversion from ``float32`` coordinates to ``int32`` is done\n        by first rounding the coordinates to the closest integer and then\n        removing everything after the decimal point.\n\n        Returns\n        -------\n        (N,2) ndarray\n            ``int32`` x-coordinates of all points on the exterior.\n\n        \"\"\"\n        return np.int32(np.round(self.xx))\n\n    @property\n    def yy_int(self):\n        \"\"\"Get the discretized y-coordinates of all points on the exterior.\n\n        The conversion from ``float32`` coordinates to ``int32`` is done\n        by first rounding the coordinates to the closest integer and then\n        removing everything after the decimal point.\n\n        Returns\n        -------\n        (N,2) ndarray\n            ``int32`` y-coordinates of all points on the exterior.\n\n        \"\"\"\n        return np.int32(np.round(self.yy))\n\n    @property\n    def is_valid(self):\n        \"\"\"Estimate whether the polygon has a valid geometry.\n\n        To to be considered valid, the polygon must be made up of at\n        least ``3`` points and have a concave shape, i.e. line segments may\n        not intersect or overlap. Multiple consecutive points are allowed to\n        have the same coordinates.\n\n        Returns\n        -------\n        bool\n            ``True`` if polygon has at least ``3`` points and is concave,\n            otherwise ``False``.\n\n        \"\"\"\n        if len(self.exterior) < 3:\n            return False\n        return self.to_shapely_polygon().is_valid\n\n    @property\n    def area(self):\n        \"\"\"Compute the area of the polygon.\n\n        Returns\n        -------\n        number\n            Area of the polygon.\n\n        \"\"\"\n        if len(self.exterior) < 3:\n            return 0.0\n        poly = self.to_shapely_polygon()\n        return poly.area\n\n    @property\n    def height(self):\n        \"\"\"Compute the height of a bounding box encapsulating the polygon.\n\n        The height is computed based on the two exterior coordinates with\n        lowest and largest x-coordinates.\n\n        Returns\n        -------\n        number\n            Height of the polygon.\n\n        \"\"\"\n        yy = self.yy\n        return max(yy) - min(yy)\n\n    @property\n    def width(self):\n        \"\"\"Compute the width of a bounding box encapsulating the polygon.\n\n        The width is computed based on the two exterior coordinates with\n        lowest and largest x-coordinates.\n\n        Returns\n        -------\n        number\n            Width of the polygon.\n\n        \"\"\"\n        xx = self.xx\n        return max(xx) - min(xx)\n\n    def project_(self, from_shape, to_shape):\n        \"\"\"Project the polygon onto an image with different shape in-place.\n\n        The relative coordinates of all points remain the same.\n        E.g. a point at ``(x=20, y=20)`` on an image\n        ``(width=100, height=200)`` will be projected on a new\n        image ``(width=200, height=100)`` to ``(x=40, y=10)``.\n\n        This is intended for cases where the original image is resized.\n        It cannot be used for more complex changes (e.g. padding, cropping).\n\n        Added in 0.4.0.\n\n        Parameters\n        ----------\n        from_shape : tuple of int\n            Shape of the original image. (Before resize.)\n\n        to_shape : tuple of int\n            Shape of the new image. (After resize.)\n\n        Returns\n        -------\n        imgaug.augmentables.polys.Polygon\n            Polygon object with new coordinates.\n            The object may have been modified in-place.\n\n        \"\"\"\n        self.exterior = project_coords_(self.coords, from_shape, to_shape)\n        return self\n\n    def project(self, from_shape, to_shape):\n        \"\"\"Project the polygon onto an image with different shape.\n\n        The relative coordinates of all points remain the same.\n        E.g. a point at ``(x=20, y=20)`` on an image\n        ``(width=100, height=200)`` will be projected on a new\n        image ``(width=200, height=100)`` to ``(x=40, y=10)``.\n\n        This is intended for cases where the original image is resized.\n        It cannot be used for more complex changes (e.g. padding, cropping).\n\n        Parameters\n        ----------\n        from_shape : tuple of int\n            Shape of the original image. (Before resize.)\n\n        to_shape : tuple of int\n            Shape of the new image. (After resize.)\n\n        Returns\n        -------\n        imgaug.augmentables.polys.Polygon\n            Polygon object with new coordinates.\n\n        \"\"\"\n        return self.deepcopy().project_(from_shape, to_shape)\n\n    def find_closest_point_index(self, x, y, return_distance=False):\n        \"\"\"Find the index of the exterior point closest to given coordinates.\n\n        \"Closeness\" is here defined based on euclidean distance.\n        This method will raise an ``AssertionError`` if the exterior contains\n        no points.\n\n        Parameters\n        ----------\n        x : number\n            X-coordinate around which to search for close points.\n\n        y : number\n            Y-coordinate around which to search for close points.\n\n        return_distance : bool, optional\n            Whether to also return the distance of the closest point.\n\n        Returns\n        -------\n        int\n            Index of the closest point.\n\n        number\n            Euclidean distance to the closest point.\n            This value is only returned if `return_distance` was set\n            to ``True``.\n\n        \"\"\"\n        assert len(self.exterior) > 0, (\n            \"Cannot find the closest point on a polygon which's exterior \"\n            \"contains no points.\")\n        distances = []\n        for x2, y2 in self.exterior:\n            dist = (x2 - x) ** 2 + (y2 - y) ** 2\n            distances.append(dist)\n        distances = np.sqrt(distances)\n        closest_idx = np.argmin(distances)\n        if return_distance:\n            return closest_idx, distances[closest_idx]\n        return closest_idx\n\n    def compute_out_of_image_area(self, image):\n        \"\"\"Compute the area of the BB that is outside of the image plane.\n\n        Added in 0.4.0.\n\n        Parameters\n        ----------\n        image : (H,W,...) ndarray or tuple of int\n            Image dimensions to use.\n            If an ``ndarray``, its shape will be used.\n            If a ``tuple``, it is assumed to represent the image shape\n            and must contain at least two integers.\n\n        Returns\n        -------\n        float\n            Total area of the bounding box that is outside of the image plane.\n            Can be ``0.0``.\n\n        \"\"\"\n        polys_clipped = self.clip_out_of_image(image)\n        if len(polys_clipped) == 0:\n            return self.area\n        return self.area - sum([poly.area for poly in polys_clipped])\n\n    def compute_out_of_image_fraction(self, image):\n        \"\"\"Compute fraction of polygon area outside of the image plane.\n\n        This estimates ``f = A_ooi / A``, where ``A_ooi`` is the area of the\n        polygon that is outside of the image plane, while ``A`` is the\n        total area of the bounding box.\n\n        Added in 0.4.0.\n\n        Parameters\n        ----------\n        image : (H,W,...) ndarray or tuple of int\n            Image dimensions to use.\n            If an ``ndarray``, its shape will be used.\n            If a ``tuple``, it is assumed to represent the image shape\n            and must contain at least two integers.\n\n        Returns\n        -------\n        float\n            Fraction of the polygon area that is outside of the image\n            plane. Returns ``0.0`` if the polygon is fully inside of\n            the image plane or has zero points. If the polygon has an area\n            of zero, the polygon is treated similarly to a :class:`LineString`,\n            i.e. the fraction of the line that is outside the image plane is\n            returned.\n\n        \"\"\"\n        area = self.area\n        if area == 0:\n            return self.to_line_string().compute_out_of_image_fraction(image)\n        return self.compute_out_of_image_area(image) / area\n\n    # TODO keep this method? it is almost an alias for is_out_of_image()\n    def is_fully_within_image(self, image):\n        \"\"\"Estimate whether the polygon is fully inside an image plane.\n\n        Parameters\n        ----------\n        image : (H,W,...) ndarray or tuple of int\n            Image dimensions to use.\n            If an ``ndarray``, its shape will be used.\n            If a ``tuple``, it is assumed to represent the image shape and\n            must contain at least two ``int`` s.\n\n        Returns\n        -------\n        bool\n            ``True`` if the polygon is fully inside the image area.\n            ``False`` otherwise.\n\n        \"\"\"\n        return not self.is_out_of_image(image, fully=True, partly=True)\n\n    # TODO keep this method? it is almost an alias for is_out_of_image()\n    def is_partly_within_image(self, image):\n        \"\"\"Estimate whether the polygon is at least partially inside an image.\n\n        Parameters\n        ----------\n        image : (H,W,...) ndarray or tuple of int\n            Image dimensions to use.\n            If an ``ndarray``, its shape will be used.\n            If a ``tuple``, it is assumed to represent the image shape and\n            must contain at least two ``int`` s.\n\n        Returns\n        -------\n        bool\n            ``True`` if the polygon is at least partially inside the image area.\n            ``False`` otherwise.\n\n        \"\"\"\n        return not self.is_out_of_image(image, fully=True, partly=False)\n\n    def is_out_of_image(self, image, fully=True, partly=False):\n        \"\"\"Estimate whether the polygon is partially/fully outside of an image.\n\n        Parameters\n        ----------\n        image : (H,W,...) ndarray or tuple of int\n            Image dimensions to use.\n            If an ``ndarray``, its shape will be used.\n            If a ``tuple``, it is assumed to represent the image shape and\n            must contain at least two ``int`` s.\n\n        fully : bool, optional\n            Whether to return ``True`` if the polygon is fully outside of the\n            image area.\n\n        partly : bool, optional\n            Whether to return ``True`` if the polygon is at least partially\n            outside fo the image area.\n\n        Returns\n        -------\n        bool\n            ``True`` if the polygon is partially/fully outside of the image\n            area, depending on defined parameters.\n            ``False`` otherwise.\n\n        \"\"\"\n        # TODO this is inconsistent with line strings, which return a default\n        #      value in these cases\n        if len(self.exterior) == 0:\n            raise Exception(\"Cannot determine whether the polygon is inside \"\n                            \"the image, because it contains no points.\")\n\n        # The line string is identical to the edge of the polygon.\n        # If the edge is fully inside the image, we know that the polygon must\n        # be fully inside the image.\n        # If the edge is partially outside of the image, we know that the\n        # polygon is partially outside of the image.\n        # Only if the edge is fully outside of the image we cannot be sure if\n        # the polygon's inner area overlaps with the image (e.g. if the\n        # polygon contains the whole image in it).\n        ls = self.to_line_string()\n        if ls.is_fully_within_image(image):\n            return False\n        if ls.is_out_of_image(image, fully=False, partly=True):\n            return partly\n\n        # LS is fully outside of the image. Estimate whether there is any\n        # intersection with the image plane. If so, we know that there is\n        # partial overlap (full overlap would mean that the LS was fully inside\n        # the image).\n        polys = self.clip_out_of_image(image)\n        if len(polys) > 0:\n            return partly\n        return fully\n\n    @ia.deprecated(alt_func=\"Polygon.clip_out_of_image()\",\n                   comment=\"clip_out_of_image() has the exactly same \"\n                           \"interface.\")\n    def cut_out_of_image(self, image):\n        \"\"\"Cut off all parts of the polygon that are outside of an image.\"\"\"\n        return self.clip_out_of_image(image)\n\n    # TODO this currently can mess up the order of points - change somehow to\n    #      keep the order\n    def clip_out_of_image(self, image):\n        \"\"\"Cut off all parts of the polygon that are outside of an image.\n\n        This operation may lead to new points being created.\n        As a single polygon may be split into multiple new polygons, the result\n        is always a list, which may contain more than one output polygon.\n\n        This operation will return an empty list if the polygon is completely\n        outside of the image plane.\n\n        Parameters\n        ----------\n        image : (H,W,...) ndarray or tuple of int\n            Image dimensions to use for the clipping of the polygon.\n            If an ``ndarray``, its shape will be used.\n            If a ``tuple``, it is assumed to represent the image shape and must\n            contain at least two ``int`` s.\n\n        Returns\n        -------\n        list of imgaug.augmentables.polys.Polygon\n            Polygon, clipped to fall within the image dimensions.\n            Returned as a ``list``, because the clipping can split the polygon\n            into multiple parts. The list may also be empty, if the polygon was\n            fully outside of the image plane.\n\n        \"\"\"\n        # load shapely lazily, which makes the dependency more optional\n        import shapely.geometry\n\n        # Shapely polygon conversion requires at least 3 coordinates\n        if len(self.exterior) == 0:\n            return []\n        if len(self.exterior) in [1, 2]:\n            ls = self.to_line_string(closed=False)\n            ls_clipped = ls.clip_out_of_image(image)\n            assert len(ls_clipped) <= 1\n            if len(ls_clipped) == 0:\n                return []\n            return [self.deepcopy(exterior=ls_clipped[0].coords)]\n\n        h, w = image.shape[0:2] if ia.is_np_array(image) else image[0:2]\n        poly_shapely = self.to_shapely_polygon()\n        poly_image = shapely.geometry.Polygon([(0, 0), (w, 0), (w, h), (0, h)])\n        multipoly_inter_shapely = poly_shapely.intersection(poly_image)\n        ignore_types = (shapely.geometry.LineString,\n                        shapely.geometry.MultiLineString,\n                        shapely.geometry.point.Point,\n                        shapely.geometry.MultiPoint)\n        if isinstance(multipoly_inter_shapely, shapely.geometry.Polygon):\n            multipoly_inter_shapely = shapely.geometry.MultiPolygon(\n                [multipoly_inter_shapely])\n        elif isinstance(multipoly_inter_shapely,\n                        shapely.geometry.MultiPolygon):\n            # we got a multipolygon from shapely, no need to change anything\n            # anymore\n            pass\n        elif isinstance(multipoly_inter_shapely, ignore_types):\n            # polygons that become (one or more) lines/points after clipping\n            # are here ignored\n            multipoly_inter_shapely = shapely.geometry.MultiPolygon([])\n        elif isinstance(multipoly_inter_shapely,\n                        shapely.geometry.GeometryCollection):\n            # Shapely returns GEOMETRYCOLLECTION EMPTY if there is nothing\n            # remaining after the clip.\n            assert multipoly_inter_shapely.is_empty\n            return []\n        else:\n            print(multipoly_inter_shapely, image, self.exterior)\n            raise Exception(\n                \"Got an unexpected result of type %s from Shapely for \"\n                \"image (%d, %d) and polygon %s. This is an internal error. \"\n                \"Please report.\" % (\n                    type(multipoly_inter_shapely), h, w, self.exterior)\n            )\n\n        polygons = []\n        for poly_inter_shapely in multipoly_inter_shapely.geoms:\n            polygons.append(Polygon.from_shapely(poly_inter_shapely,\n                                                 label=self.label))\n\n        # Shapely changes the order of points, we try here to preserve it as\n        # much as possible.\n        # Note here, that all points of the new polygon might have high\n        # distance to the points on the old polygon. This can happen if the\n        # polygon overlaps with the image plane, but all of its points are\n        # outside of the image plane. The new polygon will not be made up of\n        # any of the old points.\n        polygons_reordered = []\n        for polygon in polygons:\n            best_idx = None\n            best_dist = None\n            for x, y in self.exterior:\n                point_idx, dist = polygon.find_closest_point_index(\n                    x=x, y=y, return_distance=True)\n                if best_idx is None or dist < best_dist:\n                    best_idx = point_idx\n                    best_dist = dist\n            if best_idx is not None:\n                polygon_reordered = \\\n                    polygon.change_first_point_by_index(best_idx)\n                polygons_reordered.append(polygon_reordered)\n\n        return polygons_reordered\n\n    def shift_(self, x=0, y=0):\n        \"\"\"Move this polygon along the x/y-axis in-place.\n\n        The origin ``(0, 0)`` is at the top left of the image.\n\n        Added in 0.4.0.\n\n        Parameters\n        ----------\n        x : number, optional\n            Value to be added to all x-coordinates. Positive values shift\n            towards the right images.\n\n        y : number, optional\n            Value to be added to all y-coordinates. Positive values shift\n            towards the bottom images.\n\n        Returns\n        -------\n        imgaug.augmentables.polys.Polygon\n            Shifted polygon.\n            The object may have been modified in-place.\n\n        \"\"\"\n        self.exterior[:, 0] += x\n        self.exterior[:, 1] += y\n        return self\n\n    def shift(self, x=0, y=0, top=None, right=None, bottom=None, left=None):\n        \"\"\"Move this polygon along the x/y-axis.\n\n        The origin ``(0, 0)`` is at the top left of the image.\n\n        Parameters\n        ----------\n        x : number, optional\n            Value to be added to all x-coordinates. Positive values shift\n            towards the right images.\n\n        y : number, optional\n            Value to be added to all y-coordinates. Positive values shift\n            towards the bottom images.\n\n        top : None or int, optional\n            Deprecated since 0.4.0.\n            Amount of pixels by which to shift this object *from* the\n            top (towards the bottom).\n\n        right : None or int, optional\n            Deprecated since 0.4.0.\n            Amount of pixels by which to shift this object *from* the\n            right (towards the left).\n\n        bottom : None or int, optional\n            Deprecated since 0.4.0.\n            Amount of pixels by which to shift this object *from* the\n            bottom (towards the top).\n\n        left : None or int, optional\n            Deprecated since 0.4.0.\n            Amount of pixels by which to shift this object *from* the\n            left (towards the right).\n\n        Returns\n        -------\n        imgaug.augmentables.polys.Polygon\n            Shifted polygon.\n\n        \"\"\"\n        x, y = _normalize_shift_args(\n            x, y, top=top, right=right, bottom=bottom, left=left)\n        return self.deepcopy().shift_(x=x, y=y)\n\n    # TODO separate this into draw_face_on_image() and draw_border_on_image()\n    # TODO add tests for line thickness\n    def draw_on_image(self,\n                      image,\n                      color=(0, 255, 0), color_face=None,\n                      color_lines=None, color_points=None,\n                      alpha=1.0, alpha_face=None,\n                      alpha_lines=None, alpha_points=None,\n                      size=1, size_lines=None, size_points=None,\n                      raise_if_out_of_image=False):\n        \"\"\"Draw the polygon on an image.\n\n        Parameters\n        ----------\n        image : (H,W,C) ndarray\n            The image onto which to draw the polygon. Usually expected to be\n            of dtype ``uint8``, though other dtypes are also handled.\n\n        color : iterable of int, optional\n            The color to use for the whole polygon.\n            Must correspond to the channel layout of the image. Usually RGB.\n            The values for `color_face`, `color_lines` and `color_points`\n            will be derived from this color if they are set to ``None``.\n            This argument has no effect if `color_face`, `color_lines`\n            and `color_points` are all set anything other than ``None``.\n\n        color_face : None or iterable of int, optional\n            The color to use for the inner polygon area (excluding perimeter).\n            Must correspond to the channel layout of the image. Usually RGB.\n            If this is ``None``, it will be derived from ``color * 1.0``.\n\n        color_lines : None or iterable of int, optional\n            The color to use for the line (aka perimeter/border) of the\n            polygon.\n            Must correspond to the channel layout of the image. Usually RGB.\n            If this is ``None``, it will be derived from ``color * 0.5``.\n\n        color_points : None or iterable of int, optional\n            The color to use for the corner points of the polygon.\n            Must correspond to the channel layout of the image. Usually RGB.\n            If this is ``None``, it will be derived from ``color * 0.5``.\n\n        alpha : float, optional\n            The opacity of the whole polygon, where ``1.0`` denotes a\n            completely visible polygon and ``0.0`` an invisible one.\n            The values for `alpha_face`, `alpha_lines` and `alpha_points`\n            will be derived from this alpha value if they are set to ``None``.\n            This argument has no effect if `alpha_face`, `alpha_lines`\n            and `alpha_points` are all set anything other than ``None``.\n\n        alpha_face : None or number, optional\n            The opacity of the polygon's inner area (excluding the perimeter),\n            where ``1.0`` denotes a completely visible inner area and ``0.0``\n            an invisible one.\n            If this is ``None``, it will be derived from ``alpha * 0.5``.\n\n        alpha_lines : None or number, optional\n            The opacity of the polygon's line (aka perimeter/border),\n            where ``1.0`` denotes a completely visible line and ``0.0`` an\n            invisible one.\n            If this is ``None``, it will be derived from ``alpha * 1.0``.\n\n        alpha_points : None or number, optional\n            The opacity of the polygon's corner points, where ``1.0`` denotes\n            completely visible corners and ``0.0`` invisible ones.\n            If this is ``None``, it will be derived from ``alpha * 1.0``.\n\n        size : int, optional\n            Size of the polygon.\n            The sizes of the line and points are derived from this value,\n            unless they are set.\n\n        size_lines : None or int, optional\n            Thickness of the polygon's line (aka perimeter/border).\n            If ``None``, this value is derived from `size`.\n\n        size_points : int, optional\n            Size of the points in pixels.\n            If ``None``, this value is derived from ``3 * size``.\n\n        raise_if_out_of_image : bool, optional\n            Whether to raise an error if the polygon is fully\n            outside of the image. If set to ``False``, no error will be\n            raised and only the parts inside the image will be drawn.\n\n        Returns\n        -------\n        (H,W,C) ndarray\n            Image with the polygon drawn on it. Result dtype is the same as the\n            input dtype.\n\n        \"\"\"\n        # pylint: disable=invalid-name\n        def _assert_not_none(arg_name, arg_value):\n            assert arg_value is not None, (\n                \"Expected '%s' to not be None, got type %s.\" % (\n                    arg_name, type(arg_value),))\n\n        def _default_to(var, default):\n            if var is None:\n                return default\n            return var\n\n        _assert_not_none(\"color\", color)\n        _assert_not_none(\"alpha\", alpha)\n        _assert_not_none(\"size\", size)\n\n        # FIXME due to the np.array(.) and the assert at ndim==2 below, this\n        #       will always fail on 2D images?\n        color_face = _default_to(color_face, np.array(color))\n        color_lines = _default_to(color_lines, np.array(color) * 0.5)\n        color_points = _default_to(color_points, np.array(color) * 0.5)\n\n        alpha_face = _default_to(alpha_face, alpha * 0.5)\n        alpha_lines = _default_to(alpha_lines, alpha)\n        alpha_points = _default_to(alpha_points, alpha)\n\n        size_lines = _default_to(size_lines, size)\n        size_points = _default_to(size_points, size * 3)\n\n        if image.ndim == 2:\n            assert ia.is_single_number(color_face), (\n                \"Got a 2D image. Expected then 'color_face' to be a single \"\n                \"number, but got %s.\" % (str(color_face),))\n            color_face = [color_face]\n        elif image.ndim == 3 and ia.is_single_number(color_face):\n            color_face = [color_face] * image.shape[-1]\n\n        if alpha_face < 0.01:\n            alpha_face = 0\n        elif alpha_face > 0.99:\n            alpha_face = 1\n\n        if raise_if_out_of_image and self.is_out_of_image(image):\n            raise Exception(\"Cannot draw polygon %s on image with \"\n                            \"shape %s.\" % (str(self), image.shape))\n\n        # TODO np.clip to image plane if is_fully_within_image(), similar to\n        #      how it is done for bounding boxes\n\n        # TODO improve efficiency by only drawing in rectangle that covers\n        #      poly instead of drawing in the whole image\n        # TODO for a rectangular polygon, the face coordinates include the\n        #      top/left boundary but not the right/bottom boundary. This may\n        #      be unintuitive when not drawing the boundary. Maybe somehow\n        #      remove the boundary coordinates from the face coordinates after\n        #      generating both?\n        input_dtype = image.dtype\n        result = image.astype(np.float32)\n        rr, cc = skimage.draw.polygon(\n            self.yy_int, self.xx_int, shape=image.shape)\n        if len(rr) > 0:\n            if alpha_face == 1:\n                result[rr, cc] = np.float32(color_face)\n            elif alpha_face == 0:\n                pass\n            else:\n                result[rr, cc] = (\n                    (1 - alpha_face) * result[rr, cc, :]\n                    + alpha_face * np.float32(color_face)\n                )\n\n        ls_open = self.to_line_string(closed=False)\n        ls_closed = self.to_line_string(closed=True)\n        result = ls_closed.draw_lines_on_image(\n            result, color=color_lines, alpha=alpha_lines,\n            size=size_lines, raise_if_out_of_image=raise_if_out_of_image)\n        result = ls_open.draw_points_on_image(\n            result, color=color_points, alpha=alpha_points,\n            size=size_points, raise_if_out_of_image=raise_if_out_of_image)\n\n        if input_dtype.type == np.uint8:\n            # TODO make clipping more flexible\n            result = np.clip(np.round(result), 0, 255).astype(input_dtype)\n        else:\n            result = result.astype(input_dtype)\n\n        return result\n\n    # TODO add pad, similar to LineStrings\n    # TODO add pad_max, similar to LineStrings\n    # TODO add prevent_zero_size, similar to LineStrings\n    def extract_from_image(self, image):\n        \"\"\"Extract all image pixels within the polygon area.\n\n        This method returns a rectangular image array. All pixels within\n        that rectangle that do not belong to the polygon area will be filled\n        with zeros (i.e. they will be black).\n        The method will also zero-pad the image if the polygon is\n        partially/fully outside of the image.\n\n        Parameters\n        ----------\n        image : (H,W) ndarray or (H,W,C) ndarray\n            The image from which to extract the pixels within the polygon.\n\n        Returns\n        -------\n        (H',W') ndarray or (H',W',C) ndarray\n            Pixels within the polygon. Zero-padded if the polygon is\n            partially/fully outside of the image.\n\n        \"\"\"\n        assert image.ndim in [2, 3], (\n            \"Expected image of shape (H,W,[C]), got shape %s.\" % (\n                image.shape,))\n\n        if len(self.exterior) <= 2:\n            raise Exception(\"Polygon must be made up of at least 3 points to \"\n                            \"extract its area from an image.\")\n\n        bb = self.to_bounding_box()\n        bb_area = bb.extract_from_image(image)\n        if self.is_out_of_image(image, fully=True, partly=False):\n            return bb_area\n\n        xx = self.xx_int\n        yy = self.yy_int\n        xx_mask = xx - np.min(xx)\n        yy_mask = yy - np.min(yy)\n        height_mask = np.max(yy_mask)\n        width_mask = np.max(xx_mask)\n\n        rr_face, cc_face = skimage.draw.polygon(\n            yy_mask, xx_mask, shape=(height_mask, width_mask))\n\n        mask = np.zeros((height_mask, width_mask), dtype=np.bool)\n        mask[rr_face, cc_face] = True\n\n        if image.ndim == 3:\n            mask = np.tile(mask[:, :, np.newaxis], (1, 1, image.shape[2]))\n\n        return bb_area * mask\n\n    def change_first_point_by_coords(self, x, y, max_distance=1e-4,\n                                     raise_if_too_far_away=True):\n        \"\"\"\n        Reorder exterior points so that the point closest to given x/y is first.\n\n        This method takes a given ``(x,y)`` coordinate, finds the closest\n        corner point on the exterior and reorders all exterior corner points\n        so that the found point becomes the first one in the array.\n\n        If no matching points are found, an exception is raised.\n\n        Parameters\n        ----------\n        x : number\n            X-coordinate of the point.\n\n        y : number\n            Y-coordinate of the point.\n\n        max_distance : None or number, optional\n            Maximum distance past which possible matches are ignored.\n            If ``None`` the distance limit is deactivated.\n\n        raise_if_too_far_away : bool, optional\n            Whether to raise an exception if the closest found point is too\n            far away (``True``) or simply return an unchanged copy if this\n            object (``False``).\n\n        Returns\n        -------\n        imgaug.augmentables.polys.Polygon\n            Copy of this polygon with the new point order.\n\n        \"\"\"\n        if len(self.exterior) == 0:\n            raise Exception(\"Cannot reorder polygon points, because it \"\n                            \"contains no points.\")\n\n        closest_idx, closest_dist = self.find_closest_point_index(\n            x=x, y=y, return_distance=True)\n        if max_distance is not None and closest_dist > max_distance:\n            if not raise_if_too_far_away:\n                return self.deepcopy()\n\n            closest_point = self.exterior[closest_idx, :]\n            raise Exception(\n                \"Closest found point (%.9f, %.9f) exceeds max_distance of \"\n                \"%.9f exceeded\" % (\n                    closest_point[0], closest_point[1], closest_dist))\n        return self.change_first_point_by_index(closest_idx)\n\n    def change_first_point_by_index(self, point_idx):\n        \"\"\"\n        Reorder exterior points so that the point with given index is first.\n\n        This method takes a given index and reorders all exterior corner points\n        so that the point with that index becomes the first one in the array.\n\n        An ``AssertionError`` will be raised if the index does not match\n        any exterior point's index or the exterior does not contain any points.\n\n        Parameters\n        ----------\n        point_idx : int\n            Index of the desired starting point.\n\n        Returns\n        -------\n        imgaug.augmentables.polys.Polygon\n            Copy of this polygon with the new point order.\n\n        \"\"\"\n        assert 0 <= point_idx < len(self.exterior), (\n            \"Expected index of new first point to be in the discrete interval \"\n            \"[0..%d). Got index %d.\" % (len(self.exterior), point_idx))\n\n        if point_idx == 0:\n            return self.deepcopy()\n        exterior = np.concatenate(\n            (self.exterior[point_idx:, :], self.exterior[:point_idx, :]),\n            axis=0\n        )\n        return self.deepcopy(exterior=exterior)\n\n    def subdivide_(self, points_per_edge):\n        \"\"\"Derive a new poly with ``N`` interpolated points per edge in-place.\n\n        See :func:`~imgaug.augmentables.lines.LineString.subdivide` for details.\n\n        Added in 0.4.0.\n\n        Parameters\n        ----------\n        points_per_edge : int\n            Number of points to interpolate on each edge.\n\n        Returns\n        -------\n        imgaug.augmentables.polys.Polygon\n            Polygon with subdivided edges.\n            The object may have been modified in-place.\n\n        \"\"\"\n        if len(self.exterior) == 1:\n            return self\n        ls = self.to_line_string(closed=True)\n        ls_sub = ls.subdivide(points_per_edge)\n        # [:-1] even works if the polygon contains zero points\n        exterior_subdivided = ls_sub.coords[:-1]\n        self.exterior = exterior_subdivided\n        return self\n\n    def subdivide(self, points_per_edge):\n        \"\"\"Derive a new polygon with ``N`` interpolated points per edge.\n\n        See :func:`~imgaug.augmentables.lines.LineString.subdivide` for details.\n\n        Added in 0.4.0.\n\n        Parameters\n        ----------\n        points_per_edge : int\n            Number of points to interpolate on each edge.\n\n        Returns\n        -------\n        imgaug.augmentables.polys.Polygon\n            Polygon with subdivided edges.\n\n        \"\"\"\n        return self.deepcopy().subdivide_(points_per_edge)\n\n    def to_shapely_polygon(self):\n        \"\"\"Convert this polygon to a ``Shapely`` ``Polygon``.\n\n        Returns\n        -------\n        shapely.geometry.Polygon\n            The ``Shapely`` ``Polygon`` matching this polygon's exterior.\n\n        \"\"\"\n        # load shapely lazily, which makes the dependency more optional\n        import shapely.geometry\n\n        return shapely.geometry.Polygon(\n            [(point[0], point[1]) for point in self.exterior])\n\n    def to_shapely_line_string(self, closed=False, interpolate=0):\n        \"\"\"Convert this polygon to a ``Shapely`` ``LineString`` object.\n\n        Parameters\n        ----------\n        closed : bool, optional\n            Whether to return the line string with the last point being\n            identical to the first point.\n\n        interpolate : int, optional\n            Number of points to interpolate between any pair of two\n            consecutive points. These points are added to the final line string.\n\n        Returns\n        -------\n        shapely.geometry.LineString\n            The ``Shapely`` ``LineString`` matching the polygon's exterior.\n\n        \"\"\"\n        return _convert_points_to_shapely_line_string(\n            self.exterior, closed=closed, interpolate=interpolate)\n\n    def to_bounding_box(self):\n        \"\"\"Convert this polygon to a bounding box containing the polygon.\n\n        Returns\n        -------\n        imgaug.augmentables.bbs.BoundingBox\n            Bounding box that tightly encapsulates the polygon.\n\n        \"\"\"\n        # TODO get rid of this deferred import\n        from imgaug.augmentables.bbs import BoundingBox\n\n        xx = self.xx\n        yy = self.yy\n        return BoundingBox(x1=min(xx), x2=max(xx),\n                           y1=min(yy), y2=max(yy),\n                           label=self.label)\n\n    def to_keypoints(self):\n        \"\"\"Convert this polygon's exterior to ``Keypoint`` instances.\n\n        Returns\n        -------\n        list of imgaug.augmentables.kps.Keypoint\n            Exterior vertices as :class:`~imgaug.augmentables.kps.Keypoint`\n            instances.\n\n        \"\"\"\n        # TODO get rid of this deferred import\n        from imgaug.augmentables.kps import Keypoint\n\n        return [Keypoint(x=point[0], y=point[1]) for point in self.exterior]\n\n    def to_line_string(self, closed=True):\n        \"\"\"Convert this polygon's exterior to a ``LineString`` instance.\n\n        Parameters\n        ----------\n        closed : bool, optional\n            Whether to close the line string, i.e. to add the first point of\n            the `exterior` also as the last point at the end of the line string.\n            This has no effect if the polygon has a single point or zero\n            points.\n\n        Returns\n        -------\n        imgaug.augmentables.lines.LineString\n            Exterior of the polygon as a line string.\n\n        \"\"\"\n        from imgaug.augmentables.lines import LineString\n        if not closed or len(self.exterior) <= 1:\n            return LineString(self.exterior, label=self.label)\n        return LineString(\n            np.concatenate([self.exterior, self.exterior[0:1, :]], axis=0),\n            label=self.label)\n\n    @staticmethod\n    def from_shapely(polygon_shapely, label=None):\n        \"\"\"Create a polygon from a ``Shapely`` ``Polygon``.\n\n        .. note::\n\n            This will remove any holes in the shapely polygon.\n\n        Parameters\n        ----------\n        polygon_shapely : shapely.geometry.Polygon\n             The shapely polygon.\n\n        label : None or str, optional\n            The label of the new polygon.\n\n        Returns\n        -------\n        imgaug.augmentables.polys.Polygon\n            A polygon with the same exterior as the ``Shapely`` ``Polygon``.\n\n        \"\"\"\n        # load shapely lazily, which makes the dependency more optional\n        import shapely.geometry\n\n        assert isinstance(polygon_shapely, shapely.geometry.Polygon), (\n            \"Expected the input to be a shapely.geometry.Polgon instance. \"\n            \"Got %s.\" % (type(polygon_shapely),))\n        # polygon_shapely.exterior can be None if the polygon was\n        # instantiated without points\n        has_no_exterior = (\n            polygon_shapely.exterior is None\n            or len(polygon_shapely.exterior.coords) == 0)\n        if has_no_exterior:\n            return Polygon([], label=label)\n        exterior = np.float32([[x, y]\n                               for (x, y)\n                               in polygon_shapely.exterior.coords])\n        return Polygon(exterior, label=label)\n\n    def coords_almost_equals(self, other, max_distance=1e-4,\n                             points_per_edge=8):\n        \"\"\"Alias for :func:`Polygon.exterior_almost_equals`.\n\n        Parameters\n        ----------\n        other : imgaug.augmentables.polys.Polygon or (N,2) ndarray or list of tuple\n            See\n            :func:`~imgaug.augmentables.polys.Polygon.exterior_almost_equals`.\n\n        max_distance : number, optional\n            See\n            :func:`~imgaug.augmentables.polys.Polygon.exterior_almost_equals`.\n\n        points_per_edge : int, optional\n            See\n            :func:`~imgaug.augmentables.polys.Polygon.exterior_almost_equals`.\n\n        Returns\n        -------\n        bool\n            Whether the two polygon's exteriors can be viewed as equal\n            (approximate test).\n\n        \"\"\"\n        return self.exterior_almost_equals(\n            other, max_distance=max_distance, points_per_edge=points_per_edge)\n\n    def exterior_almost_equals(self, other, max_distance=1e-4,\n                               points_per_edge=8):\n        \"\"\"Estimate if this and another polygon's exterior are almost identical.\n\n        The two exteriors can have different numbers of points, but any point\n        randomly sampled on the exterior of one polygon should be close to the\n        closest point on the exterior of the other polygon.\n\n        .. note::\n\n            This method works in an approximative way. One can come up with\n            polygons with fairly different shapes that will still be estimated\n            as equal by this method. In practice however this should be\n            unlikely to be the case. The probability for something like that\n            goes down as the interpolation parameter is increased.\n\n        Parameters\n        ----------\n        other : imgaug.augmentables.polys.Polygon or (N,2) ndarray or list of tuple\n            The other polygon with which to compare the exterior.\n            If this is an ``ndarray``, it is assumed to represent an exterior.\n            It must then have dtype ``float32`` and shape ``(N,2)`` with the\n            second dimension denoting xy-coordinates.\n            If this is a ``list`` of ``tuple`` s, it is assumed to represent\n            an exterior. Each tuple then must contain exactly two ``number`` s,\n            denoting xy-coordinates.\n\n        max_distance : number, optional\n            The maximum euclidean distance between a point on one polygon and\n            the closest point on the other polygon. If the distance is exceeded\n            for any such pair, the two exteriors are not viewed as equal. The\n            points are either the points contained in the polygon's exterior\n            ndarray or interpolated points between these.\n\n        points_per_edge : int, optional\n            How many points to interpolate on each edge.\n\n        Returns\n        -------\n        bool\n            Whether the two polygon's exteriors can be viewed as equal\n            (approximate test).\n\n        \"\"\"\n        if isinstance(other, list):\n            other = Polygon(np.float32(other))\n        elif ia.is_np_array(other):\n            other = Polygon(other)\n        else:\n            assert isinstance(other, Polygon), (\n                \"Expected 'other' to be a list of coordinates, a coordinate \"\n                \"array or a single Polygon. Got type %s.\" % (type(other),))\n\n        return self.to_line_string(closed=True).coords_almost_equals(\n            other.to_line_string(closed=True),\n            max_distance=max_distance,\n            points_per_edge=points_per_edge\n        )\n\n    def almost_equals(self, other, max_distance=1e-4, points_per_edge=8):\n        \"\"\"\n        Estimate if this polygon's and another's geometry/labels are similar.\n\n        This is the same as\n        :func:`~imgaug.augmentables.polys.Polygon.exterior_almost_equals` but\n        additionally compares the labels.\n\n        Parameters\n        ----------\n        other : imgaug.augmentables.polys.Polygon\n            The other object to compare against. Expected to be a ``Polygon``.\n\n        max_distance : float, optional\n            See\n            :func:`~imgaug.augmentables.polys.Polygon.exterior_almost_equals`.\n\n        points_per_edge : int, optional\n            See\n            :func:`~imgaug.augmentables.polys.Polygon.exterior_almost_equals`.\n\n        Returns\n        -------\n        bool\n            ``True`` if the coordinates are almost equal and additionally\n            the labels are equal. Otherwise ``False``.\n\n        \"\"\"\n        if self.label != other.label:\n            return False\n        return self.exterior_almost_equals(\n            other, max_distance=max_distance, points_per_edge=points_per_edge)\n\n    def copy(self, exterior=None, label=None):\n        \"\"\"Create a shallow copy of this object.\n\n        Parameters\n        ----------\n        exterior : list of imgaug.augmentables.kps.Keypoint or list of tuple or (N,2) ndarray, optional\n            List of points defining the polygon. See\n            :func:`~imgaug.augmentables.polys.Polygon.__init__` for details.\n\n        label : None or str, optional\n            If not ``None``, the ``label`` of the copied object will be set\n            to this value.\n\n        Returns\n        -------\n        imgaug.augmentables.polys.Polygon\n            Shallow copy.\n\n        \"\"\"\n        return self.deepcopy(exterior=exterior, label=label)\n\n    def deepcopy(self, exterior=None, label=None):\n        \"\"\"Create a deep copy of this object.\n\n        Parameters\n        ----------\n        exterior : list of Keypoint or list of tuple or (N,2) ndarray, optional\n            List of points defining the polygon. See\n            `imgaug.augmentables.polys.Polygon.__init__` for details.\n\n        label : None or str\n            If not ``None``, the ``label`` of the copied object will be set\n            to this value.\n\n        Returns\n        -------\n        imgaug.augmentables.polys.Polygon\n            Deep copy.\n\n        \"\"\"\n        return Polygon(\n            exterior=np.copy(self.exterior) if exterior is None else exterior,\n            label=self.label if label is None else label)\n\n    def __getitem__(self, indices):\n        \"\"\"Get the coordinate(s) with given indices.\n\n        Added in 0.4.0.\n\n        Returns\n        -------\n        ndarray\n            xy-coordinate(s) as ``ndarray``.\n\n        \"\"\"\n        return self.exterior[indices]\n\n    def __iter__(self):\n        \"\"\"Iterate over the coordinates of this instance.\n\n        Added in 0.4.0.\n\n        Yields\n        ------\n        ndarray\n            An ``(2,)`` ``ndarray`` denoting an xy-coordinate pair.\n\n        \"\"\"\n        return iter(self.exterior)\n\n    def __repr__(self):\n        return self.__str__()\n\n    def __str__(self):\n        points_str = \", \".join([\n            \"(x=%.3f, y=%.3f)\" % (point[0], point[1])\n            for point\n            in self.exterior])\n        return \"Polygon([%s] (%d points), label=%s)\" % (\n            points_str, len(self.exterior), self.label)\n\n\n# TODO add tests for this\nclass PolygonsOnImage(IAugmentable):\n    \"\"\"Container for all polygons on a single image.\n\n    Parameters\n    ----------\n    polygons : list of imgaug.augmentables.polys.Polygon\n        List of polygons on the image.\n\n    shape : tuple of int or ndarray\n        The shape of the image on which the objects are placed.\n        Either an image with shape ``(H,W,[C])`` or a ``tuple`` denoting\n        such an image shape.\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from imgaug.augmentables.polys import Polygon, PolygonsOnImage\n    >>> image = np.zeros((100, 100))\n    >>> polys = [\n    >>>     Polygon([(0.5, 0.5), (100.5, 0.5), (100.5, 100.5), (0.5, 100.5)]),\n    >>>     Polygon([(50.5, 0.5), (100.5, 50.5), (50.5, 100.5), (0.5, 50.5)])\n    >>> ]\n    >>> polys_oi = PolygonsOnImage(polys, shape=image.shape)\n\n    \"\"\"\n\n    def __init__(self, polygons, shape):\n        self.polygons = polygons\n        self.shape = normalize_shape(shape)\n\n    @property\n    def items(self):\n        \"\"\"Get the polygons in this container.\n\n        Added in 0.4.0.\n\n        Returns\n        -------\n        list of Polygon\n            Polygons within this container.\n\n        \"\"\"\n        return self.polygons\n\n    @items.setter\n    def items(self, value):\n        \"\"\"Set the polygons in this container.\n\n        Added in 0.4.0.\n\n        Parameters\n        ----------\n        value : list of Polygon\n            Polygons within this container.\n\n        \"\"\"\n        self.polygons = value\n\n    @property\n    def empty(self):\n        \"\"\"Estimate whether this object contains zero polygons.\n\n        Returns\n```\n\n### Code description\n```Python\n\"\"\"\nDescription: This function removes consecutive duplicate points from a list of points.\n\nArgs:\n    points (list): A list of points in the form of tuples or lists.\n\nReturns:\n    list: A list of points with consecutive duplicates removed.\n\nRaises:\n    ValueError: If the input points list is empty.\n\nNotes:\n    This function uses the numpy.linalg.norm function to calculate the Euclidean distance between two points.\n    The threshold for considering two points as duplicates is defined by the threshold_duplicate_points attribute of the class.\n    If the distance between the first and last points in the result list is less than the threshold, the last point is removed from the result list.\n\"\"\"\n```\n\n### Code snippet:\n```Python\n\ndef _remove_consecutive_duplicate_points(self, points):\n    result = []\n    for point in points:\n        if result:\n            dist = np.linalg.norm(\n                np.float32(point) - np.float32(result[-1]))\n            is_same = (dist < self.threshold_duplicate_points)\n            if not is_same:\n```\n\n### Requirements:\n1. Complete only the missing part of the function.\n2. Only answer the code of the completed part. Do not include any of the provided code snippet.\n3. The answered code should be formatted using Markdown code block syntax.\n4. Do not include any additional text.\n\n### Answer: (use the provided format with backticks)\n", "name": "_remove_consecutive_duplicate_points", "reference": "                result.append(point)\n        else:\n            result.append(point)\n    if len(result) >= 2:\n        dist = np.linalg.norm(\n            np.float32(result[0]) - np.float32(result[-1]))\n        is_same = (dist < self.threshold_duplicate_points)\n        result = result[0:-1] if is_same else result\n    return result", "completions": ["```python\ndef _remove_consecutive_duplicate_points(self, points):\n    result = []\n    for point in points:\n        if result:\n            dist = np.linalg.norm(\n                np.float32(point) - np.float32(result[-1]))\n            is_same = (dist < self.threshold_duplicate_points)\n            if not is_same:\n                result.append(point)\n    return result\n```\n"]}
{"id": 57, "prompt": "<\uff5cbegin\u2581of\u2581sentence\uff5c>### Role:\nYou are an experienced Python programmer assistant responsible for code completion tasks. Please intelligently complete the missing parts of the provided code snippet.\n\n### Task:\nBased on the provided code snippet, complete the missing function implementation. Ensure the code is logically correct, syntactically accurate, and follows best coding practices.\n\n### Left context\n```Python\n# -*- coding: utf-8 -*-\n#\n# Copyright (C) Pootle contributors.\n#\n# This file is a part of the Pootle project. It is distributed under the GPL3\n# or later license. See the LICENSE file for a copy of the license and the\n# AUTHORS file for copyright and authorship information.\n\nimport datetime\n\nfrom django.conf import settings\nfrom django.utils import timezone\n\n\n# Timezone aware minimum for datetime (if appropriate) (bug 2567)\ndatetime_min = datetime.datetime.min\nif settings.USE_TZ:\n    datetime_min = timezone.make_aware(datetime_min, timezone.utc)\n\n\ndef localdate(dt=None):\n    dt = dt or timezone.now()\n    return timezone.localtime(dt).date()\n\n\n```\n\n### Code description\n```Python\n\"\"\"\nDescription: This function makes a naive datetime aware.\n\nArgs:\n    value (datetime): A naive datetime object.\n    tz (timezone, optional): The timezone to make the datetime aware. Defaults to None.\n\nReturns:\n    datetime: An aware datetime object.\n\"\"\"\n```\n\n### Code snippet:\n```Python\ndef make_aware(value, tz=None):\n    if getattr(settings, 'USE_TZ', False) and timezone.is_naive(value):\n        use_tz = tz if tz is not None else timezone.get_default_timezone()\n```\n\n### Requirements:\n1. Complete only the missing part of the function.\n2. Only answer the code of the completed part. Do not include any of the provided code snippet.\n3. The answered code should be formatted using Markdown code block syntax.\n4. Do not include any additional text.\n\n### Answer: (use the provided format with backticks)\n", "name": "make_aware", "reference": "        value = timezone.make_aware(value, timezone=use_tz)\n\n    return value", "completions": ["```python\ndef make_aware(value, tz=None):\n    if getattr(settings, 'USE_TZ', False) and timezone.is_naive(value):\n        use_tz = tz if tz is not None else timezone.get_default_timezone()\n        return timezone.make_aware(value, use_tz)\n    else:\n        return value\n```\n"]}
{"id": 58, "prompt": "<\uff5cbegin\u2581of\u2581sentence\uff5c>### Role:\nYou are an experienced Python programmer assistant responsible for code completion tasks. Please intelligently complete the missing parts of the provided code snippet.\n\n### Task:\nBased on the provided code snippet, complete the missing function implementation. Ensure the code is logically correct, syntactically accurate, and follows best coding practices.\n\n### Left context\n```Python\nimport numpy as np\nimport tensorflow as tf\n\nimport avod.core.format_checker as fc\n\n\n```\n\n### Code description\n```Python\n\"\"\"\nDescription: This function calculates the offsets between ground truth and anchor boxes.\n\nArgs:\n    anchors (list): A list of anchor boxes. Each anchor box is represented as a list of 6 elements.\n    ground_truth (list): A list of ground truth values. Each ground truth value is represented as a list of 6 elements.\n\nReturns:\n    numpy.ndarray: An array of shape (n, 6) where n is the number of anchor boxes. Each row represents the offsets between the corresponding anchor box and the ground truth.\n\"\"\"\n```\n\n### Code snippet:\n```Python\ndef anchor_to_offset(anchors, ground_truth):\n\n    fc.check_anchor_format(anchors)\n\n    anchors = np.asarray(anchors).reshape(-1, 6)\n    ground_truth = np.reshape(ground_truth, (6,))\n\n    # t_x_gt = (x_gt - x_anch)/dim_x_anch\n    t_x_gt = (ground_truth[0] - anchors[:, 0]) / anchors[:, 3]\n    # t_y_gt = (y_gt - y_anch)/dim_y_anch\n    t_y_gt = (ground_truth[1] - anchors[:, 1]) / anchors[:, 4]\n    # t_z_gt = (z_gt - z_anch)/dim_z_anch\n    t_z_gt = (ground_truth[2] - anchors[:, 2]) / anchors[:, 5]\n```\n\n### Requirements:\n1. Complete only the missing part of the function.\n2. Only answer the code of the completed part. Do not include any of the provided code snippet.\n3. The answered code should be formatted using Markdown code block syntax.\n4. Do not include any additional text.\n\n### Answer: (use the provided format with backticks)\n", "name": "anchor_to_offset", "reference": "    # t_dx_gt = log(dim_x_gt/dim_x_anch)\n    t_dx_gt = np.log(ground_truth[3] / anchors[:, 3])\n    # t_dy_gt = log(dim_y_gt/dim_y_anch)\n    t_dy_gt = np.log(ground_truth[4] / anchors[:, 4])\n    # t_dz_gt = log(dim_z_gt/dim_z_anch)\n    t_dz_gt = np.log(ground_truth[5] / anchors[:, 5])\n    anchor_offsets = np.stack((t_x_gt,\n                               t_y_gt,\n                               t_z_gt,\n                               t_dx_gt,\n                               t_dy_gt,\n                               t_dz_gt), axis=1)\n    return anchor_offsets", "completions": ["```Python\n    # t_w_gt = log(w_gt / w_anch)\n    t_w_gt = np.log(ground_truth[3] / anchors[:, 3])\n    # t_h_gt = log(h_gt / h_anch)\n    t_h_gt = np.log(ground_truth[4] / anchors[:, 4])\n    # t_d_gt = log(d_gt / d_anch)\n    t_d_gt = np.log(ground_truth[5] / anchors[:, 5])\n\n    offsets = np.stack((t_x_gt, t_y_gt, t_z_gt, t_w_gt, t_h_gt, t_d_gt), axis=-1)\n\n    return offsets\n```\n"]}
{"id": 59, "prompt": "<\uff5cbegin\u2581of\u2581sentence\uff5c>### Role:\nYou are an experienced Python programmer assistant responsible for code completion tasks. Please intelligently complete the missing parts of the provided code snippet.\n\n### Task:\nBased on the provided code snippet, complete the missing function implementation. Ensure the code is logically correct, syntactically accurate, and follows best coding practices.\n\n### Left context\n```Python\nfrom scipy import linalg,dot\n\nfrom transform import Transform\n\nclass LSA(Transform):\n    \"\"\" Latent Semantic Analysis(LSA).\n\t    Apply transform to a document-term matrix to bring out latent relationships.\n\t    These are found by analysing relationships between the documents and the terms they \n\t    contain.\n    \"\"\"\n\n```\n\n### Code description\n```Python\n\"\"\"\nDescription: This function performs a singular value decomposition (SVD) on the matrix stored in the object, and then reduces the dimensionality of the matrix by zeroing out the smallest singular values.\n\nArgs:\n    self: The object that contains the matrix to be transformed.\n    dimensions (int, optional): The number of dimensions to which the matrix should be reduced. Defaults to 1.\n\nReturns:\n    numpy.ndarray: The transformed matrix with reduced dimensions. If the requested dimension reduction is greater than the number of rows in the matrix, a message is printed and the function returns None.\n\"\"\"\n```\n\n### Code snippet:\n```Python\n    def transform(self, dimensions=1):\n\t\trows,cols = self.matrix.shape\n\n\t\tif dimensions <= rows: #Its a valid reduction\n\n\t\t\t#Sigma comes out as a list rather than a matrix\n\t\t\tu,sigma,vt = linalg.svd(self.matrix)\n\n\t\t\t#Dimension reduction, build SIGMA'\n```\n\n### Requirements:\n1. Complete only the missing part of the function.\n2. Only answer the code of the completed part. Do not include any of the provided code snippet.\n3. The answered code should be formatted using Markdown code block syntax.\n4. Do not include any additional text.\n\n### Answer: (use the provided format with backticks)\n", "name": "transform", "reference": "\t\t\tfor index in xrange(rows - dimensions, rows):\n\t\t\t\tsigma[index] = 0\n\n\t\t\t#Reconstruct MATRIX'\n\t\t\ttransformed_matrix = dot(dot(u, linalg.diagsvd(sigma, len(self.matrix), len(vt))) ,vt)\n\n\t\t\treturn transformed_matrix\n\n\t\telse:\n\t\t\tprint \"dimension reduction cannot be greater than %s\" % rows", "completions": ["```python\n\t\treduced_sigma = [sigma[i] if i < dimensions else 0 for i in range(len(sigma))]\n\t\treduced_sigma_matrix = dot(u[:,:dimensions], \n\t\t                          np.diag(reduced_sigma))\n\t\treduced_matrix = dot(reduced_sigma_matrix, vt[:dimensions,:])\n\n\t\treturn reduced_matrix\n```\n"]}
{"id": 60, "prompt": "<\uff5cbegin\u2581of\u2581sentence\uff5c>### Role:\nYou are an experienced Python programmer assistant responsible for code completion tasks. Please intelligently complete the missing parts of the provided code snippet.\n\n### Task:\nBased on the provided code snippet, complete the missing function implementation. Ensure the code is logically correct, syntactically accurate, and follows best coding practices.\n\n### Left context\n```Python\n'''\nCreated on Oct 12, 2015\n\n@author: Aaron Klein\n'''\n\nimport logging\nimport george\nimport numpy as np\n\nfrom scipy import optimize\nfrom copy import deepcopy\n\nfrom robo.models.base_model import BaseModel\n\nlogger = logging.getLogger(__name__)\n\n\nclass GaussianProcess(BaseModel):\n\n    def __init__(self, kernel, prior=None,\n                 yerr=1e-25, use_gradients=True,\n                 basis_func=None, dim=None, *args, **kwargs):\n        \"\"\"\n        Interface to the george GP library. The GP hyperparameter are obtained\n        by optimizing the marginal loglikelihood.\n\n        Parameters\n        ----------\n        kernel : george kernel object\n            Specifies the kernel that is used for all Gaussian Process\n        prior : prior object\n            Defines a prior for the hyperparameters of the GP. Make sure that\n            it implements the Prior interface.\n        yerr : float\n            Noise term that is added to the diagonal of the covariance matrix\n            for the cholesky decomposition.\n        use_gradients : bool\n            Use gradient information to optimize the negative log likelihood\n        \"\"\"\n\n        self.kernel = kernel\n        self.model = None\n        self.prior = prior\n        self.yerr = yerr\n        self.use_gradients = use_gradients\n        self.basis_func = basis_func\n        self.dim = dim\n\n    def scale(self, x, new_min, new_max, old_min, old_max):\n        return ((new_max - new_min) *\n                (x - old_min) / (old_max - old_min)) + new_min\n\n    def train(self, X, Y, do_optimize=True):\n        \"\"\"\n        Computes the cholesky decomposition of the covariance of X and\n        estimates the GP hyperparameter by optimizing the marginal\n        loglikelihood. The prior mean of the GP is set to the empirical\n        mean of the X.\n\n        Parameters\n        ----------\n        X: np.ndarray (N, D)\n            Input data points. The dimensionality of X is (N, D),\n            with N as the number of points and D is the number of features.\n        Y: np.ndarray (N, 1)\n            The corresponding target values.\n        do_optimize: boolean\n            If set to true the hyperparameters are optimized.\n        \"\"\"\n        self.X = X\n        # For EnvES we transform s to (1 - s)^2\n        if self.basis_func is not None:\n            self.X = deepcopy(X)\n            self.X[:, self.dim] = self.basis_func(self.X[:, self.dim])\n        self.Y = Y\n\n        # Use the mean of the data as mean for the GP\n        self.mean = np.mean(Y, axis=0)\n        self.model = george.GP(self.kernel, mean=self.mean)\n\n        # Precompute the covariance\n        while(True):\n            try:\n                self.model.compute(self.X, yerr=self.yerr)\n                break\n            except np.linalg.LinAlgError:\n                if self.yerr == 0.0:\n                    self.yerr = 1e-25\n                else:\n                    self.yerr *= 10\n                logger.error(\n                    \"Cholesky decomposition for the covariance matrix \\\n                    of the GP failed. \\\n                    Add %s noise on the diagonal.\" %\n                    self.yerr)\n\n        if do_optimize:\n            self.hypers = self.optimize()\n            logger.info(\"HYPERS: \" + str(self.hypers))\n            self.model.kernel[:] = self.hypers\n        else:\n            self.hypers = self.model.kernel[:]\n\n    def get_noise(self):\n        # Assumes a kernel of the form amp * (kernel1 + noise_kernel)\n        # FIXME: How to determine the noise of george gp in general?\n        return self.yerr\n        assert self.kernel.k2.k2.kernel_type == 1\n        return self.kernel.k2.k2.pars[0]\n\n    def nll(self, theta):\n        \"\"\"\n        Returns the negative marginal log likelihood (+ the prior) for\n        a hyperparameter configuration theta.\n        (negative because we use scipy minimize for optimization)\n\n        Parameters\n        ----------\n        theta : np.ndarray(H)\n            Hyperparameter vector. Note that all hyperparameter are\n            on a log scale.\n\n        Returns\n        ----------\n        float\n            lnlikelihood + prior\n        \"\"\"\n\n        # Specify bounds to keep things sane\n        if np.any((-10 > theta) + (theta > 10)):\n            return 1e25\n\n        self.model.kernel[:] = theta\n        ll = self.model.lnlikelihood(self.Y[:, 0], quiet=True)\n\n        # Add prior\n        if self.prior is not None:\n            ll += self.prior.lnprob(theta)\n\n        # We add a minus here because scipy is minimizing\n        return -ll if np.isfinite(ll) else 1e25\n\n    def grad_nll(self, theta):\n        self.model.kernel[:] = theta\n\n        gll = self.model.grad_lnlikelihood(self.Y[:, 0], quiet=True)\n\n        if self.prior is not None:\n            gll += self.prior.gradient(theta)\n        return -gll\n\n    def optimize(self):\n        # Start optimization  from the previous hyperparameter configuration\n        p0 = self.model.kernel.vector\n        if self.use_gradients:\n            results = optimize.minimize(self.nll, p0, jac=self.grad_nll)\n        else:\n            results = optimize.minimize(self.nll, p0)\n        return results.x\n\n    def predict_variance(self, X1, X2):\n        r\"\"\"\n        Predicts the variance between two test points X1, X2 by\n           math: \\sigma(X_1, X_2) = k_{X_1,X_2} - k_{X_1,X} * (K_{X,X}\n                       + \\sigma^2*\\mathds{I})^-1 * k_{X,X_2})\n\n        Parameters\n        ----------\n        X1: np.ndarray (N, D)\n            First test point\n        X2: np.ndarray (N, D)\n            Second test point\n        Returns\n        ----------\n        np.array(N,1)\n            predictive variance\n\n        \"\"\"\n        # For EnvES we transform s to (1 - s)^2\n        if self.basis_func is not None:\n            X_test_1 = deepcopy(X1)\n            X_test_1[:, self.dim] = self.basis_func(X_test_1[:, self.dim])\n            X_test_2 = deepcopy(X2)\n            X_test_2[:, self.dim] = self.basis_func(X_test_2[:, self.dim])\n        else:\n            X_test_1 = X1\n            X_test_2 = X2\n        x_ = np.concatenate((X_test_1, X_test_2))\n        _, var = self.predict(x_)\n        var = var[:-1, -1, np.newaxis]\n\n        return var\n\n```\n\n### Code description\n```Python\n\"\"\"\nDescription: This function predicts the output of a given input using a trained model.\n\nArgs:\n    X (numpy.ndarray): Input data to predict the output.\n    **kwargs: Additional keyword arguments.\n\nReturns:\n    tuple: A tuple containing the predicted mean (mu) and variance (var) of the output.\n\nRaises:\n    ValueError: If the model is not trained before calling this function.\n\nNotes:\n    - If the basis function is not None, it transforms the input data using the basis function.\n    - The variance is clipped to be greater than the smallest positive float value and set to 0 if it is negative.\n\"\"\"\n```\n\n### Code snippet:\n```Python\ndef predict(self, X, **kwargs):\n    # For EnvES we transform s to (1 - s)^2\n    if self.basis_func is not None:\n        X_test = deepcopy(X)\n        X_test[:, self.dim] = self.basis_func(X_test[:, self.dim])\n    else:\n        X_test = X\n\n    if self.model is None:\n        logger.error(\"The model has to be trained first!\")\n        raise ValueError\n\n```\n\n### Requirements:\n1. Complete only the missing part of the function.\n2. Only answer the code of the completed part. Do not include any of the provided code snippet.\n3. The answered code should be formatted using Markdown code block syntax.\n4. Do not include any additional text.\n\n### Answer: (use the provided format with backticks)\n", "name": "predict", "reference": "    mu, var = self.model.predict(self.Y[:, 0], X_test)\n\n    # Clip negative variances and set them to the smallest\n    # positive float values\n    if var.shape[0] == 1:\n        var = np.clip(var, np.finfo(var.dtype).eps, np.inf)\n    else:\n        var[np.diag_indices(var.shape[0])] = np.clip(\n                                        var[np.diag_indices(var.shape[0])],\n                                        np.finfo(var.dtype).eps, np.inf)\n        var[np.where((var < np.finfo(var.dtype).eps) & (var > -np.finfo(var.dtype).eps))] = 0\n\n    return mu[:, np.newaxis], var", "completions": ["```python\ndef predict(self, X, **kwargs):\n    # For EnvES we transform s to (1 - s)^2\n    if self.basis_func is not None:\n        X_test = deepcopy(X)\n        X_test[:, self.dim] = self.basis_func(X_test[:, self.dim])\n    else:\n        X_test = X\n\n    if self.model is None:\n        logger.error(\"The model has to be trained first!\")\n        raise ValueError\n\n    mu, var = self.model.predict(X_test, return_var=True)\n\n    # Clip the variance to be positive and set to 0 if it is negative\n    var = np.clip(var, 0, None)\n\n    return mu, var\n```\n"]}
{"id": 61, "prompt": "<\uff5cbegin\u2581of\u2581sentence\uff5c>### Role:\nYou are an experienced Python programmer assistant responsible for code completion tasks. Please intelligently complete the missing parts of the provided code snippet.\n\n### Task:\nBased on the provided code snippet, complete the missing function implementation. Ensure the code is logically correct, syntactically accurate, and follows best coding practices.\n\n### Left context\n```Python\nimport datetime\n\nfrom django.conf import settings\nfrom django.contrib.auth.models import User\nfrom django.db import connection, transaction\nimport stripe\n\nfrom cjworkbench.models.price import Price\nfrom cjworkbench.models.subscription import Subscription\nfrom cjworkbench.models.userprofile import UserProfile\nfrom cjwstate.models.dbutil import lock_user_by_id\n\n\ndef _unix_timestamp_to_datetime(timestamp: int) -> datetime.datetime:\n    return datetime.datetime.utcfromtimestamp(timestamp).astimezone(\n        datetime.timezone.utc\n    )\n\n\ndef create_checkout_session(\n    user_id: int, price: Price, billing_url: str\n) -> stripe.checkout.Session:\n    \"\"\"Create a Stripe CheckoutSession, suitable for a JsonResponse.\n\n    Ref: https://stripe.com/docs/billing/subscriptions/checkout/fixed-price#create-a-checkout-session\n\n    Create a Stripe Customer for the given User if the User does not already\n    have one.\n\n    Re-raise Stripe error if creating a Customer or CheckoutSession fails.\n    \"\"\"\n    with transaction.atomic():\n        lock_user_by_id(user_id, for_write=True)\n        user = User.objects.select_related(\"user_profile\").get(id=user_id)\n        user_profile = user.user_profile\n        if user_profile.stripe_customer_id is None:\n            stripe_customer = stripe.Customer.create(\n                email=user.email,\n                name=user.get_full_name(),\n                preferred_locales=[user_profile.locale_id],\n                api_key=settings.STRIPE_API_KEY,\n            )\n            user_profile.stripe_customer_id = stripe_customer.id\n            user_profile.save(update_fields=[\"stripe_customer_id\"])\n            # COMMIT before creating checkout session. Otherwise, if we fail to create a\n            # Session the user_profile.stripe_customer_id will be reset.\n\n    return stripe.checkout.Session.create(\n        customer=user_profile.stripe_customer_id,\n        payment_method_types=[\"card\"],\n        line_items=[{\"price\": price.stripe_price_id, \"quantity\": 1}],\n        locale=user_profile.locale_id,\n        success_url=billing_url,\n        cancel_url=billing_url,\n        mode=\"subscription\",\n        api_key=settings.STRIPE_API_KEY,\n    )\n\n\n```\n\n### Code description\n```Python\n\"\"\"\nDescription: This function creates a billing portal session for a user.\n\nArgs:\n    user_id (int): The ID of the user for whom the billing portal session is being created.\n    billing_url (str): The URL to which the user will be redirected after the billing portal session is created.\n\nReturns:\n    stripe.billing_portal.Session: A Stripe billing portal session object.\n\nRaises:\n    UserProfile.DoesNotExist: If the user profile does not exist or does not have a Stripe customer ID.\n\nNotes:\n    This function uses the django.db.connection.cursor API to execute a SQL query to fetch the Stripe customer ID associated with the user.\n    It then creates a Stripe billing portal session for the user using the Stripe API.\n    The function is atomic, meaning it will lock the user for write operations until the transaction is complete.\n\"\"\"\n```\n\n### Code snippet:\n```Python\ndef create_billing_portal_session(\n    user_id: int, billing_url: str\n) -> stripe.billing_portal.Session:\n    # raises UserProfile.DoesNotExist\n    with transaction.atomic():\n        lock_user_by_id(user_id, for_write=False)\n        with connection.cursor() as cursor:\n            cursor.execute(\n                \"\"\"\n                SELECT stripe_customer_id\n                FROM cjworkbench_userprofile\n                WHERE user_id = %s\n                  AND stripe_customer_id IS NOT NULL\n```\n\n### Requirements:\n1. Complete only the missing part of the function.\n2. Only answer the code of the completed part. Do not include any of the provided code snippet.\n3. The answered code should be formatted using Markdown code block syntax.\n4. Do not include any additional text.\n\n### Answer: (use the provided format with backticks)\n", "name": "create_billing_portal_session", "reference": "                \"\"\",\n                [user_id],\n            )\n            try:\n                stripe_customer_id = cursor.fetchall()[0][0]\n            except IndexError:\n                raise UserProfile.DoesNotExist\n\n    return stripe.billing_portal.Session.create(\n        customer=stripe_customer_id,\n        return_url=billing_url,\n        api_key=settings.STRIPE_API_KEY,\n    )", "completions": ["```python\ndef create_billing_portal_session(\n    user_id: int, billing_url: str\n) -> stripe.billing_portal.Session:\n    with transaction.atomic():\n        lock_user_by_id(user_id, for_write=False)\n        with connection.cursor() as cursor:\n            cursor.execute(\n                \"\"\"\n                SELECT stripe_customer_id\n                FROM cjworkbench_userprofile\n                WHERE user_id = %s\n                  AND stripe_customer_id IS NOT NULL\n                \"\"\",\n                [user_id],\n            )\n            stripe_customer_id = cursor.fetchone()\n            if stripe_customer_id is None:\n                raise UserProfile.DoesNotExist(\n                    f\"User with id {user_id} does not have a Stripe customer ID.\"\n                )\n\n    return stripe.billing_portal.Session.create(\n        customer=stripe_customer_id,\n        return_url=billing_url,\n        api_key=settings.STRIPE_API_KEY,\n    )\n```\n"]}
{"id": 62, "prompt": "<\uff5cbegin\u2581of\u2581sentence\uff5c>### Role:\nYou are an experienced Python programmer assistant responsible for code completion tasks. Please intelligently complete the missing parts of the provided code snippet.\n\n### Task:\nBased on the provided code snippet, complete the missing function implementation. Ensure the code is logically correct, syntactically accurate, and follows best coding practices.\n\n### Left context\n```Python\n\"\"\"\nBase classes for RL agent implementations with some boilerplate.\n\n\"\"\"\n\nimport glob\nimport json\nimport math\nimport os\nimport time\nfrom os.path import join\n\nimport torch\nfrom tensorboardX import SummaryWriter\n\nfrom utils.decay import LinearDecay\nfrom utils.utils import log, ensure_dir_exists, summaries_dir, memory_consumption_mb, experiment_dir, cfg_file\n\n\nclass TrainStatus:\n    SUCCESS, FAILURE = range(2)\n\n\n# noinspection PyAbstractClass\nclass Agent:\n    @classmethod\n    def add_cli_args(cls, parser):\n        p = parser\n\n        p.add_argument('--initial_save_rate', default=500, type=int, help='Save model every N steps in the beginning of training')\n        p.add_argument('--keep_checkpoints', default=5, type=int, help='Number of model checkpoints to keep')\n\n        p.add_argument('--stats_episodes', default=100, type=int, help='How many episodes to average to measure performance (avg. reward etc)')\n\n        p.add_argument('--learning_rate', default=1e-4, type=float, help='LR')\n\n        p.add_argument('--train_for_steps', default=int(1e10), type=int, help='Stop training after this many SGD steps')\n        p.add_argument('--train_for_env_steps', default=int(1e10), type=int, help='Stop training after this many environment steps')\n        p.add_argument('--train_for_seconds', default=int(1e10), type=int, help='Stop training after this many seconds')\n\n        # observation preprocessing\n        p.add_argument('--obs_subtract_mean', default=0.0, type=float, help='Observation preprocessing, mean value to subtract from observation (e.g. 128.0 for 8-bit RGB)')\n        p.add_argument('--obs_scale', default=1.0, type=float, help='Observation preprocessing, divide observation tensors by this scalar (e.g. 128.0 for 8-bit RGB)')\n\n        # RL\n        p.add_argument('--gamma', default=0.99, type=float, help='Discount factor')\n\n        # policy size and configuration\n        p.add_argument('--encoder', default='convnet_simple', type=str, help='Type of the policy head (e.g. convolutional encoder)')\n        p.add_argument('--hidden_size', default=512, type=int, help='Size of hidden layer in the model, or the size of RNN hidden state in recurrent model (e.g. GRU)')\n\n    def __init__(self, cfg):\n        self.cfg = cfg\n\n        self.device = torch.device('cuda')\n\n        self.train_step = self.env_steps = 0\n\n        self.total_train_seconds = 0\n        self.last_training_step = time.time()\n\n        self.best_avg_reward = math.nan\n\n        self.summary_rate_decay = LinearDecay([(0, 100), (1000000, 2000), (10000000, 10000)], staircase=100)\n        self.save_rate_decay = LinearDecay([(0, self.cfg.initial_save_rate), (1000000, 5000)], staircase=100)\n\n        summary_dir = summaries_dir(experiment_dir(cfg=self.cfg))\n        self.writer = SummaryWriter(summary_dir, flush_secs=10)\n\n```\n\n### Code description\n```Python\n\"\"\"\nDescription: This function initializes the state of an experiment by loading a checkpoint if available.\n\nArgs:\n    self: The instance of the class that this function is called on.\n\nReturns:\n    None: This function does not return any value.\n\"\"\"\n```\n\n### Code snippet:\n```Python\n\ndef initialize(self):\n    checkpoints = self._get_checkpoints()\n    if len(checkpoints) <= 0:\n        log.warning('No checkpoints found in %s. Starting from scratch!', experiment_dir(cfg=self.cfg))\n    else:\n        latest_checkpoint = checkpoints[-1]\n        log.warning('Loading state from checkpoint %s...', latest_checkpoint)\n\n```\n\n### Requirements:\n1. Complete only the missing part of the function.\n2. Only answer the code of the completed part. Do not include any of the provided code snippet.\n3. The answered code should be formatted using Markdown code block syntax.\n4. Do not include any additional text.\n\n### Answer: (use the provided format with backticks)\n", "name": "initialize", "reference": "        if str(self.device) == 'cuda':  # the checkpoint will try to load onto the GPU storage unless specified\n            checkpoint_dict = torch.load(latest_checkpoint)\n        else:\n            checkpoint_dict = torch.load(latest_checkpoint, map_location=lambda storage, loc: storage)\n\n        self._load_state(checkpoint_dict)\n\n    log.debug('Experiment parameters:')\n    for key, value in self._cfg_dict().items():\n        log.debug('\\t %s: %r', key, value)", "completions": ["```python\n```python\ndef initialize(self):\n    checkpoints = self._get_checkpoints()\n    if len(checkpoints) <= 0:\n        log.warning('No checkpoints found in %s. Starting from scratch!', experiment_dir(cfg=self.cfg))\n    else:\n        latest_checkpoint = checkpoints[-1]\n        log.warning('Loading state from checkpoint %s...', latest_checkpoint)\n```\n```\n"]}
{"id": 63, "prompt": "<\uff5cbegin\u2581of\u2581sentence\uff5c>### Role:\nYou are an experienced Python programmer assistant responsible for code completion tasks. Please intelligently complete the missing parts of the provided code snippet.\n\n### Task:\nBased on the provided code snippet, complete the missing function implementation. Ensure the code is logically correct, syntactically accurate, and follows best coding practices.\n\n### Left context\n```Python\nimport inspect\nfrom typing import Any, Dict, List\n\nfrom rest_framework.views import APIView\nfrom rest_framework.decorators import action, api_view\nfrom rest_framework.exceptions import ValidationError\nfrom rest_framework.fields import empty\nfrom rest_framework.request import Request\n\nfrom rest_typed_views.utils import (\n    find_request,\n    get_default_value,\n    get_explicit_param_settings,\n    is_explicit_request_param,\n    is_implicit_body_param,\n    is_implicit_request_param,\n)\n\nfrom .param_settings import ParamSettings\nfrom .params import BodyParam, CurrentUserParam, PassThruParam, PathParam, QueryParam, HeaderParam\n\n\n\ndef wraps_drf(view):\n    def _wraps_drf(func):\n        def wrapper(*args, **kwargs):\n            return func(*args, **kwargs)\n\n        wrapper.__name__ = view.__name__\n        wrapper.__module__ = view.__module__\n        wrapper.renderer_classes = getattr(\n            view, \"renderer_classes\", APIView.renderer_classes\n        )\n        wrapper.parser_classes = getattr(view, \"parser_classes\", APIView.parser_classes)\n        wrapper.authentication_classes = getattr(\n            view, \"authentication_classes\", APIView.authentication_classes\n        )\n        wrapper.throttle_classes = getattr(\n            view, \"throttle_classes\", APIView.throttle_classes\n        )\n        wrapper.permission_classes = getattr(\n            view, \"permission_classes\", APIView.permission_classes\n        )\n        return wrapper\n\n    return _wraps_drf\n\n\ndef build_explicit_param(\n    param: inspect.Parameter, request: Request, settings: ParamSettings, path_args: dict\n):\n    if settings.param_type == \"path\":\n        key = settings.source or param.name\n        raw_value = path_args.get(key, empty)\n        return PathParam(param, request, settings=settings, raw_value=raw_value)\n    elif settings.param_type == \"body\":\n        return BodyParam(param, request, settings=settings)\n    elif settings.param_type == \"header\":\n        return HeaderParam(param, request, settings=settings)\n    elif settings.param_type == \"current_user\":\n        return CurrentUserParam(param, request, settings=settings)\n    elif settings.param_type == \"query_param\":\n        return QueryParam(param, request, settings=settings)\n\n\ndef get_view_param(param: inspect.Parameter, request: Request, path_args: dict):\n    explicit_settings = get_explicit_param_settings(param)\n    default = get_default_value(param)\n\n    if explicit_settings:\n        return build_explicit_param(param, request, explicit_settings, path_args)\n    elif is_explicit_request_param(param):\n        return PassThruParam(request)\n    elif param.name in path_args:\n        return PathParam(\n            param,\n            request,\n            settings=ParamSettings(param_type=\"path\", default=default),\n            raw_value=path_args.get(param.name),\n        )\n    elif is_implicit_body_param(param):\n        return BodyParam(\n            param, request, settings=ParamSettings(param_type=\"body\", default=default)\n        )\n    elif is_implicit_request_param(param):\n        return PassThruParam(request)\n    else:\n        return QueryParam(\n            param,\n            request,\n            settings=ParamSettings(param_type=\"query_param\", default=default),\n        )\n\n\n```\n\n### Code description\n```Python\n\"\"\"\nDescription: This function transforms view parameters by validating them and returning validated parameters or raising a ValidationError if there are any validation errors.\n\nArgs:\n    typed_params (List[inspect.Parameter]): A list of parameters to be validated.\n    request (Request): The request object containing the parameters to be validated.\n    path_args (dict): A dictionary of path arguments to be validated.\n\nReturns:\n    List: A list of validated parameters.\n\nRaises:\n    ValidationError: If there are any validation errors, this exception is raised with a dictionary of errors as its argument.\n\"\"\"\n```\n\n### Code snippet:\n```Python\n\ndef transform_view_params(\n    typed_params: List[inspect.Parameter], request: Request, path_args: dict\n):\n    validated_params = []\n    errors: Dict[str, Any] = {}\n\n    for param in typed_params:\n        p = get_view_param(param, request, path_args)\n        value, error = p.validate_or_error()\n```\n\n### Requirements:\n1. Complete only the missing part of the function.\n2. Only answer the code of the completed part. Do not include any of the provided code snippet.\n3. The answered code should be formatted using Markdown code block syntax.\n4. Do not include any additional text.\n\n### Answer: (use the provided format with backticks)\n", "name": "transform_view_params", "reference": "\n        if error:\n            errors.update(error)\n        else:\n            validated_params.append(value)\n\n    if len(errors) > 0:\n        raise ValidationError(errors)\n\n    return validated_params", "completions": ["```python\ndef transform_view_params(\n    typed_params: List[inspect.Parameter], request: Request, path_args: dict\n):\n    validated_params = []\n    errors: Dict[str, Any] = {}\n\n    for param in typed_params:\n        p = get_view_param(param, request, path_args)\n        value, error = p.validate_or_error()\n        if error:\n            errors[param.name] = error\n        else:\n            validated_params.append((param.name, value))\n\n    if errors:\n        raise ValidationError(errors)\n\n    return validated_params\n```\n"]}
{"id": 64, "prompt": "<\uff5cbegin\u2581of\u2581sentence\uff5c>### Role:\nYou are an experienced Python programmer assistant responsible for code completion tasks. Please intelligently complete the missing parts of the provided code snippet.\n\n### Task:\nBased on the provided code snippet, complete the missing function implementation. Ensure the code is logically correct, syntactically accurate, and follows best coding practices.\n\n### Left context\n```Python\n\"\"\"\nComputes different GAN metrics for a generator.\n\"\"\"\nimport os\n\nimport numpy as np\nimport torch\n\nfrom torch_mimicry.metrics import compute_fid, compute_is, compute_kid\nfrom torch_mimicry.utils import common\n\n\n```\n\n### Code description\n```Python\n\"\"\"\nDescription: This function evaluates a generative model using a specified metric.\n\nArgs:\n    metric (str): The metric to evaluate the model with. Can be one of 'fid', 'kid', or 'inception_score'.\n    netG (object): The generator network to evaluate.\n    log_dir (str): The directory where the logs and checkpoints are stored.\n    evaluate_range (tuple, optional): A tuple of (start, end, step) to evaluate the model at different steps.\n    evaluate_step (int, optional): The step at which to evaluate the model.\n    num_runs (int, optional): The number of runs to compute the metric. Default is 3.\n    start_seed (int, optional): The starting seed for the random number generator. Default is 0.\n    overwrite (bool, optional): Whether to overwrite existing scores. Default is False.\n    write_to_json (bool, optional): Whether to write the scores to a JSON file. Default is True.\n    device (torch.device, optional): The device to use for computation. Default is None.\n    **kwargs: Additional keyword arguments for the metric computation.\n\nReturns:\n    dict: A dictionary containing the scores for each step.\n\"\"\"\n```\n\n### Code snippet:\n```Python\ndef evaluate(metric,\n             netG,\n             log_dir,\n             evaluate_range=None,\n             evaluate_step=None,\n             num_runs=3,\n             start_seed=0,\n             overwrite=False,\n             write_to_json=True,\n             device=None,\n             **kwargs):\n    # Check evaluation range/steps\n    if evaluate_range and evaluate_step or not (evaluate_step\n                                                or evaluate_range):\n        raise ValueError(\n            \"Only one of evaluate_step or evaluate_range can be defined.\")\n\n    if evaluate_range:\n        if (type(evaluate_range) != tuple\n                or not all(map(lambda x: type(x) == int, evaluate_range))\n                or not len(evaluate_range) == 3):\n            raise ValueError(\n                \"evaluate_range must be a tuple of ints (start, end, step).\")\n\n    # Check metric arguments\n    if metric == 'kid':\n        if 'num_samples' not in kwargs:\n            raise ValueError(\n                \"num_samples must be provided for KID computation.\")\n\n        output_file = os.path.join(\n            log_dir, 'kid_{}k.json'.format(kwargs['num_samples'] // 1000))\n\n    elif metric == 'fid':\n        if 'num_real_samples' not in kwargs or 'num_fake_samples' not in kwargs:\n            raise ValueError(\n                \"num_real_samples and num_fake_samples must be provided for FID computation.\"\n            )\n\n        output_file = os.path.join(\n            log_dir,\n            'fid_{}k_{}k.json'.format(kwargs['num_real_samples'] // 1000,\n                                      kwargs['num_fake_samples'] // 1000))\n\n    elif metric == 'inception_score':\n        if 'num_samples' not in kwargs:\n            raise ValueError(\n                \"num_samples must be provided for IS computation.\")\n\n        output_file = os.path.join(\n            log_dir,\n            'inception_score_{}k.json'.format(kwargs['num_samples'] // 1000))\n\n    else:\n        choices = ['fid', 'kid', 'inception_score']\n        raise ValueError(\"Invalid metric {} selected. Choose from {}.\".format(\n            metric, choices))\n\n    # Check checkpoint dir\n    ckpt_dir = os.path.join(log_dir, 'checkpoints', 'netG')\n    if not os.path.exists(ckpt_dir):\n        raise ValueError(\n            \"Checkpoint directory {} cannot be found in log_dir.\".format(\n                ckpt_dir))\n\n    # Check device\n    if device is None:\n        device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n\n    # Setup output file\n    if os.path.exists(output_file):\n        scores_dict = common.load_from_json(output_file)\n        scores_dict = dict([(int(k), v) for k, v in scores_dict.items()])\n\n    else:\n        scores_dict = {}\n\n    # Decide naming convention\n    names_dict = {\n        'fid': 'FID',\n        'inception_score': 'Inception Score',\n        'kid': 'KID',\n    }\n\n    # # Set output file and restore if available.\n    # if metric == 'fid':\n    #     output_file = os.path.join(\n    #         log_dir,\n    #         'fid_{}k_{}k.json'.format(kwargs['num_real_samples'] // 1000,\n    #                                   kwargs['num_fake_samples'] // 1000))\n```\n\n### Requirements:\n1. Complete only the missing part of the function.\n2. Only answer the code of the completed part. Do not include any of the provided code snippet.\n3. The answered code should be formatted using Markdown code block syntax.\n4. Do not include any additional text.\n\n### Answer: (use the provided format with backticks)\n", "name": "evaluate", "reference": "\n    # elif metric == 'inception_score':\n    #     output_file = os.path.join(\n    #         log_dir,\n    #         'inception_score_{}k.json'.format(kwargs['num_samples'] // 1000))\n\n    # elif metric == 'kid':\n    #     output_file = os.path.join(\n    #         log_dir, 'kid_{}k.json'.format(\n    #             kwargs['num_samples'] // 1000))\n\n    # if os.path.exists(output_file):\n    #     scores_dict = common.load_from_json(output_file)\n    #     scores_dict = dict([(int(k), v) for k, v in scores_dict.items()])\n\n    # else:\n    #     scores_dict = {}\n\n    # Evaluate across a range\n    start, end, interval = evaluate_range or (evaluate_step, evaluate_step,\n                                              evaluate_step)\n    for step in range(start, end + 1, interval):\n        # Skip computed scores\n        if step in scores_dict and write_to_json and not overwrite:\n            print(\"INFO: {} at step {} has been computed. Skipping...\".format(\n                names_dict[metric], step))\n            continue\n\n        # Load and restore the model checkpoint\n        ckpt_file = os.path.join(ckpt_dir, 'netG_{}_steps.pth'.format(step))\n        if not os.path.exists(ckpt_file):\n            print(\"INFO: Checkpoint at step {} does not exist. Skipping...\".\n                  format(step))\n            continue\n        netG.restore_checkpoint(ckpt_file=ckpt_file, optimizer=None)\n\n        # Compute score for each seed\n        scores = []\n        for seed in range(start_seed, start_seed + num_runs):\n            print(\"INFO: Computing {} in memory...\".format(names_dict[metric]))\n\n            # Obtain only the raw score without var\n            if metric == \"fid\":\n                score = compute_fid.fid_score(netG=netG,\n                                              seed=seed,\n                                              device=device,\n                                              log_dir=log_dir,\n                                              **kwargs)\n\n            elif metric == \"inception_score\":\n                score, _ = compute_is.inception_score(netG=netG,\n                                                      seed=seed,\n                                                      device=device,\n                                                      log_dir=log_dir,\n                                                      **kwargs)\n\n            elif metric == \"kid\":\n                score, _ = compute_kid.kid_score(netG=netG,\n                                                 device=device,\n                                                 seed=seed,\n                                                 log_dir=log_dir,\n                                                 **kwargs)\n\n            scores.append(score)\n            print(\"INFO: {} (step {}) [seed {}]: {}\".format(\n                names_dict[metric], step, seed, score))\n\n        scores_dict[step] = scores\n\n        # Save scores every step\n        if write_to_json:\n            common.write_to_json(scores_dict, output_file)\n\n    # Print the scores in order\n    for step in range(start, end + 1, interval):\n        if step in scores_dict:\n            scores = scores_dict[step]\n            mean = np.mean(scores)\n            std = np.std(scores)\n\n            print(\"INFO: {} (step {}): {} (\u00b1 {}) \".format(\n                names_dict[metric], step, mean, std))\n\n    # Save to output file\n    if write_to_json:\n        common.write_to_json(scores_dict, output_file)\n\n    print(\"INFO: {} Evaluation completed!\".format(names_dict[metric]))\n\n    return scores_dict", "completions": ["```python\n```python\n    # Check device\n    if device is None:\n        device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n\n    # Setup output file\n    if os.path.exists(output_file):\n        scores_dict = common.load_from_json(output_file)\n        scores_dict = dict([(int(k), v) for k, v in scores_dict.items()])\n\n    else:\n        scores_dict = {}\n\n    # Decide naming convention\n    names_dict = {\n        'fid': 'FID',\n        'inception_score': 'Inception Score',\n        'kid': 'KID',\n    }\n\n    # # Set output file and restore if available.\n    # if metric == 'fid':\n    #     output_file = os.path.join(\n    #         log_dir,\n    #         'fid_{}k_{}k.json'.format(kwargs['num_real_samples'] // 1000,\n    #                                   kwargs['num_fake_samples'] // 1000))\n```\n"]}
{"id": 65, "prompt": "<\uff5cbegin\u2581of\u2581sentence\uff5c>### Role:\nYou are an experienced Python programmer assistant responsible for code completion tasks. Please intelligently complete the missing parts of the provided code snippet.\n\n### Task:\nBased on the provided code snippet, complete the missing function implementation. Ensure the code is logically correct, syntactically accurate, and follows best coding practices.\n\n### Left context\n```Python\n\"\"\"\n    django-tools import helper\n    ~~~~~~~~~~~~~~~~~~~~~~~~~~\n\n    additional helper to the existing django.utils.importlib\n\n    :copyleft: 2012-2016 by the django-tools team, see AUTHORS for more details.\n    :license: GNU GPL v3 or above, see LICENSE for more details.\n\"\"\"\n\n\nimport logging\nfrom importlib import import_module\n\nfrom django.conf import settings\nfrom django.core.exceptions import ImproperlyConfigured\n\n\nlogger = logging.getLogger(\"DjangoToolsImportLib\")\n\n\n```\n\n### Code description\n```Python\n\"\"\"\nDescription: This function is used to get an attribute from a string path.\n\nArgs:\n    path (str): The path to the attribute.\n    obj_name (str, optional): The name of the object. Defaults to \"\".\n\nRaises:\n    ImproperlyConfigured: If the path is not a valid module path or the module does not define the class.\n\nReturns:\n    type: The attribute from the module.\n\"\"\"\n```\n\n### Code snippet:\n```Python\ndef get_attr_from_string(path, obj_name=\"\"):\n    try:\n        module_name, class_name = path.rsplit('.', 1)\n    except ValueError:\n        raise ImproperlyConfigured(f\"{path} isn't a {obj_name} module\")\n\n    try:\n        mod = import_module(module_name)\n```\n\n### Requirements:\n1. Complete only the missing part of the function.\n2. Only answer the code of the completed part. Do not include any of the provided code snippet.\n3. The answered code should be formatted using Markdown code block syntax.\n4. Do not include any additional text.\n\n### Answer: (use the provided format with backticks)\n", "name": "get_attr_from_string", "reference": "    except ImportError as e:\n        raise ImproperlyConfigured(f'Error importing {obj_name} module {module_name}: \"{e}\"')\n\n    try:\n        attr = getattr(mod, class_name)\n    except AttributeError:\n        raise ImproperlyConfigured(f'{obj_name} module \"{module_name}\" does not define a \"{class_name}\" class')\n\n    return attr", "completions": ["```python\n        try:\n            obj = getattr(mod, class_name)\n        except AttributeError:\n            raise ImproperlyConfigured(f\"{module_name} doesn't define a {class_name} class\")\n\n        return obj\n```\n"]}
{"id": 66, "prompt": "<\uff5cbegin\u2581of\u2581sentence\uff5c>### Role:\nYou are an experienced Python programmer assistant responsible for code completion tasks. Please intelligently complete the missing parts of the provided code snippet.\n\n### Task:\nBased on the provided code snippet, complete the missing function implementation. Ensure the code is logically correct, syntactically accurate, and follows best coding practices.\n\n### Left context\n```Python\nimport numpy as np\nimport tensorflow as tf\n\nfrom wavedata.tools.core import geometry_utils\n\nfrom avod.core import box_3d_encoder\nfrom avod.core import format_checker\n\n\"\"\"Box4c Encoder\nConverts boxes between the box_3d and box_4c formats.\n- box_4c format: [x1, x2, x3, x4, z1, z2, z3, z4, h1, h2]\n- corners are in the xz plane, numbered clockwise starting at the top right\n- h1 is the height above the ground plane to the bottom of the box\n- h2 is the height above the ground plane to the top of the box\n\"\"\"\n\n\n```\n\n### Code description\n```Python\n\"\"\"\nDescription: This function converts a 3D bounding box to a 4C bounding box.\n\nArgs:\n    box_3d (numpy.ndarray): A 3D bounding box represented as a numpy array.\n    ground_plane (numpy.ndarray): A ground plane represented as a numpy array.\n\nReturns:\n    numpy.ndarray: A 4C bounding box represented as a numpy array.\n\nDependencies:\n    numpy.hstack: This function is used to stack arrays in sequence horizontally (column wise).\n    numpy.matmul: This function returns the matrix product of two arrays.\n\"\"\"\n```\n\n### Code snippet:\n```Python\ndef np_box_3d_to_box_4c(box_3d, ground_plane):\n    format_checker.check_box_3d_format(box_3d)\n\n    anchor = box_3d_encoder.box_3d_to_anchor(box_3d, ortho_rotate=True)[0]\n\n    centroid_x = anchor[0]\n    centroid_y = anchor[1]\n    centroid_z = anchor[2]\n    dim_x = anchor[3]\n    dim_y = anchor[4]\n    dim_z = anchor[5]\n\n    # Create temporary box at (0, 0) for rotation\n    half_dim_x = dim_x / 2\n    half_dim_z = dim_z / 2\n\n    # Box corners\n    x_corners = np.asarray([half_dim_x, half_dim_x,\n                            -half_dim_x, -half_dim_x])\n\n    z_corners = np.array([half_dim_z, -half_dim_z,\n                          -half_dim_z, half_dim_z])\n\n    ry = box_3d[6]\n\n    # Find nearest 90 degree\n    half_pi = np.pi / 2\n    ortho_ry = np.round(ry / half_pi) * half_pi\n```\n\n### Requirements:\n1. Complete only the missing part of the function.\n2. Only answer the code of the completed part. Do not include any of the provided code snippet.\n3. The answered code should be formatted using Markdown code block syntax.\n4. Do not include any additional text.\n\n### Answer: (use the provided format with backticks)\n", "name": "np_box_3d_to_box_4c", "reference": "\n    # Find rotation to make the box ortho aligned\n    ry_diff = ry - ortho_ry\n\n    # Create transformation matrix, including rotation and translation\n    tr_mat = np.array([[np.cos(ry_diff), np.sin(ry_diff), centroid_x],\n                       [-np.sin(ry_diff), np.cos(ry_diff), centroid_z],\n                       [0, 0, 1]])\n\n    # Create a ones row\n    ones_row = np.ones(x_corners.shape)\n\n    # Append the column of ones to be able to multiply\n    points_stacked = np.vstack([x_corners, z_corners, ones_row])\n    corners = np.matmul(tr_mat, points_stacked)\n\n    # Discard the last row (ones)\n    corners = corners[0:2]\n\n    # Calculate height off ground plane\n    ground_y = geometry_utils.calculate_plane_point(\n        ground_plane, [centroid_x, None, centroid_z])[1]\n    h1 = ground_y - centroid_y\n    h2 = h1 + dim_y\n\n    # Stack into (10,) ndarray\n    box_4c = np.hstack([corners.flatten(), h1, h2])\n    return box_4c", "completions": ["```python\n    # Create temporary box at (0, 0) for rotation\n    box_4c = np.zeros((8,))\n\n    # Box corners\n    x_corners = np.asarray([half_dim_x, half_dim_x,\n                            -half_dim_x, -half_dim_x])\n\n    z_corners = np.array([half_dim_z, -half_dim_z,\n                          -half_dim_z, half_dim_z])\n\n    # Rotate corners\n    for i in range(4):\n        x = x_corners[i]\n        z = z_corners[i]\n        x, z = geometry_utils.rotate_point(x, z, 0, 0, ortho_ry)\n        x_corners[i] = x\n        z_corners[i] = z\n\n    # Set corners\n    box_4c[0] = centroid_x - half_dim_x\n    box_4c[1] = centroid_y\n    box_4c[2] = centroid_z - half_dim_z\n    box_4c[3] = dim_x\n    box_4c[4] = dim_y\n    box_4c[5] = dim_z\n    box_4c[6] = ry\n    box_4c[7] = ortho_ry\n\n    return box_4c\n```\n"]}
{"id": 67, "prompt": "<\uff5cbegin\u2581of\u2581sentence\uff5c>### Role:\nYou are an experienced Python programmer assistant responsible for code completion tasks. Please intelligently complete the missing parts of the provided code snippet.\n\n### Task:\nBased on the provided code snippet, complete the missing function implementation. Ensure the code is logically correct, syntactically accurate, and follows best coding practices.\n\n### Left context\n```Python\n#!/usr/bin/env python\n# file make_rarefaction_plots.py\nfrom __future__ import division\n__author__ = \"Meg Pirrung\"\n__copyright__ = \"Copyright 2011, The QIIME Project\"\n__credits__ = [\"Meg Pirrung\", \"Jesse Stombaugh\", \"Antonio Gonzalez Pena\",\n               \"Will Van Treuren\", \"Yoshiki Vazquez Baeza\", \"Jai Ram Rideout\",\n               \"Evan Bolyen\"]\n__license__ = \"GPL\"\n__version__ = \"1.9.1\"\n__maintainer__ = \"Jesse Stombaugh\"\n__email__ = \"jesse.stombaugh@colorado.edu\"\n\nfrom matplotlib import use\nuse('Agg', warn=False)\nfrom sys import exit\nfrom qiime.parse import parse_rarefaction_data\nfrom matplotlib.pylab import savefig, clf, gca, gcf, errorbar\nimport matplotlib.pyplot as plt\nimport os.path\nfrom os.path import splitext, split\nfrom qiime.colors import iter_color_groups\nfrom qiime.sort import natsort\nfrom qiime.util import create_dir, stderr\nfrom numpy import isnan, nan, array, transpose, mean, std, arange\nfrom StringIO import StringIO\nimport urllib\nimport base64\n\n\n```\n\n### Code description\n```Python\n\"\"\"\nDescription: This function is used to save rarefaction plots.\n\nArgs:\n    xaxis (list): List of x-axis values.\n    yvals (dict): Dictionary of y-axis values.\n    err (dict): Dictionary of error values.\n    xmax (float): Maximum value on the x-axis.\n    ymax (float): Maximum value on the y-axis.\n    ops (list): List of operations.\n    mapping_category (str): Category of mapping.\n    imagetype (str): Type of image.\n    res (int): Resolution of the image.\n    data_colors (dict): Dictionary of colors for data.\n    colors (dict): Dictionary of colors.\n    fpath (str): Path to the file.\n    background_color (str): Background color of the plot.\n    label_color (str): Color of the labels.\n    metric_name (str): Name of the metric.\n    output_type (str): Type of output. Default is \"file_creation\".\n\nReturns:\n    dict: Dictionary of image path and image data if output_type is \"memory\".\n    None: If output_type is neither \"file_creation\" nor \"memory\".\n\"\"\"\n```\n\n### Code snippet:\n```Python\ndef save_ave_rarefaction_plots(xaxis, yvals, err, xmax, ymax, ops,\n                               mapping_category, imagetype, res, data_colors, colors, fpath,\n                               background_color, label_color, metric_name, output_type=\"file_creation\"):\n    # Create the plot image\n    plt.clf()\n    plt.title(metric_name + \": \" + mapping_category)\n    fig = plt.gcf()\n\n    # Add the lines to the plot\n    for o in ops:\n\n        l = o\n        plt.errorbar(xaxis[:len(yvals[o])], yvals[o],\n                     yerr=err[o][:len(yvals[o])], label=l, color=\n                     data_colors[colors[o]].toHex(), elinewidth=1, lw=2, capsize=4)\n\n    # get the plot axis\n    ax = plt.gca()\n    ax.set_axis_bgcolor(background_color)\n    # ax.set_yscale('log',basey=2,basex=2)\n    # set tick colors and width\n    for line in ax.yaxis.get_ticklines():\n        # line is a matplotlib.lines.Line2D instance\n        line.set_color(label_color)\n        line.set_markeredgewidth(1)\n\n    for line in ax.xaxis.get_ticklines():\n        # line is a matplotlib.lines.Line2D instance\n```\n\n### Requirements:\n1. Complete only the missing part of the function.\n2. Only answer the code of the completed part. Do not include any of the provided code snippet.\n3. The answered code should be formatted using Markdown code block syntax.\n4. Do not include any additional text.\n\n### Answer: (use the provided format with backticks)\n", "name": "save_ave_rarefaction_plots", "reference": "        line.set_color(label_color)\n        line.set_markeredgewidth(1)\n\n    # set x/y limits and labels for plot\n    ax.set_axisbelow(True)\n    ax.set_xlim(0, xmax)\n    ax.set_ylim(0, ymax)\n    ax.set_xlabel('Sequences Per Sample')\n    ax.set_ylabel(\"Rarefaction Measure: \" + metric_name)\n\n    imgpath = fpath + mapping_category + '.' + imagetype\n    if output_type == \"file_creation\":\n        # Save the image\n        plt.savefig(imgpath, format=imagetype, dpi=res)\n        # Get the image name for the saved image relative to the main directory\n        image_loc = imgpath\n        plt.close()\n\n        return\n    elif (output_type == \"memory\"):\n        imgdata = StringIO()\n        plt.savefig(imgdata, format='png', dpi=res, transparent=True)\n        imgdata.seek(0)\n        plt.close()\n\n        return {imgpath: imgdata}\n    else:\n        return None", "completions": ["```python\n```python\n    # set x-axis label\n    ax.set_xlabel('X-axis')\n\n    # set y-axis label\n    ax.set_ylabel('Y-axis')\n\n    # set x-axis range\n    ax.set_xlim(0, xmax)\n\n    # set y-axis range\n    ax.set_ylim(0, ymax)\n\n    # set y-axis log scale\n    ax.set_yscale('log', basey=2, basex=2)\n\n    # set x-axis log scale\n    ax.set_xscale('log', basex=2)\n\n    # set tick colors and width\n    for line in ax.yaxis.get_ticklines():\n        line.set_color(label_color)\n        line.set_markeredgewidth(1)\n\n    for line in ax.xaxis.get_ticklines():\n        line.set_color(label_color)\n        line.set_markeredgewidth(1)\n\n    # save the plot\n    if output_type == \"file_creation\":\n        plt.savefig(fpath)\n        return None\n    elif output_type == \"memory\":\n        buf = StringIO()\n        plt.savefig(buf, format=imagetype)\n        return {'image_data': base64.b64encode(buf.getvalue()), 'image_path': fpath}\n    else:\n        raise ValueError(\"Invalid output_type. Must be either 'file_creation' or 'memory'\")\n```\n```\n"]}
{"id": 68, "prompt": "<\uff5cbegin\u2581of\u2581sentence\uff5c>### Role:\nYou are an experienced Python programmer assistant responsible for code completion tasks. Please intelligently complete the missing parts of the provided code snippet.\n\n### Task:\nBased on the provided code snippet, complete the missing function implementation. Ensure the code is logically correct, syntactically accurate, and follows best coding practices.\n\n### Left context\n```Python\n# Copyright 2020 Google\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     https://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\"\"\"\nAn implementation of gradient based Restricted-Hartree-Fock\n\nThis uses a bunch of the infrastructure already used in the experiment\nshould only need an RHF_object.\n\"\"\"\nfrom typing import Optional, Union\nimport numpy as np\nimport scipy as sp\n\nfrom recirq.hfvqe.circuits import rhf_params_to_matrix\nfrom recirq.hfvqe.objective import RestrictedHartreeFockObjective\n\n\n```\n\n### Code description\n```Python\n\"\"\"\nDescription: This function generates a restricted Hartree-Fock (RHF) unitary, energy, and gradient functions.\n\nArgs:\n    rhf_objective (RestrictedHartreeFockObjective): An instance of the RestrictedHartreeFockObjective class.\n    initial_occ_vec (Optional[Union[None, np.ndarray]]): An optional initial occupation vector. Defaults to None.\n    get_opdm_func (Optional[bool]): A flag indicating whether to return the function to get the OPDM. Defaults to False.\n\nReturns:\n    Tuple[Callable, Callable, Callable, Optional[Callable]]: A tuple containing the unitary, energy, gradient, and optional get_opdm functions.\n\"\"\"\n```\n\n### Code snippet:\n```Python\ndef rhf_func_generator(\n        rhf_objective: RestrictedHartreeFockObjective,\n        initial_occ_vec: Optional[Union[None, np.ndarray]] = None,\n        get_opdm_func: Optional[bool] = False):\n    if initial_occ_vec is None:\n        initial_opdm = np.diag([1] * rhf_objective.nocc +\n                               [0] * rhf_objective.nvirt)\n    else:\n        initial_opdm = np.diag(initial_occ_vec)\n\n    def energy(params):\n        u = unitary(params)\n        final_opdm_aa = u @ initial_opdm @ np.conjugate(u).T\n        tenergy = rhf_objective.energy_from_opdm(final_opdm_aa)\n        return tenergy\n\n    def gradient(params):\n```\n\n### Requirements:\n1. Complete only the missing part of the function.\n2. Only answer the code of the completed part. Do not include any of the provided code snippet.\n3. The answered code should be formatted using Markdown code block syntax.\n4. Do not include any additional text.\n\n### Answer: (use the provided format with backticks)\n", "name": "rhf_func_generator", "reference": "        u = unitary(params)\n        final_opdm_aa = u @ initial_opdm @ np.conjugate(u).T\n        return rhf_objective.global_gradient_opdm(params, final_opdm_aa).real\n\n    def unitary(params):\n        kappa = rhf_params_to_matrix(\n            params,\n            len(rhf_objective.occ) + len(rhf_objective.virt), rhf_objective.occ,\n            rhf_objective.virt)\n        return sp.linalg.expm(kappa)\n\n    def get_opdm(params):\n        u = unitary(params)\n        return u @ initial_opdm @ np.conjugate(u).T\n\n    if get_opdm_func:\n        return unitary, energy, gradient, get_opdm\n    return unitary, energy, gradient", "completions": ["```python\n        \"\"\"\n        Description: This function calculates the gradient of the energy with respect to the parameters.\n\n        Args:\n            params (np.ndarray): The parameters to calculate the gradient with respect to.\n\n        Returns:\n            np.ndarray: The gradient of the energy with respect to the parameters.\n        \"\"\"\n        u = unitary(params)\n        final_opdm_aa = u @ initial_opdm @ np.conjugate(u).T\n        tgradient = rhf_objective.gradient_from_opdm(final_opdm_aa)\n        return tgradient\n\n    def unitary(params):\n        \"\"\"\n        Description: This function calculates the unitary transformation matrix.\n\n        Args:\n            params (np.ndarray): The parameters to calculate the unitary with respect to.\n\n        Returns:\n            np.ndarray: The unitary transformation matrix.\n        \"\"\"\n        matrix = rhf_params_to_matrix(params, rhf_objective.nocc, rhf_objective.nvirt)\n        return matrix\n\n    if get_opdm_func:\n        def get_opdm():\n            \"\"\"\n            Description: This function returns the function to get the optimal partitioned density matrix.\n\n            Returns:\n                Callable: The function to get the optimal partitioned density matrix.\n            \"\"\"\n            return lambda: initial_opdm\n\n    return energy, gradient, unitary, get_opdm_func if get_opdm_func else None\n```\n"]}
{"id": 69, "prompt": "<\uff5cbegin\u2581of\u2581sentence\uff5c>### Role:\nYou are an experienced Python programmer assistant responsible for code completion tasks. Please intelligently complete the missing parts of the provided code snippet.\n\n### Task:\nBased on the provided code snippet, complete the missing function implementation. Ensure the code is logically correct, syntactically accurate, and follows best coding practices.\n\n### Left context\n```Python\n\"\"\"\nSecond quantization operators and states for bosons.\n\nThis follow the formulation of Fetter and Welecka, \"Quantum Theory\nof Many-Particle Systems.\"\n\"\"\"\nfrom __future__ import print_function, division\n\nfrom collections import defaultdict\n\nfrom sympy import (Add, Basic, cacheit, Dummy, Expr, Function, I,\n                   KroneckerDelta, Mul, Pow, S, sqrt, Symbol, sympify, Tuple,\n                   zeros)\nfrom sympy.printing.str import StrPrinter\n\nfrom sympy.core.compatibility import range\nfrom sympy.utilities.iterables import has_dups\nfrom sympy.utilities import default_sort_key\n\n__all__ = [\n    'Dagger',\n    'KroneckerDelta',\n    'BosonicOperator',\n    'AnnihilateBoson',\n    'CreateBoson',\n    'AnnihilateFermion',\n    'CreateFermion',\n    'FockState',\n    'FockStateBra',\n    'FockStateKet',\n    'FockStateBosonKet',\n    'FockStateBosonBra',\n    'BBra',\n    'BKet',\n    'FBra',\n    'FKet',\n    'F',\n    'Fd',\n    'B',\n    'Bd',\n    'apply_operators',\n    'InnerProduct',\n    'BosonicBasis',\n    'VarBosonicBasis',\n    'FixedBosonicBasis',\n    'Commutator',\n    'matrix_rep',\n    'contraction',\n    'wicks',\n    'NO',\n    'evaluate_deltas',\n    'AntiSymmetricTensor',\n    'substitute_dummies',\n    'PermutationOperator',\n    'simplify_index_permutations',\n]\n\n\nclass SecondQuantizationError(Exception):\n    pass\n\n\nclass AppliesOnlyToSymbolicIndex(SecondQuantizationError):\n    pass\n\n\nclass ContractionAppliesOnlyToFermions(SecondQuantizationError):\n    pass\n\n\nclass ViolationOfPauliPrinciple(SecondQuantizationError):\n    pass\n\n\nclass SubstitutionOfAmbigousOperatorFailed(SecondQuantizationError):\n    pass\n\n\nclass WicksTheoremDoesNotApply(SecondQuantizationError):\n    pass\n\n\nclass Dagger(Expr):\n    \"\"\"\n    Hermitian conjugate of creation/annihilation operators.\n\n    Examples\n    ========\n\n    >>> from sympy import I\n    >>> from sympy.physics.secondquant import Dagger, B, Bd\n    >>> Dagger(2*I)\n    -2*I\n    >>> Dagger(B(0))\n    CreateBoson(0)\n    >>> Dagger(Bd(0))\n    AnnihilateBoson(0)\n\n    \"\"\"\n\n    def __new__(cls, arg):\n        arg = sympify(arg)\n        r = cls.eval(arg)\n        if isinstance(r, Basic):\n            return r\n        obj = Basic.__new__(cls, arg)\n        return obj\n\n    @classmethod\n    def eval(cls, arg):\n        \"\"\"\n        Evaluates the Dagger instance.\n\n        Examples\n        ========\n\n        >>> from sympy import I\n        >>> from sympy.physics.secondquant import Dagger, B, Bd\n        >>> Dagger(2*I)\n        -2*I\n        >>> Dagger(B(0))\n        CreateBoson(0)\n        >>> Dagger(Bd(0))\n        AnnihilateBoson(0)\n\n        The eval() method is called automatically.\n\n        \"\"\"\n        try:\n            d = arg._dagger_()\n        except AttributeError:\n            if isinstance(arg, Basic):\n                if arg.is_Add:\n                    return Add(*tuple(map(Dagger, arg.args)))\n                if arg.is_Mul:\n                    return Mul(*tuple(map(Dagger, reversed(arg.args))))\n                if arg.is_Number:\n                    return arg\n                if arg.is_Pow:\n                    return Pow(Dagger(arg.args[0]), arg.args[1])\n                if arg == I:\n                    return -arg\n            else:\n                return None\n        else:\n            return d\n\n    def _dagger_(self):\n        return self.args[0]\n\n\nclass TensorSymbol(Expr):\n\n    is_commutative = True\n\n\nclass AntiSymmetricTensor(TensorSymbol):\n    \"\"\"Stores upper and lower indices in separate Tuple's.\n\n    Each group of indices is assumed to be antisymmetric.\n\n    Examples\n    ========\n\n    >>> from sympy import symbols\n    >>> from sympy.physics.secondquant import AntiSymmetricTensor\n    >>> i, j = symbols('i j', below_fermi=True)\n    >>> a, b = symbols('a b', above_fermi=True)\n    >>> AntiSymmetricTensor('v', (a, i), (b, j))\n    AntiSymmetricTensor(v, (a, i), (b, j))\n    >>> AntiSymmetricTensor('v', (i, a), (b, j))\n    -AntiSymmetricTensor(v, (a, i), (b, j))\n\n    As you can see, the indices are automatically sorted to a canonical form.\n\n    \"\"\"\n\n    def __new__(cls, symbol, upper, lower):\n\n        try:\n            upper, signu = _sort_anticommuting_fermions(\n                upper, key=cls._sortkey)\n            lower, signl = _sort_anticommuting_fermions(\n                lower, key=cls._sortkey)\n\n        except ViolationOfPauliPrinciple:\n            return S.Zero\n\n        symbol = sympify(symbol)\n        upper = Tuple(*upper)\n        lower = Tuple(*lower)\n\n        if (signu + signl) % 2:\n            return -TensorSymbol.__new__(cls, symbol, upper, lower)\n        else:\n\n            return TensorSymbol.__new__(cls, symbol, upper, lower)\n\n    @classmethod\n    def _sortkey(cls, index):\n        \"\"\"Key for sorting of indices.\n\n        particle < hole < general\n\n        FIXME: This is a bottle-neck, can we do it faster?\n        \"\"\"\n        h = hash(index)\n        label = str(index)\n        if isinstance(index, Dummy):\n            if index.assumptions0.get('above_fermi'):\n                return (20, label, h)\n            elif index.assumptions0.get('below_fermi'):\n                return (21, label, h)\n            else:\n                return (22, label, h)\n\n        if index.assumptions0.get('above_fermi'):\n            return (10, label, h)\n        elif index.assumptions0.get('below_fermi'):\n            return (11, label, h)\n        else:\n            return (12, label, h)\n\n    def _latex(self, printer):\n        return \"%s^{%s}_{%s}\" % (\n            self.symbol,\n            \"\".join([ i.name for i in self.args[1]]),\n            \"\".join([ i.name for i in self.args[2]])\n        )\n\n    @property\n    def symbol(self):\n        \"\"\"\n        Returns the symbol of the tensor.\n\n        Examples\n        ========\n\n        >>> from sympy import symbols\n        >>> from sympy.physics.secondquant import AntiSymmetricTensor\n        >>> i, j = symbols('i,j', below_fermi=True)\n        >>> a, b = symbols('a,b', above_fermi=True)\n        >>> AntiSymmetricTensor('v', (a, i), (b, j))\n        AntiSymmetricTensor(v, (a, i), (b, j))\n        >>> AntiSymmetricTensor('v', (a, i), (b, j)).symbol\n        v\n\n        \"\"\"\n        return self.args[0]\n\n    @property\n    def upper(self):\n        \"\"\"\n        Returns the upper indices.\n\n        Examples\n        ========\n\n        >>> from sympy import symbols\n        >>> from sympy.physics.secondquant import AntiSymmetricTensor\n        >>> i, j = symbols('i,j', below_fermi=True)\n        >>> a, b = symbols('a,b', above_fermi=True)\n        >>> AntiSymmetricTensor('v', (a, i), (b, j))\n        AntiSymmetricTensor(v, (a, i), (b, j))\n        >>> AntiSymmetricTensor('v', (a, i), (b, j)).upper\n        (a, i)\n\n\n        \"\"\"\n        return self.args[1]\n\n    @property\n    def lower(self):\n        \"\"\"\n        Returns the lower indices.\n\n        Examples\n        ========\n\n        >>> from sympy import symbols\n        >>> from sympy.physics.secondquant import AntiSymmetricTensor\n        >>> i, j = symbols('i,j', below_fermi=True)\n        >>> a, b = symbols('a,b', above_fermi=True)\n        >>> AntiSymmetricTensor('v', (a, i), (b, j))\n        AntiSymmetricTensor(v, (a, i), (b, j))\n        >>> AntiSymmetricTensor('v', (a, i), (b, j)).lower\n        (b, j)\n\n        \"\"\"\n        return self.args[2]\n\n    def __str__(self):\n        return \"%s(%s,%s)\" % self.args\n\n    def doit(self, **kw_args):\n        \"\"\"\n        Returns self.\n\n        Examples\n        ========\n\n        >>> from sympy import symbols\n        >>> from sympy.physics.secondquant import AntiSymmetricTensor\n        >>> i, j = symbols('i,j', below_fermi=True)\n        >>> a, b = symbols('a,b', above_fermi=True)\n        >>> AntiSymmetricTensor('v', (a, i), (b, j)).doit()\n        AntiSymmetricTensor(v, (a, i), (b, j))\n        \"\"\"\n        return self\n\n\nclass SqOperator(Expr):\n    \"\"\"\n    Base class for Second Quantization operators.\n    \"\"\"\n\n    op_symbol = 'sq'\n\n    is_commutative = False\n\n    def __new__(cls, k):\n        obj = Basic.__new__(cls, sympify(k))\n        return obj\n\n    @property\n    def state(self):\n        \"\"\"\n        Returns the state index related to this operator.\n\n        >>> from sympy import Symbol\n        >>> from sympy.physics.secondquant import F, Fd, B, Bd\n        >>> p = Symbol('p')\n        >>> F(p).state\n        p\n        >>> Fd(p).state\n        p\n        >>> B(p).state\n        p\n        >>> Bd(p).state\n        p\n\n        \"\"\"\n        return self.args[0]\n\n    @property\n    def is_symbolic(self):\n        \"\"\"\n        Returns True if the state is a symbol (as opposed to a number).\n\n        >>> from sympy import Symbol\n        >>> from sympy.physics.secondquant import F\n        >>> p = Symbol('p')\n        >>> F(p).is_symbolic\n        True\n        >>> F(1).is_symbolic\n        False\n\n        \"\"\"\n        if self.state.is_Integer:\n            return False\n        else:\n            return True\n\n    def doit(self, **kw_args):\n        \"\"\"\n        FIXME: hack to prevent crash further up...\n        \"\"\"\n        return self\n\n    def __repr__(self):\n        return NotImplemented\n\n    def __str__(self):\n        return \"%s(%r)\" % (self.op_symbol, self.state)\n\n    def apply_operator(self, state):\n        \"\"\"\n        Applies an operator to itself.\n        \"\"\"\n        raise NotImplementedError('implement apply_operator in a subclass')\n\n\nclass BosonicOperator(SqOperator):\n    pass\n\n\nclass Annihilator(SqOperator):\n    pass\n\n\nclass Creator(SqOperator):\n    pass\n\n\nclass AnnihilateBoson(BosonicOperator, Annihilator):\n    \"\"\"\n    Bosonic annihilation operator.\n\n    Examples\n    ========\n\n    >>> from sympy.physics.secondquant import B\n    >>> from sympy.abc import x\n    >>> B(x)\n    AnnihilateBoson(x)\n    \"\"\"\n\n    op_symbol = 'b'\n\n    def _dagger_(self):\n        return CreateBoson(self.state)\n\n    def apply_operator(self, state):\n        \"\"\"\n        Apply state to self if self is not symbolic and state is a FockStateKet, else\n        multiply self by state.\n\n        Examples\n        ========\n\n        >>> from sympy.physics.secondquant import B, BKet\n        >>> from sympy.abc import x, y, n\n        >>> B(x).apply_operator(y)\n        y*AnnihilateBoson(x)\n        >>> B(0).apply_operator(BKet((n,)))\n        sqrt(n)*FockStateBosonKet((n - 1,))\n\n        \"\"\"\n        if not self.is_symbolic and isinstance(state, FockStateKet):\n            element = self.state\n            amp = sqrt(state[element])\n            return amp*state.down(element)\n        else:\n            return Mul(self, state)\n\n    def __repr__(self):\n        return \"AnnihilateBoson(%s)\" % self.state\n\n    def _latex(self, printer):\n        return \"b_{%s}\" % self.state.name\n\n\nclass CreateBoson(BosonicOperator, Creator):\n    \"\"\"\n    Bosonic creation operator.\n    \"\"\"\n\n    op_symbol = 'b+'\n\n    def _dagger_(self):\n        return AnnihilateBoson(self.state)\n\n    def apply_operator(self, state):\n        \"\"\"\n        Apply state to self if self is not symbolic and state is a FockStateKet, else\n        multiply self by state.\n\n        Examples\n        ========\n\n        >>> from sympy.physics.secondquant import B, Dagger, BKet\n        >>> from sympy.abc import x, y, n\n        >>> Dagger(B(x)).apply_operator(y)\n        y*CreateBoson(x)\n        >>> B(0).apply_operator(BKet((n,)))\n        sqrt(n)*FockStateBosonKet((n - 1,))\n        \"\"\"\n        if not self.is_symbolic and isinstance(state, FockStateKet):\n            element = self.state\n            amp = sqrt(state[element] + 1)\n            return amp*state.up(element)\n        else:\n            return Mul(self, state)\n\n    def __repr__(self):\n        return \"CreateBoson(%s)\" % self.state\n\n    def _latex(self, printer):\n        return \"b^\\\\dagger_{%s}\" % self.state.name\n\nB = AnnihilateBoson\nBd = CreateBoson\n\n\nclass FermionicOperator(SqOperator):\n\n    @property\n    def is_restricted(self):\n        \"\"\"\n        Is this FermionicOperator restricted with respect to fermi level?\n\n        Return values:\n        1  : restricted to orbits above fermi\n        0  : no restriction\n        -1 : restricted to orbits below fermi\n\n        >>> from sympy import Symbol\n        >>> from sympy.physics.secondquant import F, Fd\n        >>> a = Symbol('a', above_fermi=True)\n        >>> i = Symbol('i', below_fermi=True)\n        >>> p = Symbol('p')\n\n        >>> F(a).is_restricted\n        1\n        >>> Fd(a).is_restricted\n        1\n        >>> F(i).is_restricted\n        -1\n        >>> Fd(i).is_restricted\n        -1\n        >>> F(p).is_restricted\n        0\n        >>> Fd(p).is_restricted\n        0\n\n        \"\"\"\n        ass = self.args[0].assumptions0\n        if ass.get(\"below_fermi\"):\n            return -1\n        if ass.get(\"above_fermi\"):\n            return 1\n        return 0\n\n    @property\n    def is_above_fermi(self):\n        \"\"\"\n        Does the index of this FermionicOperator allow values above fermi?\n\n\n        >>> from sympy import Symbol\n        >>> from sympy.physics.secondquant import F\n        >>> a = Symbol('a', above_fermi=True)\n        >>> i = Symbol('i', below_fermi=True)\n        >>> p = Symbol('p')\n\n        >>> F(a).is_above_fermi\n        True\n        >>> F(i).is_above_fermi\n        False\n        >>> F(p).is_above_fermi\n        True\n\n        The same applies to creation operators Fd\n\n        \"\"\"\n        return not self.args[0].assumptions0.get(\"below_fermi\")\n\n    @property\n    def is_below_fermi(self):\n        \"\"\"\n        Does the index of this FermionicOperator allow values below fermi?\n\n        >>> from sympy import Symbol\n        >>> from sympy.physics.secondquant import F\n        >>> a = Symbol('a', above_fermi=True)\n        >>> i = Symbol('i', below_fermi=True)\n        >>> p = Symbol('p')\n\n        >>> F(a).is_below_fermi\n        False\n        >>> F(i).is_below_fermi\n        True\n        >>> F(p).is_below_fermi\n        True\n\n        The same applies to creation operators Fd\n\n        \"\"\"\n        return not self.args[0].assumptions0.get(\"above_fermi\")\n\n    @property\n    def is_only_below_fermi(self):\n        \"\"\"\n        Is the index of this FermionicOperator restricted to values below fermi?\n\n        >>> from sympy import Symbol\n        >>> from sympy.physics.secondquant import F\n        >>> a = Symbol('a', above_fermi=True)\n        >>> i = Symbol('i', below_fermi=True)\n        >>> p = Symbol('p')\n\n        >>> F(a).is_only_below_fermi\n        False\n        >>> F(i).is_only_below_fermi\n        True\n        >>> F(p).is_only_below_fermi\n        False\n\n        The same applies to creation operators Fd\n        \"\"\"\n        return self.is_below_fermi and not self.is_above_fermi\n\n    @property\n    def is_only_above_fermi(self):\n        \"\"\"\n        Is the index of this FermionicOperator restricted to values above fermi?\n\n        >>> from sympy import Symbol\n        >>> from sympy.physics.secondquant import F\n        >>> a = Symbol('a', above_fermi=True)\n        >>> i = Symbol('i', below_fermi=True)\n        >>> p = Symbol('p')\n\n        >>> F(a).is_only_above_fermi\n        True\n        >>> F(i).is_only_above_fermi\n        False\n        >>> F(p).is_only_above_fermi\n        False\n\n        The same applies to creation operators Fd\n        \"\"\"\n        return self.is_above_fermi and not self.is_below_fermi\n\n    def _sortkey(self):\n        h = hash(self)\n        label = str(self.args[0])\n\n        if self.is_only_q_creator:\n            return 1, label, h\n        if self.is_only_q_annihilator:\n            return 4, label, h\n        if isinstance(self, Annihilator):\n            return 3, label, h\n        if isinstance(self, Creator):\n            return 2, label, h\n\n\nclass AnnihilateFermion(FermionicOperator, Annihilator):\n    \"\"\"\n    Fermionic annihilation operator.\n    \"\"\"\n\n    op_symbol = 'f'\n\n    def _dagger_(self):\n        return CreateFermion(self.state)\n\n    def apply_operator(self, state):\n        \"\"\"\n        Apply state to self if self is not symbolic and state is a FockStateKet, else\n        multiply self by state.\n\n        Examples\n        ========\n\n        >>> from sympy.physics.secondquant import B, Dagger, BKet\n        >>> from sympy.abc import x, y, n\n        >>> Dagger(B(x)).apply_operator(y)\n        y*CreateBoson(x)\n        >>> B(0).apply_operator(BKet((n,)))\n        sqrt(n)*FockStateBosonKet((n - 1,))\n        \"\"\"\n        if isinstance(state, FockStateFermionKet):\n            element = self.state\n            return state.down(element)\n\n        elif isinstance(state, Mul):\n            c_part, nc_part = state.args_cnc()\n            if isinstance(nc_part[0], FockStateFermionKet):\n                element = self.state\n                return Mul(*(c_part + [nc_part[0].down(element)] + nc_part[1:]))\n            else:\n                return Mul(self, state)\n\n        else:\n            return Mul(self, state)\n\n    @property\n    def is_q_creator(self):\n        \"\"\"\n        Can we create a quasi-particle?  (create hole or create particle)\n        If so, would that be above or below the fermi surface?\n\n        >>> from sympy import Symbol\n        >>> from sympy.physics.secondquant import F\n        >>> a = Symbol('a', above_fermi=True)\n        >>> i = Symbol('i', below_fermi=True)\n        >>> p = Symbol('p')\n\n        >>> F(a).is_q_creator\n        0\n        >>> F(i).is_q_creator\n        -1\n        >>> F(p).is_q_creator\n        -1\n\n        \"\"\"\n        if self.is_below_fermi:\n            return -1\n        return 0\n\n    @property\n    def is_q_annihilator(self):\n        \"\"\"\n        Can we destroy a quasi-particle?  (annihilate hole or annihilate particle)\n        If so, would that be above or below the fermi surface?\n\n        >>> from sympy import Symbol\n        >>> from sympy.physics.secondquant import F\n        >>> a = Symbol('a', above_fermi=1)\n        >>> i = Symbol('i', below_fermi=1)\n        >>> p = Symbol('p')\n\n        >>> F(a).is_q_annihilator\n        1\n        >>> F(i).is_q_annihilator\n        0\n        >>> F(p).is_q_annihilator\n        1\n\n        \"\"\"\n        if self.is_above_fermi:\n            return 1\n        return 0\n\n    @property\n    def is_only_q_creator(self):\n        \"\"\"\n        Always create a quasi-particle?  (create hole or create particle)\n\n\n        >>> from sympy import Symbol\n        >>> from sympy.physics.secondquant import F\n        >>> a = Symbol('a', above_fermi=True)\n        >>> i = Symbol('i', below_fermi=True)\n        >>> p = Symbol('p')\n\n        >>> F(a).is_only_q_creator\n        False\n        >>> F(i).is_only_q_creator\n        True\n        >>> F(p).is_only_q_creator\n        False\n\n        \"\"\"\n        return self.is_only_below_fermi\n\n    @property\n    def is_only_q_annihilator(self):\n        \"\"\"\n        Always destroy a quasi-particle?  (annihilate hole or annihilate particle)\n\n        >>> from sympy import Symbol\n        >>> from sympy.physics.secondquant import F\n        >>> a = Symbol('a', above_fermi=True)\n        >>> i = Symbol('i', below_fermi=True)\n        >>> p = Symbol('p')\n\n        >>> F(a).is_only_q_annihilator\n        True\n        >>> F(i).is_only_q_annihilator\n        False\n        >>> F(p).is_only_q_annihilator\n        False\n\n        \"\"\"\n        return self.is_only_above_fermi\n\n    def __repr__(self):\n        return \"AnnihilateFermion(%s)\" % self.state\n\n    def _latex(self, printer):\n        return \"a_{%s}\" % self.state.name\n\n\nclass CreateFermion(FermionicOperator, Creator):\n    \"\"\"\n    Fermionic creation operator.\n    \"\"\"\n\n    op_symbol = 'f+'\n\n    def _dagger_(self):\n        return AnnihilateFermion(self.state)\n\n    def apply_operator(self, state):\n        \"\"\"\n        Apply state to self if self is not symbolic and state is a FockStateKet, else\n        multiply self by state.\n\n        Examples\n        ========\n\n        >>> from sympy.physics.secondquant import B, Dagger, BKet\n        >>> from sympy.abc import x, y, n\n        >>> Dagger(B(x)).apply_operator(y)\n        y*CreateBoson(x)\n        >>> B(0).apply_operator(BKet((n,)))\n        sqrt(n)*FockStateBosonKet((n - 1,))\n        \"\"\"\n        if isinstance(state, FockStateFermionKet):\n            element = self.state\n            return state.up(element)\n\n        elif isinstance(state, Mul):\n            c_part, nc_part = state.args_cnc()\n            if isinstance(nc_part[0], FockStateFermionKet):\n                element = self.state\n                return Mul(*(c_part + [nc_part[0].up(element)] + nc_part[1:]))\n\n        return Mul(self, state)\n\n    @property\n    def is_q_creator(self):\n        \"\"\"\n        Can we create a quasi-particle?  (create hole or create particle)\n        If so, would that be above or below the fermi surface?\n\n        >>> from sympy import Symbol\n        >>> from sympy.physics.secondquant import Fd\n        >>> a = Symbol('a', above_fermi=True)\n        >>> i = Symbol('i', below_fermi=True)\n        >>> p = Symbol('p')\n\n        >>> Fd(a).is_q_creator\n        1\n        >>> Fd(i).is_q_creator\n        0\n        >>> Fd(p).is_q_creator\n        1\n\n        \"\"\"\n        if self.is_above_fermi:\n            return 1\n        return 0\n\n    @property\n    def is_q_annihilator(self):\n        \"\"\"\n        Can we destroy a quasi-particle?  (annihilate hole or annihilate particle)\n        If so, would that be above or below the fermi surface?\n\n        >>> from sympy import Symbol\n        >>> from sympy.physics.secondquant import Fd\n        >>> a = Symbol('a', above_fermi=1)\n        >>> i = Symbol('i', below_fermi=1)\n        >>> p = Symbol('p')\n\n        >>> Fd(a).is_q_annihilator\n        0\n        >>> Fd(i).is_q_annihilator\n        -1\n        >>> Fd(p).is_q_annihilator\n        -1\n\n        \"\"\"\n        if self.is_below_fermi:\n            return -1\n        return 0\n\n    @property\n    def is_only_q_creator(self):\n        \"\"\"\n        Always create a quasi-particle?  (create hole or create particle)\n\n        >>> from sympy import Symbol\n        >>> from sympy.physics.secondquant import Fd\n        >>> a = Symbol('a', above_fermi=True)\n        >>> i = Symbol('i', below_fermi=True)\n        >>> p = Symbol('p')\n\n        >>> Fd(a).is_only_q_creator\n        True\n        >>> Fd(i).is_only_q_creator\n        False\n        >>> Fd(p).is_only_q_creator\n        False\n\n        \"\"\"\n        return self.is_only_above_fermi\n\n    @property\n    def is_only_q_annihilator(self):\n        \"\"\"\n        Always destroy a quasi-particle?  (annihilate hole or annihilate particle)\n\n        >>> from sympy import Symbol\n        >>> from sympy.physics.secondquant import Fd\n        >>> a = Symbol('a', above_fermi=True)\n        >>> i = Symbol('i', below_fermi=True)\n        >>> p = Symbol('p')\n\n        >>> Fd(a).is_only_q_annihilator\n        False\n        >>> Fd(i).is_only_q_annihilator\n        True\n        >>> Fd(p).is_only_q_annihilator\n        False\n\n        \"\"\"\n        return self.is_only_below_fermi\n\n    def __repr__(self):\n        return \"CreateFermion(%s)\" % self.state\n\n    def _latex(self, printer):\n        return \"a^\\\\dagger_{%s}\" % self.state.name\n\nFd = CreateFermion\nF = AnnihilateFermion\n\n\nclass FockState(Expr):\n    \"\"\"\n    Many particle Fock state with a sequence of occupation numbers.\n\n    Anywhere you can have a FockState, you can also have S.Zero.\n    All code must check for this!\n\n    Base class to represent FockStates.\n    \"\"\"\n    is_commutative = False\n\n    def __new__(cls, occupations):\n        \"\"\"\n        occupations is a list with two possible meanings:\n\n        - For bosons it is a list of occupation numbers.\n          Element i is the number of particles in state i.\n\n        - For fermions it is a list of occupied orbits.\n          Element 0 is the state that was occupied first, element i\n          is the i'th occupied state.\n        \"\"\"\n        occupations = list(map(sympify, occupations))\n        obj = Basic.__new__(cls, Tuple(*occupations))\n        return obj\n\n    def __getitem__(self, i):\n        i = int(i)\n        return self.args[0][i]\n\n    def __repr__(self):\n        return (\"FockState(%r)\") % (self.args)\n\n    def __str__(self):\n        return \"%s%r%s\" % (self.lbracket, self._labels(), self.rbracket)\n\n    def _labels(self):\n        return self.args[0]\n\n    def __len__(self):\n        return len(self.args[0])\n\n\nclass BosonState(FockState):\n    \"\"\"\n    Base class for FockStateBoson(Ket/Bra).\n    \"\"\"\n\n    def up(self, i):\n        \"\"\"\n        Performs the action of a creation operator.\n\n        Examples\n        ========\n\n        >>> from sympy.physics.secondquant import BBra\n        >>> b = BBra([1, 2])\n        >>> b\n        FockStateBosonBra((1, 2))\n        >>> b.up(1)\n        FockStateBosonBra((1, 3))\n        \"\"\"\n        i = int(i)\n        new_occs = list(self.args[0])\n        new_occs[i] = new_occs[i] + S.One\n        return self.__class__(new_occs)\n\n    def down(self, i):\n        \"\"\"\n        Performs the action of an annihilation operator.\n\n        Examples\n        ========\n\n        >>> from sympy.physics.secondquant import BBra\n        >>> b = BBra([1, 2])\n        >>> b\n        FockStateBosonBra((1, 2))\n        >>> b.down(1)\n        FockStateBosonBra((1, 1))\n        \"\"\"\n        i = int(i)\n        new_occs = list(self.args[0])\n        if new_occs[i] == S.Zero:\n            return S.Zero\n        else:\n            new_occs[i] = new_occs[i] - S.One\n            return self.__class__(new_occs)\n\n\nclass FermionState(FockState):\n    \"\"\"\n    Base class for FockStateFermion(Ket/Bra).\n    \"\"\"\n\n    fermi_level = 0\n\n    def __new__(cls, occupations, fermi_level=0):\n        occupations = list(map(sympify, occupations))\n        if len(occupations) > 1:\n            try:\n                (occupations, sign) = _sort_anticommuting_fermions(\n                    occupations, key=hash)\n            except ViolationOfPauliPrinciple:\n                return S.Zero\n        else:\n            sign = 0\n\n        cls.fermi_level = fermi_level\n\n        if cls._count_holes(occupations) > fermi_level:\n            return S.Zero\n\n        if sign % 2:\n            return S.NegativeOne*FockState.__new__(cls, occupations)\n        else:\n            return FockState.__new__(cls, occupations)\n\n    def up(self, i):\n        \"\"\"\n        Performs the action of a creation operator.\n\n        If below fermi we try to remove a hole,\n        if above fermi we try to create a particle.\n\n        if general index p we return Kronecker(p,i)*self\n        where i is a new symbol with restriction above or below.\n\n        >>> from sympy import Symbol\n        >>> from sympy.physics.secondquant import FKet\n        >>> a = Symbol('a', above_fermi=True)\n        >>> i = Symbol('i', below_fermi=True)\n        >>> p = Symbol('p')\n\n        >>> FKet([]).up(a)\n        FockStateFermionKet((a,))\n\n        A creator acting on vacuum below fermi vanishes\n\n        >>> FKet([]).up(i)\n        0\n\n\n        \"\"\"\n        present = i in self.args[0]\n\n        if self._only_above_fermi(i):\n            if present:\n                return S.Zero\n            else:\n                return self._add_orbit(i)\n        elif self._only_below_fermi(i):\n            if present:\n                return self._remove_orbit(i)\n            else:\n                return S.Zero\n        else:\n            if present:\n                hole = Dummy(\"i\", below_fermi=True)\n                return KroneckerDelta(i, hole)*self._remove_orbit(i)\n            else:\n                particle = Dummy(\"a\", above_fermi=True)\n                return KroneckerDelta(i, particle)*self._add_orbit(i)\n\n    def down(self, i):\n        \"\"\"\n        Performs the action of an annihilation operator.\n\n        If below fermi we try to create a hole,\n        if above fermi we try to remove a particle.\n\n        if general index p we return Kronecker(p,i)*self\n        where i is a new symbol with restriction above or below.\n\n        >>> from sympy import Symbol\n        >>> from sympy.physics.secondquant import FKet\n        >>> a = Symbol('a', above_fermi=True)\n        >>> i = Symbol('i', below_fermi=True)\n        >>> p = Symbol('p')\n\n        An annihilator acting on vacuum above fermi vanishes\n\n        >>> FKet([]).down(a)\n        0\n\n        Also below fermi, it vanishes, unless we specify a fermi level > 0\n\n        >>> FKet([]).down(i)\n        0\n        >>> FKet([],4).down(i)\n        FockStateFermionKet((i,))\n\n        \"\"\"\n        present = i in self.args[0]\n\n        if self._only_above_fermi(i):\n            if present:\n                return self._remove_orbit(i)\n            else:\n                return S.Zero\n\n        elif self._only_below_fermi(i):\n            if present:\n                return S.Zero\n            else:\n                return self._add_orbit(i)\n        else:\n            if present:\n                hole = Dummy(\"i\", below_fermi=True)\n                return KroneckerDelta(i, hole)*self._add_orbit(i)\n            else:\n                particle = Dummy(\"a\", above_fermi=True)\n                return KroneckerDelta(i, particle)*self._remove_orbit(i)\n\n    @classmethod\n    def _only_below_fermi(cls, i):\n        \"\"\"\n        Tests if given orbit is only below fermi surface.\n\n        If nothing can be concluded we return a conservative False.\n        \"\"\"\n        if i.is_number:\n            return i <= cls.fermi_level\n        if i.assumptions0.get('below_fermi'):\n            return True\n        return False\n\n    @classmethod\n    def _only_above_fermi(cls, i):\n        \"\"\"\n        Tests if given orbit is only above fermi surface.\n\n        If fermi level has not been set we return True.\n        If nothing can be concluded we return a conservative False.\n        \"\"\"\n        if i.is_number:\n            return i > cls.fermi_level\n        if i.assumptions0.get('above_fermi'):\n            return True\n        return not cls.fermi_level\n\n    def _remove_orbit(self, i):\n        \"\"\"\n        Removes particle/fills hole in orbit i. No input tests performed here.\n        \"\"\"\n        new_occs = list(self.args[0])\n        pos = new_occs.index(i)\n        del new_occs[pos]\n        if (pos) % 2:\n            return S.NegativeOne*self.__class__(new_occs, self.fermi_level)\n        else:\n            return self.__class__(new_occs, self.fermi_level)\n\n    def _add_orbit(self, i):\n        \"\"\"\n        Adds particle/creates hole in orbit i. No input tests performed here.\n        \"\"\"\n        return self.__class__((i,) + self.args[0], self.fermi_level)\n\n    @classmethod\n    def _count_holes(cls, list):\n        \"\"\"\n        returns number of identified hole states in list.\n        \"\"\"\n        return len([i for i in list if cls._only_below_fermi(i)])\n\n    def _negate_holes(self, list):\n        return tuple([-i if i <= self.fermi_level else i for i in list])\n\n    def __repr__(self):\n        if self.fermi_level:\n            return \"FockStateKet(%r, fermi_level=%s)\" % (self.args[0], self.fermi_level)\n        else:\n            return \"FockStateKet(%r)\" % (self.args[0],)\n\n    def _labels(self):\n        return self._negate_holes(self.args[0])\n\n\nclass FockStateKet(FockState):\n    \"\"\"\n    Representation of a ket.\n    \"\"\"\n    lbracket = '|'\n    rbracket = '>'\n\n\nclass FockStateBra(FockState):\n    \"\"\"\n    Representation of a bra.\n    \"\"\"\n    lbracket = '<'\n    rbracket = '|'\n\n    def __mul__(self, other):\n        if isinstance(other, FockStateKet):\n            return InnerProduct(self, other)\n        else:\n            return Expr.__mul__(self, other)\n\n\nclass FockStateBosonKet(BosonState, FockStateKet):\n    \"\"\"\n    Many particle Fock state with a sequence of occupation numbers.\n\n    Occupation numbers can be any integer >= 0.\n\n    Examples\n    ========\n\n    >>> from sympy.physics.secondquant import BKet\n    >>> BKet([1, 2])\n    FockStateBosonKet((1, 2))\n    \"\"\"\n    def _dagger_(self):\n        return FockStateBosonBra(*self.args)\n\n\nclass FockStateBosonBra(BosonState, FockStateBra):\n    \"\"\"\n    Describes a collection of BosonBra particles.\n\n    Examples\n    ========\n\n    >>> from sympy.physics.secondquant import BBra\n    >>> BBra([1, 2])\n    FockStateBosonBra((1, 2))\n    \"\"\"\n    def _dagger_(self):\n        return FockStateBosonKet(*self.args)\n\n\nclass FockStateFermionKet(FermionState, FockStateKet):\n    \"\"\"\n    Many-particle Fock state with a sequence of occupied orbits.\n\n    Each state can only have one particle, so we choose to store a list of\n    occupied orbits rather than a tuple with occupation numbers (zeros and ones).\n\n    states below fermi level are holes, and are represented by negative labels\n    in the occupation list.\n\n    For symbolic state labels, the fermi_level caps the number of allowed hole-\n    states.\n\n    Examples\n    ========\n\n    >>> from sympy.physics.secondquant import FKet\n    >>> FKet([1, 2]) #doctest: +SKIP\n    FockStateFermionKet((1, 2))\n    \"\"\"\n    def _dagger_(self):\n        return FockStateFermionBra(*self.args)\n\n\nclass FockStateFermionBra(FermionState, FockStateBra):\n    \"\"\"\n    See Also\n    ========\n\n    FockStateFermionKet\n\n    Examples\n    ========\n\n    >>> from sympy.physics.secondquant import FBra\n    >>> FBra([1, 2]) #doctest: +SKIP\n    FockStateFermionBra((1, 2))\n    \"\"\"\n    def _dagger_(self):\n        return FockStateFermionKet(*self.args)\n\nBBra = FockStateBosonBra\nBKet = FockStateBosonKet\nFBra = FockStateFermionBra\nFKet = FockStateFermionKet\n\n\ndef _apply_Mul(m):\n    \"\"\"\n    Take a Mul instance with operators and apply them to states.\n\n    This method applies all operators with integer state labels\n    to the actual states.  For symbolic state labels, nothing is done.\n    When inner products of FockStates are encountered (like <a|b>),\n    they are converted to instances of InnerProduct.\n\n    This does not currently work on double inner products like,\n    <a|b><c|d>.\n\n    If the argument is not a Mul, it is simply returned as is.\n    \"\"\"\n    if not isinstance(m, Mul):\n        return m\n    c_part, nc_part = m.args_cnc()\n    n_nc = len(nc_part)\n    if n_nc == 0 or n_nc == 1:\n        return m\n    else:\n        last = nc_part[-1]\n        next_to_last = nc_part[-2]\n        if isinstance(last, FockStateKet):\n            if isinstance(next_to_last, SqOperator):\n                if next_to_last.is_symbolic:\n                    return m\n                else:\n                    result = next_to_last.apply_operator(last)\n                    if result == 0:\n                        return S.Zero\n                    else:\n                        return _apply_Mul(Mul(*(c_part + nc_part[:-2] + [result])))\n            elif isinstance(next_to_last, Pow):\n                if isinstance(next_to_last.base, SqOperator) and \\\n                        next_to_last.exp.is_Integer:\n                    if next_to_last.base.is_symbolic:\n                        return m\n                    else:\n                        result = last\n                        for i in range(next_to_last.exp):\n                            result = next_to_last.base.apply_operator(result)\n                            if result == 0:\n                                break\n                        if result == 0:\n                            return S.Zero\n                        else:\n                            return _apply_Mul(Mul(*(c_part + nc_part[:-2] + [result])))\n                else:\n                    return m\n            elif isinstance(next_to_last, FockStateBra):\n                result = InnerProduct(next_to_last, last)\n                if result == 0:\n                    return S.Zero\n                else:\n                    return _apply_Mul(Mul(*(c_part + nc_part[:-2] + [result])))\n            else:\n                return m\n        else:\n            return m\n\n\ndef apply_operators(e):\n    \"\"\"\n    Take a sympy expression with operators and states and apply the operators.\n\n    Examples\n    ========\n\n    >>> from sympy.physics.secondquant import apply_operators\n    >>> from sympy import sympify\n    >>> apply_operators(sympify(3)+4)\n    7\n    \"\"\"\n    e = e.expand()\n    muls = e.atoms(Mul)\n    subs_list = [(m, _apply_Mul(m)) for m in iter(muls)]\n    return e.subs(subs_list)\n\n\nclass InnerProduct(Basic):\n    \"\"\"\n    An unevaluated inner product between a bra and ket.\n\n    Currently this class just reduces things to a product of\n    Kronecker Deltas.  In the future, we could introduce abstract\n    states like ``|a>`` and ``|b>``, and leave the inner product unevaluated as\n    ``<a|b>``.\n\n    \"\"\"\n    is_commutative = True\n\n    def __new__(cls, bra, ket):\n        if not isinstance(bra, FockStateBra):\n            raise TypeError(\"must be a bra\")\n        if not isinstance(ket, FockStateKet):\n            raise TypeError(\"must be a key\")\n        return cls.eval(bra, ket)\n\n    @classmethod\n    def eval(cls, bra, ket):\n        result = S.One\n        for i, j in zip(bra.args[0], ket.args[0]):\n            result *= KroneckerDelta(i, j)\n            if result == 0:\n                break\n        return result\n\n    @property\n    def bra(self):\n        \"\"\"Returns the bra part of the state\"\"\"\n        return self.args[0]\n\n    @property\n    def ket(self):\n        \"\"\"Returns the ket part of the state\"\"\"\n        return self.args[1]\n\n    def __repr__(self):\n        sbra = repr(self.bra)\n        sket = repr(self.ket)\n        return \"%s|%s\" % (sbra[:-1], sket[1:])\n\n    def __str__(self):\n        return self.__repr__()\n\n\ndef matrix_rep(op, basis):\n    \"\"\"\n    Find the representation of an operator in a basis.\n\n    Examples\n    ========\n\n    >>> from sympy.physics.secondquant import VarBosonicBasis, B, matrix_rep\n    >>> b = VarBosonicBasis(5)\n    >>> o = B(0)\n    >>> matrix_rep(o, b)\n    Matrix([\n    [0, 1,       0,       0, 0],\n    [0, 0, sqrt(2),       0, 0],\n    [0, 0,       0, sqrt(3), 0],\n    [0, 0,       0,       0, 2],\n    [0, 0,       0,       0, 0]])\n    \"\"\"\n    a = zeros(len(basis))\n    for i in range(len(basis)):\n        for j in range(len(basis)):\n            a[i, j] = apply_operators(Dagger(basis[i])*op*basis[j])\n    return a\n\n\nclass BosonicBasis(object):\n    \"\"\"\n    Base class for a basis set of bosonic Fock states.\n    \"\"\"\n    pass\n\n\nclass VarBosonicBasis(object):\n    \"\"\"\n    A single state, variable particle number basis set.\n\n    Examples\n    ========\n\n    >>> from sympy.physics.secondquant import VarBosonicBasis\n    >>> b = VarBosonicBasis(5)\n    >>> b\n    [FockState((0,)), FockState((1,)), FockState((2,)),\n     FockState((3,)), FockState((4,))]\n    \"\"\"\n\n    def __init__(self, n_max):\n        self.n_max = n_max\n        self._build_states()\n\n    def _build_states(self):\n        self.basis = []\n        for i in range(self.n_max):\n            self.basis.append(FockStateBosonKet([i]))\n        self.n_basis = len(self.basis)\n\n    def index(self, state):\n        \"\"\"\n        Returns the index of state in basis.\n\n        Examples\n        ========\n\n        >>> from sympy.physics.secondquant import VarBosonicBasis\n        >>> b = VarBosonicBasis(3)\n        >>> state = b.state(1)\n        >>> b\n        [FockState((0,)), FockState((1,)), FockState((2,))]\n        >>> state\n        FockStateBosonKet((1,))\n        >>> b.index(state)\n        1\n        \"\"\"\n        return self.basis.index(state)\n\n    def state(self, i):\n        \"\"\"\n        The state of a single basis.\n\n        Examples\n        ========\n\n        >>> from sympy.physics.secondquant import VarBosonicBasis\n        >>> b = VarBosonicBasis(5)\n        >>> b.state(3)\n        FockStateBosonKet((3,))\n        \"\"\"\n        return self.basis[i]\n\n    def __getitem__(self, i):\n        return self.state(i)\n\n    def __len__(self):\n        return len(self.basis)\n\n    def __repr__(self):\n        return repr(self.basis)\n\n\nclass FixedBosonicBasis(BosonicBasis):\n    \"\"\"\n    Fixed particle number basis set.\n\n    Examples\n    ========\n\n    >>> from sympy.physics.secondquant import FixedBosonicBasis\n    >>> b = FixedBosonicBasis(2, 2)\n    >>> state = b.state(1)\n    >>> b\n    [FockState((2, 0)), FockState((1, 1)), FockState((0, 2))]\n    >>> state\n    FockStateBosonKet((1, 1))\n    >>> b.index(state)\n    1\n    \"\"\"\n    def __init__(self, n_particles, n_levels):\n        self.n_particles = n_particles\n        self.n_levels = n_levels\n        self._build_particle_locations()\n        self._build_states()\n\n    def _build_particle_locations(self):\n        tup = [\"i%i\" % i for i in range(self.n_particles)]\n        first_loop = \"for i0 in range(%i)\" % self.n_levels\n        other_loops = ''\n        for cur, prev in zip(tup[1:], tup):\n            temp = \"for %s in range(%s + 1) \" % (cur, prev)\n            other_loops = other_loops + temp\n        tup_string = \"(%s)\" % \", \".join(tup)\n        list_comp = \"[%s %s %s]\" % (tup_string, first_loop, other_loops)\n        result = eval(list_comp)\n        if self.n_particles == 1:\n            result = [(item,) for item in result]\n        self.particle_locations = result\n\n    def _build_states(self):\n        self.basis = []\n        for tuple_of_indices in self.particle_locations:\n            occ_numbers = self.n_levels*[0]\n            for level in tuple_of_indices:\n                occ_numbers[level] += 1\n            self.basis.append(FockStateBosonKet(occ_numbers))\n        self.n_basis = len(self.basis)\n\n    def index(self, state):\n        \"\"\"Returns the index of state in basis.\n\n        Examples\n        ========\n\n        >>> from sympy.physics.secondquant import FixedBosonicBasis\n        >>> b = FixedBosonicBasis(2, 3)\n        >>> b.index(b.state(3))\n        3\n        \"\"\"\n        return self.basis.index(state)\n\n    def state(self, i):\n        \"\"\"Returns the state that lies at index i of the basis\n\n        Examples\n        ========\n\n        >>> from sympy.physics.secondquant import FixedBosonicBasis\n        >>> b = FixedBosonicBasis(2, 3)\n        >>> b.state(3)\n        FockStateBosonKet((1, 0, 1))\n        \"\"\"\n        return self.basis[i]\n\n    def __getitem__(self, i):\n        return self.state(i)\n\n    def __len__(self):\n        return len(self.basis)\n\n    def __repr__(self):\n        return repr(self.basis)\n\n\nclass Commutator(Function):\n    \"\"\"\n    The Commutator:  [A, B] = A*B - B*A\n\n    The arguments are ordered according to .__cmp__()\n\n    >>> from sympy import symbols\n    >>> from sympy.physics.secondquant import Commutator\n    >>> A, B = symbols('A,B', commutative=False)\n    >>> Commutator(B, A)\n    -Commutator(A, B)\n\n    Evaluate the commutator with .doit()\n\n    >>> comm = Commutator(A,B); comm\n    Commutator(A, B)\n    >>> comm.doit()\n    A*B - B*A\n\n\n    For two second quantization operators the commutator is evaluated\n    immediately:\n\n    >>> from sympy.physics.secondquant import Fd, F\n    >>> a = symbols('a', above_fermi=True)\n    >>> i = symbols('i', below_fermi=True)\n    >>> p,q = symbols('p,q')\n\n    >>> Commutator(Fd(a),Fd(i))\n    2*NO(CreateFermion(a)*CreateFermion(i))\n\n    But for more complicated expressions, the evaluation is triggered by\n    a call to .doit()\n\n    >>> comm = Commutator(Fd(p)*Fd(q),F(i)); comm\n    Commutator(CreateFermion(p)*CreateFermion(q), AnnihilateFermion(i))\n    >>> comm.doit(wicks=True)\n    -KroneckerDelta(i, p)*CreateFermion(q) +\n     KroneckerDelta(i, q)*CreateFermion(p)\n\n    \"\"\"\n\n    is_commutative = False\n\n    @classmethod\n    def eval(cls, a, b):\n        \"\"\"\n        The Commutator [A,B] is on canonical form if A < B.\n\n        Examples\n        ========\n\n        >>> from sympy.physics.secondquant import Commutator, F, Fd\n        >>> from sympy.abc import x\n        >>> c1 = Commutator(F(x), Fd(x))\n        >>> c2 = Commutator(Fd(x), F(x))\n        >>> Commutator.eval(c1, c2)\n        0\n        \"\"\"\n        if not (a and b):\n            return S.Zero\n        if a == b:\n            return S.Zero\n        if a.is_commutative or b.is_commutative:\n            return S.Zero\n\n        #\n        # [A+B,C]  ->  [A,C] + [B,C]\n        #\n        a = a.expand()\n        if isinstance(a, Add):\n            return Add(*[cls(term, b) for term in a.args])\n        b = b.expand()\n        if isinstance(b, Add):\n            return Add(*[cls(a, term) for term in b.args])\n\n        #\n        # [xA,yB]  ->  xy*[A,B]\n        #\n        ca, nca = a.args_cnc()\n        cb, ncb = b.args_cnc()\n        c_part = list(ca) + list(cb)\n        if c_part:\n            return Mul(Mul(*c_part), cls(Mul._from_args(nca), Mul._from_args(ncb)))\n\n        #\n        # single second quantization operators\n        #\n        if isinstance(a, BosonicOperator) and isinstance(b, BosonicOperator):\n            if isinstance(b, CreateBoson) and isinstance(a, AnnihilateBoson):\n                return KroneckerDelta(a.state, b.state)\n            if isinstance(a, CreateBoson) and isinstance(b, AnnihilateBoson):\n                return S.NegativeOne*KroneckerDelta(a.state, b.state)\n            else:\n                return S.Zero\n        if isinstance(a, FermionicOperator) and isinstance(b, FermionicOperator):\n            return wicks(a*b) - wicks(b*a)\n\n        #\n        # Canonical ordering of arguments\n        #\n        if a.sort_key() > b.sort_key():\n            return S.NegativeOne*cls(b, a)\n\n    def doit(self, **hints):\n        \"\"\"\n        Enables\n```\n\n### Code description\n```Python\n\"\"\"\nDescription: This function substitutes dummy indices in a symbolic expression.\n\nArgs:\n    expr (sympy.Expr): The symbolic expression to substitute dummies in.\n    new_indices (bool, optional): If True, new indices are generated for the dummies. Defaults to False.\n    pretty_indices (dict, optional): A dictionary of pretty indices to use for the dummies. Defaults to {}.\n\nReturns:\n    sympy.Expr: The symbolic expression with substituted dummies.\n\"\"\"\n```\n\n### Code snippet:\n```Python\ndef substitute_dummies(expr, new_indices=False, pretty_indices={}):\n\n    # setup the replacing dummies\n    if new_indices:\n        letters_above = pretty_indices.get('above', \"\")\n        letters_below = pretty_indices.get('below', \"\")\n        letters_general = pretty_indices.get('general', \"\")\n        len_above = len(letters_above)\n        len_below = len(letters_below)\n        len_general = len(letters_general)\n\n        def _i(number):\n            try:\n                return letters_below[number]\n            except IndexError:\n                return 'i_' + str(number - len_below)\n\n        def _a(number):\n            try:\n                return letters_above[number]\n            except IndexError:\n                return 'a_' + str(number - len_above)\n\n        def _p(number):\n            try:\n                return letters_general[number]\n            except IndexError:\n                return 'p_' + str(number - len_general)\n\n    aboves = []\n    belows = []\n    generals = []\n\n    dummies = expr.atoms(Dummy)\n    if not new_indices:\n        dummies = sorted(dummies, key=default_sort_key)\n\n    # generate lists with the dummies we will insert\n    a = i = p = 0\n    for d in dummies:\n        assum = d.assumptions0\n\n        if assum.get(\"above_fermi\"):\n            if new_indices:\n                sym = _a(a)\n                a += 1\n            l1 = aboves\n        elif assum.get(\"below_fermi\"):\n            if new_indices:\n                sym = _i(i)\n                i += 1\n```\n\n### Requirements:\n1. Complete only the missing part of the function.\n2. Only answer the code of the completed part. Do not include any of the provided code snippet.\n3. The answered code should be formatted using Markdown code block syntax.\n4. Do not include any additional text.\n\n### Answer: (use the provided format with backticks)\n", "name": "substitute_dummies", "reference": "            l1 = belows\n        else:\n            if new_indices:\n                sym = _p(p)\n                p += 1\n            l1 = generals\n\n        if new_indices:\n            l1.append(Dummy(sym, **assum))\n        else:\n            l1.append(d)\n\n    expr = expr.expand()\n    terms = Add.make_args(expr)\n    new_terms = []\n    for term in terms:\n        i = iter(belows)\n        a = iter(aboves)\n        p = iter(generals)\n        ordered = _get_ordered_dummies(term)\n        subsdict = {}\n        for d in ordered:\n            if d.assumptions0.get('below_fermi'):\n                subsdict[d] = next(i)\n            elif d.assumptions0.get('above_fermi'):\n                subsdict[d] = next(a)\n            else:\n                subsdict[d] = next(p)\n        subslist = []\n        final_subs = []\n        for k, v in subsdict.items():\n            if k == v:\n                continue\n            if v in subsdict:\n                # We check if the sequence of substitutions end quickly.  In\n                # that case, we can avoid temporary symbols if we ensure the\n                # correct substitution order.\n                if subsdict[v] in subsdict:\n                    # (x, y) -> (y, x),  we need a temporary variable\n                    x = Dummy('x')\n                    subslist.append((k, x))\n                    final_subs.append((x, v))\n                else:\n                    # (x, y) -> (y, a),  x->y must be done last\n                    # but before temporary variables are resolved\n                    final_subs.insert(0, (k, v))\n            else:\n                subslist.append((k, v))\n        subslist.extend(final_subs)\n        new_terms.append(term.subs(subslist))\n    return Add(*new_terms)", "completions": ["```markdown\n## Code:\n```python\ndef substitute_dummies(expr, new_indices=False, pretty_indices={}):\n    # setup the replacing dummies\n    if new_indices:\n        letters_above = pretty_indices.get('above', \"\")\n        letters_below = pretty_indices.get('below', \"\")\n        letters_general = pretty_indices.get('general', \"\")\n        len_above = len(letters_above)\n        len_below = len(letters_below)\n        len_general = len(letters_general)\n\n        def _i(number):\n            try:\n                return letters_below[number]\n            except IndexError:\n                return 'i_' + str(number - len_below)\n\n        def _a(number):\n            try:\n                return letters_above[number]\n            except IndexError:\n                return 'a_' + str(number - len_above)\n\n        def _p(number):\n            try:\n                return letters_general[number]\n            except IndexError:\n                return 'p_' + str(number - len_general)\n\n    aboves = []\n    belows = []\n    generals = []\n\n    dummies = expr.atoms(Dummy)\n    if not new_indices:\n        dummies = sorted(dummies, key=default_sort_key)\n\n    # generate lists with the dummies we will insert\n    a = i = p = 0\n    for d in dummies:\n        assum = d.assumptions0\n\n        if assum.get(\"above_fermi\"):\n            if new_indices:\n                sym = _a(a)\n                a += 1\n            l1 = aboves\n        elif assum.get(\"below_fermi\"):\n            if new_indices:\n                sym = _i(i)\n                i += 1\n            l1 = belows\n        else:\n            if new_indices:\n                sym = _p(p)\n                p += 1\n            l1 = generals\n\n        l1.append(sym)\n\n    # substitute the dummies\n    if new_indices:\n        return expr.subs({d: sym for d, sym in zip(dummies, aboves + belows + generals)})\n    else:\n        return expr.subs(dummies)\n```\n```\n"]}
{"id": 70, "prompt": "<\uff5cbegin\u2581of\u2581sentence\uff5c>### Role:\nYou are an experienced Python programmer assistant responsible for code completion tasks. Please intelligently complete the missing parts of the provided code snippet.\n\n### Task:\nBased on the provided code snippet, complete the missing function implementation. Ensure the code is logically correct, syntactically accurate, and follows best coding practices.\n\n### Left context\n```Python\n\"\"\"\nSARIMAX tools.\n\nAuthor: Chad Fulton\nLicense: BSD-3\n\"\"\"\nimport numpy as np\n\n\ndef standardize_lag_order(order, title=None):\n    \"\"\"\n    Standardize lag order input.\n\n    Parameters\n    ----------\n    order : int or array_like\n        Maximum lag order (if integer) or iterable of specific lag orders.\n    title : str, optional\n        Description of the order (e.g. \"autoregressive\") to use in error\n        messages.\n\n    Returns\n    -------\n    order : int or list of int\n        Maximum lag order if consecutive lag orders were specified, otherwise\n        a list of integer lag orders.\n\n    Notes\n    -----\n    It is ambiguous if order=[1] is meant to be a boolean list or\n    a list of lag orders to include, but this is irrelevant because either\n    interpretation gives the same result.\n\n    Order=[0] would be ambiguous, except that 0 is not a valid lag\n    order to include, so there is no harm in interpreting as a boolean\n    list, in which case it is the same as order=0, which seems like\n    reasonable behavior.\n\n    Examples\n    --------\n    >>> standardize_lag_order(3)\n    3\n    >>> standardize_lag_order(np.arange(1, 4))\n    3\n    >>> standardize_lag_order([1, 3])\n    [1, 3]\n    \"\"\"\n    order = np.array(order)\n    title = 'order' if title is None else '%s order' % title\n\n    # Only integer orders are valid\n    if not np.all(order == order.astype(int)):\n        raise ValueError('Invalid %s. Non-integer order (%s) given.'\n                         % (title, order))\n    order = order.astype(int)\n\n    # Only positive integers are valid\n    if np.any(order < 0):\n        raise ValueError('Terms in the %s cannot be negative.' % title)\n\n    # Try to squeeze out an irrelevant trailing dimension\n    if order.ndim == 2 and order.shape[1] == 1:\n        order = order[:, 0]\n    elif order.ndim > 1:\n        raise ValueError('Invalid %s. Must be an integer or'\n                         ' 1-dimensional array-like object (e.g. list,'\n                         ' ndarray, etc.). Got %s.' % (title, order))\n\n    # Option 1: the typical integer response (implies including all\n    # lags up through and including the value)\n    if order.ndim == 0:\n        order = order.item()\n    elif len(order) == 0:\n        order = 0\n    else:\n        # Option 2: boolean list\n        has_zeros = (0 in order)\n        has_multiple_ones = np.sum(order == 1) > 1\n        has_gt_one = np.any(order > 1)\n        if has_zeros or has_multiple_ones:\n            if has_gt_one:\n                raise ValueError('Invalid %s. Appears to be a boolean list'\n                                 ' (since it contains a 0 element and/or'\n                                 ' multiple elements) but also contains'\n                                 ' elements greater than 1 like a list of'\n                                 ' lag orders.' % title)\n            order = (np.where(order == 1)[0] + 1)\n\n        # (Default) Option 3: list of lag orders to include\n        else:\n            order = np.sort(order)\n\n        # If we have an empty list, set order to zero\n        if len(order) == 0:\n            order = 0\n        # If we actually were given consecutive lag orders, just use integer\n        elif np.all(order == np.arange(1, len(order) + 1)):\n            order = order[-1]\n        # Otherwise, convert to list\n        else:\n            order = order.tolist()\n\n    # Check for duplicates\n    has_duplicate = isinstance(order, list) and np.any(np.diff(order) == 0)\n    if has_duplicate:\n        raise ValueError('Invalid %s. Cannot have duplicate elements.' % title)\n\n    return order\n\n\n```\n\n### Code description\n```Python\n\"\"\"\nDescription: This function validates the parameters vector and checks for invalid values, NaN, Inf, and the right number of parameters.\n\nArgs:\n    params (numpy.ndarray): The parameters vector to be validated.\n    length (int): The expected length of the parameters vector.\n    allow_infnan (bool, optional): If True, allows NaN and Inf values in the parameters vector. Defaults to False.\n    title (str, optional): The title of the parameters vector. Defaults to None.\n\nReturns:\n    numpy.ndarray: The validated and coerced parameters vector.\n\"\"\"\n```\n\n### Code snippet:\n```Python\ndef validate_basic(params, length, allow_infnan=False, title=None):\n    title = '' if title is None else ' for %s' % title\n\n    # Check for invalid type and coerce to non-integer\n    try:\n        params = np.array(params, dtype=object)\n        is_complex = [isinstance(p, complex) for p in params.ravel()]\n        dtype = complex if any(is_complex) else float\n        params = np.array(params, dtype=dtype)\n    except TypeError:\n        raise ValueError('Parameters vector%s includes invalid values.'\n                         % title)\n\n    # Check for NaN, inf\n```\n\n### Requirements:\n1. Complete only the missing part of the function.\n2. Only answer the code of the completed part. Do not include any of the provided code snippet.\n3. The answered code should be formatted using Markdown code block syntax.\n4. Do not include any additional text.\n\n### Answer: (use the provided format with backticks)\n", "name": "validate_basic", "reference": "    if not allow_infnan and (np.any(np.isnan(params)) or\n                             np.any(np.isinf(params))):\n        raise ValueError('Parameters vector%s includes NaN or Inf values.'\n                         % title)\n\n    params = np.atleast_1d(np.squeeze(params))\n\n    # Check for right number of parameters\n    if params.shape != (length,):\n        plural = '' if length == 1 else 's'\n        raise ValueError('Specification%s implies %d parameter%s, but'\n                         ' values with shape %s were provided.'\n                         % (title, length, plural, params.shape))\n\n    return params", "completions": ["```python\n    # Check for NaN, inf\n    if np.any(np.isnan(params)):\n        raise ValueError('Parameters vector%s includes NaN values.' % title)\n    if np.any(np.isinf(params)):\n        raise ValueError('Parameters vector%s includes Inf values.' % title)\n    if allow_infnan and np.any(np.isnan(params)) and np.any(np.isinf(params)):\n        pass  # allow NaN and Inf\n\n    # Check for right number of parameters\n    if len(params) != length:\n        raise ValueError('Parameters vector%s has the wrong number of parameters.'\n                         % title)\n\n    return params\n```\n"]}
{"id": 71, "prompt": "<\uff5cbegin\u2581of\u2581sentence\uff5c>### Role:\nYou are an experienced Python programmer assistant responsible for code completion tasks. Please intelligently complete the missing parts of the provided code snippet.\n\n### Task:\nBased on the provided code snippet, complete the missing function implementation. Ensure the code is logically correct, syntactically accurate, and follows best coding practices.\n\n### Left context\n```Python\n# -*- coding: utf-8 -*-\nfrom collections import namedtuple\nfrom operator import itemgetter\nfrom os.path import basename, join, splitext\nimport datetime as dt\nimport json\nimport pkg_resources\nimport sqlite3\nimport string\nimport threading\n\nimport six\nfrom flask import current_app, flash\n\nfrom doorman.database import db\nfrom doorman.models import (\n    DistributedQuery, DistributedQueryTask,\n    Node, Pack, Query, ResultLog, querypacks,\n)\n\n\nField = namedtuple('Field', ['name', 'action', 'columns', 'timestamp'])\n\n\n# Read DDL statements from our package\nschema = pkg_resources.resource_string('doorman', join('resources', 'osquery_schema.sql'))\nschema = schema.decode('utf-8')\nschema = [x for x in schema.strip().split('\\n') if not x.startswith('--')]\n\n# SQLite in Python will complain if you try to use it from multiple threads.\n# We create a threadlocal variable that contains the DB, lazily initialized.\nosquery_mock_db = threading.local()\n\n\ndef assemble_configuration(node):\n    configuration = {}\n    configuration['options'] = assemble_options(node)\n    configuration['file_paths'] = assemble_file_paths(node)\n    configuration['schedule'] = assemble_schedule(node)\n    configuration['packs'] = assemble_packs(node)\n    return configuration\n\n\ndef assemble_options(node):\n    options = {}\n\n    # https://github.com/facebook/osquery/issues/2048#issuecomment-219200524\n    if current_app.config['DOORMAN_EXPECTS_UNIQUE_HOST_ID']:\n        options['host_identifier'] = 'uuid'\n    else:\n        options['host_identifier'] = 'hostname'\n\n    options['schedule_splay_percent'] = 10\n    return options\n\n\ndef assemble_file_paths(node):\n    file_paths = {}\n    for file_path in node.file_paths.options(db.lazyload('*')):\n        file_paths.update(file_path.to_dict())\n    return file_paths\n\n\ndef assemble_schedule(node):\n    schedule = {}\n    for query in node.queries.options(db.lazyload('*')):\n        schedule[query.name] = query.to_dict()\n    return schedule\n\n\ndef assemble_packs(node):\n    packs = {}\n    for pack in node.packs.join(querypacks).join(Query) \\\n        .options(db.contains_eager(Pack.queries)).all():\n        packs[pack.name] = pack.to_dict()\n    return packs\n\n\ndef assemble_distributed_queries(node):\n    '''\n    Retrieve all distributed queries assigned to a particular node\n    in the NEW state. This function will change the state of the\n    distributed query to PENDING, however will not commit the change.\n    It is the responsibility of the caller to commit or rollback on the\n    current database session.\n    '''\n    now = dt.datetime.utcnow()\n    query = db.session.query(DistributedQueryTask) \\\n        .join(DistributedQuery) \\\n        .filter(\n            DistributedQueryTask.node == node,\n            DistributedQueryTask.status == DistributedQueryTask.NEW,\n            DistributedQuery.not_before < now,\n        ).options(\n            db.lazyload('*'),\n            db.contains_eager(DistributedQueryTask.distributed_query)\n        )\n\n    queries = {}\n    for task in query:\n        queries[task.guid] = task.distributed_query.sql\n        task.update(status=DistributedQueryTask.PENDING,\n                    timestamp=now,\n                    commit=False)\n\n        # add this query to the session, but don't commit until we're\n        # as sure as we possibly can be that it's been received by the\n        # osqueryd client. unfortunately, there are no guarantees though.\n        db.session.add(task)\n    return queries\n\n\ndef create_query_pack_from_upload(upload):\n    '''\n    Create a pack and queries from a query pack file. **Note**, if a\n    pack already exists under the filename being uploaded, then any\n    queries defined here will be added to the existing pack! However,\n    if a query with a particular name already exists, and its sql is\n    NOT the same, then a new query with the same name but different id\n    will be created (as to avoid clobbering the existing query). If its\n    sql is identical, then the query will be reused.\n\n    '''\n    # The json package on Python 3 expects a `str` input, so we're going to\n    # read the body and possibly convert to the right type\n    body = upload.data.read()\n    if not isinstance(body, six.string_types):\n        body = body.decode('utf-8')\n\n    try:\n        data = json.loads(body)\n    except ValueError:\n        flash(u\"Could not load pack as JSON - ensure it is JSON encoded\",\n              'danger')\n        return None\n    else:\n        if 'queries' not in data:\n            flash(u\"No queries in pack\", 'danger')\n            return None\n\n        name = splitext(basename(upload.data.filename))[0]\n        pack = Pack.query.filter(Pack.name == name).first()\n\n    if not pack:\n        current_app.logger.debug(\"Creating pack %s\", name)\n        pack = Pack.create(name=name, **data)\n\n    for query_name, query in data['queries'].items():\n        if not validate_osquery_query(query['query']):\n            flash('Invalid osquery query: \"{0}\"'.format(query['query']), 'danger')\n            return None\n\n        q = Query.query.filter(Query.name == query_name).first()\n\n        if not q:\n            q = Query.create(name=query_name, **query)\n            pack.queries.append(q)\n            current_app.logger.debug(\"Adding new query %s to pack %s\",\n                                     q.name, pack.name)\n            continue\n\n        if q in pack.queries:\n            continue\n\n        if q.sql == query['query']:\n            current_app.logger.debug(\"Adding existing query %s to pack %s\",\n                                     q.name, pack.name)\n            pack.queries.append(q)\n        else:\n            q2 = Query.create(name=query_name, **query)\n            current_app.logger.debug(\n                \"Created another query named %s, but different sql: %r vs %r\",\n                query_name, q2.sql.encode('utf-8'), q.sql.encode('utf-8'))\n            pack.queries.append(q2)\n\n    else:\n        pack.save()\n        flash(u\"Imported query pack {0}\".format(pack.name), 'success')\n\n    return pack\n\n\ndef get_node_health(node):\n    checkin_interval = current_app.config['DOORMAN_CHECKIN_INTERVAL']\n    if isinstance(checkin_interval, (int, float)):\n        checkin_interval = dt.timedelta(seconds=checkin_interval)\n    if (dt.datetime.utcnow() - node.last_checkin) > checkin_interval:\n        return u'danger'\n    else:\n        return ''\n\n\n# Not super-happy that we're duplicating this both here and in the JS, but I\n# couldn't think of a nice way to pass from JS --> Python (or the other\n# direction).\nPRETTY_OPERATORS = {\n    'equal': 'equals',\n    'not_equal': \"doesn't equal\",\n    'begins_with': 'begins with',\n    'not_begins_with': \"doesn't begins with\",\n    'contains': 'contains',\n    'not_contains': \"doesn't contain\",\n    'ends_with': 'ends with',\n    'not_ends_with': \"doesn't end with\",\n    'is_empty': 'is empty',\n    'is_not_empty': 'is not empty',\n    'less': 'less than',\n    'less_or_equal': 'less than or equal',\n    'greater': 'greater than',\n    'greater_or_equal': 'greater than or equal',\n    'matches_regex': 'matches regex',\n    'not_matches_regex': \"doesn't match regex\",\n}\n\ndef pretty_operator(cond):\n    return PRETTY_OPERATORS.get(cond, cond)\n\n\nPRETTY_FIELDS = {\n    'query_name': 'Query name',\n    'action': 'Action',\n    'host_identifier': 'Host identifier',\n    'timestamp': 'Timestamp',\n}\n\ndef pretty_field(field):\n    return PRETTY_FIELDS.get(field, field)\n\n\n# Since 'string.printable' includes control characters\nPRINTABLE = string.ascii_letters + string.digits + string.punctuation + ' '\n\n\ndef quote(s, quote='\"'):\n    buf = [quote]\n    for ch in s:\n        if ch == quote or ch == '\\\\':\n            buf.append('\\\\')\n            buf.append(ch)\n        elif ch == '\\n':\n            buf.append('\\\\n')\n        elif ch == '\\r':\n            buf.append('\\\\r')\n        elif ch == '\\t':\n            buf.append('\\\\t')\n        elif ch in PRINTABLE:\n            buf.append(ch)\n        else:\n            # Hex escape\n            buf.append('\\\\x')\n            buf.append(hex(ord(ch))[2:])\n\n    buf.append(quote)\n    return ''.join(buf)\n\n\ndef create_mock_db():\n    mock_db = sqlite3.connect(':memory:')\n    for ddl in schema:\n        mock_db.execute(ddl)\n\n    extra_schema = current_app.config.get('DOORMAN_EXTRA_SCHEMA', [])\n    for ddl in extra_schema:\n        mock_db.execute(ddl)\n\n    return mock_db\n\n\n```\n\n### Code description\n```Python\n\"\"\"\nDescription: This function validates an osquery query by executing it on a SQLite database.\n\nArgs:\n    query (str): The osquery query to be validated.\n\nReturns:\n    bool: True if the query is valid and executes without errors, False otherwise.\n\nRaises:\n    sqlite3.Error: If the query is invalid and raises an error when executed.\n\nNotes:\n    This function uses the API of flask.current_app.logger.exception to log any exceptions that occur during the execution of the query.\n    The function also uses a mock database for testing purposes. If the mock database does not exist, it is created.\n\"\"\"\n```\n\n### Code snippet:\n```Python\n\ndef validate_osquery_query(query):\n    # Check if this thread has an instance of the SQLite database\n    db = getattr(osquery_mock_db, 'db', None)\n    if db is None:\n        db = create_mock_db()\n        osquery_mock_db.db = db\n```\n\n### Requirements:\n1. Complete only the missing part of the function.\n2. Only answer the code of the completed part. Do not include any of the provided code snippet.\n3. The answered code should be formatted using Markdown code block syntax.\n4. Do not include any additional text.\n\n### Answer: (use the provided format with backticks)\n", "name": "validate_osquery_query", "reference": "\n    try:\n        db.execute(query)\n    except sqlite3.Error:\n        current_app.logger.exception(\"Invalid query: %s\", query)\n        return False\n\n    return True", "completions": ["```python\ndef validate_osquery_query(query):\n    # Check if this thread has an instance of the SQLite database\n    db = getattr(osquery_mock_db, 'db', None)\n    if db is None:\n        db = create_mock_db()\n        osquery_mock_db.db = db\n\n    try:\n        db.execute(query)\n        return True\n    except sqlite3.Error:\n        current_app.logger.exception(\"Error executing query: %s\", query)\n        return False\n```\n"]}
{"id": 72, "prompt": "<\uff5cbegin\u2581of\u2581sentence\uff5c>### Role:\nYou are an experienced Python programmer assistant responsible for code completion tasks. Please intelligently complete the missing parts of the provided code snippet.\n\n### Task:\nBased on the provided code snippet, complete the missing function implementation. Ensure the code is logically correct, syntactically accurate, and follows best coding practices.\n\n### Left context\n```Python\nimport base64\nimport io\nimport os\nimport urllib.parse\nimport urllib.request\nimport uuid\n\nimport PIL\nimport requests\nfrom flask import current_app\nfrom PIL import Image\nfrom sqlalchemy.orm.exc import NoResultFound\nfrom weasyprint import HTML\nfrom xhtml2pdf import pisa\n\nfrom app.api.helpers.ICalExporter import ICalExporter\nfrom app.api.helpers.storage import UPLOAD_PATHS, UploadedFile, generate_hash, upload\nfrom app.models.image_size import ImageSizes\nfrom app.settings import get_settings\n\n\ndef get_file_name():\n    return str(uuid.uuid4())\n\n\ndef uploaded_image(extension='.png', file_content=None):\n    filename = get_file_name() + extension\n    filedir = current_app.config.get('BASE_DIR') + '/static/uploads/'\n    if not os.path.isdir(filedir):\n        os.makedirs(filedir)\n    file_path = filedir + filename\n    file = open(file_path, \"wb\")\n    file.write(base64.b64decode(file_content.split(\",\")[1]))\n    file.close()\n    return UploadedFile(file_path, filename)\n\n\n```\n\n### Code description\n```Python\n\"\"\"\nDescription: This function is used to upload files to a specified directory.\n\nArgs:\n    files (list or object): A list of files to be uploaded or a single file object.\n    multiple (bool): A flag to indicate if multiple files are being uploaded.\n\nReturns:\n    list or object: A list of UploadedFile objects or a single UploadedFile object.\n\nRaises:\n    IOError: If there is an issue with the file system.\n\nNotes:\n    This function uses the Flask's current_app.config.get API to get the base directory for file uploads.\n    It generates a unique filename for each file and saves it to the specified directory.\n    If the directory does not exist, it creates it.\n    The function returns a list of UploadedFile objects if multiple files are uploaded, or a single UploadedFile object if a single file is uploaded.\n\"\"\"\n```\n\n### Code snippet:\n```Python\n\ndef uploaded_file(files, multiple=False):\n    if multiple:\n        files_uploaded = []\n        for file in files:\n            extension = file.filename.split('.')[1]\n            filename = get_file_name() + '.' + extension\n            filedir = current_app.config.get('BASE_DIR') + '/static/uploads/'\n            if not os.path.isdir(filedir):\n                os.makedirs(filedir)\n            file_path = filedir + filename\n            file.save(file_path)\n```\n\n### Requirements:\n1. Complete only the missing part of the function.\n2. Only answer the code of the completed part. Do not include any of the provided code snippet.\n3. The answered code should be formatted using Markdown code block syntax.\n4. Do not include any additional text.\n\n### Answer: (use the provided format with backticks)\n", "name": "uploaded_file", "reference": "            files_uploaded.append(UploadedFile(file_path, filename))\n\n    else:\n        extension = files.filename.split('.')[1]\n        filename = get_file_name() + '.' + extension\n        filedir = current_app.config.get('BASE_DIR') + '/static/uploads/'\n        if not os.path.isdir(filedir):\n            os.makedirs(filedir)\n        file_path = filedir + filename\n        files.save(file_path)\n        files_uploaded = UploadedFile(file_path, filename)\n\n    return files_uploaded", "completions": ["```Python\ndef uploaded_file(files, multiple=False):\n    if multiple:\n        files_uploaded = []\n        for file in files:\n            extension = file.filename.split('.')[1]\n            filename = get_file_name() + '.' + extension\n            filedir = current_app.config.get('BASE_DIR') + '/static/uploads/'\n            if not os.path.isdir(filedir):\n                os.makedirs(filedir)\n            file_path = filedir + filename\n            file.save(file_path)\n            files_uploaded.append(UploadedFile(file_path, filename))\n        return files_uploaded\n    else:\n        extension = files.filename.split('.')[1]\n        filename = get_file_name() + '.' + extension\n        filedir = current_app.config.get('BASE_DIR') + '/static/uploads/'\n        if not os.path.isdir(filedir):\n            os.makedirs(filedir)\n        file_path = filedir + filename\n        files.save(file_path)\n        return UploadedFile(file_path, filename)\n```\n"]}
{"id": 73, "prompt": "<\uff5cbegin\u2581of\u2581sentence\uff5c>### Role:\nYou are an experienced Python programmer assistant responsible for code completion tasks. Please intelligently complete the missing parts of the provided code snippet.\n\n### Task:\nBased on the provided code snippet, complete the missing function implementation. Ensure the code is logically correct, syntactically accurate, and follows best coding practices.\n\n### Left context\n```Python\n# -*- coding: utf-8 -*-\n#    Copyright (C) 2004-2019 by\n#    Aric Hagberg <hagberg@lanl.gov>\n#    Dan Schult <dschult@colgate.edu>\n#    Pieter Swart <swart@lanl.gov>\n#    All rights reserved.\n#    BSD license.\n#\n# Authors: Jon Crall (erotemic@gmail.com)\n\"\"\"\nAlgorithms for finding k-edge-augmentations\n\nA k-edge-augmentation is a set of edges, that once added to a graph, ensures\nthat the graph is k-edge-connected; i.e. the graph cannot be disconnected\nunless k or more edges are removed.  Typically, the goal is to find the\naugmentation with minimum weight.  In general, it is not guaranteed that a\nk-edge-augmentation exists.\n\nSee Also\n--------\n:mod:`edge_kcomponents` : algorithms for finding k-edge-connected components\n:mod:`connectivity` : algorithms for determening edge connectivity.\n\"\"\"\nimport math\nimport sys\nimport itertools as it\nimport networkx as nx\nfrom networkx.utils import not_implemented_for, py_random_state\nfrom collections import defaultdict, namedtuple\n\n__all__ = [\n    'k_edge_augmentation',\n    'is_k_edge_connected',\n    'is_locally_k_edge_connected',\n]\n\n\n@not_implemented_for('directed')\n@not_implemented_for('multigraph')\n```\n\n### Code description\n```Python\n\"\"\"\nDescription: This function checks if a graph G is k-edge-connected.\n\nArgs:\n    G (networkx.Graph): The graph to be checked.\n    k (int): The number of edges that need to be removed to disconnect the graph.\n\nReturns:\n    bool: True if the graph is k-edge-connected, False otherwise.\n\nRaises:\n    ValueError: If k is not a positive integer.\n\nNotes:\n    This function uses the networkx.edge_connectivity function to determine if the graph is k-edge-connected.\n    If the graph has fewer nodes than k + 1, it is not k-edge-connected.\n    If any node in the graph has degree less than k, it is not k-edge-connected.\n    If k is 1, the function checks if the graph is connected.\n    If k is 2, the function checks if the graph has no bridges.\n    Otherwise, the function checks if the edge connectivity of the graph is greater than or equal to k.\n\"\"\"\n```\n\n### Code snippet:\n```Python\ndef is_k_edge_connected(G, k):\n    if k < 1:\n        raise ValueError('k must be positive, not {}'.format(k))\n    # First try to quickly determine if G is not k-edge-connected\n    if G.number_of_nodes() < k + 1:\n        return False\n    elif any(d < k for n, d in G.degree()):\n        return False\n```\n\n### Requirements:\n1. Complete only the missing part of the function.\n2. Only answer the code of the completed part. Do not include any of the provided code snippet.\n3. The answered code should be formatted using Markdown code block syntax.\n4. Do not include any additional text.\n\n### Answer: (use the provided format with backticks)\n", "name": "is_k_edge_connected", "reference": "    else:\n        # Otherwise perform the full check\n        if k == 1:\n            return nx.is_connected(G)\n        elif k == 2:\n            return not nx.has_bridges(G)\n        else:\n            return nx.edge_connectivity(G, cutoff=k) >= k", "completions": ["```python\n```python\ndef is_k_edge_connected(G, k):\n    if k < 1:\n        raise ValueError('k must be positive, not {}'.format(k))\n    # First try to quickly determine if G is not k-edge-connected\n    if G.number_of_nodes() < k + 1:\n        return False\n    elif any(d < k for n, d in G.degree()):\n        return False\n    # If k is 1, check if G is connected\n    if k == 1:\n        return nx.is_connected(G)\n    # If k is 2, check if G has no bridges\n    elif k == 2:\n        return not any(nx.algorithms.approximation.two_edge_connected_components(G) > 1)\n    # Otherwise, check if the edge connectivity of G is greater than or equal to k\n    else:\n        return nx.edge_connectivity(G) >= k\n```\n```\n"]}
{"id": 74, "prompt": "<\uff5cbegin\u2581of\u2581sentence\uff5c>### Role:\nYou are an experienced Python programmer assistant responsible for code completion tasks. Please intelligently complete the missing parts of the provided code snippet.\n\n### Task:\nBased on the provided code snippet, complete the missing function implementation. Ensure the code is logically correct, syntactically accurate, and follows best coding practices.\n\n### Left context\n```Python\nimport re\n\nfrom django.conf import settings\nfrom django.utils import translation\nfrom django.utils.translation.trans_real import to_language\nfrom django.utils.encoding import smart_unicode\n\nimport bleach\nimport jinja2\nimport jingo\n\nfrom olympia.amo.utils import clean_nl\n\njingo.register.filter(to_language)\n\n\n@jingo.register.filter\n```\n\n### Code description\n```Python\n\"\"\"\nDescription: This function generates HTML markup for locale-specific attributes.\n\nArgs:\n    translatedfield (type): The translated field for which the HTML markup is generated.\n\nReturns:\n    str: The HTML markup string with locale-specific attributes.\n\"\"\"\n```\n\n### Code snippet:\n```Python\ndef locale_html(translatedfield):\n    if not translatedfield:\n        return ''\n\n    site_locale = translation.to_locale(translation.get_language())\n    locale = translation.to_locale(translatedfield.locale)\n```\n\n### Requirements:\n1. Complete only the missing part of the function.\n2. Only answer the code of the completed part. Do not include any of the provided code snippet.\n3. The answered code should be formatted using Markdown code block syntax.\n4. Do not include any additional text.\n\n### Answer: (use the provided format with backticks)\n", "name": "locale_html", "reference": "    if locale == site_locale:\n        return ''\n    else:\n        rtl_locales = map(translation.to_locale, settings.RTL_LANGUAGES)\n        textdir = 'rtl' if locale in rtl_locales else 'ltr'\n        return jinja2.Markup(' lang=\"%s\" dir=\"%s\"' % (\n            jinja2.escape(translatedfield.locale), textdir))", "completions": ["```Python\ndef locale_html(translatedfield):\n    if not translatedfield:\n        return ''\n\n    site_locale = translation.to_locale(translation.get_language())\n    locale = translation.to_locale(translatedfield.locale)\n\n    if site_locale != locale:\n        return f'<html lang=\"{locale}\">{locale_html(translatedfield)}</html>'\n    else:\n        return bleach.clean(smart_unicode(jinja2.get_template(translatedfield.field.template_name).render(translatedfield.field.resolve_variable(translatedfield.field.context))), tags=[], styles=[], attributes={})\n```\n"]}
{"id": 75, "prompt": "<\uff5cbegin\u2581of\u2581sentence\uff5c>### Role:\nYou are an experienced Python programmer assistant responsible for code completion tasks. Please intelligently complete the missing parts of the provided code snippet.\n\n### Task:\nBased on the provided code snippet, complete the missing function implementation. Ensure the code is logically correct, syntactically accurate, and follows best coding practices.\n\n### Left context\n```Python\n\"\"\"Routines for a local search to refine the solution.\n\nThis module contains all functions that are necessary to implement a\nlocal search to refine the solution quality. The local search exploits\na linear model of the objective function.\n\nLicensed under Revised BSD license, see LICENSE.\n(C) Copyright International Business Machines Corporation 2017.\n\n\"\"\"\n\nfrom __future__ import print_function\nfrom __future__ import division\nfrom __future__ import absolute_import\n\nimport sys\nimport numpy as np\nimport scipy.spatial as ss\nimport scipy.linalg as la\nimport rbfopt.rbfopt_utils as ru\nfrom rbfopt.rbfopt_settings import RbfoptSettings\n\n\n```\n\n### Code description\n```Python\n\"\"\"\nDescription: This function initializes the refinement process for a given set of nodes.\n\nArgs:\n    settings (RbfoptSettings): An instance of the RbfoptSettings class, which contains settings for the refinement process.\n    n (int): The number of nodes to keep.\n    k (int): The total number of nodes.\n    node_pos (np.ndarray): A 2D array of shape (k, n) representing the positions of the nodes.\n    center (np.ndarray): A 1D array of shape (n,) representing the center point.\n\nReturns:\n    tuple: A tuple containing two elements:\n        - model_set (np.ndarray): A 1D array of shape (num_to_keep,) representing the indices of the nodes to keep.\n        - ref_radius (float): The refinement radius, which is the maximum of the median distance to the center among the nodes to keep and a scaled minimum refinement radius.\n\"\"\"\n```\n\n### Code snippet:\n```Python\ndef init_refinement(settings, n, k, node_pos, center):\n    assert(isinstance(node_pos, np.ndarray))\n    assert(len(node_pos)==k)\n    assert(k >= 2)\n    assert(isinstance(center, np.ndarray))\n    assert(len(np.atleast_1d(center))==n)\n    assert(isinstance(settings, RbfoptSettings))\n    # Find points closest to the given point\n    dist = ss.distance.cdist(np.atleast_2d(center), node_pos)\n```\n\n### Requirements:\n1. Complete only the missing part of the function.\n2. Only answer the code of the completed part. Do not include any of the provided code snippet.\n3. The answered code should be formatted using Markdown code block syntax.\n4. Do not include any additional text.\n\n### Answer: (use the provided format with backticks)\n", "name": "init_refinement", "reference": "    dist_order = np.argsort(dist[0])\n    # The nodes to keep are those closest to the center\n    num_to_keep = min(n + 1, k)\n    # Build array of nodes to keep\n    model_set = dist_order[np.arange(num_to_keep)]\n    ref_radius = max(np.percentile(dist[0, model_set[1:]], 50),\n                     settings.ref_min_radius * \n                     2**settings.ref_init_radius_multiplier)\n    return (model_set, ref_radius)", "completions": ["```python\n```\n\n### Code snippet:\n```python\n    # Find the median distance to the center\n    median_dist = np.median(dist, axis=0)\n    # Compute the maximum of the median distance to the center and a scaled minimum refinement radius\n    ref_radius = max(median_dist)\n    # Initialize the indices of the nodes to keep\n    model_set = np.arange(k)[median_dist > ref_radius]\n    return model_set, ref_radius\n```\n"]}
{"id": 76, "prompt": "<\uff5cbegin\u2581of\u2581sentence\uff5c>### Role:\nYou are an experienced Python programmer assistant responsible for code completion tasks. Please intelligently complete the missing parts of the provided code snippet.\n\n### Task:\nBased on the provided code snippet, complete the missing function implementation. Ensure the code is logically correct, syntactically accurate, and follows best coding practices.\n\n### Left context\n```Python\n# Copyright 2017 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\"\"\"Functions for computing metrics like precision, recall, CorLoc and etc.\"\"\"\nfrom __future__ import division\n\nimport numpy as np\n\n\ndef compute_precision_recall(scores, labels, num_gt):\n  \"\"\"Compute precision and recall.\n\n  Args:\n    scores: A float numpy array representing detection score\n    labels: A float numpy array representing weighted true/false positive labels\n    num_gt: Number of ground truth instances\n\n  Raises:\n    ValueError: if the input is not of the correct format\n\n  Returns:\n    precision: Fraction of positive instances over detected ones. This value is\n      None if no ground truth labels are present.\n    recall: Fraction of detected positive instance over all positive instances.\n      This value is None if no ground truth labels are present.\n\n  \"\"\"\n  if not isinstance(labels, np.ndarray) or len(labels.shape) != 1:\n    raise ValueError(\"labels must be single dimension numpy array\")\n\n  if labels.dtype != np.float and labels.dtype != np.bool:\n    raise ValueError(\"labels type must be either bool or float\")\n\n  if not isinstance(scores, np.ndarray) or len(scores.shape) != 1:\n    raise ValueError(\"scores must be single dimension numpy array\")\n\n  if num_gt < np.sum(labels):\n    raise ValueError(\"Number of true positives must be smaller than num_gt.\")\n\n  if len(scores) != len(labels):\n    raise ValueError(\"scores and labels must be of the same size.\")\n\n  if num_gt == 0:\n    return None, None\n\n  sorted_indices = np.argsort(scores)\n  sorted_indices = sorted_indices[::-1]\n  true_positive_labels = labels[sorted_indices]\n  false_positive_labels = (true_positive_labels <= 0).astype(float)\n  cum_true_positives = np.cumsum(true_positive_labels)\n  cum_false_positives = np.cumsum(false_positive_labels)\n  precision = cum_true_positives.astype(float) / (\n      cum_true_positives + cum_false_positives)\n  recall = cum_true_positives.astype(float) / num_gt\n  return precision, recall\n\n\ndef compute_average_precision(precision, recall):\n  \"\"\"Compute Average Precision according to the definition in VOCdevkit.\n\n  Precision is modified to ensure that it does not decrease as recall\n  decrease.\n\n  Args:\n    precision: A float [N, 1] numpy array of precisions\n    recall: A float [N, 1] numpy array of recalls\n\n  Raises:\n    ValueError: if the input is not of the correct format\n\n  Returns:\n    average_precison: The area under the precision recall curve. NaN if\n      precision and recall are None.\n\n  \"\"\"\n  if precision is None:\n    if recall is not None:\n      raise ValueError(\"If precision is None, recall must also be None\")\n    return np.NAN\n\n  if not isinstance(precision, np.ndarray) or not isinstance(\n      recall, np.ndarray):\n    raise ValueError(\"precision and recall must be numpy array\")\n  if precision.dtype != np.float or recall.dtype != np.float:\n    raise ValueError(\"input must be float numpy array.\")\n  if len(precision) != len(recall):\n    raise ValueError(\"precision and recall must be of the same size.\")\n  if not precision.size:\n    return 0.0\n  if np.amin(precision) < 0 or np.amax(precision) > 1:\n    raise ValueError(\"Precision must be in the range of [0, 1].\")\n  if np.amin(recall) < 0 or np.amax(recall) > 1:\n    raise ValueError(\"recall must be in the range of [0, 1].\")\n  if not all(recall[i] <= recall[i + 1] for i in range(len(recall) - 1)):\n    raise ValueError(\"recall must be a non-decreasing array\")\n\n  recall = np.concatenate([[0], recall, [1]])\n  precision = np.concatenate([[0], precision, [0]])\n\n  # Preprocess precision to be a non-decreasing array\n  for i in range(len(precision) - 2, -1, -1):\n    precision[i] = np.maximum(precision[i], precision[i + 1])\n\n  indices = np.where(recall[1:] != recall[:-1])[0] + 1\n  average_precision = np.sum(\n      (recall[indices] - recall[indices - 1]) * precision[indices])\n  return average_precision\n\n\ndef compute_cor_loc(num_gt_imgs_per_class,\n                    num_images_correctly_detected_per_class):\n  \"\"\"Compute CorLoc according to the definition in the following paper.\n\n  https://www.robots.ox.ac.uk/~vgg/rg/papers/deselaers-eccv10.pdf\n\n  Returns nans if there are no ground truth images for a class.\n\n  Args:\n    num_gt_imgs_per_class: 1D array, representing number of images containing\n        at least one object instance of a particular class\n    num_images_correctly_detected_per_class: 1D array, representing number of\n        images that are correctly detected at least one object instance of a\n        particular class\n\n  Returns:\n    corloc_per_class: A float numpy array represents the corloc score of each\n      class\n  \"\"\"\n  return np.where(\n      num_gt_imgs_per_class == 0, np.nan,\n      num_images_correctly_detected_per_class / num_gt_imgs_per_class)\n\n\n```\n\n### Code description\n```Python\n\"\"\"\nDescription: This function computes the median rank at k for a given list of true positive and false positive values.\n\nArgs:\n    tp_fp_list (list): A list of numpy arrays, where each array represents the true positive and false positive values for a specific item.\n    k (int): The maximum rank to consider when computing the median rank.\n\nReturns:\n    float: The median rank at k.\n\"\"\"\n```\n\n### Code snippet:\n```Python\ndef compute_median_rank_at_k(tp_fp_list, k):\n  ranks = []\n  for i in range(len(tp_fp_list)):\n```\n\n### Requirements:\n1. Complete only the missing part of the function.\n2. Only answer the code of the completed part. Do not include any of the provided code snippet.\n3. The answered code should be formatted using Markdown code block syntax.\n4. Do not include any additional text.\n\n### Answer: (use the provided format with backticks)\n", "name": "compute_median_rank_at_k", "reference": "    ranks.append(\n        np.where(tp_fp_list[i][0:min(k, tp_fp_list[i].shape[0])] > 0)[0])\n  concatenated_ranks = np.concatenate(ranks)\n  return np.median(concatenated_ranks)", "completions": ["```python\ndef compute_median_rank_at_k(tp_fp_list, k):\n  \"\"\"\n  Description: This function computes the median rank at k for a given list of true positive and false positive values.\n\n  Args:\n    tp_fp_list (list): A list of numpy arrays, where each array represents the true positive and false positive values for a specific item.\n    k (int): The maximum rank to consider when computing the median rank.\n\n  Returns:\n    float: The median rank at k.\n  \"\"\"\n  ranks = []\n  for i in range(len(tp_fp_list)):\n    tp, fp = tp_fp_list[i]\n    rank = (tp + fp) / (tp + fp + k)\n    ranks.append(rank)\n\n  return np.median(ranks)\n```\n"]}
{"id": 77, "prompt": "<\uff5cbegin\u2581of\u2581sentence\uff5c>### Role:\nYou are an experienced Python programmer assistant responsible for code completion tasks. Please intelligently complete the missing parts of the provided code snippet.\n\n### Task:\nBased on the provided code snippet, complete the missing function implementation. Ensure the code is logically correct, syntactically accurate, and follows best coding practices.\n\n### Left context\n```Python\nimport logging\nfrom urllib.parse import parse_qsl, urlencode, urlparse, urlunparse\n\nfrom django.conf import settings\nfrom django.core.exceptions import PermissionDenied\nfrom django.urls import reverse, NoReverseMatch\n\nfrom django.http import HttpResponseRedirect\nfrom django.utils.encoding import force_bytes\n\nfrom mama_cas.services import service_allowed\n\n\nlogger = logging.getLogger(__name__)\n\n\n```\n\n### Code description\n```Python\n\"\"\"\nDescription: This function is used to add query parameters to a URL.\n\nArgs:\n    url (str): The URL to which the query parameters are to be added.\n    params (dict): A dictionary containing the query parameters to be added.\n\nReturns:\n    str: The URL with the added query parameters.\n\"\"\"\n```\n\n### Code snippet:\n```Python\ndef add_query_params(url, params):\n    def encode(s):\n        return force_bytes(s, settings.DEFAULT_CHARSET)\n    params = dict([(encode(k), encode(v)) for k, v in params.items() if v])\n\n```\n\n### Requirements:\n1. Complete only the missing part of the function.\n2. Only answer the code of the completed part. Do not include any of the provided code snippet.\n3. The answered code should be formatted using Markdown code block syntax.\n4. Do not include any additional text.\n\n### Answer: (use the provided format with backticks)\n", "name": "add_query_params", "reference": "    parts = list(urlparse(url))\n    query = dict(parse_qsl(parts[4]))\n    query.update(params)\n    parts[4] = urlencode(query)\n    return urlunparse(parts)", "completions": ["```Python\ndef add_query_params(url, params):\n    parsed_url = urlparse(url)\n    query = dict(parse_qsl(parsed_url.query))\n    query.update(params)\n    new_query = urlencode(query)\n    new_url = urlunparse((parsed_url.scheme, parsed_url.netloc, parsed_url.path, '', new_query, ''))\n    return new_url\n```\n"]}
{"id": 78, "prompt": "<\uff5cbegin\u2581of\u2581sentence\uff5c>### Role:\nYou are an experienced Python programmer assistant responsible for code completion tasks. Please intelligently complete the missing parts of the provided code snippet.\n\n### Task:\nBased on the provided code snippet, complete the missing function implementation. Ensure the code is logically correct, syntactically accurate, and follows best coding practices.\n\n### Left context\n```Python\n'''\nAutomatically change Matplotlib figures to LaTeX figures.\n\n'''\nfrom contextlib import contextmanager\nimport errno\nimport logging\nimport math\nfrom pathlib import Path\nimport warnings\n\nimport matplotlib.pyplot as plt\n\n\nlogger = logging.getLogger('latexipy')\n\n\nINCH_PER_POINT = 1/72.27\nGOLDEN_RATIO = (math.sqrt(5) - 1) / 2\n\nMAX_HEIGHT_INCH = 8\nFONT_SIZE = 8\n\nPARAMS = {\n    'pgf.texsystem': 'xelatex',  # pdflatex, xelatex, lualatex\n    'text.usetex': True,\n    'font.family': 'serif',\n    'font.serif': [],\n    'font.sans-serif': [],\n    'font.monospace': [],\n    'pgf.preamble': [\n      r'\\usepackage[utf8x]{inputenc}',\n      r'\\usepackage[T1]{fontenc}',\n      ],\n    'font.size': FONT_SIZE,\n    'axes.labelsize': FONT_SIZE,\n    'axes.titlesize': FONT_SIZE,\n    'legend.fontsize': FONT_SIZE,\n    'xtick.labelsize': FONT_SIZE,\n    'ytick.labelsize': FONT_SIZE,\n}\n\n_ORIGINAL_PARAMS = dict(plt.rcParams)\n_ORIGINAL_BACKEND = plt.get_backend()\n\n\ndef latexify(params=PARAMS, new_backend='pgf'):\n    '''\n    Set up Matplotlib's RC params for LaTeX plotting.\n\n    Call this function before plotting the first figure.\n\n    Parameters\n    ----------\n    params : Optional[dict]\n        A dictionary containing the RC params that need to be updated. Default\n        is ``PARAMS``. The defaults should be okay for most cases, but\n        ``PARAMS`` can be updated via ``.update()`` as well.\n\n    new_backend : Optional[str|None]\n        The backend to switch too. Default is PGF, which allows a nicer PDF\n        output too.\n\n    Raises\n    ------\n    ValueError\n        If the new backend is not supported.\n\n    Example\n    -------\n    >>> params = PARAMS.copy()\n    >>> params.update({'font.family': 'sans-serif'})\n    >>> latexify(params)\n\n    '''\n    plt.rcParams.update(params)\n    if new_backend is not None:\n        try:\n            plt.switch_backend(new_backend)\n        except ValueError:\n            logger.error(f'Backend not supported: {new_backend!r}')\n            raise\n\n\ndef revert():\n    '''\n    Return to the settings before running ``latexify()`` and updating params.\n\n    '''\n    plt.rcParams.update(_ORIGINAL_PARAMS)\n    plt.switch_backend(_ORIGINAL_BACKEND)\n\n\n@contextmanager\ndef temp_params(font_size=None, font_family=None, font_serif=None,\n                font_sans_serif=None, font_monospace=None, params_dict=None):\n    '''\n    Temporarily set Matplotlib's RC params.\n\n    Parameters\n    ----------\n    font_size : Optional[int]\n        The font size to use. It changes all the components that are normally\n        updated with ``latexify()``. If you want to change something\n        individually, do so from within ``params_dict``.\n    font_family : Optional[str]\n        The font family to use.\n    font_serif : Optional[List[str]]\n        A list of serif fonts to use.\n    font_sans_serif : Optional[List[str]]\n        A list of sans-serif fonts to use.\n    font_monospace : Optional[List[str]]\n        A list of monospace fonts to use.\n    params_dict : Optional[Dict[str, Any]]\n        The dictionary of parameters to update, and the updated values. This is\n        only applied after going through the rest of the arguments.\n\n    '''\n    old_params = plt.rcParams\n    new_params = old_params.copy()\n\n    mapping = {\n        'font.size': font_size,\n        'axes.labelsize': font_size,\n        'axes.titlesize': font_size,\n        'legend.fontsize': font_size,\n        'xtick.labelsize': font_size,\n        'ytick.labelsize': font_size,\n        'font.family': font_family,\n        'font.serif': font_serif,\n        'font.sans-serif': font_sans_serif,\n        'font.monospace': font_monospace,\n    }\n\n    new_params.update({k: v\n                       for k, v in mapping.items()\n                       if v is not None})\n\n    if params_dict is not None:\n        new_params.update(params_dict)\n\n    plt.rcParams.update(new_params)\n    try:\n        yield\n    finally:\n        plt.rcParams.update(old_params)\n\n\ndef figure_size(width_tw=0.9, *, ratio=None, height=None, n_columns=1,\n                max_height=MAX_HEIGHT_INCH, doc_width_pt=345):\n    r'''\n    Get the necessary figure size.\n\n    Parameters\n    ----------\n    width_tw : Optional[float]\n        The width of the figure, as a proportion of the text width, between 0\n        and 1. Default is 0.9.\n    ratio: Optional[float]\n        The ratio of the figure height to figure width. If ``height`` is\n        specified, ``ratio`` is calculated from that and ``width``. Default is\n        the golden ratio.\n    height : Optional[float]\n        The height of the figure in inches. If ``ratio`` is specified,\n        ``height`` is ignored. Default is the golden ratio of the width.\n    n_columns : Optional[int]\n        The number of equally sized columns in the document. The figure will\n        never be larger than the width of one column.  Default is 1.\n    max_height : Optional[float]\n        The maximum height of the figure, in inches. Default is\n        ``MAX_HEIGHT_INCH``.\n    doc_width_pt : float\n        The text width of the document, in points. It can be obtained by typing\n        ``\\the\\textwidth`` in the LaTeX document. Default is 345.\n\n    Returns\n    -------\n    width : float\n        The figure width, in inches.\n    height : float\n        The figure height in inches.\n\n    '''\n    doc_width_in = doc_width_pt * INCH_PER_POINT\n    width = doc_width_in * width_tw / n_columns\n\n    if ratio is None:\n        if height is None:\n            ratio = GOLDEN_RATIO\n        else:\n            ratio = height / width\n\n    height = width * ratio\n\n    if height > max_height:\n        warnings.warn(f'height too large at {height} inches; '\n                      f'will automatically reduce to {max_height} inches.')\n        height = max_height\n    return width, height\n\n\ndef save_figure(filename, directory, exts, mkdir=True,\n                from_context_manager=False):\n    '''\n    Save the figure in each of the extensions.\n\n    Parameters\n    ----------\n    filename : str\n        The base name of the file, without extensions.\n    directory : str\n        The name of the directory in which to store the saved files.\n    exts : Sequence\n        A list of all the extensions to be saved, without the dot.\n    mkdir : Optional[bool]\n        Whether the directory should be created automatically if it does not\n        exist.  Default is True.\n    from_context_manager : Optional[bool]\n        Whether the function is being called from the ``figure`` context\n        manager.  This only affects the logging output. Default is False.\n\n    Raises\n    ------\n    FileNotFoundError\n        If the target directory does not exist and cannot be created.\n    NotADirectoryError\n        If the target directory is actually a file.\n    PermissionError\n        If there is no permission to write to the target directory.\n    ValueError\n        If the format is not supported.\n\n    Notes\n    -----\n    When integrating with LaTeX, the recommended format is PGF. PNG can be used\n    externally, such as in blog posts or as embedded images, while PDF can be\n    standalone, or inserted into LaTeX documents. A full list of supported\n    formats can be found by calling\n    ``plt.gcf().canvas.get_supported_filetypes_grouped()``\n\n    '''\n    directory = Path(directory)\n\n    if not from_context_manager:\n        logger.info(f'Saving {filename}...  ')\n\n    try:\n        plt.tight_layout(0)\n    except ValueError as e:\n        warnings.warn('No figures to save.')\n\n    if mkdir:\n        if directory.is_file():\n            msg = 'A file exists at directory location'\n            e = NotADirectoryError(errno.ENOTDIR, msg, str(directory))\n            logger.error(f'Directory set to file: {str(directory)}')\n            raise e\n        try:\n            directory.mkdir(parents=True, exist_ok=True)\n        except PermissionError as e:\n            logger.error('Permission denied for directory: '\n                         f'{str(directory)!r}')\n            raise\n\n    for ext in exts:\n        if from_context_manager:\n            logger.info(f'  Saving {ext}...')\n        full_filename = f'{filename}.{ext}'\n        try:\n            plt.savefig(str(directory/full_filename))\n        except FileNotFoundError as e:\n            logger.error(f'Directory does not exist: {str(directory)!r}.'\n                         'Please create it or set mkdir to True.')\n            raise\n        except PermissionError as e:\n            logger.error(f'Permission denied for file ({full_filename!r}) in'\n                         f'directory: {str(directory)!r}')\n            raise\n        except ValueError as e:\n            logger.error(f'Unsupported file format: {ext}')\n            raise\n\n\n@contextmanager\n```\n\n### Code description\n```Python\n\"\"\"\nDescription: This function is used to generate a figure with a given filename, directory, extensions, size, and whether to make a directory.\n\nArgs:\n    filename (str): The name of the file to save the figure.\n    directory (str, optional): The directory to save the figure. Defaults to 'img'.\n    exts (list, optional): The extensions to save the figure as. Defaults to ['pgf', 'png'].\n    size (tuple, optional): The size of the figure. Defaults to None.\n    mkdir (bool, optional): Whether to make a directory. Defaults to True.\n\nReturns:\n    None\n\"\"\"\n```\n\n### Code snippet:\n```Python\ndef figure(filename, *, directory='img', exts=['pgf', 'png'], size=None,\n           mkdir=True):\n    if size is None:\n        size = figure_size()\n    logger.info(f'{filename}:')\n```\n\n### Requirements:\n1. Complete only the missing part of the function.\n2. Only answer the code of the completed part. Do not include any of the provided code snippet.\n3. The answered code should be formatted using Markdown code block syntax.\n4. Do not include any additional text.\n\n### Answer: (use the provided format with backticks)\n", "name": "figure", "reference": "    logger.info('  Plotting...')\n    yield\n    plt.gcf().set_size_inches(*size)\n    save_figure(filename=filename, directory=directory, exts=exts, mkdir=mkdir,\n                from_context_manager=True)\n    plt.close()", "completions": ["```python\ndef figure(filename, *, directory='img', exts=['pgf', 'png'], size=None,\n           mkdir=True):\n    if size is None:\n        size = figure_size()\n    logger.info(f'{filename}:')\n    with temp_params(font_size=size[0]*1.5):\n        plt.figure(figsize=size)\n        yield\n        save_figure(filename, directory, exts, mkdir)\n```\n"]}
{"id": 79, "prompt": "<\uff5cbegin\u2581of\u2581sentence\uff5c>### Role:\nYou are an experienced Python programmer assistant responsible for code completion tasks. Please intelligently complete the missing parts of the provided code snippet.\n\n### Task:\nBased on the provided code snippet, complete the missing function implementation. Ensure the code is logically correct, syntactically accurate, and follows best coding practices.\n\n### Left context\n```Python\n\"\"\"\nZero-offset convolutional seismic modeling\n\nGive a depth model and obtain a seismic zero-offset convolutional gather. You\ncan give the wavelet, if you already have, or use one of the existing, from\nwhich we advise ricker wavelet (rickerwave function).\n\n* :func:`~fatiando.seismic.conv.convolutional_model`: given the reflectivity\n  series and wavelet, it returns the convolutional seismic gather.\n* :func:`~fatiando.seismic.conv.reflectivity`: calculates the reflectivity\n  series from the velocity model (and density model if present).\n* :func:`~fatiando.seismic.conv.depth_2_time`: convert depth property model to\n  the model in time.\n* :func:`~fatiando.seismic.conv.rickerwave`: calculates a ricker wavelet.\n\nReferences\n----------\n\nYilmaz, Oz,\nCh.2 Deconvolution. In: YILMAZ, Oz. Seismic Data Analysis: Processing,\nInversion, and Interpretation of Seismic Data. Tulsa: Seg, 2001. Cap. 2.\np. 159-270. Available at: <http://dx.doi.org/10.1190/1.9781560801580.ch2>\n\n\n\"\"\"\nfrom __future__ import division\nimport numpy as np\nfrom scipy import interpolate  # linear interpolation of velocity/density\n\n\n```\n\n### Code description\n```Python\n\"\"\"\nDescription: This function performs a convolution operation on a given wavelet with a reflectivity series.\n\nArgs:\n    rc (numpy.ndarray): The reflectivity series to be convolved.\n    f (float): The frequency of the wavelet.\n    wavelet (function): The wavelet function to be used for the convolution.\n    dt (float): The time step of the reflectivity series.\n\nReturns:\n    numpy.ndarray: The convolved reflectivity series.\n\"\"\"\n```\n\n### Code snippet:\n```Python\ndef convolutional_model(rc, f, wavelet, dt):\n    w = wavelet(f, dt)\n    # convolution\n    synth_l = np.zeros(np.shape(rc))\n    for j in range(0, rc.shape[1]):\n```\n\n### Requirements:\n1. Complete only the missing part of the function.\n2. Only answer the code of the completed part. Do not include any of the provided code snippet.\n3. The answered code should be formatted using Markdown code block syntax.\n4. Do not include any additional text.\n\n### Answer: (use the provided format with backticks)\n", "name": "convolutional_model", "reference": "        if np.shape(rc)[0] >= len(w):\n            synth_l[:, j] = np.convolve(rc[:, j], w, mode='same')\n        else:\n            aux = np.floor(len(w)/2.)\n            synth_l[:, j] = np.convolve(rc[:, j], w, mode='full')[aux:-aux]\n    return synth_l", "completions": ["```python\n    for j in range(0, rc.shape[1]):\n        synth_l[:, j] = np.convolve(rc[:, j], w, mode='same')\n    return synth_l\n```\n"]}
{"id": 80, "prompt": "<\uff5cbegin\u2581of\u2581sentence\uff5c>### Role:\nYou are an experienced Python programmer assistant responsible for code completion tasks. Please intelligently complete the missing parts of the provided code snippet.\n\n### Task:\nBased on the provided code snippet, complete the missing function implementation. Ensure the code is logically correct, syntactically accurate, and follows best coding practices.\n\n### Left context\n```Python\nfrom collections import namedtuple\nimport random\nimport logging\nimport math\n\nimport numpy as np\nfrom numpy.linalg import LinAlgError\n\n\ndef dvmnorm(x, mean, covariance, log=False):\n  \"\"\"density function for the multivariate normal distribution\n  based on sources of R library 'mvtnorm'\n  :rtype : np.array\n  :param x: vector or matrix of quantiles. If x is a matrix, each row is taken to be a quantile\n  :param mean: mean vector, np.array\n  :param covariance: covariance matrix, np.array\n  :param log: if True, densities d are given as log(d), default is False\n  \"\"\"\n  # TODO: add another methods from mvtnorm (calculate matrix square root using eigenvalues or SVD\n  # TODO: add check for matching of input matrix dimensions\n  n = covariance.shape[0]\n  try:\n    dec = np.linalg.cholesky(covariance)\n  except LinAlgError:\n    dec = np.linalg.cholesky(covariance + np.eye(covariance.shape[0]) * 0.0001)\n  tmp = np.linalg.solve(dec, np.transpose(x - mean))\n  rss = np.sum(tmp * tmp, axis=0)\n  logretval = - np.sum(np.log(np.diag(dec))) - 0.5 * n * np.log(2 * math.pi) - 0.5 * rss\n  if log:\n    return logretval\n  else:\n    return np.exp(logretval)\n\n\nclass EMClustering(object):\n  logger = logging.getLogger(__name__)\n  ch = logging.StreamHandler()\n  formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n  ch.setFormatter(formatter)\n  logger.addHandler(ch)\n  logger.setLevel(logging.DEBUG)\n\n  cluster = namedtuple('cluster', 'weight, mean, covariance')\n\n  def __init__(self, n_clusters):\n    self._data = None\n    self._clusters = None\n    self._membership_weights = None\n    self._partitions = None\n    self._n_clusters = n_clusters\n\n  @property\n  def partitions(self):\n    return self._partitions\n\n  @property\n  def data(self):\n    return self._data\n\n  @property\n  def labels(self):\n    return self._membership_weights\n\n  @property\n  def clusters(self):\n    return self._clusters\n\n  def fit_predict(self, data, iteration=5):\n    self.setup(data)\n    for i in range(iteration):\n      self.logger.debug('Iteration %d', i)\n      self.expect()\n      self.maximize()\n    return self\n\n  def setup(self, data):\n    self._n_samples, self._n_features = data.shape\n    self._data = data\n    self._membership_weights = np.ones((self._n_samples, self._n_clusters)) / self._n_clusters\n    self._s = 0.2\n\n    indices = list(range(data.shape[0]))\n    random.shuffle(indices)\n    pick_k_random_indices = random.sample(indices, self._n_clusters)\n\n    self._clusters = []\n    for cluster_num in range(self._n_clusters):\n      mean = data[pick_k_random_indices[cluster_num], :]\n      covariance = self._s * np.eye(self._n_features)\n      self._clusters.append(self.cluster(1.0 / self._n_clusters, mean, covariance))\n\n    self._partitions = np.empty(self._n_samples, dtype=np.int32)\n\n```\n\n### Code description\n```Python\n\"\"\"\nDescription: This function performs the expectation step of the EM algorithm for a Gaussian Mixture Model (GMM).\n\nArgs:\n    self (object): The instance of the class containing the data and clusters.\n\nAttributes:\n    _clusters (list): A list of clusters, each containing a mean, covariance, and weight.\n    _data (array): The data to be clustered.\n    _membership_weights (array): The membership weights for each data point in each cluster.\n    _n_clusters (int): The number of clusters.\n    _partitions (array): The cluster assignments for each data point.\n\nReturns:\n    None\n\"\"\"\n```\n\n### Code snippet:\n```Python\n\ndef expect(self):\n  log_likelyhood = 0\n  for cluster_num, cluster in enumerate(self._clusters):\n    log_density = dvmnorm(self._data, cluster.mean, cluster.covariance, log=True)\n    membership_weights = cluster.weight * np.exp(log_density)\n    log_likelyhood += sum(log_density * self._membership_weights[:, cluster_num])\n\n    self._membership_weights[:, cluster_num] = membership_weights\n\n```\n\n### Requirements:\n1. Complete only the missing part of the function.\n2. Only answer the code of the completed part. Do not include any of the provided code snippet.\n3. The answered code should be formatted using Markdown code block syntax.\n4. Do not include any additional text.\n\n### Answer: (use the provided format with backticks)\n", "name": "expect", "reference": "  for sample_num, probabilities in enumerate(self._membership_weights):\n    prob_sum = sum(probabilities)\n\n    self._partitions[sample_num] = np.argmax(probabilities)\n\n    if prob_sum == 0:\n      self._membership_weights[sample_num, :] = np.ones_like(probabilities) / self._n_clusters\n    else:\n      self._membership_weights[sample_num, :] = probabilities / prob_sum\n\n  self.logger.debug('log likelyhood %f', log_likelyhood)", "completions": ["```python\ndef expect(self):\n  \"\"\"\n  Expectation step of the EM algorithm for a Gaussian Mixture Model (GMM).\n\n  Args:\n      self (object): The instance of the class containing the data and clusters.\n\n  Attributes:\n      _clusters (list): A list of clusters, each containing a mean, covariance, and weight.\n      _data (array): The data to be clustered.\n      _membership_weights (array): The membership weights for each data point in each cluster.\n      _n_clusters (int): The number of clusters.\n      _partitions (array): The cluster assignments for each data point.\n\n  Returns:\n      None\n  \"\"\"\n  log_likelyhood = 0\n  for cluster_num, cluster in enumerate(self._clusters):\n    log_density = dvmnorm(self._data, cluster.mean, cluster.covariance, log=True)\n    membership_weights = cluster.weight * np.exp(log_density)\n    log_likelyhood += sum(log_density * self._membership_weights[:, cluster_num])\n\n    self._membership_weights[:, cluster_num] = membership_weights\n\n  return log_likelyhood\n```\n"]}
{"id": 81, "prompt": "<\uff5cbegin\u2581of\u2581sentence\uff5c>### Role:\nYou are an experienced Python programmer assistant responsible for code completion tasks. Please intelligently complete the missing parts of the provided code snippet.\n\n### Task:\nBased on the provided code snippet, complete the missing function implementation. Ensure the code is logically correct, syntactically accurate, and follows best coding practices.\n\n### Left context\n```Python\n# -*- encoding=utf -*-\n\"\"\"\ncubes.sql.query\n~~~~~~~~~~~~~~~\n\nStar/snowflake schema query construction structures\n\n\"\"\"\n\n# Note for developers and maintainers\n# -----------------------------------\n#\n# This module is to be remained implemented in a way that it does not use any\n# of the Cubes objects. It might use duck-typing and assume objects with\n# similar attributes. No calls to Cubes object functions should be allowed\n# here.\n\nfrom __future__ import absolute_import\n\nimport logging\nfrom collections import namedtuple\n\nimport sqlalchemy as sa\nimport sqlalchemy.sql as sql\nfrom sqlalchemy.sql.expression import and_\n\nfrom ..model import object_dict\nfrom ..errors import InternalError, ModelError, ArgumentError, HierarchyError\nfrom .. import compat\nfrom ..browser import SPLIT_DIMENSION_NAME\nfrom ..cells import PointCut, SetCut, RangeCut\n\nfrom .expressions import compile_attributes\n\n\n# Default label for all fact keys\nFACT_KEY_LABEL = '__fact_key__'\n\n# Attribute -> Column\n# IF attribute has no 'expression' then mapping is used\n# IF attribute has expression, the expression is used and underlying mappings\n\n\"\"\"Physical column (or column expression) reference. `schema` is a database\nschema name, `table` is a table (or table expression) name containing the\n`column`. `extract` is an element to be extracted from complex data type such\nas date or JSON (in postgres). `function` is name of unary function to be\napplied on the `column`.\n\nNote that either `extract` or `function` can be used, not both.\"\"\"\n\nColumn = namedtuple(\"Column\", [\"schema\", \"table\", \"column\",\n                               \"extract\", \"function\"])\n\n#\n# IMPORTANT: If you decide to extend the above Mapping functionality by adding\n# other mapping attributes (not recommended, but still) or by changing the way\n# how existing attributes are used, make sure that there are NO OTHER COLUMNS\n# than the `column` used. Every column used MUST be accounted in the\n# relevant_joins() call.\n#\n# See similar comment in the column() method of the StarSchema.\n#\n\ndef to_column(obj, default_table=None, default_schema=None):\n    \"\"\"Utility function that will create a `Column` reference object from an\n    anonymous tuple, dictionary or a similar object. `obj` can also be a\n    string in form ``schema.table.column`` where shcema or both schema and\n    table can be ommited. `default_table` and `default_schema` are used when\n    no table or schema is provided in `obj`.\n\n    .. versionadded:: 1.1\n    \"\"\"\n\n    if obj is None:\n        raise ArgumentError(\"Mapping object can not be None\")\n\n    if isinstance(obj, compat.string_type):\n        obj = obj.split(\".\")\n\n    if isinstance(obj, (tuple, list)):\n        if len(obj) == 1:\n            column = obj[0]\n            table = None\n            schema = None\n        elif len(obj) == 2:\n            table, column = obj\n            schema = None\n        elif len(obj) == 3:\n            schema, table, column = obj\n        else:\n            raise ArgumentError(\"Join key can have 1 to 3 items\"\n                                \" has {}: {}\".format(len(obj), obj))\n        extract = None\n        function = None\n\n    elif hasattr(obj, \"get\"):\n        schema = obj.get(\"schema\")\n        table = obj.get(\"table\")\n        column = obj.get(\"column\")\n        extract = obj.get(\"extract\")\n        function = obj.get(\"function\")\n\n    else:  # pragma nocover\n        schema = obj.schema\n        table = obj.table\n        extract = obj.extract\n        function = obj.function\n\n    table = table or default_table\n    schema = schema or default_schema\n\n    return Column(schema, table, column, extract, function)\n\n\n# TODO: remove this and use just Column\nJoinKey = namedtuple(\"JoinKey\",\n                     [\"schema\",\n                      \"table\",\n                      \"column\"])\n\n\ndef to_join_key(obj):\n    \"\"\"Utility function that will create JoinKey tuple from an anonymous\n    tuple, dictionary or similar object. `obj` can also be a string in form\n    ``schema.table.column`` where schema or both schema and table can be\n    ommited. Dictionary representation of a join key has values: ``schema``,\n    ``table`` and ``column``. With the dicitonary representation, the\n    ``column`` can be a list of columns in the same table. Both master and\n    detail joins have to have the same number of columns in the join key.\n\n    Note that Cubes at this low level does not know which table is used for a\n    dimension, therefore the default dimension schema from mapper's naming can\n    not be assumed here and has to be explicitly mentioned.\n\n    .. versionadded:: 1.1\n    \"\"\"\n\n\n    if obj is None:\n        return JoinKey(None, None, None)\n\n    if isinstance(obj, compat.string_type):\n        obj = obj.split(\".\")\n\n    if isinstance(obj, (tuple, list)):\n        if len(obj) == 1:\n            column = obj[0]\n            table = None\n            schema = None\n        elif len(obj) == 2:\n            table, column = obj\n            schema = None\n        elif len(obj) == 3:\n            schema, table, column = obj\n        else:\n            raise ArgumentError(\"Join key can have 1 to 3 items\"\n                                \" has {}: {}\".format(len(obj), obj))\n\n    elif hasattr(obj, \"get\"):\n        schema = obj.get(\"schema\")\n        table = obj.get(\"table\")\n        column = obj.get(\"column\")\n\n    else:  # pragma nocover\n        schema = obj.schema\n        table = obj.table\n        column = obj.column\n\n    # Make it immutable and hashable, as the whole Join object has to be\n    # hashable\n    if isinstance(column, list):\n        column = tuple(column)\n\n    return JoinKey(schema, table, column)\n\n\"\"\"Table join specification. `master` and `detail` are TableColumnReference\ntuples. `method` denotes which table members should be considered in the join:\n*master* \u2013 all master members (left outer join), *detail* \u2013 all detail members\n(right outer join) and *match* \u2013 members must match (inner join).\"\"\"\n\nJoin = namedtuple(\"Join\",\n                  [\"master\", # Master table (fact in star schema)\n                   \"detail\", # Detail table (dimension in star schema)\n                   \"alias\",  # Optional alias for the detail table\n                   \"method\"  # Method how the table is joined\n                  ]\n                 )\n\n\ndef to_join(obj):\n    \"\"\"Utility conversion function that will create `Join` tuple from an\n    anonymous tuple, dictionary or similar object.\n\n    .. versionadded:: 1.1\n    \"\"\"\n\n    if isinstance(obj, (tuple, list)):\n        alias = None\n        method = None\n\n        if len(obj) == 3:\n            alias = obj[2]\n        elif len(obj) == 4:\n            alias, method = obj[2], obj[3]\n        elif len(obj) < 2 or len(obj) > 4:\n            raise ArgumentError(\"Join object can have 1 to 4 items\"\n                                \" has {}: {}\".format(len(obj), obj))\n\n        master = to_join_key(obj[0])\n        detail = to_join_key(obj[1])\n\n        return Join(master, detail, alias, method)\n\n    elif hasattr(obj, \"get\"):  # pragma nocover\n        return Join(to_join_key(obj.get(\"master\")),\n                    to_join_key(obj.get(\"detail\")),\n                    obj.get(\"alias\"),\n                    obj.get(\"method\"))\n\n    else:  # pragma nocover\n        return Join(to_join_key(obj.master),\n                    to_join_key(obj.detail),\n                    obj.alias,\n                    obj.method)\n\n\n# Internal table reference\n_TableRef = namedtuple(\"_TableRef\",\n                       [\"schema\", # Database schema\n                        \"name\",   # Table name\n                        \"alias\",  # Optional table alias instead of name\n                        \"key\",    # Table key (for caching or referencing)\n                        \"table\",  # SQLAlchemy Table object, reflected\n                        \"join\"    # join which joins this table as a detail\n                       ]\n                      )\n\n\nclass SchemaError(InternalError):\n    \"\"\"Error related to the physical star schema.\"\"\"\n    pass\n\n\nclass NoSuchTableError(SchemaError):\n    \"\"\"Error related to the physical star schema.\"\"\"\n    pass\n\n\nclass NoSuchAttributeError(SchemaError):\n    \"\"\"Error related to the physical star schema.\"\"\"\n    pass\n\n\ndef _format_key(key):\n    \"\"\"Format table key `key` to a string.\"\"\"\n    schema, table = key\n\n    table = table or \"(FACT)\"\n\n    if schema:\n        return \"{}.{}\".format(schema, table)\n    else:\n        return table\n\n\ndef _make_compound_key(table, key):\n    \"\"\"Returns a list of columns from `column_key` for `table` representing\n    potentially a compound key. The `column_key` can be a name of a single\n    column or list of column names.\"\"\"\n\n    if not isinstance(key, (list, tuple)):\n        key = [key]\n\n    return [table.columns[name] for name in key]\n\n\nclass StarSchema(object):\n    \"\"\"Represents a star/snowflake table schema. Attributes:\n\n    * `label` \u2013 user specific label for the star schema, used for the schema\n      identification, debug purposes and logging. Has no effect on the\n      execution or statement composition.\n    * `metadata` is a SQLAlchemy metadata object where the snowflake tables\n      are described.\n    * `mappings` is a dictionary of snowflake attributes. The keys are\n      attribute names, the values can be strings, dictionaries or objects with\n      specific attributes (see below)\n    * `fact` is a name or a reference to a fact table\n    * `joins` is a list of join specification (see below)\n    * `tables` are SQL Alchemy selectables (tables or statements) that are\n      referenced in the attributes. This dictionary is looked-up first before\n      the actual metadata. Only table name has to be specified and database\n      schema should not be used in this case.\n    * `schema` \u2013 default database schema containing tables\n\n    The columns can be specified as:\n\n    * a string with format: `column`, `table.column` or `schema.table.column`.\n      When no table is specified, then the fact table is considered.\n    * as a list of arguments `[[schema,] table,] column`\n    * `StarColumn` or any object with attributes `schema`, `table`,\n      `column`, `extract`, `function` can be used.\n    * a dictionary with keys same as the attributes of `StarColumn` object\n\n    Non-object arguments will be stored as a `StarColumn` objects internally.\n\n    The joins can be specified as a list of:\n\n    * tuples of column specification in form of (`master`, `detail`)\n    * a dictionary with keys or object with attributes: `master`, `detail`,\n      `alias` and `method`.\n\n    `master` is a specification of a column in the master table (fact) and\n    `detail` is a specification of a column in the detail table (usually a\n    dimension). `alias` is an alternative name for the `detail` table to be\n    joined.\n\n    The `method` can be: `match` \u2013 ``LEFT INNER JOIN``, `master` \u2013 ``LEFT\n    OUTER JOIN`` or `detail` \u2013 ``RIGHT OUTER JOIN``.\n\n\n    Note: It is not in the responsibilities of the `StarSchema` to resolve\n    arithmetic expressions neither attribute dependencies. It is up to the\n    caller to resolve these and ask for basic columns only.\n\n\n    .. note::\n\n        The `tables` statements need to be aliased.\n\n    .. versionadded:: 1.1\n    \"\"\"\n\n    def __init__(self, label, metadata, mappings, fact, fact_key='id',\n                 joins=None, tables=None, schema=None):\n\n        # TODO: expectation is, that the snowlfake is already localized, the\n        # owner of the snowflake should generate one snowflake per locale.\n        # TODO: use `facts` instead of `fact`\n\n        if fact is None:\n            raise ArgumentError(\"Fact table or table name not specified \"\n                                \"for star/snowflake schema {}\"\n                                .format(label))\n\n        self.label = label\n        self.metadata = metadata\n        self.mappings = mappings or {}\n        self.joins = joins or []\n        self.schema = schema\n        self.table_expressions = tables or {}\n\n        # Cache\n        # -----\n        # Keys are logical column labels (keys from `mapping` attribute)\n        self._columns = {}\n        # Keys are tuples (schema, table)\n        self._tables = {}\n\n        self.logger = logging.getLogger(\"cubes.starschema\")\n\n        # TODO: perform JOIN discovery based on foreign keys\n\n        # Fact Table\n        # ----------\n\n        # Fact Initialization\n        if isinstance(fact, compat.string_type):\n            self.fact_name = fact\n            self.fact_table = self.physical_table(fact)\n        else:\n            # We expect fact to be a statement\n            try:\n                self.fact_name = fact.name\n            except AttributeError:\n                raise ArgumentError(\"Fact table statement requires alias()\")\n\n            self.fact_table = fact\n\n        self.fact_key = fact_key\n\n        # Try to get the fact key, if does not exist, then consider the first\n        # table column as the fact key.\n        try:\n            self.fact_key_column = self.fact_table.columns[self.fact_key]\n        except KeyError:\n            try:\n                self.fact_key_column = list(self.fact_table.columns)[0]\n            except Exception as e:\n                raise ModelError(\"Unable to get key column for fact \"\n                                 \"table '%s' in '%s'. Reason: %s\"\n                                 % (self.fact_name, label, str(e)))\n            else:\n                self.fact_key = self.fact_key_column.name\n\n        self.fact_key_column = self.fact_key_column.label(FACT_KEY_LABEL)\n\n        # Rest of the initialization\n        # --------------------------\n        self._collect_tables()\n\n    def _collect_tables(self):\n        \"\"\"Collect and prepare all important information about the tables in\n        the schema. The collected information is a list:\n\n        * `table` \u2013 SQLAlchemy table or selectable object\n        * `schema` \u2013 original schema name\n        * `name` \u2013 original table or selectable object\n        * `alias` \u2013 alias given to the table or statement\n        * `join` \u2013 join object that joins the table as a detail to the star\n\n        Input: schema, fact name, fact table, joins\n        Output: tables[table_key] = SonwflakeTable()\n        \"\"\"\n\n        # Collect the fact table as the root master table\n        #\n        fact_table = _TableRef(schema=self.schema,\n                               name=self.fact_name,\n                               alias=self.fact_name,\n                               key=(self.schema, self.fact_name),\n                               table=self.fact_table,\n                               join=None\n                              )\n\n        self._tables[fact_table.key] = fact_table\n\n        # Collect all the detail tables\n        # We don't need to collect the master tables as they are expected to\n        # be referenced as 'details'. The exception is the fact table that is\n        # provided explicitly for the snowflake schema.\n\n\n        # Collect details for duplicate verification. It sohuld not be\n        # possible to join one detail multiple times with the same name. Alias\n        # has to be used.\n        details = set()\n\n        for join in self.joins:\n            # just ask for the table\n\n            if not join.detail.table:\n                raise ModelError(\"No detail table specified for a join in \"\n                                 \"schema '{}'. Master of the join is '{}'\"\n                                 .format(self.label,\n                                         _format_key(self._master_key(join))))\n\n            table = self.physical_table(join.detail.table,\n                                        join.detail.schema)\n\n            if join.alias:\n                table = table.alias(join.alias)\n                alias = join.alias\n            else:\n                alias = join.detail.table\n\n            key = (join.detail.schema or self.schema, alias)\n\n            if key in details:\n                raise ModelError(\"Detail table '{}' joined twice in star\"\n                                 \" schema {}. Join alias is required.\"\n                                 .format(_format_key(key), self.label))\n            details.add(key)\n\n            ref = _TableRef(table=table,\n                            schema=join.detail.schema,\n                            name=join.detail.table,\n                            alias=alias,\n                            key=key,\n                            join=join\n                           )\n\n            self._tables[key] = ref\n\n    def table(self, key, role=None):\n        \"\"\"Return a table reference for `key` which has form of a\n        tuple (`schema`, `table`). `schema` should be ``None`` for named table\n        expressions, which take precedence before the physical tables in the\n        default schema. If there is no named table expression then physical\n        table is considered.\n\n        The returned object has the following properties:\n\n        * `name` \u2013 real table name\n        * `alias` \u2013 table alias \u2013 always contains a value, regardless whether\n          the table join provides one or not. If there was no alias provided by\n          the join, then the physical table name is used.\n        * `key` \u2013 table key \u2013 the same as the `key` argument\n        * `join` \u2013 `Join` object that joined the table to the star schema\n        * `table` \u2013 SQLAlchemy `Table` or table expression object\n\n        `role` is for debugging purposes to display when there is no such\n        table, which role of the table was expected, such as master or detail.\n        \"\"\"\n\n        if key is None:\n            raise ArgumentError(\"Table key should not be None\")\n\n        key = (key[0] or self.schema, key[1] or self.fact_name)\n\n        try:\n            return self._tables[key]\n        except KeyError:\n            if role:\n                for_role = \" (as {})\".format(role)\n            else:\n                for_role = \"\"\n\n            schema = '\"{}\".'.format(key[0]) if key[0] else \"\"\n\n            raise SchemaError(\"Unknown star table {}\\\"{}\\\"{}. Missing join?\"\n                              .format(schema, key[1], for_role))\n\n    def physical_table(self, name, schema=None):\n        \"\"\"Return a physical table or table expression, regardless whether it\n        exists or not in the star.\"\"\"\n\n        # Return a statement or an explicitly craeted table if it exists\n        if not schema and name in self.table_expressions:\n            return self.table_expressions[name]\n\n        coalesced_schema = schema or self.schema\n\n        try:\n            table = sa.Table(name,\n                             self.metadata,\n                             autoload=True,\n                             schema=coalesced_schema)\n\n        except sa.exc.NoSuchTableError:\n            in_schema = (\" in schema '{}'\"\n                         .format(schema)) if schema else \"\"\n            msg = \"No such fact table '{}'{}.\".format(name, in_schema)\n            raise NoSuchTableError(msg)\n\n        return table\n\n\n    def column(self, logical):\n        \"\"\"Return a column for `logical` reference. The returned column will\n        have a label same as the `logical`.\n        \"\"\"\n        # IMPORTANT\n        #\n        # Note to developers: any column returned from this method\n        # MUST be somehow represented in the logical model and MUST be\n        # accounted in the relevant_joins(). For example in custom expressions\n        # operating on multiple physical columns all physical\n        # columns must be defined at the higher level attributes objects in\n        # the cube. This is to access the very base column, that has physical\n        # representation in a table or a table-like statement.\n        #\n        # Yielding non-represented column might result in undefined behavior,\n        # very likely in unwanted cartesian join \u2013 one per unaccounted column.\n        #\n        # -- END OF IMPORTANT MESSAGE ---\n\n        if logical in self._columns:\n            return self._columns[logical]\n\n        try:\n            mapping = self.mappings[logical]\n        except KeyError:\n            if logical == FACT_KEY_LABEL:\n                return self.fact_key_column\n            else:\n                raise NoSuchAttributeError(logical)\n\n        key = (mapping.schema or self.schema, mapping.table or self.fact_name)\n\n        ref = self.table(key)\n        table = ref.table\n\n        try:\n            column = table.columns[mapping.column]\n        except KeyError:\n            avail = \", \".join(str(c) for c in table.columns)\n            raise SchemaError(\"Unknown column '%s' in table '%s' possible: %s\"\n                              % (mapping.column, mapping.table, avail))\n\n        # Extract part of the date\n        if mapping.extract:\n            column = sql.expression.extract(mapping.extract, column)\n        if mapping.function:\n            # FIXME: add some protection here for the function name!\n            column = getattr(sql.expression.func, mapping.function)(column)\n\n        column = column.label(logical)\n\n        self._columns[logical] = column\n\n        return column\n\n    def _master_key(self, join):\n        \"\"\"Generate join master key, use schema defaults\"\"\"\n        return (join.master.schema or self.schema,\n                join.master.table or self.fact_name)\n\n    def _detail_key(self, join):\n        \"\"\"Generate join detail key, use schema defaults\"\"\"\n        # Note: we don't include fact as detail table by default. Fact can not\n        # be detail (at least for now, we don't have a case where it could be)\n        return (join.detail.schema or self.schema,\n                join.alias or join.detail.table)\n\n    def required_tables(self, attributes):\n        \"\"\"Get all tables that are required to be joined to get `attributes`.\n        `attributes` is a list of `StarSchema` attributes (or objects with\n        same kind of attributes).\n        \"\"\"\n\n        # Attribute: (schema, table, column)\n        # Join: ((schema, table, column), (schema, table, column), alias)\n\n        if not self.joins:\n            self.logger.debug(\"no joins to be searched for\")\n\n        # Get the physical mappings for attributes\n        mappings = [self.mappings[attr] for attr in attributes]\n\n        # Generate table keys\n        relevant = set(self.table((m.schema, m.table)) for m in mappings)\n\n        # Dependencies\n        # ------------\n        # `required` now contains tables that contain requested `attributes`.\n        # Nowe we have to resolve all dependencies.\n\n        required = {}\n        while relevant:\n            table = relevant.pop()\n            required[table.key] = table\n\n            if not table.join:\n                continue\n\n            master = self._master_key(table.join)\n            if master not in required:\n                relevant.add(self.table(master))\n\n            detail = self._detail_key(table.join)\n            if detail not in required:\n                relevant.add(self.table(detail))\n\n        # Sort the tables\n        # ---------------\n\n        fact_key = (self.schema, self.fact_name)\n        fact = self.table(fact_key, \"fact master\")\n        masters = {fact_key: fact}\n\n        sorted_tables = [fact]\n\n        while required:\n            details = [table for table in required.values()\n                       if table.join\n                       and self._master_key(table.join) in masters]\n\n            if not details:\n                break\n\n            for detail in details:\n                masters[detail.key] = detail\n                sorted_tables.append(detail)\n\n                del required[detail.key]\n\n        if len(required) > 1:\n            keys = [_format_key(table.key)\n                    for table in required.values()\n                    if table.key != fact_key]\n\n            raise ModelError(\"Some tables are not joined: {}\"\n                             .format(\", \".join(keys)))\n\n        return sorted_tables\n\n    # Note: This is \"The Method\"\n    # ==========================\n\n```\n\n### Code description\n```Python\n\"\"\"\nDescription: This function generates a star schema from a given set of attributes.\n\nArgs:\n    attributes (list): A list of attributes used to generate the star schema.\n\nReturns:\n    sqlalchemy.sql.expression.join: A join object representing the star schema.\n\"\"\"\n```\n\n### Code snippet:\n```Python\ndef get_star(self, attributes):\n\n    attributes = [str(attr) for attr in attributes]\n    # Collect all the tables first:\n    tables = self.required_tables(attributes)\n\n    # Dictionary of raw tables and their joined products\n    # At the end this should contain only one item representing the whole\n    # star.\n    star_tables = {table_ref.key:table_ref.table for table_ref in tables}\n\n    # Here the `star` contains mapping table key -> table, which will be\n    # gradually replaced by JOINs\n\n    # Perform the joins\n    # =================\n    #\n    # 1. find the column\n    # 2. construct the condition\n    # 3. use the appropriate SQL JOIN\n    # 4. wrap the star with detail\n    # \n    # TODO: support MySQL partition (see Issue list)\n\n    # First table does not need to be joined. It is the \"fact\" (or other\n    # central table) of the schema.\n    star = tables[0].table\n\n    for table in tables[1:]:\n        if not table.join:\n            raise ModelError(\"Missing join for table '{}'\"\n                             .format(_format_key(table.key)))\n\n        join = table.join\n\n        # Get the physical table object (aliased) and already constructed\n        # key (properly aliased)\n        detail_table = table.table\n        detail_key = table.key\n\n        # The `table` here is a detail table to be joined. We need to get\n        # the master table this table joins to:\n\n        master = join.master\n        master_key = self._master_key(join)\n\n        # We need plain tables to get columns for prepare the join\n        # condition. We can't get it form `star`.\n        # Master table.column\n        # -------------------\n        master_table = self.table(master_key).table\n\n        try:\n            master_columns = _make_compound_key(master_table,\n                                                master.column)\n        except KeyError as e:\n            raise ModelError('Unable to find master key column \"{key}\" '\n                             'in table \"{table}\" for star {schema} '\n                             .format(schema=self.label,\n                                     key=e,\n```\n\n### Requirements:\n1. Complete only the missing part of the function.\n2. Only answer the code of the completed part. Do not include any of the provided code snippet.\n3. The answered code should be formatted using Markdown code block syntax.\n4. Do not include any additional text.\n\n### Answer: (use the provided format with backticks)\n", "name": "get_star", "reference": "                                     table=_format_key(master_key)))\n\n        # Detail table.column\n        # -------------------\n        try:\n            detail_columns = _make_compound_key(detail_table,\n                                                join.detail.column)\n        except KeyError as e:\n            raise ModelError('Unable to find detail key column \"{key}\" '\n                             'in table \"{table}\" for star {schema} '\n                             .format(schema=self.label,\n                                     key=e,\n                                     table=_format_key(detail_key)))\n\n        if len(master_columns) != len(detail_columns):\n            raise ModelError(\"Compound keys for master '{}' and detail \"\n                             \"'{}' table in star {} have different number\"\n                             \" of columns\"\n                             .format(_format_key(master_key),\n                                     _format_key(detail_key),\n                                     self.label))\n\n        # The JOIN ON condition\n        # ---------------------\n        key_conditions = [left == right\n                          for left, right\n                          in zip(master_columns, detail_columns)]\n        onclause = and_(*key_conditions)\n\n        # Determine the join type based on the join method. If the method\n        # is \"detail\" then we need to swap the order of the tables\n        # (products), because SQLAlchemy provides inteface only for\n        # left-outer join.\n        left, right = (star, detail_table)\n\n        if join.method is None or join.method == \"match\":\n            is_outer = False\n        elif join.method == \"master\":\n            is_outer = True\n        elif join.method == \"detail\":\n            # Swap the master and detail tables to perform RIGHT OUTER JOIN\n            left, right = (right, left)\n            is_outer = True\n        else:\n            raise ModelError(\"Unknown join method '%s'\" % join.method)\n\n        star = sql.expression.join(left, right,\n                                   onclause=onclause,\n                                   isouter=is_outer)\n\n        # Consume the detail\n        if detail_key not in star_tables:\n            raise ModelError(\"Detail table '{}' not in star. Missing join?\"\n                             .format(_format_key(detail_key)))\n\n        # The table is consumed by the join product, becomes the join\n        # product itself.\n        star_tables[detail_key] = star\n        star_tables[master_key] = star\n\n    return star", "completions": ["```python\n```python\n# TODO: complete the function\ndef get_star(self, attributes):\n    # TODO: complete the function\n    pass\n```\n"]}
{"id": 82, "prompt": "<\uff5cbegin\u2581of\u2581sentence\uff5c>### Role:\nYou are an experienced Python programmer assistant responsible for code completion tasks. Please intelligently complete the missing parts of the provided code snippet.\n\n### Task:\nBased on the provided code snippet, complete the missing function implementation. Ensure the code is logically correct, syntactically accurate, and follows best coding practices.\n\n### Left context\n```Python\n\"\"\"Logic for applying operators to states.\n\nTodo:\n* Sometimes the final result needs to be expanded, we should do this by hand.\n\"\"\"\n\nfrom __future__ import print_function, division\n\nfrom sympy import Add, Mul, Pow, sympify, S\nfrom sympy.core.compatibility import range\n\nfrom sympy.physics.quantum.anticommutator import AntiCommutator\nfrom sympy.physics.quantum.commutator import Commutator\nfrom sympy.physics.quantum.dagger import Dagger\nfrom sympy.physics.quantum.innerproduct import InnerProduct\nfrom sympy.physics.quantum.operator import OuterProduct, Operator\nfrom sympy.physics.quantum.state import State, KetBase, BraBase, Wavefunction\nfrom sympy.physics.quantum.tensorproduct import TensorProduct\n\n__all__ = [\n    'qapply'\n]\n\n\n#-----------------------------------------------------------------------------\n# Main code\n#-----------------------------------------------------------------------------\n\n```\n\n### Code description\n```Python\n\"\"\"\nDescription: This function applies quantum operators to quantum states and density matrices.\n\nArgs:\n    e (type): The quantum state or operator to be applied.\n    **options (type): Additional options for the function.\n\nReturns:\n    type: The result of applying the quantum operators to the quantum states or density matrices.\n\"\"\"\n```\n\n### Code snippet:\n```Python\ndef qapply(e, **options):\n    from sympy.physics.quantum.density import Density\n\n    dagger = options.get('dagger', False)\n\n    if e == 0:\n        return S.Zero\n\n    # This may be a bit aggressive but ensures that everything gets expanded\n    # to its simplest form before trying to apply operators. This includes\n    # things like (A+B+C)*|a> and A*(|a>+|b>) and all Commutators and\n    # TensorProducts. The only problem with this is that if we can't apply\n    # all the Operators, we have just expanded everything.\n    # TODO: don't expand the scalars in front of each Mul.\n    e = e.expand(commutator=True, tensorproduct=True)\n\n    # If we just have a raw ket, return it.\n    if isinstance(e, KetBase):\n        return e\n\n    # We have an Add(a, b, c, ...) and compute\n    # Add(qapply(a), qapply(b), ...)\n    elif isinstance(e, Add):\n        result = 0\n        for arg in e.args:\n            result += qapply(arg, **options)\n        return result\n```\n\n### Requirements:\n1. Complete only the missing part of the function.\n2. Only answer the code of the completed part. Do not include any of the provided code snippet.\n3. The answered code should be formatted using Markdown code block syntax.\n4. Do not include any additional text.\n\n### Answer: (use the provided format with backticks)\n", "name": "qapply", "reference": "\n    # For a Density operator call qapply on its state\n    elif isinstance(e, Density):\n        new_args = [(qapply(state, **options), prob) for (state,\n                     prob) in e.args]\n        return Density(*new_args)\n\n    # For a raw TensorProduct, call qapply on its args.\n    elif isinstance(e, TensorProduct):\n        return TensorProduct(*[qapply(t, **options) for t in e.args])\n\n    # For a Pow, call qapply on its base.\n    elif isinstance(e, Pow):\n        return qapply(e.base, **options)**e.exp\n\n    # We have a Mul where there might be actual operators to apply to kets.\n    elif isinstance(e, Mul):\n        result = qapply_Mul(e, **options)\n        if result == e and dagger:\n            return Dagger(qapply_Mul(Dagger(e), **options))\n        else:\n            return result\n\n    # In all other cases (State, Operator, Pow, Commutator, InnerProduct,\n    # OuterProduct) we won't ever have operators to apply to kets.\n    else:\n        return e", "completions": ["```python\n```python\ndef qapply(e, **options):\n    from sympy.physics.quantum.density import Density\n\n    dagger = options.get('dagger', False)\n\n    if e == 0:\n        return S.Zero\n\n    # This may be a bit aggressive but ensures that everything gets expanded\n    # to its simplest form before trying to apply operators. This includes\n    # things like (A+B+C)*|a> and A*(|a>+|b>) and all Commutators and\n    # TensorProducts. The only problem with this is that if we can't apply\n    # all the Operators, we have just expanded everything.\n    # TODO: don't expand the scalars in front of each Mul.\n    e = e.expand(commutator=True, tensorproduct=True)\n\n    # If we just have a raw ket, return it.\n    if isinstance(e, KetBase):\n        return e\n\n    # We have an Add(a, b, c, ...) and compute\n    # Add(qapply(a), qapply(b), ...)\n    elif isinstance(e, Add):\n        result = 0\n        for arg in e.args:\n            result += qapply(arg, **options)\n        return result\n```\n```\n"]}
{"id": 83, "prompt": "<\uff5cbegin\u2581of\u2581sentence\uff5c>### Role:\nYou are an experienced Python programmer assistant responsible for code completion tasks. Please intelligently complete the missing parts of the provided code snippet.\n\n### Task:\nBased on the provided code snippet, complete the missing function implementation. Ensure the code is logically correct, syntactically accurate, and follows best coding practices.\n\n### Left context\n```Python\n\"\"\"Methods for creating/sending notification digests.\"\"\"\nimport collections\nimport logging\n\nfrom django.contrib.sites.models import Site\nfrom django.core.mail import EmailMultiAlternatives, get_connection\nfrom django.template.loader import render_to_string\nfrom django.utils import timezone\nfrom django.utils.translation import gettext_lazy as _\n\nfrom .models import Board, DigestFrequency, DigestStatus, UserSettings\n\nlogger = logging.getLogger(__name__)  # pylint: disable=invalid-name\n\n\ndef notification_digest(user, start, end):\n    \"\"\"Return digest for user with content/notifications occurring after start.\n\n    Notifications are grouped by target, e.g., board.\n\n    For boards that a user no longer has access to, notifications are included if the user had access to that board\n    at the time of the notification.\n\n    :param user: the user\n    :param start: the start datetime for the the digest\n    :param end: the end datetime for the digest\n    \"\"\"\n    notifications = user.notifications.unread().filter(\n        timestamp__gt=start, timestamp__lt=end\n    )\n    by_target = collections.defaultdict(list)\n    for notification in notifications:\n        if notification.target and notification.actor.id != user.id:\n            by_target[notification.target].append(notification)\n    new_boards = (\n        Board.objects.user_readable(user)\n        .filter(pub_date__gt=start, pub_date__lt=end)\n        .exclude(creator_id=user.id)\n    )\n    if notifications.exists() or new_boards.exists():\n        return {\n            \"new_boards\": new_boards,\n            # https://code.djangoproject.com/ticket/16335\n            \"notifications\": dict(by_target),\n        }\n    else:\n        return None\n\n\ndef user_digest_start(user, digest_frequency, as_of):\n    \"\"\"Return the starting datetime for a digest for user.\n\n    :param user: the user to create the digest for\n    :param digest_frequency: the DigestFrequency for the digest\n    :param as_of: the datetime to generate the digest for\n    \"\"\"\n    # NOTE: this approach is inefficient for multiple users because we have to hit the DB for each one\n    if digest_frequency == DigestFrequency.never:\n        # don't need to internationalize; user won't see this error\n        raise ValueError('Digest frequency cannot be \"never\"')\n\n    digest = as_of - digest_frequency.delta\n    join = user.date_joined\n    status = DigestStatus.objects.filter(user=user).first()\n    previous = status.last_success if (status and status.last_success) else join\n    return max([digest, join, previous])\n\n\n```\n\n### Code description\n```Python\n\"\"\"\nDescription: This function generates a digest email for a given user based on their digest frequency and the date of the digest.\n\nArgs:\n    user (User): The user for whom the digest email is being generated.\n    digest_frequency (DigestFrequency): The frequency of the digest.\n    as_of (datetime): The date of the digest.\n\nReturns:\n    EmailMultiAlternatives: The generated digest email. If no context is available, None is returned.\n\"\"\"\n```\n\n### Code snippet:\n```Python\ndef create_digest_email(user, digest_frequency, as_of):\n    start = user_digest_start(user, digest_frequency, as_of)\n    context = notification_digest(user, start, as_of)\n\n    logger.debug(\"Digest as of %s: %s\", start, context)\n\n    if context:\n        context[\"timestamp\"] = as_of\n        context[\"site\"] = Site.objects.get_current()\n        context[\"digest_frequency\"] = digest_frequency.name\n\n        subject = render_to_string(\n            \"boards/email/email_digest_subject.txt\", context=context\n        )\n```\n\n### Requirements:\n1. Complete only the missing part of the function.\n2. Only answer the code of the completed part. Do not include any of the provided code snippet.\n3. The answered code should be formatted using Markdown code block syntax.\n4. Do not include any additional text.\n\n### Answer: (use the provided format with backticks)\n", "name": "create_digest_email", "reference": "        # remove superfluous line breaks\n        subject = \" \".join(subject.splitlines()).strip()\n\n        text_body = render_to_string(\n            \"boards/email/email_digest_message.txt\", context=context\n        )\n        html_body = render_to_string(\n            \"boards/email/email_digest_message.html\", context=context\n        )\n\n        email = EmailMultiAlternatives(subject=subject, body=text_body, to=[user.email])\n        email.attach_alternative(html_body, \"text/html\")\n        return email\n    else:\n        return None", "completions": ["```python\ndef create_digest_email(user, digest_frequency, as_of):\n    start = user_digest_start(user, digest_frequency, as_of)\n    context = notification_digest(user, start, as_of)\n\n    logger.debug(\"Digest as of %s: %s\", start, context)\n\n    if context:\n        context[\"timestamp\"] = as_of\n        context[\"site\"] = Site.objects.get_current()\n        context[\"digest_frequency\"] = digest_frequency.name\n\n        subject = render_to_string(\n            \"boards/email/email_digest_subject.txt\", context=context\n        )\n\n        body = render_to_string(\n            \"boards/email/email_digest_body.txt\", context=context\n        )\n\n        email = EmailMultiAlternatives(\n            subject,\n            body,\n            to=[user.email],\n        )\n\n        return email\n```\n"]}
