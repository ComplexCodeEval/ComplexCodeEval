{"id": 0, "prompt": "def calculate_psnr_pt(img, img2, crop_border, test_y_channel=False, **kwargs):\n\n    assert img.shape == img2.shape, (f'Image shapes are different: {img.shape}, {img2.shape}.')\n\n    if crop_border != 0:\n        img = img[:, :, crop_border:-crop_border, crop_border:-crop_border]\n        img2 = img2[:, :, crop_border:-crop_border, crop_border:-crop_border]\n\n    if test_y_channel:\n        img = rgb2ycbcr_pt(img, y_only=True)\n        img2 = rgb2ycbcr_pt(img2, y_only=True)\n\n    img = img.to(torch.float64)\n    img2 = img2.to(torch.float64)\n\n    mse = torch.", "completion_part": "mean((img - img2)**2, dim=[1, 2, 3])", "after_completion": "\n    return 10. * torch.log10(1. / (mse + 1e-8))", "namespace": "torch.mean", "completions": ["mean((img - img2) ** 2"]}
{"id": 1, "prompt": "def spherical_bessel_smooth(r: Tensor, cutoff: float = 5.0, max_n: int = 10) -> Tensor:\n    n = torch.arange(max_n).type(dtype=matgl.float_th)[None, :]\n    r = r[:, None]\n    fnr = (\n        (-1) ** n\n        * ", "completion_part": "sqrt(2.0)", "after_completion": "\n        * pi\n        / cutoff**1.5\n        * (n + 1)\n        * (n + 2)\n        / torch.sqrt(2 * n**2 + 6 * n + 5)\n        * (_sinc(r * (n + 1) * pi / cutoff) + _sinc(r * (n + 2) * pi / cutoff))\n    )\n    en = n**2 * (n + 2) ** 2 / (4 * (n + 1) ** 4 + 1)\n    dn = [torch.tensor(1.0)]\n    for i in range(1, max_n):\n        dn_value = 1 - en[0, i] / dn[-1]\n        dn.append(dn_value)\n    dn = torch.stack(dn)  # type: ignore\n    gn = [fnr[:, 0]]\n    for i in range(1, max_n):\n        gn_value = 1 / torch.sqrt(dn[i]) * (fnr[:, i] + torch.sqrt(en[0, i] / dn[i - 1]) * gn[-1])\n        gn.append(gn_value)\n\n    return torch.t(torch.stack(gn))", "namespace": "torch.sqrt", "completions": ["2 ** (n - 1)\n        *"]}
{"id": 2, "prompt": "\ndef edge_index_to_vector(\n    edge_index: Tensor,\n    size: Tuple[int, int],\n    bipartite: bool,\n    force_undirected: bool = False,\n) -> Tuple[Tensor, int]:\n\n    row, col = edge_index\n\n    if bipartite:  # No need to account for self-loops.\n        idx = (row * size[1]).add_(col)\n        population = size[0] * size[1]\n        return idx, population\n\n    elif force_undirected:\n        assert size[0] == size[1]\n        num_nodes = size[0]\n\n        # We only operate on the upper triangular matrix:\n        mask = row < col\n        row, col = row[mask], col[mask]\n        offset = torch.", "completion_part": "arange(1, num_nodes, device=row.device)", "after_completion": ".cumsum(0)[row]\n        idx = row.mul_(num_nodes).add_(col).sub_(offset)\n        population = (num_nodes * (num_nodes + 1)) // 2 - num_nodes\n        return idx, population\n\n    else:\n        assert size[0] == size[1]\n        num_nodes = size[0]\n\n        # We remove self-loops as we do not want to take them into account\n        # when sampling negative values.\n        mask = row != col\n        row, col = row[mask], col[mask]\n        col[row < col] -= 1\n        idx = row.mul_(num_nodes - 1).add_(col)\n        population = num_nodes * num_nodes - num_nodes\n        return idx, population", "namespace": "torch.arange", "completions": ["arange(num_nodes, dtype=torch."]}
{"id": 3, "prompt": "    def set_xdata(self, x):\n        if not np.iterable(x):\n            # When deprecation cycle is completed\n            # raise RuntimeError('x must be a sequence')\n            _api.", "completion_part": "warn_deprecated(\n                since=3.7,\n                message=\"Setting data with a non sequence type \"\n                \"is deprecated since %(since)s and will be \"\n                \"remove %(removal)s\")", "after_completion": "\n            x = [x, ]\n        self._xorig = copy.copy(x)\n        self._invalidx = True\n        self.stale = True", "namespace": "matplotlib._api.warn_deprecated", "completions": ["warn_deprecated(\n                \"2.2\","]}
{"id": 4, "prompt": "def loss(logprobs, values, old_logprobs, old_values, advantages, returns, mask, logits, config=None):\n    clip_ratio = config.clip_ratio\n    cliprange_value = config.cliprange_value\n    cliprange = config.cliprange\n    vf_coef = config.vf_coef\n    ent_coef = config.ent_coef\n    values_clipped = torch.clamp(\n        values,\n        old_values - cliprange_value,\n        old_values + cliprange_value,\n    )\n    n = mask.sum()\n\n    vf_loss1 = (values - returns) ** 2\n    vf_loss2 = (values_clipped - returns) ** 2\n    vf_loss = 0.5 * torch.sum(torch.max(vf_loss1, vf_loss2) * mask) / n\n    vf_clipfrac = torch.sum((vf_loss2 > vf_loss1).float() * mask) / n\n\n    log_ratio = (logprobs - old_logprobs) * mask\n    ratio = torch.exp(log_ratio)\n    # Unbiased KL-div estimates (`k3`). Ref: http://joschu.net/blog/kl-approx.html\n    if clip_ratio:\n        ratio = torch.clamp(ratio, 1.0 - cliprange, 1.0 + cliprange)\n\n    with torch.no_grad():\n        approx_kl = torch.", "completion_part": "mean((ratio - 1) - log_ratio)", "after_completion": "  # noqa: F841\n\n    pg_loss1 = -advantages * ratio\n    pg_loss2 = -advantages * torch.clamp(\n        ratio,\n        1.0 - cliprange,\n        1.0 + cliprange,\n    )\n    pg_loss = torch.sum(torch.max(pg_loss1, pg_loss2) * mask) / n\n    pg_clipfrac = torch.sum((pg_loss2 > pg_loss1).float() * mask) / n\n\n    # compared with trlx loss function, entroy loss is added\n    dist = dist_fn(logits)\n    ent_loss = torch.sum(dist.entropy() * mask) / n\n\n    loss = pg_loss + vf_coef * vf_loss - ent_coef * ent_loss\n\n    stats = dict(\n        losses=dict(\n            total_loss=loss.item(),\n            policy_loss=pg_loss.item(),\n            value_loss=vf_loss.item(),\n            entropy_loss=ent_loss.item(),\n        ),\n        values=dict(\n            get_tensor_stats(values, mask, n),\n            values_error=(torch.sum(((values - returns) * mask) ** 2) / n).item(),\n            clipfrac=vf_clipfrac.item(),\n        ),\n        old_values=get_tensor_stats(old_values, mask, n),\n        returns=get_tensor_stats(returns, mask, n),\n        policy=dict(approx_kl=approx_kl.item(), clipfrac=pg_clipfrac.item()),\n        ratio=((ratio * mask).sum().item() / n).item(),\n        padding_percentage=(n / mask.numel()).item(),\n    )\n    return loss, stats", "namespace": "torch.mean", "completions": ["mean(log_ratio - logprobs)\n       "]}
{"id": 5, "prompt": "def remap_bert_state_dict(\n    state_dict,\n    config: PretrainedConfig,\n    remove_bert=False,\n    remove_cls_weights=False,\n    add_pooling_layer=False,\n):\n\n    def add_bert_prefix(key):\n        # prepend bert. to the key\n        if key.startswith(\"bert.\") or key.startswith(\"cls.\"):\n            return key\n        return f\"bert.{key}\"\n\n    state_dict = OrderedDict((add_bert_prefix(k), v) for k, v in state_dict.items())\n\n    # LayerNorm\n    def key_mapping_ln_gamma_beta(key):\n        key = re.sub(r\"LayerNorm.gamma$\", \"LayerNorm.weight\", key)\n        key = re.sub(r\"LayerNorm.beta$\", \"LayerNorm.bias\", key)\n        return key\n\n    state_dict = OrderedDict((key_mapping_ln_gamma_beta(k), v) for k, v in state_dict.items())\n\n    # Layers\n    def key_mapping_layers(key):\n        return re.sub(r\"^bert.encoder.layer\\.\", \"bert.encoder.layers.\", key)\n\n    state_dict = OrderedDict((key_mapping_layers(k), v) for k, v in state_dict.items())\n\n    # LayerNorm\n    def key_mapping_ln(key):\n        key = re.sub(r\"^bert.embeddings.LayerNorm.\", \"bert.emb_ln.\", key)\n        key = re.sub(\n            r\"^bert.encoder.layers.(\\d+).attention.output.LayerNorm.(weight|bias)\",\n            r\"bert.encoder.layers.\\1.norm1.\\2\",\n            key,\n        )\n        key = re.sub(\n            r\"^bert.encoder.layers.(\\d+).output.LayerNorm.(weight|bias)\",\n            r\"bert.encoder.layers.\\1.norm2.\\2\",\n            key,\n        )\n        key = re.sub(\n            r\"^cls.predictions.transform.LayerNorm.(weight|bias)\",\n            r\"cls.predictions.transform.layer_norm.\\1\",\n            key,\n        )\n        return key\n\n    state_dict = OrderedDict((key_mapping_ln(k), v) for k, v in state_dict.items())\n\n    # MLP\n    def key_mapping_mlp(key):\n        key = re.sub(\n            r\"^bert.encoder.layers.(\\d+).intermediate.dense.(weight|bias)\",\n            r\"bert.encoder.layers.\\1.mlp.fc1.\\2\",\n            key,\n        )\n        key = re.sub(\n            r\"^bert.encoder.layers.(\\d+).output.dense.(weight|bias)\",\n            r\"bert.encoder.layers.\\1.mlp.fc2.\\2\",\n            key,\n        )\n        return key\n\n    state_dict = OrderedDict((key_mapping_mlp(k), v) for k, v in state_dict.items())\n\n    # Attention\n    last_layer_subset = getattr(config, \"last_layer_subset\", False)\n    for d in range(config.num_hidden_layers):\n        if f\"bert.encoder.layers.{d}.attention.self.query.weight\" not in state_dict:\n            continue\n        Wq = state_dict.pop(f\"bert.encoder.layers.{d}.attention.self.query.weight\")\n        Wk = state_dict.pop(f\"bert.encoder.layers.{d}.attention.self.key.weight\")\n        Wv = state_dict.pop(f\"bert.encoder.layers.{d}.attention.self.value.weight\")\n        bq = state_dict.pop(f\"bert.encoder.layers.{d}.attention.self.query.bias\")\n        bk = state_dict.pop(f\"bert.encoder.layers.{d}.attention.self.key.bias\")\n        bv = state_dict.pop(f\"bert.encoder.layers.{d}.attention.self.value.bias\")\n        if not (last_layer_subset and d == config.num_hidden_layers - 1):\n            state_dict[f\"bert.encoder.layers.{d}.attn.Wqkv.weight\"] = torch.cat([Wq, Wk, Wv], dim=0)\n            state_dict[f\"bert.encoder.layers.{d}.attn.Wqkv.bias\"] = torch.cat([bq, bk, bv], dim=0)\n        else:\n            state_dict[f\"bert.encoder.layers.{d}.attn.Wq.weight\"] = Wq\n            state_dict[f\"bert.encoder.layers.{d}.attn.Wkv.weight\"] = torch.cat([Wk, Wv], dim=0)\n            state_dict[f\"bert.encoder.layers.{d}.attn.Wq.bias\"] = bq\n            state_dict[f\"bert.encoder.layers.{d}.attn.Wkv.bias\"] = torch.cat([bk, bv], dim=0)\n\n    def key_mapping_attn(key):\n        return re.sub(\n            r\"^bert.encoder.layers.(\\d+).attention.output.dense.(weight|bias)\",\n            r\"bert.encoder.layers.\\1.attn.out_proj.\\2\",\n            key,\n        )\n\n    state_dict = OrderedDict((key_mapping_attn(k), v) for k, v in state_dict.items())\n\n    def key_mapping_decoder_bias(key):\n        return re.sub(r\"^cls.predictions.bias\", \"cls.predictions.decoder.bias\", key)\n\n    # remove nsp weights, we don't use\n    state_dict.pop(\"cls.seq_relationship.weight\", None)\n    state_dict.pop(\"cls.seq_relationship.bias\", None)\n    state_dict.pop(\"bert.embeddings.position_ids\", None)\n\n    state_dict = OrderedDict((key_mapping_decoder_bias(k), v) for k, v in state_dict.items())\n\n    if remove_cls_weights:\n        cls_weights = [\n            \"cls.predictions.decoder.bias\",\n            \"cls.predictions.transform.dense.weight\",\n            \"cls.predictions.transform.dense.bias\",\n            \"cls.predictions.transform.layer_norm.weight\",\n            \"cls.predictions.transform.layer_norm.bias\",\n            \"cls.predictions.decoder.weight\",\n        ]\n        for weight in cls_weights:\n            state_dict.pop(weight, None)\n\n    # Word embedding\n    pad_vocab_size_multiple = getattr(config, \"pad_vocab_size_multiple\", 1)\n    if pad_vocab_size_multiple > 1:\n        word_embeddings = state_dict[\"bert.embeddings.word_embeddings.weight\"]\n        state_dict[\"bert.embeddings.word_embeddings.weight\"] = F.", "completion_part": "pad(\n            word_embeddings, (0, 0, 0, config.vocab_size - word_embeddings.shape[0])\n        )", "after_completion": "\n        if not remove_cls_weights:\n            decoder_weight = state_dict[\"cls.predictions.decoder.weight\"]\n            state_dict[\"cls.predictions.decoder.weight\"] = F.pad(\n                decoder_weight, (0, 0, 0, config.vocab_size - decoder_weight.shape[0])\n            )\n            # If the vocab was padded, we want to set the decoder bias for those padded indices to be\n            # strongly negative (i.e. the decoder shouldn't predict those indices).\n            # TD [2022-05-09]: I don't think it affects the MLPerf training.\n            if \"cls.predictions.decoder.bias\" in state_dict:\n                decoder_bias = state_dict[\"cls.predictions.decoder.bias\"]\n                state_dict[\"cls.predictions.decoder.bias\"] = F.pad(\n                    decoder_bias, (0, config.vocab_size - decoder_bias.shape[0]), value=-100.0\n                )\n\n    if add_pooling_layer is False:\n        pooler_weights = [\n            \"bert.pooler.dense.weight\",\n            \"bert.pooler.dense.bias\",\n        ]\n        for key in pooler_weights:\n            state_dict.pop(key, None)\n\n    if remove_bert:\n\n        def remove_bert_prefix(key):\n            key = re.sub(r\"^bert.\", \"\", key)\n            return key\n\n        state_dict = OrderedDict((remove_bert_prefix(k), v) for k, v in state_dict.items())\n\n    return state_dict", "namespace": "torch.nn.functional.pad", "completions": ["pad(\n            word_embeddings,\n            (0"]}
{"id": 6, "prompt": "\n    def init_weights(self):\n        if isinstance(self.init_cfg, dict) and \\\n                self.init_cfg.get('type') in ['Pretrained', 'Pretrained_Part']:\n            checkpoint = CheckpointLoader.load_checkpoint(\n                self.init_cfg['checkpoint'], logger=None, map_location='cpu')\n\n            if self.init_cfg.get('type') == 'Pretrained':\n                if 'state_dict' in checkpoint:\n                    state_dict = checkpoint['state_dict']\n                else:\n                    state_dict = checkpoint\n\n            elif self.init_cfg.get('type') == 'Pretrained_Part':\n                state_dict = checkpoint.copy()\n                para_prefix = 'image_encoder'\n                prefix_len = len(para_prefix) + 1\n                for k, v in checkpoint.items():\n                    state_dict.pop(k)\n                    if para_prefix in k:\n                        state_dict[k[prefix_len:]] = v\n\n            if 'pos_embed' in state_dict.keys():\n                if self.pos_embed.shape != state_dict['pos_embed'].shape:\n                    print_log(msg=f'Resize the pos_embed shape from '\n                              f'{state_dict[\"pos_embed\"].shape} to '\n                              f'{self.pos_embed.shape}')\n                    h, w = self.img_size\n                    pos_size = int(\n                        math.sqrt(state_dict['pos_embed'].shape[1] - 1))\n                    state_dict['pos_embed'] = self.resize_pos_embed(\n                        state_dict['pos_embed'],\n                        (h // self.patch_size, w // self.patch_size),\n                        (pos_size, pos_size), self.interpolate_mode)\n\n            load_state_dict(self, state_dict, strict=False, logger=None)\n        elif self.init_cfg is not None:\n            super().init_weights()\n        else:\n            # We only implement the 'jax_impl' initialization implemented at\n            # https://github.com/rwightman/pytorch-image-models/blob/master/timm/models/vision_transformer.py#L353  # noqa: E501\n            trunc_normal_(self.pos_embed, std=.02)\n            trunc_normal_(self.cls_token, std=.02)\n            for n, m in self.named_modules():\n                if isinstance(m, nn.Linear):\n                    trunc_normal_(m.weight, std=.02)\n                    if m.bias is not None:\n                        if 'ffn' in n:\n                            nn.init.", "completion_part": "normal_(m.bias, mean=0., std=1e-6)", "after_completion": "\n                        else:\n                            nn.init.constant_(m.bias, 0)\n                elif isinstance(m, nn.Conv2d):\n                    kaiming_init(m, mode='fan_in', bias=0.)\n                elif isinstance(m, (_BatchNorm, nn.GroupNorm, nn.LayerNorm)):\n                    constant_init(m, val=1.0, bias=0.)", "namespace": "torch.nn.init.normal_", "completions": ["constant_(m.bias, 0)\n                       "]}
{"id": 7, "prompt": "    def _do_iteration(\n        self,\n        data: dict[str, Any],\n        loss_fns: Optional[dict[str, callable]] = None,\n        regularizer_fns: Optional[dict[str, callable]] = None,\n    ) -> DoIterationOutput:\n\n        # loss_fns can be None, e.g. during validation\n        if loss_fns is None:\n            loss_fns = {}\n\n        data = dict_to_device(data, self.device)\n\n        output_image: TensorOrNone\n        output_kspace: TensorOrNone\n\n        with ", "completion_part": "autocast(enabled=self.mixed_precision)", "after_completion": ":\n            output_images, output_kspace = self.forward_function(data)\n            output_images = [T.modulus_if_complex(_, complex_axis=self._complex_dim) for _ in output_images]\n            loss_dict = {k: torch.tensor([0.0], dtype=data[\"target\"].dtype).to(self.device) for k in loss_fns.keys()}\n\n            auxiliary_loss_weights = torch.logspace(-1, 0, steps=len(output_images)).to(output_images[0])\n            for i, output_image in enumerate(output_images):\n                loss_dict = self.compute_loss_on_data(\n                    loss_dict, loss_fns, data, output_image, None, auxiliary_loss_weights[i]\n                )\n            # Compute loss on k-space\n            loss_dict = self.compute_loss_on_data(loss_dict, loss_fns, data, None, output_kspace)\n\n            loss = sum(loss_dict.values())  # type: ignore\n\n        if self.model.training:\n            self._scaler.scale(loss).backward()\n\n        loss_dict = detach_dict(loss_dict)  # Detach dict, only used for logging.\n\n        output_image = output_images[-1]\n        return DoIterationOutput(\n            output_image=output_image,\n            sensitivity_map=data[\"sensitivity_map\"],\n            data_dict={**loss_dict},\n        )", "namespace": "torch.cuda.amp.autocast", "completions": [" torch.set_grad_enabled(self."]}
{"id": 8, "prompt": "def precision_score(\n        y_true,\n        y_pred,\n        sample_weight=None):\n    actual_category = pd.", "completion_part": "unique(y_true)", "after_completion": "\n    pred_category = pd.unique(y_pred)\n    labels = pd.unique(np.concatenate([actual_category, pred_category]))\n    precisions_array = metrics.precision_score(\n        y_true=y_true,\n        y_pred=y_pred,\n        average=None,\n        labels=labels,\n        sample_weight=sample_weight,\n        zero_division=0\n    )\n    precisions = {}\n    for label, precision in zip(labels, precisions_array):\n        precisions[label] = precision\n    return precisions", "namespace": "pandas.unique", "completions": ["Series(y_true)\n    predicted_category"]}
{"id": 9, "prompt": "def interpolation(\n    query_points: Tensor,\n    context_grid: Tensor,\n    grid: List[Tuple[float, float, int]],\n    interpolation_type: str = \"smooth_step_2\",\n    mem_speed_trade: bool = True,\n) -> Tensor:\n\n    # set stride TODO this will be replaced with InterpolationType later\n    if interpolation_type == \"nearest_neighbor\":\n        stride = 1\n    elif interpolation_type == \"linear\":\n        stride = 2\n    elif interpolation_type == \"smooth_step_1\":\n        stride = 2\n    elif interpolation_type == \"smooth_step_2\":\n        stride = 2\n    elif interpolation_type == \"gaussian\":\n        stride = 5\n    else:\n        raise RuntimeError(f\"Interpolation type {interpolation_type} not supported\")\n\n    # set device\n    device = query_points.device\n\n    # useful values\n    dims = len(grid)\n    nr_channels = context_grid.size(0)\n    dx = [((x[1] - x[0]) / (x[2] - 1)) for x in grid]\n\n    # generate mesh grid of position information [grid_dim_1, grid_dim_2, ..., 2-3]\n    # NOTE the mesh grid is padded by stride//2\n    k = stride // 2\n    linspace = [\n        torch.linspace(x[0] - k * dx_i, x[1] + k * dx_i, x[2] + 2 * k)\n        for x, dx_i in zip(grid, dx)\n    ]\n    meshgrid = torch.meshgrid(linspace)\n    meshgrid = torch.stack(meshgrid, dim=-1).to(device)\n\n    # pad context grid by k to avoid cuts on corners\n    padding = dims * (k, k)\n    context_grid = F.", "completion_part": "pad(context_grid, padding)", "after_completion": "\n\n    # reshape query points, context grid and mesh grid for easier indexing\n    # [1, grid_dim_1*grid_dim_2*..., 2-4]\n    nr_grid_points = int(torch.tensor([x[2] + 2 * k for x in grid]).prod())\n    meshgrid = meshgrid.view(1, nr_grid_points, dims)\n    context_grid = torch.reshape(context_grid, [1, nr_channels, nr_grid_points])\n    context_grid = torch.swapaxes(context_grid, 1, 2)\n    query_points = query_points.unsqueeze(0)\n\n    # compute index of nearest neighbor on grid to query points\n    idx = _grid_knn_idx(query_points, grid, stride, padding=True)\n\n    # index mesh grid to get distance vector\n    if mem_speed_trade:\n        mesh_grid_idx = index_values_low_mem(meshgrid, idx)\n    else:\n        mesh_grid_idx = index_values_high_mem(meshgrid, idx)\n    dist_vec = query_points.unsqueeze(2) - mesh_grid_idx\n\n    # make tf dx vec (for interpolation function)\n    dx = torch.tensor(dx, dtype=torch.float32)\n    dx = torch.reshape(dx, [1, 1, 1, dims]).to(device)\n\n    # compute bump function\n    if interpolation_type == \"nearest_neighbor\":\n        weights = nearest_neighbor_weighting(dist_vec, dx)\n    elif interpolation_type == \"linear\":\n        weights = linear_weighting(dist_vec, dx)\n    elif interpolation_type == \"smooth_step_1\":\n        weights = smooth_step_1_weighting(dist_vec, dx)\n    elif interpolation_type == \"smooth_step_2\":\n        weights = smooth_step_2_weighting(dist_vec, dx)\n    elif interpolation_type == \"gaussian\":\n        weights = gaussian_weighting(dist_vec, dx)\n    else:\n        raise RuntimeError\n\n    # index context grid with index\n    if mem_speed_trade:\n        context_grid_idx = index_values_low_mem(context_grid, idx)\n    else:\n        context_grid_idx = index_values_high_mem(context_grid, idx)\n\n    # interpolate points\n    product = weights * context_grid_idx\n    interpolated_points = product.sum(dim=2)\n\n    return interpolated_points[0]", "namespace": "torch.nn.functional.pad", "completions": ["pad(context_grid, padding, mode=\""]}
{"id": 10, "prompt": "def generate_random_split(length: int, seed: int, train_ratio: float = 0.8,\n                          val_ratio: float = 0.1,\n                          include_test: bool = True) -> np.ndarray:\n    assert train_ratio > 0\n    assert val_ratio > 0\n\n    if include_test:\n        assert train_ratio + val_ratio < 1\n        train_num = int(length * train_ratio)\n        val_num = int(length * val_ratio)\n        test_num = length - train_num - val_num\n        arr = np.concatenate([\n            np.full(train_num, SPLIT_TO_NUM['train']),\n            np.full(val_num, SPLIT_TO_NUM['val']),\n            np.full(test_num, SPLIT_TO_NUM['test'])\n        ])\n    else:\n        assert train_ratio + val_ratio == 1\n        train_num = int(length * train_ratio)\n        val_num = length - train_num\n        arr = np.concatenate([\n            np.full(train_num, SPLIT_TO_NUM['train']),\n            np.full(val_num, SPLIT_TO_NUM['val']),\n        ])\n    np.random.", "completion_part": "seed(seed)", "after_completion": "\n    np.random.shuffle(arr)\n\n    return arr", "namespace": "numpy.random.seed", "completions": ["seed(seed)\n    np.random.shuffle"]}
{"id": 11, "prompt": "\ndef parse_form_metadata(data: MultiDict) -> Metadata:\n    # We construct a RawMetdata using the form data, which we will later pass\n    # to Metadata to get a validated metadata.\n    #\n    # NOTE: Form data is very similiar to the email format where the only difference\n    #       between a list and a single value is whether or not the same key is used\n    #       multiple times. Thus we will handle things in a similiar way, always\n    #       fetching things as a list and then determining what to do based on the\n    #       field type and how many values we found.\n    #\n    #       In general, large parts of this have been taken directly from\n    #       packaging.metadata and adjusted to work with form data.\n    raw: dict[str, str | list[str] | dict[str, str]] = {}\n    unparsed: dict[str, list[str]] = {}\n\n    for name in frozenset(data.keys()):\n        # We have to be lenient in the face of \"extra\" data, because the data\n        # value here might contain unrelated form data, so we'll skip thing for\n        # fields that aren't in our list of values.\n        raw_name = _FORM_TO_RAW_MAPPING.get(name)\n        if raw_name is None:\n            continue\n\n        # We use getall() here, even for fields that aren't multiple use,\n        # because otherwise someone could have e.g. two Name fields, and we\n        # would just silently ignore it rather than doing something about it.\n        value = data.getall(name) or []\n\n        # An empty string is invalid for all fields, treat it as if it wasn't\n        # provided in the first place\n        if value == [\"\"]:\n            continue\n\n        # If this is one of our string fields, then we'll check to see if our\n        # value is a list of a single item. If it is then we'll assume that\n        # it was emitted as a single string, and unwrap the str from inside\n        # the list.\n        #\n        # If it's any other kind of data, then we haven't the faintest clue\n        # what we should parse it as, and we have to just add it to our list\n        # of unparsed stuff.\n        if raw_name in _STRING_FIELDS and len(value) == 1:\n            raw[raw_name] = value[0]\n        # If this is one of our list of string fields, then we can just assign\n        # the value, since forms *only* have strings, and our getall() call\n        # above ensures that this is a list.\n        elif raw_name in _LIST_FIELDS:\n            raw[raw_name] = value\n        # Special Case: Keywords\n        # The keywords field is implemented in the metadata spec as a str,\n        # but it conceptually is a list of strings, and is serialized using\n        # \", \".join(keywords), so we'll do some light data massaging to turn\n        # this into what it logically is.\n        elif raw_name == \"keywords\" and len(value) == 1:\n            raw[raw_name] = _parse_keywords(value[0])\n        # Special Case: Project-URL\n        # The project urls is implemented in the metadata spec as a list of\n        # specially-formatted strings that represent a key and a value, which\n        # is fundamentally a mapping, however the email format doesn't support\n        # mappings in a sane way, so it was crammed into a list of strings\n        # instead.\n        #\n        # We will do a little light data massaging to turn this into a map as\n        # it logically should be.\n        elif raw_name == \"project_urls\":\n            try:\n                raw[raw_name] = _parse_project_urls(value)\n            except KeyError:\n                unparsed[name] = value\n        # Nothing that we've done has managed to parse this, so it'll just\n        # throw it in our unparseable data and move on.\n        else:\n            unparsed[name] = value\n\n    # If we have any unparsed data, then we treat that as an error\n    if unparsed:\n        raise ExceptionGroup(\n            \"unparsed\",\n            [", "completion_part": "InvalidMetadata(key, f\"{key!r} has invalid data\")", "after_completion": " for key in unparsed],\n        )\n\n    # We need to cast our `raw` to a metadata, because a TypedDict only support\n    # literal key names, but we're computing our key names on purpose, but the\n    # way this function is implemented, our `TypedDict` can only have valid key\n    # names.\n    return Metadata.from_raw(typing.cast(RawMetadata, raw))", "namespace": "packaging.metadata.InvalidMetadata", "completions": ["\n                Exception(\n                    f\"unparsed form data"]}
{"id": 12, "prompt": "def import_mesh(path):\n    gltf = GLTF2.load(path)\n    gltf.convert_buffers(BufferFormat.BINARYBLOB)\n    materials = _get_materials(gltf)\n    meshes = _get_meshes(gltf)\n    for sampler in gltf.samplers:\n        if sampler.wrapS != 10497 or sampler.wrapT != 10497:\n            warnings.warn(\n                \"wrapping mode is not support yet. Please make a github request if needed.\",\n                UserWarning\n            )\n    default_scene = gltf.scenes[gltf.scene]\n    scene_meshes = []\n\n    has_tangents = False\n    has_uvs = False\n    has_normals = False\n    for mesh in meshes:\n        if mesh.has_attribute('vertex_tangents'):\n            has_tangents = True\n        if mesh.has_attribute('uvs'):\n            has_uvs = True\n        if mesh.has_attribute('normals'):\n            has_normals = True\n\n    def _traverse_scene(node_idx, cur_transform):\n        node = gltf.nodes[node_idx]\n        if node.matrix is not None:\n            node_transform = torch.tensor(node.matrix, dtype=torch.double).reshape(4, 4)\n        else:\n            node_transform = None\n            if node.scale is not None:\n                node_transform = torch.tensor([\n                    [node.scale[0], 0., 0., 0.],\n                    [0., node.scale[1], 0., 0.],\n                    [0., 0., node.scale[2], 0.],\n                    [0., 0., 0., 1.]\n                ], dtype=torch.double)\n            if node.rotation is not None:\n                rotation_mat = _make_rotation_mat(node.rotation)\n                if node_transform is None:\n                    node_transform = rotation_mat\n                else:\n                    node_transform = node_transform @ rotation_mat\n\n            if node.translation is not None:\n                translation_mat = torch.tensor([\n                    [1., 0., 0., 0.],\n                    [0., 1., 0., 0.],\n                    [0., 0., 1., 0.],\n                    [node.translation[0], node.translation[1], node.translation[2], 1.]\n                ], dtype=torch.double)\n                if node_transform is None:\n                    node_transform = translation_mat\n                else:\n                    node_transform = node_transform @ translation_mat\n        if node_transform is not None:\n            cur_transform = node_transform @ cur_transform\n        if node.mesh is not None:\n            mesh = copy.copy(meshes[node.mesh])\n            if mesh is not None:\n                vertices = torch.nn.functional.pad(\n                    mesh.vertices, (0, 1), value=1., mode='constant')\n                vertices = vertices @ cur_transform.float()\n                mesh.vertices = vertices[..., :3]\n                if has_tangents:\n                    tangents = mesh.vertex_tangents @ cur_transform[:3, :3].float()\n                    mesh.vertex_tangents = torch.nn.functional.normalize(\n                        tangents, dim=-1)\n                if has_normals:\n                    inv_cur_transform = torch.linalg.inv(cur_transform[:3, :3]).T.float()\n                    normals = mesh.vertex_normals @ inv_cur_transform\n                    mesh.vertex_normals = torch.nn.functional.normalize(\n                        normals, dim=-1)\n\n                scene_meshes.append(mesh)\n        for next_node_idx in node.children:\n            _traverse_scene(next_node_idx, cur_transform)\n    for node_idx in default_scene.nodes:\n        _traverse_scene(node_idx, torch.", "completion_part": "eye(4, dtype=torch.double)", "after_completion": ")\n    outputs = _join_meshes(scene_meshes)\n    outputs.materials = materials\n    return outputs", "namespace": "torch.eye", "completions": ["eye(4, dtype=torch.double))"]}
{"id": 13, "prompt": "\ndef clip_grad_norm_(params, max_norm, aggregate_norm_fn=None) -> torch.Tensor:\n    def grad_exists(p):\n        return p is not None and getattr(p, \"grad\", None) is not None\n\n    if isinstance(params, torch.Tensor):\n        params = [params]\n    params = list(params)\n    grads = [\n        p.grad.detach() for p in params if grad_exists(p) and not hasattr(p, \"expert\")\n    ]\n    expert_grads = [\n        p.grad.detach() for p in params if grad_exists(p) and hasattr(p, \"expert\")\n    ]\n\n    if len(grads) == 0:\n        if len(params) > 0:\n            return params[0].new_tensor(0.0)\n        else:\n            return torch.", "completion_part": "tensor(0.0)", "after_completion": "\n\n    if len(grads) == 1:\n        total_norm = torch.norm(grads[0], p=2, dtype=torch.float32)\n    else:\n        if multi_tensor_l2norm_available:\n            total_norm = multi_tensor_total_norm(grads)\n        else:\n            if torch.cuda.is_available():\n                warnings.warn(\n                    \"amp_C fused kernels unavailable, disabling multi_tensor_l2norm; \"\n                    \"you may get better performance by installing NVIDIA's apex library\"\n                )\n                device = torch.cuda.current_device()\n            elif grads[0].device.type == \"xla\":\n                device = grads[0].device\n            else:\n                device = torch.device(\"cpu\")\n            total_norm = torch.norm(\n                torch.stack(\n                    [torch.norm(g, p=2, dtype=torch.float32).to(device) for g in grads]\n                )\n            )\n\n    if aggregate_norm_fn is not None:\n        total_norm = aggregate_norm_fn(total_norm)\n\n    if max_norm > 0:\n        max_norm = float(max_norm)\n        clip_coef = (max_norm / (total_norm + 1e-6)).clamp_(max=1)\n        for g in grads + expert_grads:\n            g.mul_(clip_coef)\n    return total_norm", "namespace": "torch.tensor", "completions": ["tensor(0.0)\n\n    if aggregate_"]}
{"id": 14, "prompt": "\n    def set_locs(self, locs):\n        self.locs = np.array(locs)\n        self._labelled.clear()\n\n        if not self._minor:\n            return None\n        if all(\n            _is_decade(x, rtol=1e-7)\n            or _is_decade(1 - x, rtol=1e-7)\n            or (_is_close_to_int(2 * x) and\n                int(np.round(2 * x)) == 1)\n            for x in locs\n        ):\n            # minor ticks are subsample from ideal, so no label\n            return None\n        if len(locs) < self._minor_threshold:\n            if len(locs) < self._minor_number:\n                self._labelled.update(locs)\n            else:\n                # we do not have a lot of minor ticks, so only few decades are\n                # displayed, then we choose some (spaced) minor ticks to label.\n                # Only minor ticks are known, we assume it is sufficient to\n                # choice which ticks are displayed.\n                # For each ticks we compute the distance between the ticks and\n                # the previous, and between the ticks and the next one. Ticks\n                # with smallest minimum are chosen. As tiebreak, the ticks\n                # with smallest sum is chosen.\n                diff = np.diff(-np.log(1 / self.locs - 1))\n                space_pessimistic = np.", "completion_part": "minimum(\n                    np.concatenate(((np.inf,), diff)),\n                    np.concatenate((diff, (np.inf,))),\n                )", "after_completion": "\n                space_sum = (\n                    np.concatenate(((0,), diff))\n                    + np.concatenate((diff, (0,)))\n                )\n                good_minor = sorted(\n                    range(len(self.locs)),\n                    key=lambda i: (space_pessimistic[i], space_sum[i]),\n                )[-self._minor_number:]\n                self._labelled.update(locs[i] for i in good_minor)", "namespace": "numpy.minimum", "completions": ["argmin(diff)\n                space_optimistic"]}
{"id": 15, "prompt": "    def _iter_collection_raw_paths(self, master_transform, paths,\n                                   all_transforms):\n        Npaths = len(paths)\n        Ntransforms = len(all_transforms)\n        N = max(Npaths, Ntransforms)\n\n        if Npaths == 0:\n            return\n\n        transform = transforms.", "completion_part": "IdentityTransform()", "after_completion": "\n        for i in range(N):\n            path = paths[i % Npaths]\n            if Ntransforms:\n                transform = Affine2D(all_transforms[i % Ntransforms])\n            yield path, transform + master_transform", "namespace": "matplotlib.transforms.IdentityTransform", "completions": ["Compose(all_transforms)\n        for i in"]}
{"id": 16, "prompt": "def _spatial_matching(\n    x,\n    y=None,\n    n_matches=5,\n    metric=\"euclidean\",\n    solver=None,\n    return_mip=False,\n    allow_partial_match=False,\n    **metric_kwargs,\n):\n    try:\n        import pulp\n    except ImportError as error:\n        raise ImportError(\"spatial matching requires the pulp library\") from error\n    if metric == \"precomputed\":\n        distance_matrix = x\n        match_between = y is not None\n    elif y is not None:\n        x, x_ids, _ = _validate_geometry_input(\n            x, ids=None, valid_geometry_types=_VALID_GEOMETRY_TYPES\n        )\n        y, y_ids, _ = _validate_geometry_input(\n            y, ids=None, valid_geometry_types=_VALID_GEOMETRY_TYPES\n        )\n        distance_matrix = pairwise_distances(x, y, metric=metric)\n        match_between = True\n    else:\n        x, x_ids, _ = _validate_geometry_input(\n            x, ids=None, valid_geometry_types=_VALID_GEOMETRY_TYPES\n        )\n        y_ids = x_ids\n        distance_matrix = pairwise_distances(x, metric=metric, **metric_kwargs)\n\n        match_between = False\n\n    n_targets, n_sources = distance_matrix.shape\n\n    if match_between:\n        row, col = numpy.", "completion_part": "meshgrid(\n            numpy.arange(n_targets), numpy.arange(n_sources), indexing=\"ij\"\n        )", "after_completion": "\n        row = row.flatten()\n        col = col.flatten()\n    else:\n        # if we are matching within, we need to\n        row, col = numpy.triu_indices(\n            n=n_targets, m=n_sources, k=int(not match_between)\n        )\n\n    mp = pulp.LpProblem(\"optimal-neargraph\", sense=pulp.LpMinimize)\n    # a match is as binary decision variable, connecting observation i to observation j\n    match_vars = pulp.LpVariable.dicts(\n        \"match\",\n        lowBound=0,\n        upBound=1,\n        indices=zip(row, col, strict=True),\n        cat=\"Continuous\" if allow_partial_match else \"Binary\",\n    )\n    # we want to minimize the geographic distance of links in the graph\n    mp.objective = pulp.lpSum(\n        [\n            match_vars[i, j] * distance_matrix[i, j]\n            for i, j in zip(row, col, strict=True)\n        ]\n    )\n\n    # for each observation\n    for j in range(n_targets):\n        # there must be exactly k other matched observations, which might live\n        if match_between:\n            summand = pulp.lpSum(\n                [\n                    # over the whole match matrix\n                    match_vars[j, i]\n                    for i in range(n_sources)\n                ]\n            )\n            sense = 1\n        else:\n            summand = pulp.lpSum(\n                [\n                    # in the \"upper\" triangle, or \"lower\" triangle\n                    match_vars.get((i, j), match_vars.get((j, i)))\n                    for i in range(n_sources)\n                    if (i != j)\n                ]\n            )\n            sense = int(not allow_partial_match)\n        mp += pulp.LpConstraint(summand, sense=sense, rhs=n_matches)\n    if match_between:  # but, we may choose to ignore some sources\n        for i in range(n_sources):\n            summand = pulp.lpSum([match_vars[j, i] for j in range(n_targets)])\n            mp += pulp.LpConstraint(summand, sense=-1, rhs=n_matches)\n\n    status = mp.solve(solver)\n\n    if (status != 1) & (not allow_partial_match):\n        warnings.warn(\n            f\"Problem is {pulp.LpStatus[status]}, so edge weights may be non-integer!\",\n            stacklevel=2,\n        )\n\n    edges = [\n        (*key, value.value()) for key, value in match_vars.items() if value.value() > 0\n    ]\n    if not match_between:\n        edges.extend([(*tuple(reversed(edge[:-1])), edge[-1]) for edge in edges])\n\n    heads, tails, weights = map(numpy.asarray, zip(*sorted(edges), strict=True))\n\n    if return_mip:\n        return x_ids[heads], y_ids[tails], weights, mp\n    return x_ids[heads], y_ids[tails], weights", "namespace": "numpy.meshgrid", "completions": ["unravel_index(\n            numpy.argpartition"]}
{"id": 17, "prompt": "def average_transforms(\n    R: torch.Tensor,\n    t: torch.Tensor,\n    w: torch.Tensor,\n    mask: torch.Tensor,\n    dim: int,\n    t_edge: Optional[torch.Tensor] = None,\n    dither: Optional[bool] = True,\n    dither_eps: float = 1e-4,\n) -> Tuple[torch.Tensor, torch.Tensor]:\n    assert dim >= 0, \"dimension must index from the left\"\n    w = torch.where(\n        mask[..., None].bool(), w, torch.full_like(w, torch.finfo(w.dtype).min)\n    )\n\n    # We use different averaging models based on the number of weights\n    num_transform_weights = w.size(-1)\n    if num_transform_weights == 1:\n        # Share a single scalar weight between t and R.\n        probs = w.softmax(dim)\n        t_probs = probs\n        R_probs = probs[..., None]\n\n        # Average translation.\n        t_avg = (t * t_probs).sum(dim)\n    elif num_transform_weights == 2:\n        # Use separate scalar weights for each of t and R.\n        probs = w.softmax(dim)\n        t_probs, R_probs = probs.unbind(-1)\n        t_probs = t_probs[..., None]\n        R_probs = R_probs[..., None, None]\n\n        # Average translation.\n        t_avg = (t * t_probs).sum(dim)\n    elif num_transform_weights == 3:\n        # For R use a signed scalar weight.\n        R_probs = w[..., 2].softmax(dim)[..., None, None]\n\n        # For t use a two-parameter precision matrix P = P_isometric + P_radial.\n        # We need to hand compute softmax over the shared dim x 2 elements.\n        w_t = w[..., :2]\n        w_t_total = w_t.logsumexp([dim, -1], True)\n        p_iso, p_rad = (w_t - w_t_total).exp().unbind(-1)\n\n        # Use Gaussian fusion for translation.\n        t_edge = t_edge * mask.to(t_edge.dtype)[..., None]\n        t_avg, _ = fuse_gaussians_isometric_plus_radial(t, p_iso, p_rad, t_edge, dim)\n    else:\n        raise NotImplementedError\n\n    # Average rotation via SVD\n    R_avg_unc = (R * R_probs).sum(dim)\n    R_avg_unc = R_avg_unc + dither_eps * torch.randn_like(R_avg_unc)\n    U, S, Vh = torch.linalg.svd(R_avg_unc, full_matrices=True)\n    R_avg = U @ Vh\n\n    # Enforce that matrix is rotation matrix\n    d = torch.linalg.det(R_avg)\n    d_expand = F.", "completion_part": "pad(d[..., None, None], (2, 0), value=1.0)", "after_completion": "\n    Vh = Vh * d_expand\n    R_avg = U @ Vh\n    return R_avg, t_avg", "namespace": "torch.nn.functional.pad", "completions": ["pad(d, (0, 1),"]}
{"id": 59, "prompt": "    def fit(\n        self,\n        timedelta_unit: DATETIME_UNITS = \"s\",\n        log_scale: bool | tuple[bool, bool] | None = None,\n        lower_cutoff_quantile: float | None = None,\n        upper_cutoff_quantile: float | None = None,\n        bins: int | Literal[BINS_ESTIMATORS] = 20,\n    ) -> None:\n        called_params = {\n            \"timedelta_unit\": timedelta_unit,\n            \"log_scale\": log_scale,\n            \"lower_cutoff_quantile\": lower_cutoff_quantile,\n            \"upper_cutoff_quantile\": upper_cutoff_quantile,\n            \"bins\": bins,\n        }\n        not_hash_values = [\"timedelta_unit\"]\n\n        self.log_scale, self.upper_cutoff_quantile, self.lower_cutoff_quantile = self.__validate_input(\n            log_scale,\n            lower_cutoff_quantile,\n            upper_cutoff_quantile,\n        )\n\n        self.timedelta_unit = timedelta_unit\n        self.bins = bins\n\n        data = self.__eventstream.to_dataframe(copy=True).groupby(self.user_col)[self.time_col].agg([\"min\", \"max\"])\n        data[\"time_passed\"] = data[\"max\"] - data[\"min\"]\n        values_to_plot = (data[\"time_passed\"] / np.", "completion_part": "timedelta64(1, self.timedelta_unit)", "after_completion": ").reset_index(  # type: ignore\n            drop=True\n        )\n\n        if self._remove_cutoff_values:  # type: ignore\n            values_to_plot = self._remove_cutoff_values(values_to_plot).to_numpy()\n        if self.log_scale[0]:\n            log_adjustment = np.timedelta64(100, \"ms\") / np.timedelta64(1, self.timedelta_unit)\n            values_to_plot = np.where(values_to_plot != 0, values_to_plot, values_to_plot + log_adjustment)  # type: ignore\n            bins_to_show = np.power(10, np.histogram_bin_edges(np.log10(values_to_plot), bins=self.bins))\n        else:\n            bins_to_show = np.histogram_bin_edges(values_to_plot, bins=self.bins)\n        if len(values_to_plot) == 0:\n            bins_to_show = np.array([])\n\n        self.bins_to_show = bins_to_show\n        self.values_to_plot = values_to_plot  # type: ignore\n        collect_data_performance(\n            scope=\"user_lifetime_hist\",\n            event_name=\"metadata\",\n            called_params=called_params,\n            not_hash_values=not_hash_values,\n            performance_data={},\n            eventstream_index=self.__eventstream._eventstream_index,\n        )", "namespace": "numpy.timedelta64", "completions": ["timedelta64(1, self.timedelta_"]}
{"id": 19, "prompt": "\ndef main():\n    parser = argparse.ArgumentParser()\n\n    # Required parameters\n    parser.add_argument(\n        \"--data_dir\",\n        default=None,\n        type=str,\n        required=True,\n        help=\"The input data dir. Should contain the .tsv files (or other data files) for the task.\",\n    )\n    parser.add_argument(\n        \"--model_type\",\n        default=None,\n        type=str,\n        required=True,\n        help=\"Model type selected in the list: \" + \", \".join(MODEL_CLASSES.", "completion_part": "keys()", "after_completion": "),\n    )\n    parser.add_argument(\n        \"--model_name_or_path\",\n        default=None,\n        type=str,\n        required=True,\n        help=\"Path to pre-trained model or shortcut name.\",\n    )\n    parser.add_argument(\n        \"--task_name\",\n        default=None,\n        type=str,\n        required=True,\n        help=\"The name of the task to train selected in the list: \" + \", \".join(processors.keys()),\n    )\n    parser.add_argument(\n        \"--output_dir\",\n        default=None,\n        type=str,\n        required=True,\n        help=\"The output directory where the model predictions and checkpoints will be written.\",\n    )\n    parser.add_argument(\n        \"--patience\",\n        default=\"0\",\n        type=str,\n        required=False,\n    )\n    parser.add_argument(\n        \"--regression_threshold\",\n        default=0,\n        type=float,\n        required=False,\n    )\n\n    # Other parameters\n    parser.add_argument(\n        \"--config_name\",\n        default=\"\",\n        type=str,\n        help=\"Pretrained config name or path if not the same as model_name\",\n    )\n    parser.add_argument(\n        \"--tokenizer_name\",\n        default=\"\",\n        type=str,\n        help=\"Pretrained tokenizer name or path if not the same as model_name\",\n    )\n    parser.add_argument(\n        \"--cache_dir\",\n        default=\"\",\n        type=str,\n        help=\"Where do you want to store the pre-trained models downloaded from huggingface.co\",\n    )\n    parser.add_argument(\n        \"--max_seq_length\",\n        default=128,\n        type=int,\n        help=(\n            \"The maximum total input sequence length after tokenization. Sequences longer \"\n            \"than this will be truncated, sequences shorter will be padded.\"\n        ),\n    )\n    parser.add_argument(\"--do_train\", action=\"store_true\", help=\"Whether to run training.\")\n    parser.add_argument(\"--do_eval\", action=\"store_true\", help=\"Whether to run eval on the dev set.\")\n    parser.add_argument(\n        \"--evaluate_during_training\",\n        action=\"store_true\",\n        help=\"Run evaluation during training at each logging step.\",\n    )\n    parser.add_argument(\n        \"--do_lower_case\",\n        action=\"store_true\",\n        help=\"Set this flag if you are using an uncased model.\",\n    )\n\n    parser.add_argument(\n        \"--per_gpu_train_batch_size\",\n        default=8,\n        type=int,\n        help=\"Batch size per GPU/CPU for training.\",\n    )\n    parser.add_argument(\n        \"--per_gpu_eval_batch_size\",\n        default=1,\n        type=int,\n        help=\"Batch size per GPU/CPU for evaluation.\",\n    )\n    parser.add_argument(\n        \"--gradient_accumulation_steps\",\n        type=int,\n        default=1,\n        help=\"Number of updates steps to accumulate before performing a backward/update pass.\",\n    )\n    parser.add_argument(\n        \"--learning_rate\",\n        default=5e-5,\n        type=float,\n        help=\"The initial learning rate for Adam.\",\n    )\n    parser.add_argument(\"--weight_decay\", default=0.0, type=float, help=\"Weight decay if we apply some.\")\n    parser.add_argument(\"--adam_epsilon\", default=1e-8, type=float, help=\"Epsilon for Adam optimizer.\")\n    parser.add_argument(\"--max_grad_norm\", default=1.0, type=float, help=\"Max gradient norm.\")\n    parser.add_argument(\n        \"--num_train_epochs\",\n        default=3.0,\n        type=float,\n        help=\"Total number of training epochs to perform.\",\n    )\n    parser.add_argument(\n        \"--max_steps\",\n        default=-1,\n        type=int,\n        help=\"If > 0: set total number of training steps to perform. Override num_train_epochs.\",\n    )\n    parser.add_argument(\"--warmup_steps\", default=0, type=int, help=\"Linear warmup over warmup_steps.\")\n\n    parser.add_argument(\"--logging_steps\", type=int, default=500, help=\"Log every X updates steps.\")\n    parser.add_argument(\n        \"--save_steps\",\n        type=int,\n        default=500,\n        help=\"Save checkpoint every X updates steps.\",\n    )\n    parser.add_argument(\n        \"--eval_all_checkpoints\",\n        action=\"store_true\",\n        help=\"Evaluate all checkpoints starting with the same prefix as model_name ending and ending with step number\",\n    )\n    parser.add_argument(\"--no_cuda\", action=\"store_true\", help=\"Avoid using CUDA when available\")\n    parser.add_argument(\n        \"--overwrite_output_dir\",\n        action=\"store_true\",\n        help=\"Overwrite the content of the output directory\",\n    )\n    parser.add_argument(\n        \"--overwrite_cache\",\n        action=\"store_true\",\n        help=\"Overwrite the cached training and evaluation sets\",\n    )\n    parser.add_argument(\"--seed\", type=int, default=42, help=\"random seed for initialization\")\n\n    parser.add_argument(\n        \"--fp16\",\n        action=\"store_true\",\n        help=\"Whether to use 16-bit (mixed) precision (through NVIDIA apex) instead of 32-bit\",\n    )\n    parser.add_argument(\n        \"--fp16_opt_level\",\n        type=str,\n        default=\"O1\",\n        help=(\n            \"For fp16: Apex AMP optimization level selected in ['O0', 'O1', 'O2', and 'O3'].\"\n            \"See details at https://nvidia.github.io/apex/amp.html\"\n        ),\n    )\n    parser.add_argument(\n        \"--local_rank\",\n        type=int,\n        default=-1,\n        help=\"For distributed training: local_rank\",\n    )\n    parser.add_argument(\"--server_ip\", type=str, default=\"\", help=\"For distant debugging.\")\n    parser.add_argument(\"--server_port\", type=str, default=\"\", help=\"For distant debugging.\")\n    args = parser.parse_args()\n\n    if (\n        os.path.exists(args.output_dir)\n        and os.listdir(args.output_dir)\n        and args.do_train\n        and not args.overwrite_output_dir\n    ):\n        raise ValueError(\n            \"Output directory ({}) already exists and is not empty. Use --overwrite_output_dir to overcome.\".format(\n                args.output_dir\n            )\n        )\n\n    # Setup distant debugging if needed\n    if args.server_ip and args.server_port:\n        # Distant debugging - see https://code.visualstudio.com/docs/python/debugging#_attach-to-a-local-script\n        import ptvsd\n\n        print(\"Waiting for debugger attach\")\n        ptvsd.enable_attach(address=(args.server_ip, args.server_port), redirect_output=True)\n        ptvsd.wait_for_attach()\n\n    # Setup CUDA, GPU & distributed training\n    if args.local_rank == -1 or args.no_cuda:\n        device = torch.device(\"cuda\" if torch.cuda.is_available() and not args.no_cuda else \"cpu\")\n        args.n_gpu = torch.cuda.device_count()\n    else:  # Initializes the distributed backend which will take care of sychronizing nodes/GPUs\n        torch.cuda.set_device(args.local_rank)\n        device = torch.device(\"cuda\", args.local_rank)\n        torch.distributed.init_process_group(backend=\"nccl\")\n        args.n_gpu = 1\n    args.device = device\n\n    # Setup logging\n    logging.basicConfig(\n        format=\"%(asctime)s - %(levelname)s - %(name)s - %(message)s\",\n        datefmt=\"%m/%d/%Y %H:%M:%S\",\n        level=logging.INFO if args.local_rank in [-1, 0] else logging.WARN,\n    )\n    logger.warning(\n        \"Process rank: %s, device: %s, n_gpu: %s, distributed training: %s, 16-bits training: %s\",\n        args.local_rank,\n        device,\n        args.n_gpu,\n        bool(args.local_rank != -1),\n        args.fp16,\n    )\n    # Set the verbosity to info of the Transformers logger (on main process only):\n    if is_main_process(args.local_rank):\n        transformers.utils.logging.set_verbosity_info()\n        transformers.utils.logging.enable_default_handler()\n        transformers.utils.logging.enable_explicit_format()\n    # Set seed\n    set_seed(args)\n\n    # Prepare GLUE task\n    args.task_name = args.task_name.lower()\n    if args.task_name not in processors:\n        raise ValueError(\"Task not found: %s\" % (args.task_name))\n    processor = processors[args.task_name]()\n    args.output_mode = output_modes[args.task_name]\n    label_list = processor.get_labels()\n    num_labels = len(label_list)\n\n    if args.patience != \"0\" and args.per_gpu_eval_batch_size != 1:\n        raise ValueError(\"The eval batch size must be 1 with PABEE inference on.\")\n\n    # Load pretrained model and tokenizer\n    if args.local_rank not in [-1, 0]:\n        torch.distributed.barrier()  # Make sure only the first process in distributed training will download model & vocab\n\n    args.model_type = args.model_type.lower()\n    config_class, model_class, tokenizer_class = MODEL_CLASSES[args.model_type]\n    config = config_class.from_pretrained(\n        args.config_name if args.config_name else args.model_name_or_path,\n        num_labels=num_labels,\n        finetuning_task=args.task_name,\n        cache_dir=args.cache_dir if args.cache_dir else None,\n    )\n    tokenizer = tokenizer_class.from_pretrained(\n        args.tokenizer_name if args.tokenizer_name else args.model_name_or_path,\n        do_lower_case=args.do_lower_case,\n        cache_dir=args.cache_dir if args.cache_dir else None,\n    )\n    model = model_class.from_pretrained(\n        args.model_name_or_path,\n        from_tf=bool(\".ckpt\" in args.model_name_or_path),\n        config=config,\n        cache_dir=args.cache_dir if args.cache_dir else None,\n    )\n\n    if args.local_rank == 0:\n        torch.distributed.barrier()  # Make sure only the first process in distributed training will download model & vocab\n\n    model.to(args.device)\n\n    print(\"Total Model Parameters:\", sum(param.numel() for param in model.parameters()))\n    output_layers_param_num = sum(param.numel() for param in model.classifiers.parameters())\n    print(\"Output Layers Parameters:\", output_layers_param_num)\n    single_output_layer_param_num = sum(param.numel() for param in model.classifiers[0].parameters())\n    print(\n        \"Added Output Layers Parameters:\",\n        output_layers_param_num - single_output_layer_param_num,\n    )\n\n    logger.info(\"Training/evaluation parameters %s\", args)\n\n    # Training\n    if args.do_train:\n        train_dataset = load_and_cache_examples(args, args.task_name, tokenizer, evaluate=False)\n        global_step, tr_loss = train(args, train_dataset, model, tokenizer)\n        logger.info(\" global_step = %s, average loss = %s\", global_step, tr_loss)\n\n    # Saving best-practices: if you use defaults names for the model, you can reload it using from_pretrained()\n    if args.do_train and (args.local_rank == -1 or torch.distributed.get_rank() == 0):\n        logger.info(\"Saving model checkpoint to %s\", args.output_dir)\n        # Save a trained model, configuration and tokenizer using `save_pretrained()`.\n        # They can then be reloaded using `from_pretrained()`\n        model_to_save = (\n            model.module if hasattr(model, \"module\") else model\n        )  # Take care of distributed/parallel training\n        model_to_save.save_pretrained(args.output_dir)\n        tokenizer.save_pretrained(args.output_dir)\n\n        # Good practice: save your training arguments together with the trained model\n        torch.save(args, os.path.join(args.output_dir, \"training_args.bin\"))\n\n        # Load a trained model and vocabulary that you have fine-tuned\n        model = model_class.from_pretrained(args.output_dir)\n        tokenizer = tokenizer_class.from_pretrained(args.output_dir)\n        model.to(args.device)\n\n    # Evaluation\n    results = {}\n    if args.do_eval and args.local_rank in [-1, 0]:\n        patience_list = [int(x) for x in args.patience.split(\",\")]\n        tokenizer = tokenizer_class.from_pretrained(args.output_dir, do_lower_case=args.do_lower_case)\n        checkpoints = [args.output_dir]\n        if args.eval_all_checkpoints:\n            checkpoints = [\n                os.path.dirname(c) for c in sorted(glob.glob(args.output_dir + \"/**/\" + WEIGHTS_NAME, recursive=True))\n            ]\n\n        logger.info(\"Evaluate the following checkpoints: %s\", checkpoints)\n\n        for checkpoint in checkpoints:\n            global_step = checkpoint.split(\"-\")[-1] if len(checkpoints) > 1 else \"\"\n            prefix = checkpoint.split(\"/\")[-1] if checkpoint.find(\"checkpoint\") != -1 else \"\"\n\n            model = model_class.from_pretrained(checkpoint)\n            model.to(args.device)\n\n            print(f\"Evaluation for checkpoint {prefix}\")\n            for patience in patience_list:\n                result = evaluate(args, model, tokenizer, prefix=prefix, patience=patience)\n                result = {k + \"_{}\".format(global_step): v for k, v in result.items()}\n                results.update(result)\n    return results", "namespace": "transformers.glue_processors.keys", "completions": ["keys()),\n    )\n    parser.add_argument"]}
{"id": 20, "prompt": "def frechet_distance_exact(\n    means1: NDArray,\n    cov1: NDArray,\n    means2: NDArray,\n    cov2: NDArray,\n) -> float:\n    n = len(means1)\n    if (means2.shape != (n,)) or (cov1.shape != (n, n)) or (cov2.shape != (n, n)):\n        raise ValueError(\"Inputs have to be of same dimensions.\")\n\n    ssdiff = np.sum((means1 - means2) ** 2.0)\n    product = np.array(cov1 @ cov2)\n    if product.ndim < 2:\n        product = product.reshape(-1, 1)\n    covmean = scipy.linalg.", "completion_part": "sqrtm(product)", "after_completion": "\n    if np.iscomplexobj(covmean):\n        covmean = covmean.real\n    frechet_dist = ssdiff + np.trace(cov1 + cov2 - 2.0 * covmean)\n\n    return frechet_dist / n", "namespace": "scipy.linalg.sqrtm", "completions": ["sqrtm(product)\n    if not np."]}
{"id": 21, "prompt": "def as_freq(\n    data_series,\n    freq,\n    atomic_freq=\"1 Min\",\n    series_type=\"cumulative\",\n    include_coverage=False,\n):\n    # TODO(philngo): make sure this complies with CalTRACK 2.2.2.1\n    if not isinstance(data_series, pd.Series):\n        raise ValueError(\n            \"expected series, got object with class {}\".format(data_series.__class__)\n        )\n    if data_series.empty:\n        return data_series\n    series = remove_duplicates(data_series)\n    target_freq = pd.Timedelta(atomic_freq)\n    timedeltas = (series.index[1:] - series.index[:-1]).append(\n        pd.TimedeltaIndex([pd.NaT])\n    )\n\n    if series_type == \"cumulative\":\n        spread_factor = target_freq.total_seconds() / timedeltas.total_seconds()\n        series_spread = series * spread_factor\n        atomic_series = series_spread.asfreq(atomic_freq, method=\"ffill\")\n        resampled = atomic_series.resample(freq).sum()\n        resampled_with_nans = atomic_series.resample(freq).first()\n        n_coverage = atomic_series.resample(freq).count()\n        resampled = resampled[resampled_with_nans.notnull()].reindex(resampled.index)\n\n    elif series_type == \"instantaneous\":\n        atomic_series = series.asfreq(atomic_freq, method=\"ffill\")\n        resampled = atomic_series.resample(freq).mean()\n\n    if resampled.index[-1] < series.index[-1]:\n        # this adds a null at the end using the target frequency\n        last_index = pd.", "completion_part": "date_range(resampled.index[-1], freq=freq, periods=2)", "after_completion": "[1:]\n        resampled = (\n            pd.concat([resampled, pd.Series(np.nan, index=last_index)])\n            .resample(freq)\n            .mean()\n        )\n    if include_coverage:\n        n_total = resampled.resample(atomic_freq).count().resample(freq).count()\n        resampled = resampled.to_frame(\"value\")\n        resampled[\"coverage\"] = n_coverage / n_total\n        return resampled\n    else:\n        return resampled", "namespace": "pandas.date_range", "completions": ["DatetimeIndex([series.index[-1]])\n       "]}
{"id": 22, "prompt": "\n    def from_dict(cls, data: Dict[str, Any]) -> \"ExtractedTableAnswer\":\n        init_params = data.get(\"init_parameters\", {})\n        if (doc := init_params.get(\"document\")) is not None:\n            data[\"init_parameters\"][\"document\"] = Document.from_dict(doc)\n\n        if (context := init_params.get(\"context\")) is not None:\n            data[\"init_parameters\"][\"context\"] = ", "completion_part": "read_json(io.StringIO(context))", "after_completion": "\n\n        if (cells := init_params.get(\"document_cells\")) is not None:\n            data[\"init_parameters\"][\"document_cells\"] = [ExtractedTableAnswer.Cell(**c) for c in cells]\n\n        if (cells := init_params.get(\"context_cells\")) is not None:\n            data[\"init_parameters\"][\"context_cells\"] = [ExtractedTableAnswer.Cell(**c) for c in cells]\n        return default_from_dict(cls, data)", "namespace": "pandas.read_json", "completions": [" Context.from_dict(context)\n\n        if"]}
{"id": 23, "prompt": "\ndef linspace(\n    start, stop, num=50, endpoint=True, retstep=False, dtype=None, axis=0\n):\n    axis = standardize_axis_for_numpy(axis)\n    if dtype is None:\n        dtypes_to_resolve = [\n            getattr(start, \"dtype\", type(start)),\n            getattr(stop, \"dtype\", type(stop)),\n            float,\n        ]\n        dtype = dtypes.result_type(*dtypes_to_resolve)\n    return np.", "completion_part": "linspace(\n        start,\n        stop,\n        num=num,\n        endpoint=endpoint,\n        retstep=retstep,\n        dtype=dtype,\n        axis=axis,\n    )", "after_completion": "", "namespace": "numpy.linspace", "completions": ["linspace(\n        start, stop, num=num"]}
{"id": 88, "prompt": "def unflatten_lower_triangle(flattened: torch.Tensor, M: int, N: int) -> torch.Tensor:\n    # Get the indices of the lower triangular part\n    row_indices, col_indices = torch.tril_indices(M, N)\n\n    # Create an empty tensor to fill in the original shape\n    new_shape = list(flattened.shape[:-1]) + [M, N]\n    reconstructed = torch.", "completion_part": "zeros(new_shape, dtype=flattened.dtype, device=flattened.device)", "after_completion": "\n\n    # Fill in the lower triangular part of the reconstructed tensor\n    reconstructed[..., row_indices, col_indices] = flattened\n\n    return reconstructed", "namespace": "torch.zeros", "completions": ["zeros(new_shape, dtype=flattened"]}
{"id": 90, "prompt": "def unary_encoder_random_gaussian(nqubits: int, architecture: str = \"tree\", seed=None):\n    if not isinstance(nqubits, int):\n        raise_error(\n            TypeError, f\"nqubits must be type int, but it is type {type(nqubits)}.\"\n        )\n\n    if nqubits <= 0.0:\n        raise_error(\n            ValueError, f\"nqubits must be a positive integer, but it is {nqubits}.\"\n        )\n\n    if not isinstance(architecture, str):\n        raise_error(\n            TypeError,\n            f\"``architecture`` must be type str, but it is type {type(architecture)}.\",\n        )\n\n    if architecture != \"tree\":\n        raise_error(\n            NotImplementedError,\n            f\"Currently, this function only accepts ``architecture=='tree'``.\",\n        )\n\n    if not math.log2(nqubits).is_integer():\n        raise_error(ValueError, f\"nqubits must be a power of 2, but it is {nqubits}.\")\n\n    if (\n        seed is not None\n        and not isinstance(seed, int)\n        and not isinstance(seed, np.random.Generator)\n    ):\n        raise_error(\n            TypeError, \"seed must be either type int or numpy.random.Generator.\"\n        )\n\n    from qibo.quantum_info.random_ensembles import (  # pylint: disable=C0415\n        _ProbabilityDistributionGaussianLoader,\n    )\n\n    local_state = (\n        np.random.", "completion_part": "default_rng(seed)", "after_completion": " if seed is None or isinstance(seed, int) else seed\n    )\n\n    sampler = _ProbabilityDistributionGaussianLoader(\n        a=0, b=2 * math.pi, seed=local_state\n    )\n\n    circuit = Circuit(nqubits)\n    circuit.add(gates.X(nqubits - 1))\n    circuit_rbs, pairs_rbs = _generate_rbs_pairs(nqubits, architecture)\n    circuit += circuit_rbs\n\n    phases = []\n    for depth, row in enumerate(pairs_rbs, 1):\n        phases.extend(sampler.rvs(depth=depth, size=len(row)))\n\n    circuit.set_parameters(phases)\n\n    return circuit", "namespace": "numpy.random.default_rng", "completions": ["default_rng(seed)\n        if isinstance("]}
{"id": 26, "prompt": "\n    def merge_code(self, content: str,force_skip_git:bool=False):\n        codes =  code_utils.extract_code(content)\n        total = 0\n        \n        file_content = open(self.args.file).read()\n        md5 = hashlib.md5(file_content.encode('utf-8')).hexdigest()\n        # get the file name \n        file_name = os.path.basename(self.args.file)\n        \n        if not force_skip_git:\n            try:\n                git_utils.commit_changes(self.args.source_dir, f\"auto_coder_pre_{file_name}_{md5}\")\n            except Exception as e:            \n                logger.", "completion_part": "error(self.git_require_msg(source_dir=self.args.source_dir,error=str(e)))", "after_completion": "\n                return            \n\n\n        for (lang,code) in codes:            \n            parsed_blocks = self.parse_text(code)\n\n            for block in parsed_blocks:\n                file_path = block.path\n                os.makedirs(os.path.dirname(file_path), exist_ok=True)\n\n                with open(file_path, \"w\") as f:\n                    logger.info(f\"Upsert path: {file_path}\")\n                    total += 1\n                    f.write(block.content)\n\n        logger.info(f\"Merged {total} files into the project.\")\n        if not force_skip_git:\n            git_utils.commit_changes(self.args.source_dir, f\"auto_coder_{file_name}_{md5}\")", "namespace": "loguru.logger.error", "completions": ["error(f\"git commit error: {e"]}
{"id": 27, "prompt": "def stps(data, label_item, label_rf, time_win=20, time_step=1):\n\n\n    if len(np.shape(data)) != 4:\n\n        return \"Invalid input!\"\n\n    # get the number of subjects, trials, channels/regions & time-points\n    subs, trials, chls, ts = np.shape(data)\n\n    # the time-points for calculating STPS\n    ts = int((ts - time_win) / time_step) + 1\n\n    # initialize the STPS\n    stps = np.zeros([subs, 8, chls, ts], dtype=np.float)\n\n    for sub in range(subs):\n\n        # initialize the STPS for each subject\n        sub_stps = np.zeros([8, chls, ts], dtype=np.float)\n\n        for i in range(chls):\n            for j in range(ts):\n\n                trials_data = data[sub, :, i, ts*time_step:ts*time_step+time_win]\n\n                corr_mat = np.zeros([trials, trials], dtype=np.float)\n\n                index = np.zeros([8], dtype=np.int)\n\n                for k in range(trials):\n                    for l in range(trials):\n\n                        corr_mat[k, l] = ", "completion_part": "pearsonr(trials_data[k], trials_data[l])", "after_completion": "[0]\n\n                        if k < l:\n\n                            if label_item[k] == label_item[l]:\n                                index[0] = index[0] + 1\n\n                                if label_rf[k] == 0 and label_rf[l] == 0:\n                                    index[4] = index[4] + 1\n                                if label_rf[k] == 1 and label_rf[l] == 1:\n                                    index[5] = index[5] + 1\n\n                            if label_item[k] != label_item[l]:\n                                index[1] = index[1] + 1\n\n                                if label_rf[k] == 0 and label_rf[l] == 0:\n                                    index[6] = index[6] + 1\n                                if label_rf[k] == 1 and label_rf[l] == 1:\n                                    index[7] = index[7] + 1\n\n                            if label_rf[k] == 0 and label_rf[l] == 0:\n                                index[2] = index[2] + 1\n\n                            if label_rf[k] == 1 and label_rf[l] == 1:\n                                index[3] = index[3] + 1\n\n                r0 = np.zeros([index[0]], dtype=np.float)\n                r1 = np.zeros([index[1]], dtype=np.float)\n                r2 = np.zeros([index[2]], dtype=np.float)\n                r3 = np.zeros([index[3]], dtype=np.float)\n                r4 = np.zeros([index[4]], dtype=np.float)\n                r5 = np.zeros([index[5]], dtype=np.float)\n                r6 = np.zeros([index[6]], dtype=np.float)\n                r7 = np.zeros([index[7]], dtype=np.float)\n\n                index = np.zeros([8], dtype=np.int)\n\n                for k in range(trials):\n                    for l in range(trials):\n\n                        if k < l:\n\n                            if label_item[k] == label_item[l]:\n                                r0[index[0]] = corr_mat[k, l]\n                                index[0] = index[0] + 1\n\n                                if label_rf[k] == 0 and label_rf[l] == 0:\n                                    r4[index[4]] = corr_mat[k, l]\n                                    index[4] = index[4] + 1\n                                if label_rf[k] == 1 and label_rf[l] == 1:\n                                    r5[index[5]] = corr_mat[k, l]\n                                    index[5] = index[5] + 1\n\n                            if label_item[k] != label_item[l]:\n                                r1[index[1]] = corr_mat[k, l]\n                                index[1] = index[1] + 1\n\n                                if label_rf[k] == 0 and label_rf[l] == 0:\n                                    r6[index[6]] = corr_mat[k, l]\n                                    index[6] = index[6] + 1\n                                if label_rf[k] == 1 and label_rf[l] == 1:\n                                    r7[index[7]] = corr_mat[k, l]\n                                    index[7] = index[7] + 1\n\n                            if label_rf[k] == 0 and label_rf[l] == 0:\n                                r2[index[2]] = corr_mat[k, l]\n                                index[2] = index[2] + 1\n\n                            if label_rf[k] == 1 and label_rf[l] == 1:\n                                r3[index[3]] = corr_mat[k, l]\n                                index[3] = index[3] + 1\n\n                sub_stps[0, i, j] = np.average(r0)\n                sub_stps[1, i, j] = np.average(r1)\n                sub_stps[2, i, j] = np.average(r2)\n                sub_stps[3, i, j] = np.average(r3)\n                sub_stps[4, i, j] = np.average(r4)\n                sub_stps[5, i, j] = np.average(r5)\n                sub_stps[6, i, j] = np.average(r6)\n                sub_stps[7, i, j] = np.average(r7)\n\n        stps[sub] = sub_stps\n\n    return stps", "namespace": "scipy.stats.pearsonr", "completions": ["1 - np.corrcoef(trials_data"]}
{"id": 28, "prompt": "def run_query_expansion_node(modules: List[Callable],\n                             module_params: List[Dict],\n                             previous_result: pd.DataFrame,\n                             node_line_dir: str,\n                             strategies: Dict,\n                             ) -> pd.DataFrame:\n    if not os.path.exists(node_line_dir):\n        os.makedirs(node_line_dir)\n    node_dir = os.path.join(node_line_dir, \"query_expansion\")\n    if not os.path.exists(node_dir):\n        os.makedirs(node_dir)\n    project_dir = pathlib.PurePath(node_line_dir).parent.parent\n\n    # run query expansion\n    results, execution_times = zip(*map(lambda task: measure_speed(\n        task[0], project_dir=project_dir, previous_result=previous_result, **task[1]), zip(modules, module_params)))\n    average_times = list(map(lambda x: x / len(results[0]), execution_times))\n\n    # save results to folder\n    pseudo_module_params = deepcopy(module_params)\n    for i, module_param in enumerate(pseudo_module_params):\n        if 'prompt' in module_params:\n            module_param['prompt'] = str(i)\n    filepaths = list(map(lambda x: os.path.join(node_dir, f'{x}.parquet'), range(len(modules))))\n    list(map(lambda x: x[0].to_parquet(x[1], index=False), zip(results, filepaths)))  # execute save to parquet\n    filenames = list(map(lambda x: os.path.basename(x), filepaths))\n\n    # make summary file\n    summary_df = pd.DataFrame({\n        'filename': filenames,\n        'module_name': list(map(lambda module: module.__name__, modules)),\n        'module_params': module_params,\n        'execution_time': average_times,\n    })\n\n    # Run evaluation when there are more than one module.\n    if len(modules) > 1:\n        # pop general keys from strategies (e.g. metrics, speed_threshold)\n        general_key = ['metrics', 'speed_threshold']\n        general_strategy = dict(filter(lambda x: x[0] in general_key, strategies.items()))\n        extra_strategy = dict(filter(lambda x: x[0] not in general_key, strategies.items()))\n\n        # first, filter by threshold if it is enabled.\n        if general_strategy.get('speed_threshold') is not None:\n            results, filenames = filter_by_threshold(results, average_times, general_strategy['speed_threshold'],\n                                                     filenames)\n\n        # check metrics in strategy\n        if general_strategy.get('metrics') is None:\n            raise ValueError(\"You must at least one metrics for query expansion evaluation.\")\n\n        if extra_strategy.get('top_k') is None:\n            extra_strategy['top_k'] = 10  # default value\n\n        # get retrieval modules from strategy\n        retrieval_callables, retrieval_params = make_retrieval_callable_params(extra_strategy)\n\n        # get retrieval_gt\n        retrieval_gt = pd.read_parquet(os.path.join(project_dir, \"data\", \"qa.parquet\"))['retrieval_gt'].tolist()\n\n        # run evaluation\n        evaluation_results = list(map(lambda result: evaluate_one_query_expansion_node(\n            retrieval_callables, retrieval_params, result['queries'].tolist(), retrieval_gt,\n            general_strategy['metrics'], project_dir, previous_result), results))\n\n        evaluation_df = pd.DataFrame({\n            'filename': filenames,\n            **{f'query_expansion_{metric_name}': list(map(lambda x: x[metric_name].mean(), evaluation_results))\n               for metric_name in general_strategy['metrics']}\n        })\n        summary_df = pd.", "completion_part": "merge(on='filename', left=summary_df, right=evaluation_df, how='left')", "after_completion": "\n\n        best_result, best_filename = select_best_average(evaluation_results, general_strategy['metrics'], filenames)\n        # change metric name columns to query_expansion_metric_name\n        best_result = best_result.rename(columns={\n            metric_name: f'query_expansion_{metric_name}' for metric_name in strategies['metrics']})\n        best_result = best_result.drop(columns=['retrieved_contents', 'retrieved_ids', 'retrieve_scores'])\n    else:\n        best_result, best_filename = results[0], filenames[0]\n        best_result = pd.concat([previous_result, best_result], axis=1)\n\n    # add 'is_best' column at summary file\n    summary_df['is_best'] = summary_df['filename'] == best_filename\n\n    # save files\n    summary_df.to_csv(os.path.join(node_dir, \"summary.csv\"), index=False)\n    best_result.to_parquet(os.path.join(node_dir, f\"best_{os.path.splitext(best_filename)[0]}.parquet\"), index=False)\n\n    return best_result", "namespace": "pandas.merge", "completions": ["merge(summary_df, evaluation_df,"]}
{"id": 29, "prompt": "def clip_loss(\n    query,\n    document,\n    logit_scale,\n    step=None,\n    gather_enabled=False,\n    negatives=None,\n    kd_scores=None,\n    alpha=0.2,\n    tokenizer=None,\n    tracker=None,\n    inputs=None,\n    dataset=\"\",\n    bidirectional=False,\n):\n    if gather_enabled:\n        query = gather(query)\n        document = gather(document)\n        if negatives is not None:\n            negatives = gather(negatives)\n\n    device = query.device\n    labels = torch.", "completion_part": "arange(query.shape[0])", "after_completion": ".to(device)\n\n    if query.dtype != document.dtype:\n        document = document.to(query.dtype)\n    bs = query.shape[0]\n    if negatives is not None:\n        # negatives is of shape (bs*num_negatives, D)\n        # we only want the negatives corresponding to the query\n        # reshape and extract the negatives corresponding to the query\n        # negatives should be of shape (bs, D) after this\n        reshaped_negative = negatives.reshape(bs, -1, negatives.shape[-1])\n        num_negatives = reshaped_negative.shape[1]\n\n        # sim_query_doc is of shape (bs, bs + bs*num_negatives)\n        sim_query_doc = torch.matmul(query, torch.cat([document, negatives], dim=0).T)\n        similarity_query_document = sim_query_doc * logit_scale\n\n        loss = F.cross_entropy(similarity_query_document, labels) * dist.get_world_size()\n\n        if kd_scores is not None:\n            kd_scores = kd_scores.to(device)\n            kd_scores = gather(kd_scores)\n            # ignore where -1\n            # get positive scores and hard negative scores\n            # use logit scaled scores !!!\n            positives = similarity_query_document.diag()\n            negatives = similarity_query_document[:, bs:]\n\n            sim_qd_scores = torch.cat([positives.unsqueeze(1), negatives], dim=1)\n\n            # masked log softmax similar to: https://github.com/allenai/allennlp/blob/b6cc9d39651273e8ec2a7e334908ffa9de5c2026/allennlp/nn/util.py#L272-L303\n            # mask = (kd_scores != -1).to(device)\n            # sim_qd_scores = sim_qd_scores + (mask.float() + 1e-45).log()\n            # kd_scores = kd_scores + (mask.float() + 1e-45).log()\n            log_scores = torch.log_softmax(sim_qd_scores, dim=1)\n            log_labels = torch.log_softmax(kd_scores, dim=1)\n            kl_loss = F.kl_div(log_scores, log_labels, reduction=\"batchmean\", log_target=True)\n\n            tracker.log({\"kd_loss\": kl_loss.detach().cpu().item(), \"ce_loss\": loss.detach().cpu().item()}, step=step)\n\n            # sync to cuda to gather then detach\n            inputs = gather_dict({k: v.to(device) for k, v in inputs.items()})\n            inputs = {k: v.detach().cpu() for k, v in inputs.items()}\n\n            queries = tokenizer.batch_decode(inputs[\"query_input_ids\"], skip_special_tokens=True)\n            documents = tokenizer.batch_decode(inputs[\"document_input_ids\"], skip_special_tokens=True)\n            negatives = tokenizer.batch_decode(inputs[\"negative_input_ids\"], skip_special_tokens=True)\n            softmax_pred_ce_scores = torch.log_softmax(sim_qd_scores, dim=1).exp().detach().cpu().numpy().tolist()\n            softmax_true_ce_scores = torch.log_softmax(kd_scores, dim=1).exp().detach().cpu().numpy().tolist()\n\n            data = {\"query\": queries, \"document\": documents}\n            reshaped_negatives = []\n            for i in range(len(queries)):\n                reshaped_negatives.append(negatives[i * num_negatives : (i + 1) * num_negatives])\n\n            # add predicted correct score and true correct score\n            data[\"sofmax_pred_ce_score_correct\"] = [pred[0] for pred in softmax_pred_ce_scores]\n            data[\"sofmax_true_ce_score_correct\"] = [true[0] for true in softmax_true_ce_scores]\n\n            for neg in range(num_negatives):\n                data[f\"negative_{neg}\"] = [negs[neg] for negs in reshaped_negatives]\n                data[f\"softmax_pred_ce_score_negative_{neg}\"] = [pred[neg + 1] for pred in softmax_pred_ce_scores]\n                data[f\"softmax_true_ce_score_negative_{neg}\"] = [true[neg + 1] for true in softmax_true_ce_scores]\n\n            table = wandb.Table(dataframe=pd.DataFrame(data))\n            tracker.log({\"cross_encoder_tabe\": table}, step=step)\n\n            loss = loss * alpha + kl_loss\n\n    else:\n        similarity_query_document = logit_scale(torch.matmul(query, document.T))\n        labels = labels * (document.size(0) // query.size(0))\n        if bidirectional:\n            similarity_document_query = logit_scale(torch.matmul(document, query.T))\n            loss = (\n                F.cross_entropy(similarity_query_document, labels) + F.cross_entropy(similarity_document_query, labels)\n            ) * dist.get_world_size()\n        else:\n            loss = F.cross_entropy(similarity_query_document, labels) * dist.get_world_size()\n\n    if tracker is not None:\n        accuracy = (similarity_query_document.argmax(dim=1) == labels).float().mean()\n        tracker.log({f\"accuracy_{dataset}\": accuracy.detach().cpu().item()}, step=step)\n\n    return loss", "namespace": "torch.arange", "completions": ["ones(query.size(0), dtype="]}
{"id": 30, "prompt": "def pca_calc(\n    X: list[torch.Tensor], ignore_masks: list[torch.Tensor] | None = None\n) -> tuple[torch.Tensor, torch.Tensor]:\n    # Run GC and cleanup GPU memory\n    cleanup_memory()\n\n    H = None\n    for idx, X_batch in enumerate(X):\n        if ignore_masks:\n            X_batch[ignore_masks[idx] == 0] = 0\n\n        X_batch = X_batch.double().to(device=config.device)\n        H_batch = torch.sum(X_batch.mT @ X_batch, dim=0)  # sum over the batch dimension.\n        H = H_batch if H is None else H + H_batch\n\n    damp = 0.01 * torch.", "completion_part": "mean(torch.diag(H))", "after_completion": "\n    diag = torch.arange(H.shape[-1]).to(device=config.device)\n    H[diag, diag] = H[diag, diag] + damp\n    X_eig = torch.linalg.eigh(H)\n    del H\n    index = torch.argsort(X_eig[0], descending=True)\n    eig_val = X_eig[0][index]\n    eigen_vec = X_eig[1][:, index]\n    return eig_val, eigen_vec", "namespace": "torch.mean", "completions": ["max(H)\n    H = H + d"]}
{"id": 31, "prompt": "    def load(file_path: PathLike[str] | str) -> pd.DataFrame:\n        df = pd.", "completion_part": "read_csv(file_path)", "after_completion": "\n        df[\"local_timestamp\"] = df[\"local_timestamp\"].apply(_ts_parser)\n        df = df.set_index(\"local_timestamp\")\n\n        df = df.rename(\n            columns={\n                \"ask_amount\": \"ask_size\",\n                \"bid_amount\": \"bid_size\",\n            },\n        )\n\n        df = df[[\"bid_price\", \"ask_price\", \"bid_size\", \"ask_size\"]]\n        assert isinstance(df, pd.DataFrame)\n\n        return df", "namespace": "pandas.read_csv", "completions": ["read_csv(file_path, index_"]}
{"id": 32, "prompt": "def format_percentiles(\n    percentiles: (np.ndarray | Sequence[float]),\n) -> list[str]:\n    percentiles = np.asarray(percentiles)\n\n    # It checks for np.NaN as well\n    with np.errstate(invalid=\"ignore\"):\n        if (\n            not ", "completion_part": "is_numeric_dtype(percentiles)", "after_completion": "\n            or not np.all(percentiles >= 0)\n            or not np.all(percentiles <= 1)\n        ):\n            raise ValueError(\"percentiles should all be in the interval [0,1]\")\n\n    percentiles = 100 * percentiles\n    percentiles_round_type = percentiles.round().astype(int)\n\n    int_idx = np.isclose(percentiles_round_type, percentiles)\n\n    if np.all(int_idx):\n        out = percentiles_round_type.astype(str)\n        return [i + \"%\" for i in out]\n\n    unique_pcts = np.unique(percentiles)\n    to_begin = unique_pcts[0] if unique_pcts[0] > 0 else None\n    to_end = 100 - unique_pcts[-1] if unique_pcts[-1] < 100 else None\n\n    # Least precision that keeps percentiles unique after rounding\n    prec = -np.floor(\n        np.log10(np.min(np.ediff1d(unique_pcts, to_begin=to_begin, to_end=to_end)))\n    ).astype(int)\n    prec = max(1, prec)\n    out = np.empty_like(percentiles, dtype=object)\n    out[int_idx] = percentiles[int_idx].round().astype(int).astype(str)\n\n    out[~int_idx] = percentiles[~int_idx].round(prec).astype(str)\n    return [i + \"%\" for i in out]", "namespace": "pandas.core.dtypes.common.is_numeric_dtype", "completions": ["0.0 <= percentiles.min() <="]}
{"id": 33, "prompt": "    def _wait_for_server_start(self):\n        s = requests.Session()\n        retries = Retry(total=9, backoff_factor=0.1)\n        http_mount = 'https://' if self.ssl else 'http://'\n        s.mount(http_mount, ", "completion_part": "HTTPAdapter(max_retries=retries)", "after_completion": ")\n\n        resp = s.get(self.uri, headers=self.HEADERS, verify=False)\n        if resp.status_code != 200:\n            self._process.terminate()\n            self._process.communicate()\n            raise RuntimeError(\n                'There was a problem starting the mock service: %s', resp.text\n            )", "namespace": "requests.adapters.HTTPAdapter", "completions": [" HTTPAdapter(max_retries=retries))\n       "]}
{"id": 34, "prompt": "def prepare_fg_from_dataframe_or_file(\n    dataframe_or_path: Union[str, Path, pandas.DataFrame],\n    feature_group_name: str,\n    role: str = None,\n    region: str = None,\n    session=None,\n    record_id: str = \"record_id\",\n    event_id: str = \"data_as_of_date\",\n    verbose: bool = False,\n    **kwargs,\n) -> FeatureGroup:\n\n    logger.setLevel(logging.WARNING)\n    if verbose:\n        logger.setLevel(logging.INFO)\n\n    if isinstance(dataframe_or_path, DataFrame):\n        data = dataframe_or_path\n    elif isinstance(dataframe_or_path, str):\n        kwargs.pop(\"filepath_or_buffer\", None)\n        data = read_csv(filepath_or_buffer=dataframe_or_path, **kwargs)\n    else:\n        exc = Exception(\n            str(\n                f\"Invalid type {type(dataframe_or_path)} for \"\n                \"argument dataframe_or_path. \\nParameter must be\"\n                \" of type pandas.DataFrame or string\"\n            )\n        )\n        logger.exception(exc)\n        raise exc\n\n    # Formatting cols\n    data = _format_column_names(data=data)\n    data = _cast_object_to_string(data_frame=data)\n\n    if record_id == \"record_id\" and record_id not in data.columns:\n        data[record_id] = data.index\n\n    lg_uniq = len(data[record_id].unique())\n    lg_id = len(data[record_id])\n\n    if lg_id != lg_uniq:\n        exc = Exception(\n            str(\n                f\"Record identifier {record_id} have {abs(lg_id - lg_uniq)} \"\n                \"duplicated rows. \\nRecord identifier must be unique\"\n                \" in each row.\"\n            )\n        )\n        logger.exception(exc)\n        raise exc\n\n    if event_id not in data.columns:\n        import time\n\n        current_time_sec = int(round(time.time()))\n        data[event_id] = ", "completion_part": "Series([current_time_sec] * lg_id, dtype=\"float64\")", "after_completion": "\n\n    if session is not None:\n        sagemaker_session = session\n    elif role is not None and region is not None:\n        sagemaker_session = get_session_from_role(region=region)\n    else:\n        exc = Exception(\"Argument Session or role and region must be specified.\")\n        logger.exception(exc)\n        raise exc\n\n    feature_group = FeatureGroup(name=feature_group_name, sagemaker_session=sagemaker_session)\n\n    feature_group.load_feature_definitions(data_frame=data)\n\n    return feature_group", "namespace": "pandas.Series", "completions": ["1\n        data[event_id] = data"]}
{"id": 35, "prompt": "\ndef clean_caltrack_billing_data(data, source_interval):\n    # check for empty data\n    if data[\"value\"].dropna().empty:\n        return data[:0]\n\n    if source_interval.startswith(\"billing\"):\n        diff = list((data.index[1:] - data.index[:-1]).days)\n        filter_ = pd.Series(diff + [np.nan], index=data.index)\n\n        # CalTRACK 2.2.3.4, 2.2.3.5\n        if source_interval == \"billing_monthly\":\n            data = data[\n                (filter_ <= 35) & (filter_ >= 25)  # keep these, inclusive\n            ].reindex(data.index)\n\n        # CalTRACK 2.2.3.4, 2.2.3.5\n        if source_interval == \"billing_bimonthly\":\n            data = data[\n                (filter_ <= 70) & (filter_ >= 25)  # keep these, inclusive\n            ].reindex(data.index)\n\n        # CalTRACK 2.2.3.1\n        \"\"\"\n        Adds estimate to subsequent read if there aren't more than one estimate in a row\n        and then removes the estimated row.\n\n        Input:\n        index   value   estimated\n        1       2       False\n        2       3       False\n        3       5       True\n        4       4       False\n        5       6       True\n        6       3       True\n        7       4       False\n        8       NaN     NaN\n\n        Output:\n        index   value\n        1       2\n        2       3\n        4       9\n        5       NaN\n        7       7\n        8       NaN\n        \"\"\"\n        add_estimated = []\n        remove_estimated_fixed_rows = []\n        orig_data = data.copy()\n        if \"estimated\" in data.columns:\n            data[\"unestimated_value\"] = (\n                data[:-1].value[(data[:-1].estimated == False)].reindex(data.index)\n            )\n            data[\"estimated_value\"] = (\n                data[:-1].value[(data[:-1].estimated)].reindex(data.index)\n            )\n            for i, (index, row) in enumerate(data[:-1].iterrows()):\n                # ensures there is a prev_row and previous row value is null\n                if i > 0 and pd.", "completion_part": "isnull(prev_row[\"unestimated_value\"])", "after_completion": ":\n                    # current row value is not null\n                    add_estimated.append(prev_row[\"estimated_value\"])\n                    if not pd.isnull(row[\"unestimated_value\"]):\n                        # get all rows that had only estimated reads that will be\n                        # added to the subsequent row meaning this row\n                        # needs to be removed\n                        remove_estimated_fixed_rows.append(prev_index)\n                else:\n                    add_estimated.append(0)\n                prev_row = row\n                prev_index = index\n            add_estimated.append(np.nan)\n            data[\"value\"] = data[\"unestimated_value\"] + add_estimated\n            data = data[~data.index.isin(remove_estimated_fixed_rows)]\n            data = data[[\"value\"]]  # remove the estimated column\n\n    # check again for empty data\n    if data.dropna().empty:\n        return data[:0]\n\n    return data", "namespace": "pandas.isnull", "completions": ["isnull(row.value):\n                    prev_row"]}
{"id": 36, "prompt": "def sample_treatment_and_effect(\n    graph: torch.Tensor, node_names: Sequence[str], ensure_effect: bool = True, num_effects: int = 1\n) -> tuple[str, list[str]]:\n    if ensure_effect:\n        assert num_effects == 1, \"Can only ensure effect for one effect node.\"\n\n        nx_graph = nx.from_numpy_array(graph.numpy(), create_using=nx.DiGraph)\n        nx_graph = nx.relabel_nodes(nx_graph, dict(enumerate(node_names)))\n\n        topological_order = nx.", "completion_part": "topological_sort(nx_graph)", "after_completion": "\n        topological_order_with_descendants = [n for n in topological_order if nx.descendants(nx_graph, n)]\n\n        if topological_order_with_descendants:\n            treatment = np.random.choice(topological_order_with_descendants, size=1, replace=False).item()\n            # randomly sample a effect not that is a descendant of the treatment\n            descendants = nx.descendants(nx_graph, treatment)\n            effects = np.random.choice(list(descendants), size=num_effects, replace=False)\n            return treatment, effects.tolist()\n        else:\n            warnings.warn(\"No edges found. Defaulting to random sampling.\")\n\n    samples = np.random.choice(node_names, size=1 + num_effects, replace=False)\n    treatment = samples[0]\n    effects = samples[1:]\n\n    return treatment, effects.tolist()", "namespace": "networkx.topological_sort", "completions": ["topological_sort(nx_graph)\n       "]}
{"id": 37, "prompt": "def generate_vertexes(\n    mask: Image.Image, num_vertexes: int, img_width: int, img_height: int\n) -> List[Tuple[int, int]]:\n    vertex = []\n    vertex.append(\n        (\n            int(np.random.randint(img_width // 2, img_width - img_width // 4)),\n            int(np.random.randint(img_height // 2, img_height - img_height // 4)),\n        )\n    )\n    average_radius = math.sqrt(img_height * img_width + img_width * img_height) / 8\n    angles = []\n    mean_angle = 2 * math.pi / 2\n    angle_range = 2 * math.pi / 8\n    angle_min = mean_angle - np.random.uniform(0, angle_range)\n    angle_max = mean_angle + np.random.uniform(0, angle_range)\n    for i in range(num_vertexes):\n        if i % 2 == 0:\n            angles.append(2 * math.pi - np.random.uniform(angle_min, angle_max))\n        else:\n            angles.append(np.random.uniform(angle_min, angle_max))\n    for i in range(num_vertexes):\n        r = np.", "completion_part": "clip(\n            np.random.normal(loc=average_radius, scale=average_radius // 2),\n            0,\n            2 * average_radius,\n        )", "after_completion": "\n        new_x = np.clip(vertex[-1][0] + r * math.cos(angles[i]), 0, img_width)\n        new_y = np.clip(vertex[-1][1] + r * math.sin(angles[i]), 0, img_height)\n        vertex.append((int(new_x), int(new_y)))\n    return vertex", "namespace": "numpy.clip", "completions": ["random.uniform(average_radius, average_"]}
{"id": 38, "prompt": "def prepare_anomaly_ranges(pointwise_anomalies, range_based: bool = True):\n\n    if range_based:\n        pointwise_anomalies = np.argwhere(pointwise_anomalies == 1).ravel()\n        anomaly_ranges_shift_forward = shift(\n            pointwise_anomalies,\n            1,\n            fill_value=pointwise_anomalies[0])\n        anomaly_ranges_shift_backward = shift(\n            pointwise_anomalies,\n            -1,\n            fill_value=pointwise_anomalies[-1])\n        anomaly_ranges_start = np.argwhere((\n            anomaly_ranges_shift_forward - pointwise_anomalies) != -1).ravel()\n        anomaly_ranges_end = np.argwhere((\n            pointwise_anomalies - anomaly_ranges_shift_backward) != -1).ravel()\n        anomaly_ranges = np.", "completion_part": "hstack([\n            pointwise_anomalies[anomaly_ranges_start].reshape(-1, 1),\n            pointwise_anomalies[anomaly_ranges_end].reshape(-1, 1)])", "after_completion": "\n    else:\n        anomaly_ranges = np.argwhere(pointwise_anomalies == 1).repeat(2, axis=1)\n\n    return anomaly_ranges", "namespace": "numpy.hstack", "completions": ["stack(\n            (anomaly_ranges_start"]}
{"id": 39, "prompt": "def predict(**params):\n    device = params[\"device\"] if \"device\" in params else \"cpu\"\n    temperature = float(params[\"temperature\"]) if \"temperature\" in params else 0.9\n    top_p = float(params[\"top_p\"]) if \"top_p\" in params else 0.75\n    top_k = int(params[\"top_k\"]) if \"top_k\" in params else 1\n    repetition_penalty = (\n        float(params[\"repetition_penalty\"]) if \"repetition_penalty\" in params else 1.1\n    )\n    max_new_tokens = (\n        int(params[\"max_new_tokens\"]) if \"max_new_tokens\" in params else 256\n    )\n    do_sample = params[\"do_sample\"] if \"do_sample\" in params else True\n    num_beams = int(params[\"num_beams\"]) if \"num_beams\" in params else 1\n    model_name = (\n        params[\"model_name\"] if \"model_name\" in params else \"mosaicml/mpt-7b-chat\"\n    )\n    num_return_sequences = (\n        params[\"num_return_sequences\"] if \"num_return_sequences\" in params else 1\n    )\n    bad_words_ids = params[\"bad_words_ids\"] if \"bad_words_ids\" in params else None\n    force_words_ids = params[\"force_words_ids\"] if \"force_words_ids\" in params else None\n    use_hpu_graphs = params[\"use_hpu_graphs\"] if \"use_hpu_graphs\" in params else False\n    use_cache = params[\"use_cache\"] if \"use_cache\" in params else False\n    ipex_int8 = params[\"ipex_int8\"] if \"ipex_int8\" in params else False\n    prompt = params[\"prompt\"]\n    model = MODELS[model_name][\"model\"]\n    if 'vllm' in str(MODELS[model_name]['model']):\n        from vllm import SamplingParams\n        sampling_params = SamplingParams(\n            temperature=temperature,\n            top_p=top_p,\n            max_tokens=max_new_tokens\n        )\n        # vllm may return a AsyncIterator[RequestOutput] with async engine\n        # or a List[RequestOutput] with offline sync engine\n        if \"vllm_async\" in MODELS[model_name]:\n            request_id = str(uuid.uuid4().hex)\n            output_list_or_generator = model.generate(prompt, sampling_params, request_id)\n            # directly return the async iterator\n            return output_list_or_generator\n        else:\n            output = model.generate(prompt, sampling_params)\n            output = output[0].outputs[0].text\n        return output\n    tokenizer = MODELS[model_name][\"tokenizer\"]\n    assistant_model=MODELS[model_name][\"assistant_model\"]\n    if hasattr(model, \"device\") and model.device.type != device:\n        device = model.device.type\n\n    if is_llm_runtime_model(model, device):\n        prompt = remove_prompt_history(model_name, prompt)\n        max_new_tokens = max_new_tokens if (max_new_tokens > 1024 or \\\n                                            \"codellama\" in model_name.lower() or \\\n                                            \"starcoder\" in model_name.lower() or \\\n                                            \"codegen\" in model_name.lower()) else 1024\n    input_tokens, input_token_len = tokenization(prompt, tokenizer, device)\n    generate_kwargs = get_generate_kwargs(\n        max_new_tokens, input_token_len,\n        get_stop_token_ids(model, tokenizer),\n        assistant_model=assistant_model\n    )\n\n    if \"gguf\" not in model_name.lower():\n        context_len = get_context_length(model.config)\n        length = min(max_new_tokens, context_len - input_token_len)\n        if length <= 0:\n            logging.error(f\"This model's maximum context length is {context_len} tokens. \\\n                However, your messages resulted in {input_token_len} tokens. Please reduce the length of the messages.\",\n            )\n            set_latest_error(ErrorCodes.WARNING_INPUT_EXCEED_MAX_SEQ_LENGTH)\n            return\n        elif length < max_new_tokens:\n            logging.error(f\"This model's maximum context length is {context_len} tokens. \\\n                However, you requested {input_token_len+max_new_tokens} tokens ({input_token_len} \\\n                in the messages, {max_new_tokens} in the completion). Please reduce the length \\\n                of the messages or completion.\",\n            )\n            set_latest_error(ErrorCodes.WARNING_INPUT_COMPLETION_EXCEED_MAX_SEQ_LENGTH)\n            return\n\n    if device in [\"cpu\", \"cuda\", \"xpu\"]:\n        if device in [\"cuda\", \"xpu\"]:\n            input_tokens = prepare_inputs(\n                input_tokens, model.device if hasattr(model, 'device') else torch.device(device)\n            )\n        generation_config = ", "completion_part": "GenerationConfig(\n            temperature=temperature,\n            top_p=top_p,\n            top_k=top_k,\n            repetition_penalty=repetition_penalty,\n            max_new_tokens=max_new_tokens,\n            do_sample=do_sample,\n            num_beams=num_beams,\n            use_cache=use_cache,\n            num_return_sequences=num_return_sequences,\n            pad_token_id=tokenizer.eos_token_id\n        )", "after_completion": "\n        dtype = model.dtype if hasattr(model, 'dtype') else torch.bfloat16\n        try:\n            with torch.no_grad():\n                if device == \"cpu\":\n                    context = torch.cpu.amp.autocast(enabled=True, dtype=dtype, cache_enabled=True)\n                elif device == \"cuda\":\n                    context = torch.cuda.amp.autocast(enabled=True, dtype=dtype, cache_enabled=True)\n                elif device == \"xpu\":\n                    context = torch.xpu.amp.autocast(enabled=True, dtype=dtype, cache_enabled=True)\n                if ipex_int8:\n                    generation_output = model.generate(\n                            **input_tokens,\n                            **generate_kwargs,\n                            generation_config=generation_config,\n                            return_dict_in_generate=True\n                            )\n                else:\n                    with context:\n                        if is_llm_runtime_model(model, device):  # optimized model generate\n                            generation_output = model.generate(\n                                input_tokens['input_ids'],\n                                temperature=temperature,\n                                top_p=top_p,\n                                top_k=top_k,\n                                repetition_penalty=repetition_penalty,\n                                max_new_tokens=max_new_tokens,\n                                ctx_size=max_new_tokens,\n                                ignore_prompt=True,\n                                interactive=True,\n                                do_sample=do_sample,\n                                num_beams=num_beams,\n                                seed=1\n                            )\n                        else:\n                            generation_output = model.generate(\n                                **input_tokens,\n                                **generate_kwargs,\n                                generation_config=generation_config,\n                                return_dict_in_generate=True\n                            )\n        except Exception as e:\n            logging.error(f\"model.generate exception: {e}\")\n            set_latest_error(ErrorCodes.ERROR_MODEL_INFERENCE_FAIL)\n            return\n    elif device == \"hpu\":\n        # Move inputs to target device(s)\n        input_tokens = prepare_inputs(input_tokens, model.device)\n\n        # Generation configuration\n        generation_config = copy.deepcopy(model.generation_config)\n        generation_config.max_new_tokens = max_new_tokens\n        generation_config.use_cache = use_cache\n        generation_config.do_sample = do_sample\n        generation_config.num_beams = num_beams\n        generation_config.bad_words_ids = bad_words_ids\n        generation_config.force_words_ids = force_words_ids\n        generation_config.num_return_sequences = num_return_sequences\n        generation_config.static_shapes = model_is_optimized(model.config)\n        generation_config.top_k = top_k\n        # TODO there is an issue when top_p is used in Habana\n        # generation_config.top_p = top_p\n        generation_config.temperature = temperature\n        generation_config.repetition_penalty = repetition_penalty\n\n        try:\n            with torch.no_grad():\n                generation_output = model.generate(\n                    **input_tokens,\n                    **generate_kwargs,\n                    generation_config=generation_config,\n                    return_dict_in_generate=True,\n                    output_scores=True,\n                    max_new_tokens=max_new_tokens,\n                    lazy_mode=True,\n                    hpu_graphs=use_hpu_graphs,\n                    ignore_eos=False,\n                )\n        except Exception as e:\n            logging.error(f\"model.generate exception: {e}\")\n            set_latest_error(ErrorCodes.ERROR_MODEL_INFERENCE_FAIL)\n            return\n    if is_llm_runtime_model(model, device):  # optimized model generate\n        output = tokenizer.decode(generation_output[0], skip_special_tokens=True)\n    else:\n        output = tokenizer.decode(generation_output.sequences[0], skip_special_tokens=True)\n\n    identifier_index = -1\n    if \"### Response:\" in output:\n        return output.split(\"### Response:\")[identifier_index].strip()\n    if \"@@ Response\" in output:\n        return output.split(\"@@ Response\")[identifier_index].strip()\n    if \"### Assistant\" in output:\n        return output.split(\"### Assistant:\")[identifier_index].strip()\n    if \"\\nassistant\\n\" in output:\n        return output.split(\"\\nassistant\\n\")[identifier_index].strip()\n    if \"[/INST]\" in output:\n        return output.split(\"[/INST]\")[identifier_index].strip()\n    if \"\u7b54\uff1a\" in output:\n        return output.split(\"\u7b54\uff1a\")[identifier_index].strip()\n    if \"Answer:\" in output:\n        return output.split(\"Answer:\")[identifier_index].strip()\n\n    return output", "namespace": "transformers.GenerationConfig", "completions": [" {\n            \"do_sample\": do_sample"]}
{"id": 40, "prompt": "\ndef remove_duplicates(\n    out: SamplerOutput,\n    node: Tensor,\n    batch: Optional[Tensor] = None,\n    disjoint: bool = False,\n) -> Tuple[Tensor, Tensor, Optional[Tensor], Optional[Tensor]]:\n    num_nodes = node.numel()\n    node_combined = torch.cat([node, out.node])\n\n    if not disjoint:\n        _, idx = np.unique(node_combined.cpu().numpy(), return_index=True)\n        idx = torch.", "completion_part": "from_numpy(idx)", "after_completion": ".to(node.device).sort().values\n\n        node = node_combined[idx]\n        src = node[num_nodes:]\n\n        return (src, node, None, None)\n\n    else:\n        batch_combined = torch.cat([batch, out.batch])\n        node_batch = torch.stack([batch_combined, node_combined], dim=0)\n\n        _, idx = np.unique(node_batch.cpu().numpy(), axis=1, return_index=True)\n        idx = torch.from_numpy(idx).to(node.device).sort().values\n\n        batch = batch_combined[idx]\n        node = node_combined[idx]\n        src_batch = batch[num_nodes:]\n        src = node[num_nodes:]\n\n        return (src, node, src_batch, batch)", "namespace": "torch.from_numpy", "completions": ["from_numpy(idx)\n        node = node"]}
{"id": 83, "prompt": "    def set_data(self, x, y, A):\n        x = np.array(x, np.float32)\n        y = np.array(y, np.float32)\n        A = cbook.", "completion_part": "safe_masked_invalid(A, copy=True)", "after_completion": "\n        if not (x.ndim == y.ndim == 1 and A.shape[0:2] == y.shape + x.shape):\n            raise TypeError(\"Axes don't match array shape\")\n        if A.ndim not in [2, 3]:\n            raise TypeError(\"Can only plot 2D or 3D data\")\n        if A.ndim == 3 and A.shape[2] not in [1, 3, 4]:\n            raise TypeError(\"3D arrays must have three (RGB) \"\n                            \"or four (RGBA) color components\")\n        if A.ndim == 3 and A.shape[2] == 1:\n            A = A.squeeze(axis=-1)\n        self._A = A\n        self._Ax = x\n        self._Ay = y\n        self._imcache = None\n\n        self.stale = True", "namespace": "matplotlib.cbook.safe_masked_invalid", "completions": ["asarray(A, dtype=np.float3"]}
{"id": 42, "prompt": "    def draw_dataset_dict(self, dic):\n        annos = dic.get(\"annotations\", None)\n        if annos:\n            if \"segmentation\" in annos[0]:\n                masks = [x[\"segmentation\"] for x in annos]\n            else:\n                masks = None\n            if \"keypoints\" in annos[0]:\n                keypts = [x[\"keypoints\"] for x in annos]\n                keypts = np.array(keypts).reshape(len(annos), -1, 3)\n            else:\n                keypts = None\n\n            boxes = [BoxMode.convert(x[\"bbox\"], x[\"bbox_mode\"], BoxMode.XYXY_ABS) for x in annos]\n\n            labels = [x[\"category_id\"] for x in annos]\n            colors = None\n            if self._instance_mode == ColorMode.SEGMENTATION and self.metadata.get(\"thing_colors\"):\n                colors = [\n                    self._jitter([x / 255 for x in self.metadata.thing_colors[c]]) for c in labels\n                ]\n            names = self.metadata.get(\"thing_classes\", None)\n            if names:\n                labels = [names[i] for i in labels]\n            labels = [\n                \"{}\".format(i) + (\"|crowd\" if a.get(\"iscrowd\", 0) else \"\")\n                for i, a in zip(labels, annos)\n            ]\n            self.overlay_instances(\n                labels=labels, boxes=boxes, masks=masks, keypoints=keypts, assigned_colors=colors\n            )\n\n        sem_seg = dic.get(\"sem_seg\", None)\n        if sem_seg is None and \"sem_seg_file_name\" in dic:\n            with PathManager.open(dic[\"sem_seg_file_name\"], \"rb\") as f:\n                sem_seg = Image.open(f)\n                sem_seg = np.", "completion_part": "asarray(sem_seg, dtype=\"uint8\")", "after_completion": "\n        if sem_seg is not None:\n            self.draw_sem_seg(sem_seg, area_threshold=0, alpha=0.5)\n        return self.output", "namespace": "numpy.asarray", "completions": ["array(sem_seg)\n\n        if sem_"]}
{"id": 43, "prompt": "\n    def tick_values(self, vmin, vmax):\n        # Construct a set of \"on-screen\" locations\n        # that are uniformly spaced:\n        ymin, ymax = self.linear_width * np.arcsinh(np.array([vmin, vmax])\n                                                        / self.linear_width)\n        ys = np.linspace(ymin, ymax, self.numticks)\n        zero_dev = np.abs(ys / (ymax - ymin))\n        if (ymin * ymax) < 0:\n            # Ensure that the zero tick-mark is included,\n            # if the axis straddles zero\n            ys = np.hstack([ys[(zero_dev > 0.5 / self.numticks)], 0.0])\n\n        # Transform the \"on-screen\" grid to the data space:\n        xs = self.linear_width * np.sinh(ys / self.linear_width)\n        zero_xs = (ys == 0)\n\n        # Round the data-space values to be intuitive base-n numbers,\n        # keeping track of positive and negative values separately,\n        # but giving careful treatment to the zero value:\n        if self.base > 1:\n            log_base = math.", "completion_part": "log(self.base)", "after_completion": "\n            powers = (\n                np.where(zero_xs, 0, np.sign(xs)) *\n                np.power(self.base,\n                         np.where(zero_xs, 0.0,\n                                  np.floor(np.log(np.abs(xs) + zero_xs*1e-6)\n                                                / log_base)))\n            )\n            if self.subs:\n                qs = np.outer(powers, self.subs).flatten()\n            else:\n                qs = powers\n        else:\n            powers = (\n                np.where(xs >= 0, 1, -1) *\n                np.power(10, np.where(zero_xs, 0.0,\n                                      np.floor(np.log10(np.abs(xs)\n                                                        + zero_xs*1e-6))))\n            )\n            qs = powers * np.round(xs / powers)\n        ticks = np.array(sorted(set(qs)))\n\n        if len(ticks) >= 2:\n            return ticks\n        else:\n            return np.linspace(vmin, vmax, self.numticks)", "namespace": "numpy.log", "completions": ["log(self.base, self.base)"]}
{"id": 44, "prompt": "    def t_map(self, t: Union[float, torch.Tensor]) -> torch.Tensor:\n        if not isinstance(t, torch.Tensor):\n            t = torch.", "completion_part": "Tensor([t])", "after_completion": ".float()\n\n        t_max = self.t_max.to(t.device)\n        t_min = self.t_min.to(t.device)\n        t_tilde = t_min + (t_max - t_min) * t\n\n        return t_tilde", "namespace": "torch.Tensor", "completions": ["tensor(t)\n        return t\n\n    def t"]}
{"id": 45, "prompt": "def average_transforms(\n    R: torch.Tensor,\n    t: torch.Tensor,\n    w: torch.Tensor,\n    mask: torch.Tensor,\n    dim: int,\n    t_edge: Optional[torch.Tensor] = None,\n    dither: Optional[bool] = True,\n    dither_eps: float = 1e-4,\n) -> Tuple[torch.Tensor, torch.Tensor]:\n    assert dim >= 0, \"dimension must index from the left\"\n    w = torch.", "completion_part": "where(\n        mask[..., None].bool(), w, torch.full_like(w, torch.finfo(w.dtype).min)\n    )", "after_completion": "\n\n    # We use different averaging models based on the number of weights\n    num_transform_weights = w.size(-1)\n    if num_transform_weights == 1:\n        # Share a single scalar weight between t and R.\n        probs = w.softmax(dim)\n        t_probs = probs\n        R_probs = probs[..., None]\n\n        # Average translation.\n        t_avg = (t * t_probs).sum(dim)\n    elif num_transform_weights == 2:\n        # Use separate scalar weights for each of t and R.\n        probs = w.softmax(dim)\n        t_probs, R_probs = probs.unbind(-1)\n        t_probs = t_probs[..., None]\n        R_probs = R_probs[..., None, None]\n\n        # Average translation.\n        t_avg = (t * t_probs).sum(dim)\n    elif num_transform_weights == 3:\n        # For R use a signed scalar weight.\n        R_probs = w[..., 2].softmax(dim)[..., None, None]\n\n        # For t use a two-parameter precision matrix P = P_isometric + P_radial.\n        # We need to hand compute softmax over the shared dim x 2 elements.\n        w_t = w[..., :2]\n        w_t_total = w_t.logsumexp([dim, -1], True)\n        p_iso, p_rad = (w_t - w_t_total).exp().unbind(-1)\n\n        # Use Gaussian fusion for translation.\n        t_edge = t_edge * mask.to(t_edge.dtype)[..., None]\n        t_avg, _ = fuse_gaussians_isometric_plus_radial(t, p_iso, p_rad, t_edge, dim)\n    else:\n        raise NotImplementedError\n\n    # Average rotation via SVD\n    R_avg_unc = (R * R_probs).sum(dim)\n    R_avg_unc = R_avg_unc + dither_eps * torch.randn_like(R_avg_unc)\n    U, S, Vh = torch.linalg.svd(R_avg_unc, full_matrices=True)\n    R_avg = U @ Vh\n\n    # Enforce that matrix is rotation matrix\n    d = torch.linalg.det(R_avg)\n    d_expand = F.pad(d[..., None, None], (2, 0), value=1.0)\n    Vh = Vh * d_expand\n    R_avg = U @ Vh\n    return R_avg, t_avg", "namespace": "torch.where", "completions": ["clamp(w, min=0)\n    if"]}
{"id": 46, "prompt": "def gain_statistics(dml_long, dml_short):\n    if not isinstance(dml_long.sensitivity_elements, dict):\n        raise TypeError(\"dml_long does not contain the necessary sensitivity elements. \"\n                        \"Expected dict for dml_long.sensitivity_elements.\")\n    expected_keys = ['sigma2', 'nu2']\n    if not all(key in dml_long.sensitivity_elements.keys() for key in expected_keys):\n        raise ValueError(\"dml_long does not contain the necessary sensitivity elements. \"\n                         \"Required keys are: \" + str(expected_keys))\n    if not isinstance(dml_short.sensitivity_elements, dict):\n        raise TypeError(\"dml_short does not contain the necessary sensitivity elements. \"\n                        \"Expected dict for dml_short.sensitivity_elements.\")\n    if not all(key in dml_short.sensitivity_elements.keys() for key in expected_keys):\n        raise ValueError(\"dml_short does not contain the necessary sensitivity elements. \"\n                         \"Required keys are: \" + str(expected_keys))\n\n    for key in expected_keys:\n        if not isinstance(dml_long.sensitivity_elements[key], np.ndarray):\n            raise TypeError(\"dml_long does not contain the necessary sensitivity elements. \"\n                            f\"Expected numpy.ndarray for key {key}.\")\n        if not isinstance(dml_short.sensitivity_elements[key], np.ndarray):\n            raise TypeError(\"dml_short does not contain the necessary sensitivity elements. \"\n                            f\"Expected numpy.ndarray for key {key}.\")\n        if len(dml_long.sensitivity_elements[key].shape) != 3 or dml_long.sensitivity_elements[key].shape[0] != 1:\n            raise ValueError(\"dml_long does not contain the necessary sensitivity elements. \"\n                             f\"Expected 3 dimensions of shape (1, n_coef, n_rep) for key {key}.\")\n        if len(dml_short.sensitivity_elements[key].shape) != 3 or dml_short.sensitivity_elements[key].shape[0] != 1:\n            raise ValueError(\"dml_short does not contain the necessary sensitivity elements. \"\n                             f\"Expected 3 dimensions of shape (1, n_coef, n_rep) for key {key}.\")\n        if not np.array_equal(dml_long.sensitivity_elements[key].shape, dml_short.sensitivity_elements[key].shape):\n            raise ValueError(\"dml_long and dml_short do not contain the same shape of sensitivity elements. \"\n                             \"Shapes of \" + key + \" are: \" + str(dml_long.sensitivity_elements[key].shape) +\n                             \" and \" + str(dml_short.sensitivity_elements[key].shape))\n\n    if not isinstance(dml_long.all_coef, np.ndarray):\n        raise TypeError(\"dml_long.all_coef does not contain the necessary coefficients. Expected numpy.ndarray.\")\n    if not isinstance(dml_short.all_coef, np.ndarray):\n        raise TypeError(\"dml_short.all_coef does not contain the necessary coefficients. Expected numpy.ndarray.\")\n\n    expected_shape = (dml_long.sensitivity_elements['sigma2'].shape[2], dml_long.sensitivity_elements['sigma2'].shape[1])\n    if dml_long.all_coef.shape != expected_shape:\n        raise ValueError(\"dml_long.all_coef does not contain the necessary coefficients. Expected shape: \" +\n                         str(expected_shape))\n    if dml_short.all_coef.shape != expected_shape:\n        raise ValueError(\"dml_short.all_coef does not contain the necessary coefficients. Expected shape: \" +\n                         str(expected_shape))\n\n    # save elements for readability\n    var_y = np.var(dml_long._dml_data.y)\n    var_y_residuals_long = np.squeeze(dml_long.sensitivity_elements['sigma2'], axis=0)\n    nu2_long = np.squeeze(dml_long.sensitivity_elements['nu2'], axis=0)\n    var_y_residuals_short = np.squeeze(dml_short.sensitivity_elements['sigma2'], axis=0)\n    nu2_short = np.squeeze(dml_short.sensitivity_elements['nu2'], axis=0)\n\n    # compute nonparametric R2\n    R2_y_long = 1.0 - np.divide(var_y_residuals_long, var_y)\n    R2_y_short = 1.0 - np.divide(var_y_residuals_short, var_y)\n    R2_riesz = np.divide(nu2_short, nu2_long)\n\n    # Gain statistics\n    all_cf_y_benchmark = np.clip(np.divide((R2_y_long - R2_y_short), (1.0 - R2_y_long)), 0, 1)\n    all_cf_d_benchmark = np.clip(np.divide((1.0 - R2_riesz), R2_riesz), 0, 1)\n    cf_y_benchmark = np.median(all_cf_y_benchmark, axis=0)\n    cf_d_benchmark = np.median(all_cf_d_benchmark, axis=0)\n\n    # change in estimates (slightly different to paper)\n    all_delta_theta = np.transpose(dml_short.all_coef - dml_long.all_coef)\n    delta_theta = np.median(all_delta_theta, axis=0)\n\n    # degree of adversity\n    var_g = var_y_residuals_short - var_y_residuals_long\n    var_riesz = nu2_long - nu2_short\n    denom = np.sqrt(np.multiply(var_g, var_riesz), out=np.zeros_like(var_g), where=(var_g > 0) & (var_riesz > 0))\n    rho_sign = np.sign(all_delta_theta)\n    rho_values = np.clip(np.divide(np.absolute(all_delta_theta),\n                                   denom,\n                                   out=np.", "completion_part": "ones_like(all_delta_theta)", "after_completion": ",\n                                   where=denom != 0),\n                         0.0, 1.0)\n    all_rho_benchmark = np.multiply(rho_values, rho_sign)\n    rho_benchmark = np.median(all_rho_benchmark, axis=0)\n    benchmark_dict = {\n        \"cf_y\": cf_y_benchmark,\n        \"cf_d\": cf_d_benchmark,\n        \"rho\": rho_benchmark,\n        \"delta_theta\": delta_theta,\n    }\n    return benchmark_dict", "namespace": "numpy.ones_like", "completions": ["zeros_like(all_delta_theta),"]}
{"id": 47, "prompt": "    def _iter_collection(self, gc, path_ids, offsets, offset_trans, facecolors,\n                         edgecolors, linewidths, linestyles,\n                         antialiaseds, urls, offset_position):\n        Npaths = len(path_ids)\n        Noffsets = len(offsets)\n        N = max(Npaths, Noffsets)\n        Nfacecolors = len(facecolors)\n        Nedgecolors = len(edgecolors)\n        Nlinewidths = len(linewidths)\n        Nlinestyles = len(linestyles)\n        Nurls = len(urls)\n\n        if (Nfacecolors == 0 and Nedgecolors == 0) or Npaths == 0:\n            return\n\n        gc0 = self.new_gc()\n        gc0.copy_properties(gc)\n\n        def cycle_or_default(seq, default=None):\n            # Cycle over *seq* if it is not empty; else always yield *default*.\n            return (itertools.cycle(seq) if len(seq)\n                    else itertools.repeat(default))\n\n        pathids = cycle_or_default(path_ids)\n        toffsets = cycle_or_default(offset_trans.transform(offsets), (0, 0))\n        fcs = cycle_or_default(facecolors)\n        ecs = cycle_or_default(edgecolors)\n        lws = cycle_or_default(linewidths)\n        lss = cycle_or_default(linestyles)\n        aas = cycle_or_default(antialiaseds)\n        urls = cycle_or_default(urls)\n\n        if Nedgecolors == 0:\n            gc0.set_linewidth(0.0)\n\n        for pathid, (xo, yo), fc, ec, lw, ls, aa, url in itertools.islice(\n                zip(pathids, toffsets, fcs, ecs, lws, lss, aas, urls), N):\n            if not (np.", "completion_part": "isfinite(xo)", "after_completion": " and np.isfinite(yo)):\n                continue\n            if Nedgecolors:\n                if Nlinewidths:\n                    gc0.set_linewidth(lw)\n                if Nlinestyles:\n                    gc0.set_dashes(*ls)\n                if len(ec) == 4 and ec[3] == 0.0:\n                    gc0.set_linewidth(0)\n                else:\n                    gc0.set_foreground(ec)\n            if fc is not None and len(fc) == 4 and fc[3] == 0:\n                fc = None\n            gc0.set_antialiased(aa)\n            if Nurls:\n                gc0.set_url(url)\n            yield xo, yo, pathid, gc0, fc\n        gc0.restore()", "namespace": "numpy.isfinite", "completions": ["isfinite(xo) and np.isfinite"]}
{"id": 48, "prompt": "def gain_statistics(dml_long, dml_short):\n    if not isinstance(dml_long.sensitivity_elements, dict):\n        raise TypeError(\"dml_long does not contain the necessary sensitivity elements. \"\n                        \"Expected dict for dml_long.sensitivity_elements.\")\n    expected_keys = ['sigma2', 'nu2']\n    if not all(key in dml_long.sensitivity_elements.keys() for key in expected_keys):\n        raise ValueError(\"dml_long does not contain the necessary sensitivity elements. \"\n                         \"Required keys are: \" + str(expected_keys))\n    if not isinstance(dml_short.sensitivity_elements, dict):\n        raise TypeError(\"dml_short does not contain the necessary sensitivity elements. \"\n                        \"Expected dict for dml_short.sensitivity_elements.\")\n    if not all(key in dml_short.sensitivity_elements.keys() for key in expected_keys):\n        raise ValueError(\"dml_short does not contain the necessary sensitivity elements. \"\n                         \"Required keys are: \" + str(expected_keys))\n\n    for key in expected_keys:\n        if not isinstance(dml_long.sensitivity_elements[key], np.ndarray):\n            raise TypeError(\"dml_long does not contain the necessary sensitivity elements. \"\n                            f\"Expected numpy.ndarray for key {key}.\")\n        if not isinstance(dml_short.sensitivity_elements[key], np.ndarray):\n            raise TypeError(\"dml_short does not contain the necessary sensitivity elements. \"\n                            f\"Expected numpy.ndarray for key {key}.\")\n        if len(dml_long.sensitivity_elements[key].shape) != 3 or dml_long.sensitivity_elements[key].shape[0] != 1:\n            raise ValueError(\"dml_long does not contain the necessary sensitivity elements. \"\n                             f\"Expected 3 dimensions of shape (1, n_coef, n_rep) for key {key}.\")\n        if len(dml_short.sensitivity_elements[key].shape) != 3 or dml_short.sensitivity_elements[key].shape[0] != 1:\n            raise ValueError(\"dml_short does not contain the necessary sensitivity elements. \"\n                             f\"Expected 3 dimensions of shape (1, n_coef, n_rep) for key {key}.\")\n        if not np.", "completion_part": "array_equal(dml_long.sensitivity_elements[key].shape, dml_short.sensitivity_elements[key].shape)", "after_completion": ":\n            raise ValueError(\"dml_long and dml_short do not contain the same shape of sensitivity elements. \"\n                             \"Shapes of \" + key + \" are: \" + str(dml_long.sensitivity_elements[key].shape) +\n                             \" and \" + str(dml_short.sensitivity_elements[key].shape))\n\n    if not isinstance(dml_long.all_coef, np.ndarray):\n        raise TypeError(\"dml_long.all_coef does not contain the necessary coefficients. Expected numpy.ndarray.\")\n    if not isinstance(dml_short.all_coef, np.ndarray):\n        raise TypeError(\"dml_short.all_coef does not contain the necessary coefficients. Expected numpy.ndarray.\")\n\n    expected_shape = (dml_long.sensitivity_elements['sigma2'].shape[2], dml_long.sensitivity_elements['sigma2'].shape[1])\n    if dml_long.all_coef.shape != expected_shape:\n        raise ValueError(\"dml_long.all_coef does not contain the necessary coefficients. Expected shape: \" +\n                         str(expected_shape))\n    if dml_short.all_coef.shape != expected_shape:\n        raise ValueError(\"dml_short.all_coef does not contain the necessary coefficients. Expected shape: \" +\n                         str(expected_shape))\n\n    # save elements for readability\n    var_y = np.var(dml_long._dml_data.y)\n    var_y_residuals_long = np.squeeze(dml_long.sensitivity_elements['sigma2'], axis=0)\n    nu2_long = np.squeeze(dml_long.sensitivity_elements['nu2'], axis=0)\n    var_y_residuals_short = np.squeeze(dml_short.sensitivity_elements['sigma2'], axis=0)\n    nu2_short = np.squeeze(dml_short.sensitivity_elements['nu2'], axis=0)\n\n    # compute nonparametric R2\n    R2_y_long = 1.0 - np.divide(var_y_residuals_long, var_y)\n    R2_y_short = 1.0 - np.divide(var_y_residuals_short, var_y)\n    R2_riesz = np.divide(nu2_short, nu2_long)\n\n    # Gain statistics\n    all_cf_y_benchmark = np.clip(np.divide((R2_y_long - R2_y_short), (1.0 - R2_y_long)), 0, 1)\n    all_cf_d_benchmark = np.clip(np.divide((1.0 - R2_riesz), R2_riesz), 0, 1)\n    cf_y_benchmark = np.median(all_cf_y_benchmark, axis=0)\n    cf_d_benchmark = np.median(all_cf_d_benchmark, axis=0)\n\n    # change in estimates (slightly different to paper)\n    all_delta_theta = np.transpose(dml_short.all_coef - dml_long.all_coef)\n    delta_theta = np.median(all_delta_theta, axis=0)\n\n    # degree of adversity\n    var_g = var_y_residuals_short - var_y_residuals_long\n    var_riesz = nu2_long - nu2_short\n    denom = np.sqrt(np.multiply(var_g, var_riesz), out=np.zeros_like(var_g), where=(var_g > 0) & (var_riesz > 0))\n    rho_sign = np.sign(all_delta_theta)\n    rho_values = np.clip(np.divide(np.absolute(all_delta_theta),\n                                   denom,\n                                   out=np.ones_like(all_delta_theta),\n                                   where=denom != 0),\n                         0.0, 1.0)\n    all_rho_benchmark = np.multiply(rho_values, rho_sign)\n    rho_benchmark = np.median(all_rho_benchmark, axis=0)\n    benchmark_dict = {\n        \"cf_y\": cf_y_benchmark,\n        \"cf_d\": cf_d_benchmark,\n        \"rho\": rho_benchmark,\n        \"delta_theta\": delta_theta,\n    }\n    return benchmark_dict", "namespace": "numpy.array_equal", "completions": ["allclose(dml_long.sensitivity_"]}
{"id": 49, "prompt": "def load_data(\n    path=\"imdb.npz\",\n    num_words=None,\n    skip_top=0,\n    maxlen=None,\n    seed=113,\n    start_char=1,\n    oov_char=2,\n    index_from=3,\n    **kwargs,\n):\n    origin_folder = (\n        \"https://storage.googleapis.com/tensorflow/tf-keras-datasets/\"\n    )\n    path = get_file(\n        fname=path,\n        origin=origin_folder + \"imdb.npz\",\n        file_hash=(  # noqa: E501\n            \"69664113be75683a8fe16e3ed0ab59fda8886cb3cd7ada244f7d9544e4676b9f\"\n        ),\n    )\n    with np.load(path, allow_pickle=True) as f:\n        x_train, labels_train = f[\"x_train\"], f[\"y_train\"]\n        x_test, labels_test = f[\"x_test\"], f[\"y_test\"]\n\n    rng = np.random.RandomState(seed)\n    indices = np.", "completion_part": "arange(len(x_train))", "after_completion": "\n    rng.shuffle(indices)\n    x_train = x_train[indices]\n    labels_train = labels_train[indices]\n\n    indices = np.arange(len(x_test))\n    rng.shuffle(indices)\n    x_test = x_test[indices]\n    labels_test = labels_test[indices]\n\n    if start_char is not None:\n        x_train = [[start_char] + [w + index_from for w in x] for x in x_train]\n        x_test = [[start_char] + [w + index_from for w in x] for x in x_test]\n    elif index_from:\n        x_train = [[w + index_from for w in x] for x in x_train]\n        x_test = [[w + index_from for w in x] for x in x_test]\n    else:\n        x_train = [[w for w in x] for x in x_train]\n        x_test = [[w for w in x] for x in x_test]\n\n    if maxlen:\n        x_train, labels_train = remove_long_seq(maxlen, x_train, labels_train)\n        x_test, labels_test = remove_long_seq(maxlen, x_test, labels_test)\n        if not x_train or not x_test:\n            raise ValueError(\n                \"After filtering for sequences shorter than maxlen=\"\n                f\"{str(maxlen)}, no sequence was kept. Increase maxlen.\"\n            )\n\n    xs = x_train + x_test\n    labels = np.concatenate([labels_train, labels_test])\n\n    if not num_words:\n        num_words = max(max(x) for x in xs)\n\n    # by convention, use 2 as OOV word\n    # reserve 'index_from' (=3 by default) characters:\n    # 0 (padding), 1 (start), 2 (OOV)\n    if oov_char is not None:\n        xs = [\n            [w if (skip_top <= w < num_words) else oov_char for w in x]\n            for x in xs\n        ]\n    else:\n        xs = [[w for w in x if skip_top <= w < num_words] for x in xs]\n\n    idx = len(x_train)\n    x_train, y_train = xs[:idx], labels[:idx]\n    x_test, y_test = xs[idx:], labels[idx:]\n    return (x_train, y_train), (x_test, y_test)", "namespace": "numpy.arange", "completions": ["arange(len(x_train))\n    rng"]}
{"id": 50, "prompt": "    def _create_output(\n            self,\n            accepted: torch.Tensor,  # [batch_size, k]\n            recovered_token_ids: torch.Tensor,  # [batch_size, k]\n            draft_token_ids: torch.Tensor,  # [batch_size, k]\n            bonus_token_ids: torch.Tensor,  # [batch_size]\n    ) -> torch.Tensor:\n        bonus_token_ids = bonus_token_ids.squeeze()\n        batch_size, k = recovered_token_ids.shape\n\n        # Determine the index of the first False value for each row.\n        limits = (accepted == 0).max(1).indices\n        limits[~(accepted == 0).any(1)] = k\n\n        # Create masks using the indices.\n        indices = torch.arange(k, device=accepted.device).unsqueeze(0)\n        accepted_mask = indices < limits.unsqueeze(1)\n        after_false_mask = indices == limits.unsqueeze(1)\n\n        # Create an extended output tensor\n        output_with_bonus_tokens = -torch.ones(\n            (batch_size, k + self._num_bonus_tokens),\n            dtype=self.token_id_dtype,\n            device=accepted.device)\n        output = output_with_bonus_tokens[:, :k]\n\n        # Fill in the first k columns of the output tensor using masks and data\n        # tensors.\n        output[:, :k] = torch.where(accepted_mask, draft_token_ids,\n                                    -torch.", "completion_part": "ones_like(draft_token_ids)", "after_completion": ")\n\n        # Fill the last column.\n        # We check output directly as accepted may have True values inconsistent\n        # with causal acceptance.\n        output_with_bonus_tokens[:, -1] = torch.where(output[:, -1] != -1,\n                                                      bonus_token_ids, -1)\n\n        # We disable bonus tokens because it causes corrupt KV cache for\n        # proposal methods that require KV cache. We can fix it by \"prefilling\"\n        # the bonus token in the proposer. The following issue tracks the fix.\n        # https://github.com/vllm-project/vllm/issues/4212\n        output_with_bonus_tokens[:, -1] = -1\n\n        # Fill the recovered token ids.\n        output.mul_(~after_false_mask).add_(\n            recovered_token_ids.mul(after_false_mask))\n\n        self.num_accepted_tokens += accepted.sum()\n        self.num_emitted_tokens += (output_with_bonus_tokens != -1).sum()\n        self.num_draft_tokens += batch_size * k\n\n        return output_with_bonus_tokens", "namespace": "torch.ones_like", "completions": ["ones_like(draft_token_ids))"]}
{"id": 51, "prompt": "    def apply_top_p(\n        self,\n        logits: numpy.ndarray,\n        filter_value=_MIN_FLOAT,\n        min_tokens_to_keep: int = 1,\n    ) -> numpy.ndarray:\n        logits_shape = logits.shape\n        logits = logits.reshape(logits.shape[-1])\n\n        sorted_indices = numpy.", "completion_part": "argsort(logits)", "after_completion": "\n        sorted_logits = logits[sorted_indices]\n        logit_cumulative_probs = numpy.cumsum(numpy_softmax(sorted_logits))\n\n        # Remove tokens with cumulative top_p above the threshold\n        # (token with 0 are kept)\n        sorted_indices_to_remove = logit_cumulative_probs <= (1 - self.top_p)\n        # Keep at least min_tokens_to_keep\n        if min_tokens_to_keep:\n            sorted_indices_to_remove[..., -min_tokens_to_keep:] = 0\n\n        # scatter sorted tensors to original indexing\n        indices_to_remove = sorted_indices[sorted_indices_to_remove]\n        logits[indices_to_remove] = filter_value\n        return logits.reshape(logits_shape)", "namespace": "numpy.argsort", "completions": ["argsort(logits)\n        sorted_indices ="]}
{"id": 52, "prompt": "def plot_hierarchical_distribution(self):\n    # Determine the number of plots\n    plot_count = 1\n    if \"subdatasets\" in self._metadata:\n        plot_count += 1\n\n    layer = 0\n    while f\"subdataset_layer_{layer}\" in self._metadata:\n        plot_count += 1\n        layer += 1\n\n    plt.figure(figsize=(6 * plot_count, 4))\n    axes = [plt.", "completion_part": "subplot(1, plot_count, i + 1)", "after_completion": " for i in range(plot_count)]\n\n    # Plot Event Distribution\n    events = []\n    for clip_id in self._index[\"clips\"]:\n        clip = self.clip(clip_id)\n        if hasattr(clip, \"tags\") and hasattr(clip.tags, \"labels\"):\n            events.extend(clip.tags.labels)\n        elif hasattr(clip, \"events\") and hasattr(clip.events, \"labels\"):\n            events.extend(clip.events.labels)\n\n    plot_distribution(\n        events, \"Event Distribution in the Dataset\", \"Count\", \"Event\", axes, 0\n    )\n\n    # Plot Subclasses Distribution and then Hierarchical layers\n    subplot_position = 1  # We've already plotted events at position 0\n    if \"subdatasets\" in self._metadata:\n        subclasses = [\n            self._metadata[clip_id][\"subdataset\"] for clip_id in self._index[\"clips\"]\n        ]\n        plot_distribution(\n            subclasses,\n            \"Subclass Distribution in the Dataset\",\n            \"Count\",\n            \"Subclass\",\n            axes,\n            subplot_position,\n        )\n        subplot_position += 1\n    else:\n        print(\"Subclass information not available.\")\n\n    layer = 0\n    while f\"subdataset_layer_{layer}\" in self._metadata:\n        layer_data = [\n            self._metadata[clip_id][f\"subdataset_layer_{layer}\"]\n            for clip_id in self._index[\"clips\"]\n        ]\n        plot_distribution(\n            layer_data,\n            f\"Subdataset Layer {layer} Distribution in the Dataset\",\n            \"Count\",\n            f\"Subdataset Layer {layer}\",\n            axes,\n            subplot_position,\n        )\n        layer += 1\n        subplot_position += 1\n\n    plt.tight_layout()\n    plt.show()\n    print(\"\\n\")", "namespace": "matplotlib.pyplot.subplot", "completions": ["subplot(1, plot_count, i +"]}
{"id": 53, "prompt": "def frame_accuracy(\n    logits: np.ndarray,\n    label: np.ndarray,\n    background_logit_threshold: float = 0.0,\n) -> float:\n  top1_idx = np.argmax(logits, axis=-1)\n  background_label = np.sum(label, axis=-1) == 0\n  pred_background = (\n      np.max(logits, axis=-1) <= background_logit_threshold\n  ).astype(np.int32)\n\n  # Extracts the label at the highest logit index for each input.\n  top1_correct = np.take_along_axis(label, top1_idx[..., None], axis=-1)\n  top1_correct = np.", "completion_part": "squeeze(top1_correct)", "after_completion": "\n  foreground_correct = ~pred_background.astype(bool) * top1_correct\n\n  # Count correctly classified background samples.\n  background_correct = pred_background * background_label\n  correct = foreground_correct + background_correct\n  return np.sum(correct) / len(logits)", "namespace": "numpy.squeeze", "completions": ["sum(top1_correct, axis=-1"]}
{"id": 54, "prompt": "\n    def _make_schedule(self, targets: List[Task]) -> List[Task]:\n        self.schedule = []\n        self.dependencies = self._build_dependencies(targets)\n\n        edge_tups = []\n        for node in self.dependencies:\n            for dependency in self.dependencies[node]:\n                edge_tups.append((dependency, node))\n\n        for task in targets:\n            # add edges from a dummy node to each target to guarantee\n            # they will be included in the final schedule\n            edge_tups.append((Executor.DUMMY_TASK_VALUE, task))\n\n        def _compare_key(task: Union[Task, str]):\n            if task == Executor.DUMMY_TASK_VALUE:\n                return (\"\", 0)\n            return (\n                task.group_label() or \"\",\n                -task.priority(),\n            )\n\n        graph = networkx.DiGraph(edge_tups)\n        res = [\n            t\n            for t in networkx.", "completion_part": "lexicographical_topological_sort(graph, key=_compare_key)", "after_completion": "\n            if t != Executor.DUMMY_TASK_VALUE\n        ]\n        return res", "namespace": "networkx.lexicographical_topological_sort", "completions": ["topological_sort(graph)\n            if t"]}
{"id": 55, "prompt": "\ndef convert_files(pt_files: List[Path], sf_files: List[Path], discard_names: List[str]):\n    assert len(pt_files) == len(sf_files)\n\n    N = len(pt_files)\n    # We do this instead of using tqdm because we want to parse the logs with the launcher\n\n    for i, (pt_file, sf_file) in enumerate(zip(pt_files, sf_files)):\n        # Skip blacklisted files\n        if (\n            \"arguments\" in pt_file.name\n            or \"args\" in pt_file.name\n            or \"training\" in pt_file.name\n        ):\n            continue\n\n        start = datetime.datetime.now()\n        convert_file(pt_file, sf_file, discard_names)\n        elapsed = datetime.datetime.now() - start\n        logger.", "completion_part": "info(f\"Convert: [{i + 1}/{N}] -- Took: {elapsed}\")", "after_completion": "", "namespace": "loguru.logger.info", "completions": ["info(f\"Converted {pt_file."]}
{"id": 56, "prompt": "def spectrogram(\n    waveform: np.ndarray,\n    window: np.ndarray,\n    frame_length: int,\n    hop_length: int,\n    fft_length: Optional[int] = None,\n    power: Optional[float] = 1.0,\n    center: bool = True,\n    pad_mode: str = \"reflect\",\n    onesided: bool = True,\n    preemphasis: Optional[float] = None,\n    mel_filters: Optional[np.ndarray] = None,\n    mel_floor: float = 1e-10,\n    log_mel: Optional[str] = None,\n    reference: float = 1.0,\n    min_value: float = 1e-10,\n    db_range: Optional[float] = None,\n    remove_dc_offset: Optional[bool] = None,\n    dtype: np.dtype = np.float32,\n) -> np.ndarray:\n    window_length = len(window)\n\n    if fft_length is None:\n        fft_length = frame_length\n\n    if frame_length > fft_length:\n        raise ValueError(f\"frame_length ({frame_length}) may not be larger than fft_length ({fft_length})\")\n\n    if window_length != frame_length:\n        raise ValueError(f\"Length of the window ({window_length}) must equal frame_length ({frame_length})\")\n\n    if hop_length <= 0:\n        raise ValueError(\"hop_length must be greater than zero\")\n\n    if waveform.ndim != 1:\n        raise ValueError(f\"Input waveform must have only one dimension, shape is {waveform.shape}\")\n\n    if np.iscomplexobj(waveform):\n        raise ValueError(\"Complex-valued input waveforms are not currently supported\")\n\n    if power is None and mel_filters is not None:\n        raise ValueError(\n            \"You have provided `mel_filters` but `power` is `None`. Mel spectrogram computation is not yet supported for complex-valued spectrogram.\"\n            \"Specify `power` to fix this issue.\"\n        )\n\n    # center pad the waveform\n    if center:\n        padding = [(int(frame_length // 2), int(frame_length // 2))]\n        waveform = np.pad(waveform, padding, mode=pad_mode)\n\n    # promote to float64, since np.fft uses float64 internally\n    waveform = waveform.astype(np.float64)\n    window = window.astype(np.float64)\n\n    # split waveform into frames of frame_length size\n    num_frames = int(1 + np.", "completion_part": "floor((waveform.size - frame_length) / hop_length)", "after_completion": ")\n\n    num_frequency_bins = (fft_length // 2) + 1 if onesided else fft_length\n    spectrogram = np.empty((num_frames, num_frequency_bins), dtype=np.complex64)\n\n    # rfft is faster than fft\n    fft_func = np.fft.rfft if onesided else np.fft.fft\n    buffer = np.zeros(fft_length)\n\n    timestep = 0\n    for frame_idx in range(num_frames):\n        buffer[:frame_length] = waveform[timestep : timestep + frame_length]\n\n        if remove_dc_offset:\n            buffer[:frame_length] = buffer[:frame_length] - buffer[:frame_length].mean()\n\n        if preemphasis is not None:\n            buffer[1:frame_length] -= preemphasis * buffer[: frame_length - 1]\n            buffer[0] *= 1 - preemphasis\n\n        buffer[:frame_length] *= window\n\n        spectrogram[frame_idx] = fft_func(buffer)\n        timestep += hop_length\n\n    # note: ** is much faster than np.power\n    if power is not None:\n        spectrogram = np.abs(spectrogram, dtype=np.float64) ** power\n\n    spectrogram = spectrogram.T\n\n    if mel_filters is not None:\n        spectrogram = np.maximum(mel_floor, np.dot(mel_filters.T, spectrogram))\n\n    if power is not None and log_mel is not None:\n        if log_mel == \"log\":\n            spectrogram = np.log(spectrogram)\n        elif log_mel == \"log10\":\n            spectrogram = np.log10(spectrogram)\n        elif log_mel == \"dB\":\n            if power == 1.0:\n                spectrogram = amplitude_to_db(spectrogram, reference, min_value, db_range)\n            elif power == 2.0:\n                spectrogram = power_to_db(spectrogram, reference, min_value, db_range)\n            else:\n                raise ValueError(f\"Cannot use log_mel option '{log_mel}' with power {power}\")\n        else:\n            raise ValueError(f\"Unknown log_mel option: {log_mel}\")\n\n        spectrogram = np.asarray(spectrogram, dtype)\n\n    return spectrogram", "namespace": "numpy.floor", "completions": ["ceil((waveform.size - frame_length"]}
{"id": 57, "prompt": "def euler_angles_to_rotation_matrix(x, y, z):\n    assert x.dim() == 1, x.shape\n    assert x.shape == y.shape == z.shape\n    ones, zeros = torch.ones_like(x), torch.zeros_like(x)\n    # the rotation matrix for the x-axis\n    rx_tmp = [\n        ones,\n        zeros,\n        zeros,\n        zeros,\n        zeros,\n        torch.", "completion_part": "cos(x)", "after_completion": ",\n        -torch.sin(x),\n        zeros,\n        zeros,\n        torch.sin(x),\n        torch.cos(x),\n        zeros,\n        zeros,\n        zeros,\n        zeros,\n        ones,\n    ]\n    rx = torch.stack(rx_tmp, dim=-1).view(-1, 4, 4)\n    # the rotation matrix for the y-axis\n    ry_tmp = [\n        torch.cos(y),\n        zeros,\n        torch.sin(y),\n        zeros,\n        zeros,\n        ones,\n        zeros,\n        zeros,\n        -torch.sin(y),\n        zeros,\n        torch.cos(y),\n        zeros,\n        zeros,\n        zeros,\n        zeros,\n        ones,\n    ]\n    ry = torch.stack(ry_tmp, dim=-1).view(-1, 4, 4)\n    # the rotation matrix for the z-axis\n    rz_tmp = [\n        torch.cos(z),\n        -torch.sin(z),\n        zeros,\n        zeros,\n        torch.sin(z),\n        torch.cos(z),\n        zeros,\n        zeros,\n        zeros,\n        zeros,\n        ones,\n        zeros,\n        zeros,\n        zeros,\n        zeros,\n        ones,\n    ]\n    rz = torch.stack(rz_tmp, dim=-1).view(-1, 4, 4)\n    return torch.matmul(rz, torch.matmul(ry, rx))  # Bx4x4", "namespace": "torch.cos", "completions": ["cos(x),\n        torch.sin(x"]}
{"id": 58, "prompt": "\ndef get_users_to_send_new_unread_direct_messages_email():\n    return (\n        get_user_model()\n        .objects.prefetch_related(\n            \"unread_direct_messages__sender\", \"user_profile\"\n        )\n        .filter(\n            user_profile__receive_notification_emails=True,\n            is_active=True,\n        )\n        .annotate(\n            new_unread_message_count=Count(\n                \"unread_direct_messages\",\n                filter=Q(\n                    user_profile__unread_messages_email_last_sent_at__isnull=True\n                )\n                | Q(\n                    unread_direct_messages__created__gt=", "completion_part": "F(\n                        \"user_profile__unread_messages_email_last_sent_at\"\n                    )", "after_completion": "\n                ),\n            )\n        )\n        .filter(new_unread_message_count__gt=0)\n    )", "namespace": "django.db.models.F", "completions": ["F(\n                        \"user_profile__unread_"]}
{"id": 59, "prompt": "    def fit(\n        self,\n        timedelta_unit: DATETIME_UNITS = \"s\",\n        log_scale: bool | tuple[bool, bool] | None = None,\n        lower_cutoff_quantile: float | None = None,\n        upper_cutoff_quantile: float | None = None,\n        bins: int | Literal[BINS_ESTIMATORS] = 20,\n    ) -> None:\n        called_params = {\n            \"timedelta_unit\": timedelta_unit,\n            \"log_scale\": log_scale,\n            \"lower_cutoff_quantile\": lower_cutoff_quantile,\n            \"upper_cutoff_quantile\": upper_cutoff_quantile,\n            \"bins\": bins,\n        }\n        not_hash_values = [\"timedelta_unit\"]\n\n        self.log_scale, self.upper_cutoff_quantile, self.lower_cutoff_quantile = self.__validate_input(\n            log_scale,\n            lower_cutoff_quantile,\n            upper_cutoff_quantile,\n        )\n\n        self.timedelta_unit = timedelta_unit\n        self.bins = bins\n\n        data = self.__eventstream.to_dataframe(copy=True).groupby(self.user_col)[self.time_col].agg([\"min\", \"max\"])\n        data[\"time_passed\"] = data[\"max\"] - data[\"min\"]\n        values_to_plot = (data[\"time_passed\"] / np.", "completion_part": "timedelta64(1, self.timedelta_unit)", "after_completion": ").reset_index(  # type: ignore\n            drop=True\n        )\n\n        if self._remove_cutoff_values:  # type: ignore\n            values_to_plot = self._remove_cutoff_values(values_to_plot).to_numpy()\n        if self.log_scale[0]:\n            log_adjustment = np.timedelta64(100, \"ms\") / np.timedelta64(1, self.timedelta_unit)\n            values_to_plot = np.where(values_to_plot != 0, values_to_plot, values_to_plot + log_adjustment)  # type: ignore\n            bins_to_show = np.power(10, np.histogram_bin_edges(np.log10(values_to_plot), bins=self.bins))\n        else:\n            bins_to_show = np.histogram_bin_edges(values_to_plot, bins=self.bins)\n        if len(values_to_plot) == 0:\n            bins_to_show = np.array([])\n\n        self.bins_to_show = bins_to_show\n        self.values_to_plot = values_to_plot  # type: ignore\n        collect_data_performance(\n            scope=\"user_lifetime_hist\",\n            event_name=\"metadata\",\n            called_params=called_params,\n            not_hash_values=not_hash_values,\n            performance_data={},\n            eventstream_index=self.__eventstream._eventstream_index,\n        )", "namespace": "numpy.timedelta64", "completions": ["timedelta64(1, self.timedelta_"]}
{"id": 60, "prompt": "def matthews_corrcoef(\n        y_true,\n        y_pred,\n        sample_weight=None):\n    return metrics.", "completion_part": "matthews_corrcoef(\n        y_true=y_true,\n        y_pred=y_pred,\n        sample_weight=sample_weight)", "after_completion": "", "namespace": "sklearn.metrics.matthews_corrcoef", "completions": ["matthews_corrcoef(\n        y_"]}
{"id": 61, "prompt": "\n    def load_checkpoint(\n        self,\n        path: _PATH,\n        state: Optional[Union[Module, Optimizer, Dict[str, Union[Module, Optimizer, Any]]]] = None,\n        strict: bool = True,\n    ) -> Dict[str, Any]:\n        if not state:\n            raise ValueError(\n                f\"Got `FSDPStrategy.load_checkpoint(..., state={state!r})` but a state with at least\"\n                \" a model instance to reload is required. Pass it in like so:\"\n                \" `FSDPStrategy.load_checkpoint(..., state={'model': model, ...})`\"\n            )\n        # broadcast the path from rank 0 to ensure all the states are loaded from a common path\n        path = Path(self.broadcast(path))\n\n        from thunder.distributed.checkpoint import has_fsdp_modules, StateDictOptions, load_model_state_dict, load\n\n        if isinstance(state, Module):\n            if not _is_full_checkpoint(path):\n                raise ValueError(\n                    \"Failed to load checkpoint directly into the model. The given path must be a single file\"\n                    f\" containing the full state dict: {path}\"\n                )\n            state_dict = torch.", "completion_part": "load(str(path), mmap=True, map_location=\"cpu\")", "after_completion": "\n            options = StateDictOptions(full_state_dict=True, cpu_offload=True, strict=strict, rank0_only=False)\n            load_model_state_dict(state_dict, _unwrap_tom(state), options, self.local_rank)\n            return {}\n\n        if isinstance(state, Optimizer):\n            raise NotImplementedError(\n                \"Loading a single optimizer object from a checkpoint is not supported yet with the FSDP strategy.\"\n            )\n\n        modules = {key: module for key, module in state.items() if has_fsdp_modules(module)}\n        if len(modules) == 0:\n            raise ValueError(\n                \"Could not find a FSDP model in the provided checkpoint state. Please provide the model as\"\n                \" part of the state like so: `load_checkpoint(..., state={'model': model, ...})`. Make sure\"\n                \" you set up the model (and optimizers if any) through the strategy before loading the checkpoint.\"\n            )\n        if len(modules) > 1:\n            raise ValueError(\n                \"Found multiple FSDP models in the given state. Loading checkpoints with FSDP is\"\n                \" currently limited to a single model per checkpoint. To load multiple models, call the\"\n                \" load method for each model separately with a different path.\"\n            )\n        optimizers = {key: optim for key, optim in state.items() if isinstance(optim, Optimizer)}\n        module_key, module = list(modules.items())[0]\n        module = _unwrap_tom(module)\n\n        if _is_sharded_checkpoint(path):\n            options = StateDictOptions(full_state_dict=False, cpu_offload=True, strict=strict, rank0_only=False)\n            # Load the DCP state dict, which requires a holder state dict\n            converted_state, _ = _get_state_dict(state, None, options, self.local_rank)\n            load(converted_state, path)\n            load_model_state_dict(converted_state[module_key], module, options, self.local_rank)\n\n            # Load metadata (anything not a module or optimizer)\n            metadata = torch.load(path / _METADATA_FILENAME)\n            requested_metadata_keys = state.keys() - modules.keys() - optimizers.keys()\n            _validate_keys_for_strict_loading(requested_metadata_keys, metadata.keys(), strict=strict)\n            for key in requested_metadata_keys:\n                if key not in metadata:\n                    continue\n                state[key] = metadata.pop(key)\n            # return the remaining metadata that wasn't requested as part of `state`\n            return metadata\n\n        if _is_full_checkpoint(path):\n            options = StateDictOptions(full_state_dict=True, cpu_offload=True, strict=strict, rank0_only=False)\n            if not options.rank0_only or self.local_rank == 0:\n                map_location = \"cpu\" if options.cpu_offload else None\n                checkpoint = torch.load(str(path), mmap=True, map_location=map_location)\n                load_model_state_dict(checkpoint[module_key], module, options, self.local_rank)\n            else:\n                checkpoint = {}\n\n            requested_metadata_keys = state.keys() - modules.keys() - optimizers.keys()\n            _validate_keys_for_strict_loading(requested_metadata_keys, checkpoint.keys(), strict=strict)\n            # Load metadata (anything not a module or optimizer)\n            _move_state_into(source=checkpoint, destination=state, keys=requested_metadata_keys)\n            # return the remaining metadata that wasn't requested as part of `state`\n            return checkpoint\n\n        raise ValueError(\n            f\"The path {str(path)!r} does not point to a valid checkpoint. Make sure the path points to either a\"\n            \" directory with FSDP checkpoint shards, or a single file with a full checkpoint.\"\n        )", "namespace": "torch.load", "completions": ["load(path, map_location=\"cpu\")"]}
{"id": 62, "prompt": "def random_state(state: RandomState | None = None):\n    if (\n        ", "completion_part": "is_integer(state)", "after_completion": "\n        or is_array_like(state)\n        or isinstance(state, np.random.BitGenerator)\n    ):\n        # error: Argument 1 to \"RandomState\" has incompatible type \"Optional[Union[int,\n        # Union[ExtensionArray, ndarray[Any, Any]], Generator, RandomState]]\"; expected\n        # \"Union[None, Union[Union[_SupportsArray[dtype[Union[bool_, integer[Any]]]],\n        # Sequence[_SupportsArray[dtype[Union[bool_, integer[Any]]]]],\n        # Sequence[Sequence[_SupportsArray[dtype[Union[bool_, integer[Any]]]]]],\n        # Sequence[Sequence[Sequence[_SupportsArray[dtype[Union[bool_,\n        # integer[Any]]]]]]],\n        # Sequence[Sequence[Sequence[Sequence[_SupportsArray[dtype[Union[bool_,\n        # integer[Any]]]]]]]]], Union[bool, int, Sequence[Union[bool, int]],\n        # Sequence[Sequence[Union[bool, int]]], Sequence[Sequence[Sequence[Union[bool,\n        # int]]]], Sequence[Sequence[Sequence[Sequence[Union[bool, int]]]]]]],\n        # BitGenerator]\"\n        return np.random.RandomState(state)  # type: ignore[arg-type]\n    elif isinstance(state, np.random.RandomState):\n        return state\n    elif isinstance(state, np.random.Generator):\n        return state\n    elif state is None:\n        return np.random\n    else:\n        raise ValueError(\n            \"random_state must be an integer, array-like, a BitGenerator, Generator, \"\n            \"a numpy RandomState, or None\"\n        )", "namespace": "pandas.core.dtypes.common.is_integer", "completions": ["0 <= state < 2**32\n       "]}
{"id": 63, "prompt": "def euler_angles_to_rotation_matrix(x, y, z):\n    assert x.dim() == 1, x.shape\n    assert x.shape == y.shape == z.shape\n    ones, zeros = torch.ones_like(x), torch.zeros_like(x)\n    # the rotation matrix for the x-axis\n    rx_tmp = [\n        ones,\n        zeros,\n        zeros,\n        zeros,\n        zeros,\n        torch.cos(x),\n        -torch.sin(x),\n        zeros,\n        zeros,\n        torch.sin(x),\n        torch.cos(x),\n        zeros,\n        zeros,\n        zeros,\n        zeros,\n        ones,\n    ]\n    rx = torch.stack(rx_tmp, dim=-1).view(-1, 4, 4)\n    # the rotation matrix for the y-axis\n    ry_tmp = [\n        torch.cos(y),\n        zeros,\n        torch.sin(y),\n        zeros,\n        zeros,\n        ones,\n        zeros,\n        zeros,\n        -torch.sin(y),\n        zeros,\n        torch.cos(y),\n        zeros,\n        zeros,\n        zeros,\n        zeros,\n        ones,\n    ]\n    ry = torch.stack(ry_tmp, dim=-1).view(-1, 4, 4)\n    # the rotation matrix for the z-axis\n    rz_tmp = [\n        torch.cos(z),\n        -torch.sin(z),\n        zeros,\n        zeros,\n        torch.sin(z),\n        torch.cos(z),\n        zeros,\n        zeros,\n        zeros,\n        zeros,\n        ones,\n        zeros,\n        zeros,\n        zeros,\n        zeros,\n        ones,\n    ]\n    rz = torch.stack(rz_tmp, dim=-1).view(-1, 4, 4)\n    return torch.", "completion_part": "matmul(rz, torch.matmul(ry, rx))", "after_completion": "  # Bx4x4", "namespace": "torch.matmul", "completions": ["matmul(torch.matmul(rx, ry),"]}
{"id": 64, "prompt": "    def mean(self, lower=0.0, upper=np.inf):\n        loc = []\n        for idx in self.index:\n            qpd = self.qpd.loc[idx, :].values[0]\n            l, _ = ", "completion_part": "quad(exp_func, args=(qpd), a=lower, b=upper)", "after_completion": "\n            loc.append(l)\n        loc_arr = np.array(loc)\n        return pd.DataFrame(loc_arr, index=self.index, columns=self.columns)", "namespace": "scipy.integrate.quad", "completions": ["ql.qpd_to_loc(q"]}
{"id": 65, "prompt": "\ndef clip_grad_norm_(params, max_norm, aggregate_norm_fn=None) -> torch.Tensor:\n    def grad_exists(p):\n        return p is not None and getattr(p, \"grad\", None) is not None\n\n    if isinstance(params, torch.Tensor):\n        params = [params]\n    params = list(params)\n    grads = [\n        p.grad.detach() for p in params if grad_exists(p) and not hasattr(p, \"expert\")\n    ]\n    expert_grads = [\n        p.grad.detach() for p in params if grad_exists(p) and hasattr(p, \"expert\")\n    ]\n\n    if len(grads) == 0:\n        if len(params) > 0:\n            return params[0].new_tensor(0.0)\n        else:\n            return torch.tensor(0.0)\n\n    if len(grads) == 1:\n        total_norm = torch.norm(grads[0], p=2, dtype=torch.float32)\n    else:\n        if multi_tensor_l2norm_available:\n            total_norm = multi_tensor_total_norm(grads)\n        else:\n            if torch.cuda.", "completion_part": "is_available()", "after_completion": ":\n                warnings.warn(\n                    \"amp_C fused kernels unavailable, disabling multi_tensor_l2norm; \"\n                    \"you may get better performance by installing NVIDIA's apex library\"\n                )\n                device = torch.cuda.current_device()\n            elif grads[0].device.type == \"xla\":\n                device = grads[0].device\n            else:\n                device = torch.device(\"cpu\")\n            total_norm = torch.norm(\n                torch.stack(\n                    [torch.norm(g, p=2, dtype=torch.float32).to(device) for g in grads]\n                )\n            )\n\n    if aggregate_norm_fn is not None:\n        total_norm = aggregate_norm_fn(total_norm)\n\n    if max_norm > 0:\n        max_norm = float(max_norm)\n        clip_coef = (max_norm / (total_norm + 1e-6)).clamp_(max=1)\n        for g in grads + expert_grads:\n            g.mul_(clip_coef)\n    return total_norm", "namespace": "torch.cuda.is_available", "completions": ["is_available():\n                total_norm = torch"]}
{"id": 66, "prompt": "def get_engine(engine: str) -> BaseImpl:\n    if engine == \"auto\":\n        engine = ", "completion_part": "get_option(\"io.parquet.engine\")", "after_completion": "\n\n    if engine == \"auto\":\n        # try engines in this order\n        engine_classes = [PyArrowImpl, FastParquetImpl]\n\n        error_msgs = \"\"\n        for engine_class in engine_classes:\n            try:\n                return engine_class()\n            except ImportError as err:\n                error_msgs += \"\\n - \" + str(err)\n\n        raise ImportError(\n            \"Unable to find a usable engine; \"\n            \"tried using: 'pyarrow', 'fastparquet'.\\n\"\n            \"A suitable version of \"\n            \"pyarrow or fastparquet is required for parquet \"\n            \"support.\\n\"\n            \"Trying to import the above resulted in these errors:\"\n            f\"{error_msgs}\"\n        )\n\n    if engine == \"pyarrow\":\n        return PyArrowImpl()\n    elif engine == \"fastparquet\":\n        return FastParquetImpl()\n\n    raise ValueError(\"engine must be one of 'pyarrow', 'fastparquet'\")", "namespace": "pandas.get_option", "completions": ["0\n    elif engine == \"cython\":\n       "]}
{"id": 67, "prompt": "def sample_treatment_and_effect(\n    graph: torch.Tensor, node_names: Sequence[str], ensure_effect: bool = True, num_effects: int = 1\n) -> tuple[str, list[str]]:\n    if ensure_effect:\n        assert num_effects == 1, \"Can only ensure effect for one effect node.\"\n\n        nx_graph = nx.from_numpy_array(graph.numpy(), create_using=nx.DiGraph)\n        nx_graph = nx.relabel_nodes(nx_graph, dict(enumerate(node_names)))\n\n        topological_order = nx.topological_sort(nx_graph)\n        topological_order_with_descendants = [n for n in topological_order if nx.descendants(nx_graph, n)]\n\n        if topological_order_with_descendants:\n            treatment = np.random.", "completion_part": "choice(topological_order_with_descendants, size=1, replace=False)", "after_completion": ".item()\n            # randomly sample a effect not that is a descendant of the treatment\n            descendants = nx.descendants(nx_graph, treatment)\n            effects = np.random.choice(list(descendants), size=num_effects, replace=False)\n            return treatment, effects.tolist()\n        else:\n            warnings.warn(\"No edges found. Defaulting to random sampling.\")\n\n    samples = np.random.choice(node_names, size=1 + num_effects, replace=False)\n    treatment = samples[0]\n    effects = samples[1:]\n\n    return treatment, effects.tolist()", "namespace": "numpy.random.choice", "completions": ["choice(topological_order_with_desc"]}
{"id": 68, "prompt": "def as_compatible_data(\n    data: T_DuckArray | ArrayLike, fastpath: bool = False\n) -> T_DuckArray:\n    if fastpath and getattr(data, \"ndim\", 0) > 0:\n        # can't use fastpath (yet) for scalars\n        return cast(\"T_DuckArray\", _maybe_wrap_data(data))\n\n    from xarray.core.dataarray import DataArray\n\n    if isinstance(data, (Variable, DataArray)):\n        return data.data\n\n    if isinstance(data, NON_NUMPY_SUPPORTED_ARRAY_TYPES):\n        data = _possibly_convert_datetime_or_timedelta_index(data)\n        return cast(\"T_DuckArray\", _maybe_wrap_data(data))\n\n    if isinstance(data, tuple):\n        data = utils.to_0d_object_array(data)\n\n    if isinstance(data, pd.Timestamp):\n        # TODO: convert, handle datetime objects, too\n        data = np.datetime64(data.value, \"ns\")\n\n    if isinstance(data, timedelta):\n        data = np.timedelta64(getattr(data, \"value\", data), \"ns\")\n\n    # we don't want nested self-described arrays\n    if isinstance(data, (pd.Series, pd.DataFrame)):\n        data = data.values\n\n    if isinstance(data, np.ma.MaskedArray):\n        mask = np.ma.getmaskarray(data)\n        if mask.any():\n            dtype, fill_value = dtypes.maybe_promote(data.dtype)\n            data = duck_array_ops.where_method(data, ~mask, fill_value)\n        else:\n            data = np.", "completion_part": "asarray(data)", "after_completion": "\n\n    if not isinstance(data, np.ndarray) and (\n        hasattr(data, \"__array_function__\") or hasattr(data, \"__array_namespace__\")\n    ):\n        return cast(\"T_DuckArray\", data)\n\n    # validate whether the data is valid data types.\n    data = np.asarray(data)\n\n    if isinstance(data, np.ndarray) and data.dtype.kind in \"OMm\":\n        data = _possibly_convert_objects(data)\n    return _maybe_wrap_data(data)", "namespace": "numpy.asarray", "completions": ["asarray(data)\n\n    if isinstance(data,"]}
{"id": 69, "prompt": "def to_numpy(\n    evset: EventSet,\n    timestamp_to_datetime: bool = True,\n    timestamps: bool = True,\n) -> Dict[str, ndarray]:\n    timestamp_key = \"timestamp\"\n    index_names = evset.schema.index_names()\n    feature_names = evset.schema.feature_names()\n\n    column_names = index_names + feature_names\n    if timestamps:\n        column_names += [timestamp_key]\n\n    dst = {column_name: [] for column_name in column_names}\n    for index, data in evset.data.items():\n        assert isinstance(index, tuple)\n\n        if timestamps:\n            # Timestamps\n            if evset.schema.is_unix_timestamp and timestamp_to_datetime:\n                dst[timestamp_key].append(\n                    data.timestamps.astype(\"datetime64[s]\")\n                )\n            else:\n                dst[timestamp_key].append(data.timestamps)\n\n        # Features\n        for feature_name, feature in zip(feature_names, data.features):\n            dst[feature_name].append(feature)\n\n        # Indexes\n        num_timestamps = len(data.timestamps)\n        for index_name, index_item in zip(index_names, index):\n            dst[index_name].append(np.", "completion_part": "repeat(index_item, num_timestamps)", "after_completion": ")\n\n    dst = {k: np.concatenate(v) for k, v in dst.items()}\n    return dst", "namespace": "numpy.repeat", "completions": ["full(num_timestamps, index_item))"]}
{"id": 70, "prompt": "\n    def aggregate(self, pairs: List[Tuple[Model, float]]) -> Model:\n        \n        model = Model()\n\n        sample_model = pairs[0][0]\n        \n        if sample_model.encrypted:\n            raise RuntimeError(\"Cannot aggregate encrypted tensors using Federated Average.\")\n        \n        for tensor in sample_model.tensors:\n            model.tensors.append(copy.deepcopy(tensor))\n\n        total_tensors = len(model.tensors)\n\n        for var_idx in range(total_tensors):\n            var_num_values = model.tensors[var_idx].length\n\n            aggregated_tensor = np.zeros(var_num_values)\n            \n            for pair in pairs:\n                local_model = pair[0]\n                scaling_factor = pair[1]\n                local_tensor = local_model.tensors[var_idx]\n                \n                t2_r = np.", "completion_part": "frombuffer(local_tensor.value, dtype=MODEL_WEIGHTS_DTYPE)", "after_completion": " * scaling_factor\n                aggregated_tensor += t2_r\n                \n            model.tensors[var_idx].value = serialize_tensor(aggregated_tensor)\n\n        return model", "namespace": "numpy.frombuffer", "completions": ["array(local_tensor.values)\n                t"]}
{"id": 71, "prompt": "    def epoch_iterator(\n        self,\n        batch_size: tp.Optional[int] = None,\n        max_tokens: tp.Optional[int] = None,\n        order_by_length: tp.Optional[str] = None,\n        min_sample_size: int = 0,\n        rank: int = 0,\n        world_size: int = 1,\n        nb_producers: int = 10,\n        nb_prefetch: int = 2,\n        seed: tp.Optional[int] = None,\n        nb_cpu: int = 10,\n    ):\n        if not ((batch_size is None) ^ (max_tokens is None)):\n            raise ValueError(\"need to provide either `batch_size` either `max_tokens`\")\n        if max_tokens is not None and order_by_length is None:\n            raise ValueError(\n                \"`order_by_length` should be given to deal with `max_tokens`\"\n            )\n\n        np_rs = np.random.RandomState(seed)\n        with Parallel(\n            n_jobs=max(nb_cpu // 2, 1), backend=\"threading\", return_as=\"generator\"\n        ) as parallel_outer, Parallel(\n            n_jobs=nb_cpu, backend=\"threading\", return_as=\"generator\"\n        ) as parallel_inner, pyarrow_cpu(\n            nb_cpu\n        ):\n            if order_by_length is not None:\n                columns = sorted(\n                    set(self._columns_wo_partition_keys) | set([order_by_length])\n                )\n            else:\n                columns = self._columns_wo_partition_keys\n\n            def load_one_fragement(fragment):\n                fragment_table = fragment.to_table(\n                    columns=columns, use_threads=self.use_threads\n                )\n                fragment_table = self._add_partitioning_values(fragment_table, fragment)\n                if self._filter_expression is not None:\n                    fragment_table = fragment_table.filter(self._filter_expression)\n                return fragment_table\n\n            def get_table_producer(all_fragments, nb_producers, nb_prefetch):\n                def unit_table_producer():\n                    all_shuffled_fragments = np_rs.permutation(all_fragments)\n                    for block in batched(\n                        all_shuffled_fragments, nb_producers * nb_prefetch\n                    ):\n                        yield from parallel_inner(\n                            ", "completion_part": "delayed(load_one_fragement)", "after_completion": "(frag) for frag in block\n                        )\n\n                for tables in batched(unit_table_producer(), nb_producers):\n                    yield pa.concat_tables(list(tables)).combine_chunks()\n\n            def table_iterator(table):\n                if order_by_length is not None:\n                    length_col = self._compute_length(table[order_by_length])\n                    # add small perturbation to avoid same sample appear together during different epochs\n                    length_col += np_rs.randint(\n                        0, max(np.quantile(length_col, 0.001), 2), len(length_col)\n                    )\n                else:\n                    length_col = np_rs.randint(0, 2**23, len(table))\n\n                table = table.select(self.columns)\n\n                if batch_size is not None:\n                    # faster equivalent to\n                    # yield from table.take(np.argsort(length_col)).to_batches(batch_size)\n                    order_tt = pa.Table.from_arrays(\n                        [pa.array(np.argsort(length_col))], [\"order\"]\n                    )\n                    batches = order_tt.to_batches(batch_size)\n                    batches = [\n                        batches[i][\"order\"] for i in np_rs.permutation(len(batches))\n                    ]\n                    yield from parallel_outer(delayed(table.take)(bb) for bb in batches)\n                elif max_tokens is not None:\n                    splits = self._compute_length_splits(length_col, max_tokens)\n                    splits = [splits[i] for i in np_rs.permutation(len(splits))]\n                    yield from parallel_outer(delayed(table.take)(bb) for bb in splits)\n                else:\n                    raise ValueError(\"unknown batching method\")\n\n            local_fragments = np.split(\n                np.array(self._all_fragments, dtype=\"O\"), world_size\n            )[rank]\n            table_producer = get_table_producer(\n                local_fragments, nb_producers, nb_prefetch\n            )\n            for table in table_producer:\n                for out in table_iterator(table):\n                    if len(out) < min_sample_size:\n                        continue\n                    if self.to_pandas:\n                        out = out.to_pandas()\n                    yield out", "namespace": "joblib.delayed", "completions": ["1,\n                            load_one_fragement,"]}
{"id": 72, "prompt": "\ndef main():\n    parser = HfArgumentParser(\n        (ModelArguments, DataArguments, RetrieverTrainingArguments))\n\n    (model_args,\n    data_args,\n    training_args\n    ) = parser.parse_args_into_dataclasses()\n\n    model_args: ModelArguments\n    data_args: DataArguments\n    training_args: RetrieverTrainingArguments\n\n    if (\n            os.path.exists(training_args.output_dir)\n            and os.listdir(training_args.output_dir)\n            and training_args.do_train\n            and not training_args.overwrite_output_dir\n    ):\n        raise ValueError(\n            f\"Output directory ({training_args.output_dir}) already exists. Use --overwrite_output_dir to overcome.\"\n        )\n\n    # Set seed\n    set_seed(training_args.seed)\n\n    num_labels = 1\n    tokenizer = AutoTokenizer.", "completion_part": "from_pretrained(\n        model_args.tokenizer_name if model_args.tokenizer_name else model_args.model_name_or_path,\n        cache_dir=model_args.cache_dir,\n        use_fast=False,\n    )", "after_completion": "\n    config = AutoConfig.from_pretrained(\n        model_args.config_name if model_args.config_name else model_args.model_name_or_path,\n        num_labels=num_labels,\n        cache_dir=model_args.cache_dir,\n    )\n\n    model = BiEncoderModel(model_name=model_args.model_name_or_path,\n                           normalized=training_args.normalized,\n                           sentence_pooling_method=training_args.sentence_pooling_method,\n                           negatives_cross_device=training_args.negatives_cross_device,\n                           temperature=training_args.temperature)\n\n    if training_args.fix_position_embedding:\n        for k, v in model.named_parameters():\n            if \"position_embeddings\" in k:\n                v.requires_grad = False\n\n    train_dataset = TrainDatasetForEmbedding(args=data_args, tokenizer=tokenizer)\n\n    trainer = BiTrainer(\n        model=model,\n        args=training_args,\n        train_dataset=train_dataset,\n        data_collator=EmbedCollator(\n            tokenizer,\n            query_max_len=data_args.query_max_len,\n            passage_max_len=data_args.passage_max_len\n        ),\n        tokenizer=tokenizer\n    )\n\n    Path(training_args.output_dir).mkdir(parents=True, exist_ok=True)\n\n    # Training\n    trainer.train()\n    trainer.save_model()\n    # For convenience, we also re-save the tokenizer to the same directory,\n    # so that you can share your model easily on huggingface.co/models =)\n    if trainer.is_world_process_zero():\n        tokenizer.save_pretrained(training_args.output_dir)", "namespace": "transformers.AutoTokenizer.from_pretrained", "completions": ["from_pretrained(model_args.model_"]}
{"id": 73, "prompt": "\ndef parse_mutelist_file(\n    mutelist_path: str, aws_session: Session = None, aws_account: str = None\n):\n    try:\n        # Check if file is a S3 URI\n        if re.search(\"^s3://([^/]+)/(.*?([^/]+))$\", mutelist_path):\n            bucket = mutelist_path.split(\"/\")[2]\n            key = (\"/\").join(mutelist_path.split(\"/\")[3:])\n            s3_client = aws_session.client(\"s3\")\n            mutelist = yaml.", "completion_part": "safe_load(\n                s3_client.get_object(Bucket=bucket, Key=key)[\"Body\"]\n            )", "after_completion": "[\"Mutelist\"]\n        # Check if file is a Lambda Function ARN\n        elif re.search(r\"^arn:(\\w+):lambda:\", mutelist_path):\n            lambda_region = mutelist_path.split(\":\")[3]\n            lambda_client = aws_session.client(\"lambda\", region_name=lambda_region)\n            lambda_response = lambda_client.invoke(\n                FunctionName=mutelist_path, InvocationType=\"RequestResponse\"\n            )\n            lambda_payload = lambda_response[\"Payload\"].read()\n            mutelist = yaml.safe_load(lambda_payload)[\"Mutelist\"]\n        # Check if file is a DynamoDB ARN\n        elif re.search(\n            r\"^arn:aws(-cn|-us-gov)?:dynamodb:[a-z]{2}-[a-z-]+-[1-9]{1}:[0-9]{12}:table\\/[a-zA-Z0-9._-]+$\",\n            mutelist_path,\n        ):\n            mutelist = {\"Accounts\": {}}\n            table_region = mutelist_path.split(\":\")[3]\n            dynamodb_resource = aws_session.resource(\n                \"dynamodb\", region_name=table_region\n            )\n            dynamo_table = dynamodb_resource.Table(mutelist_path.split(\"/\")[1])\n            response = dynamo_table.scan(\n                FilterExpression=Attr(\"Accounts\").is_in([aws_account, \"*\"])\n            )\n            dynamodb_items = response[\"Items\"]\n            # Paginate through all results\n            while \"LastEvaluatedKey\" in dynamodb_items:\n                response = dynamo_table.scan(\n                    ExclusiveStartKey=response[\"LastEvaluatedKey\"],\n                    FilterExpression=Attr(\"Accounts\").is_in([aws_account, \"*\"]),\n                )\n                dynamodb_items.update(response[\"Items\"])\n            for item in dynamodb_items:\n                # Create mutelist for every item\n                mutelist[\"Accounts\"][item[\"Accounts\"]] = {\n                    \"Checks\": {\n                        item[\"Checks\"]: {\n                            \"Regions\": item[\"Regions\"],\n                            \"Resources\": item[\"Resources\"],\n                        }\n                    }\n                }\n                if \"Tags\" in item:\n                    mutelist[\"Accounts\"][item[\"Accounts\"]][\"Checks\"][item[\"Checks\"]][\n                        \"Tags\"\n                    ] = item[\"Tags\"]\n                if \"Exceptions\" in item:\n                    mutelist[\"Accounts\"][item[\"Accounts\"]][\"Checks\"][item[\"Checks\"]][\n                        \"Exceptions\"\n                    ] = item[\"Exceptions\"]\n        else:\n            with open(mutelist_path) as f:\n                mutelist = yaml.safe_load(f)[\"Mutelist\"]\n        try:\n            mutelist_schema.validate(mutelist)\n        except Exception as error:\n            logger.critical(\n                f\"{error.__class__.__name__} -- Mutelist YAML is malformed - {error}[{error.__traceback__.tb_lineno}]\"\n            )\n            sys.exit(1)\n        return mutelist\n    except Exception as error:\n        logger.critical(\n            f\"{error.__class__.__name__} -- {error}[{error.__traceback__.tb_lineno}]\"\n        )\n        sys.exit(1)", "namespace": "yaml.safe_load", "completions": ["safe_load(\n                s3_client."]}
{"id": 74, "prompt": "def keep_most_recent_files(pattern, maxsize=int(1e12), maxfiles=1000, debug=False):\n\n    # get the list of files in the directory\n    fnames = glob.glob(pattern)\n    # compute a list of (mtime, fname, size) triples\n    files = []\n    for fname in fnames:\n        try:\n            s = os.stat(fname)\n        except FileNotFoundError:\n            continue\n        files.append((s.st_mtime, fname, s.st_size))\n    # sort the list by mtime, most recent first\n    files.sort(reverse=True)\n    # compute an accumulated total of the file sizes in order using np.cumsum\n    sizes = np.", "completion_part": "cumsum([size for mtime, fname, size in files])", "after_completion": "\n    # compute a cutoff index based on maxsize\n    cutoff = np.searchsorted(sizes, maxsize)\n    # compute a cutoff index based on maxfiles\n    cutoff = min(cutoff, maxfiles)\n    # delete the files above the cutoff in reverse order\n    for mtime, fname, size in files[cutoff:][::-1]:\n        try:\n            os.unlink(fname)\n        except FileNotFoundError:\n            pass", "namespace": "numpy.cumsum", "completions": ["cumsum([0] + [f[2"]}
{"id": 75, "prompt": "    def _create_output(\n            self,\n            accepted: torch.Tensor,  # [batch_size, k]\n            recovered_token_ids: torch.Tensor,  # [batch_size, k]\n            draft_token_ids: torch.Tensor,  # [batch_size, k]\n            bonus_token_ids: torch.Tensor,  # [batch_size]\n    ) -> torch.Tensor:\n        bonus_token_ids = bonus_token_ids.squeeze()\n        batch_size, k = recovered_token_ids.shape\n\n        # Determine the index of the first False value for each row.\n        limits = (accepted == 0).max(1).indices\n        limits[~(accepted == 0).any(1)] = k\n\n        # Create masks using the indices.\n        indices = torch.arange(k, device=accepted.device).unsqueeze(0)\n        accepted_mask = indices < limits.unsqueeze(1)\n        after_false_mask = indices == limits.unsqueeze(1)\n\n        # Create an extended output tensor\n        output_with_bonus_tokens = -torch.", "completion_part": "ones(\n            (batch_size, k + self._num_bonus_tokens),\n            dtype=self.token_id_dtype,\n            device=accepted.device)", "after_completion": "\n        output = output_with_bonus_tokens[:, :k]\n\n        # Fill in the first k columns of the output tensor using masks and data\n        # tensors.\n        output[:, :k] = torch.where(accepted_mask, draft_token_ids,\n                                    -torch.ones_like(draft_token_ids))\n\n        # Fill the last column.\n        # We check output directly as accepted may have True values inconsistent\n        # with causal acceptance.\n        output_with_bonus_tokens[:, -1] = torch.where(output[:, -1] != -1,\n                                                      bonus_token_ids, -1)\n\n        # Fill the recovered token ids.\n        output.mul_(~after_false_mask).add_(\n            recovered_token_ids.mul(after_false_mask))\n\n        self.num_accepted_tokens += accepted.sum()\n        self.num_emitted_tokens += (output_with_bonus_tokens != -1).sum()\n        self.num_draft_tokens += batch_size * k\n\n        return output_with_bonus_tokens", "namespace": "torch.ones", "completions": ["ones(\n            (batch_size, k +"]}
{"id": 76, "prompt": "def annotations_to_instances(annos, image_size, mask_format=\"polygon\"):\n    boxes = [BoxMode.convert(obj[\"bbox\"], obj[\"bbox_mode\"], BoxMode.XYXY_ABS) for obj in annos]\n    target = Instances(image_size)\n    boxes = target.gt_boxes = Boxes(boxes)\n    boxes.clip(image_size)\n\n    classes = [obj[\"category_id\"] for obj in annos]\n    classes = torch.tensor(classes, dtype=torch.int64)\n    target.gt_classes = classes\n\n    if len(annos) and \"segmentation\" in annos[0]:\n        segms = [obj[\"segmentation\"] for obj in annos]\n        if mask_format == \"polygon\":\n            masks = PolygonMasks(segms)\n        else:\n            assert mask_format == \"bitmask\", mask_format\n            masks = []\n            for segm in segms:\n                if isinstance(segm, list):\n                    # polygon\n                    masks.append(polygons_to_bitmask(segm, *image_size))\n                elif isinstance(segm, dict):\n                    # COCO RLE\n                    masks.append(mask_util.decode(segm))\n                elif isinstance(segm, np.ndarray):\n                    assert segm.ndim == 2, \"Expect segmentation of 2 dimensions, got {}.\".format(\n                        segm.ndim\n                    )\n                    # mask array\n                    masks.append(segm)\n                else:\n                    raise ValueError(\n                        \"Cannot convert segmentation of type '{}' to BitMasks!\"\n                        \"Supported types are: polygons as list[list[float] or ndarray],\"\n                        \" COCO-style RLE as a dict, or a full-image segmentation mask \"\n                        \"as a 2D ndarray.\".format(type(segm))\n                    )\n            # torch.from_numpy does not support array with negative stride.\n            masks = BitMasks(\n                torch.stack([torch.from_numpy(np.", "completion_part": "ascontiguousarray(x)", "after_completion": ") for x in masks])\n            )\n        target.gt_masks = masks\n\n    if len(annos) and \"keypoints\" in annos[0]:\n        kpts = [obj.get(\"keypoints\", []) for obj in annos]\n        target.gt_keypoints = Keypoints(kpts)\n\n    return target", "namespace": "numpy.ascontiguousarray", "completions": ["ascontiguousarray(m)) for m in masks"]}
{"id": 77, "prompt": "def show_events_result(\n    warmup_length,\n    save_dir,\n    train_period,\n    basin_area=None,\n    prcp=None,\n):\n    # TODO: not finished\n    time = pd.read_excel(\n        \"D:/\u7814\u7a76\u751f/\u6bd5\u4e1a\u8bba\u6587/new\u6bd5\u4e1a\u8bba\u6587/\u9884\u7b54\u8fa9/\u78a7\u6d41\u6cb3\u6c34\u5e93/\u7ad9\u70b9\u4fe1\u606f/\u6d2a\u6c34\u7387\u5b9a\u65f6\u95f4.xlsx\"\n    )\n    calibrate_starttime = pd.to_datetime(\"2012-06-10 0:00:00\")\n    calibrate_endtime = pd.to_datetime(\"2019-12-31 23:00:00\")\n    basin_area = float(basin_area)\n    best_simulation = [\n        x * (basin_area * 1000000 / 1000 / 3600) for x in best_simulation\n    ]\n    obs = [x * (basin_area * 1000000 / 1000 / 3600) for x in spot_setup.evaluation()]\n    time[\"starttime\"] = pd.to_datetime(time[\"starttime\"])\n    time[\"endtime\"] = pd.to_datetime(time[\"endtime\"])\n    Prcp_list = []\n    W_obs_list = []\n    W_sim_list = []\n    W_bias_abs_list = []\n    W_bias_rela_list = []\n    Q_max_obs_list = []\n    Q_max_sim_list = []\n    Q_bias_rela_list = []\n    time_bias_list = []\n    DC_list = []\n    ID_list = []\n    for i, row in time.iterrows():\n        # for i in range(len(time)):\n        if row[\"starttime\"] < calibrate_endtime:\n            # if(time[\"starttime\",0]<calibrate_endtime):\n            start_num = (\n                row[\"starttime\"]\n                - calibrate_starttime\n                - pd.Timedelta(hours=warmup_length)\n            ) / pd.Timedelta(hours=1)\n            end_num = (\n                row[\"endtime\"] - calibrate_starttime - pd.Timedelta(hours=warmup_length)\n            ) / pd.Timedelta(hours=1)\n            start_period = (row[\"endtime\"] - calibrate_starttime) / pd.Timedelta(\n                hours=1\n            )\n            end_period = (row[\"endtime\"] - calibrate_starttime) / pd.Timedelta(hours=1)\n            start_period = int(start_period)\n            end_period = int(end_period)\n            start_num = int(start_num)\n            end_num = int(end_num)\n            t_range_train_changci = pd.", "completion_part": "date_range(\n                row[\"starttime\"], row[\"endtime\"], freq=\"H\"\n            )", "after_completion": "\n            save_fig = os.path.join(save_dir, \"train_results\" + str(i) + \".png\")\n            best_simulation_changci = best_simulation[start_num : end_num + 1]\n            plot_sim_and_obs(\n                t_range_train_changci,\n                best_simulation[start_num : end_num + 1],\n                obs[start_num : end_num + 1],\n                prcp[start_num : end_num + 1],\n                save_fig,\n            )\n            Prcp = sum(prcp[start_num : end_num + 1])\n            W_obs = (\n                sum(obs[start_num : end_num + 1]) * 3600 * 1000 / basin_area / 1000000\n            )\n            W_sim = sum(best_simulation_changci) * 3600 * 1000 / basin_area / 1000000\n            W_bias_abs = W_sim - W_obs\n            W_bias_rela = W_bias_abs / W_obs\n            Q_max_obs = np.max(obs[start_num : end_num + 1])\n            Q_max_sim = np.max(best_simulation_changci)\n            Q_bias_rela = (Q_max_sim - Q_max_obs) / Q_max_obs\n            t1 = np.argmax(best_simulation_changci)\n            t2 = np.argmax(obs[start_num : end_num + 1])\n            time_bias = t1 - t2\n            DC = NSE(obs[start_num : end_num + 1], best_simulation_changci)\n            ID = row[\"starttime\"].strftime(\"%Y%m%d\")\n            Prcp_list.append(Prcp)\n            W_obs_list.append(W_obs)\n            W_sim_list.append(W_sim)\n            W_bias_abs_list.append(W_bias_abs)\n            W_bias_rela_list.append(W_bias_rela)\n            Q_max_obs_list.append(Q_max_obs)\n            Q_max_sim_list.append(Q_max_sim)\n            Q_bias_rela_list.append(Q_bias_rela)\n            time_bias_list.append(time_bias)\n\n            DC_list.append(DC)\n            ID_list.append(ID)\n\n    bias = pd.DataFrame(\n        {\n            \"Prcp(mm)\": Prcp_list,\n            \"W_obs(mm)\": W_obs_list,\n            \"W_sim(mm)\": W_sim_list,\n            \"W_bias_abs\": W_bias_abs_list,\n            \"W_bias_rela\": W_bias_rela_list,\n            \"Q_max_obs(m3/s)\": Q_max_obs_list,\n            \"Q_max_sim(m3/s)\": Q_max_sim_list,\n            \"Q_bias_rela\": Q_bias_rela_list,\n            \"time_bias\": time_bias_list,\n            \"DC\": DC_list,\n            \"ID\": ID_list,\n        }\n    )\n    bias.to_csv(\n        os.path.join(\n            \"D:/\u7814\u7a76\u751f/\u6bd5\u4e1a\u8bba\u6587/new\u6bd5\u4e1a\u8bba\u6587/\u9884\u7b54\u8fa9/\u78a7\u6d41\u6cb3\u6c34\u5e93/\u7ad9\u70b9\u4fe1\u606f/train_metrics.csv\"\n        )\n    )\n    t_range_train = pd.to_datetime(train_period[warmup_length:]).values.astype(\n        \"datetime64[h]\"\n    )\n    save_fig = os.path.join(save_dir, \"train_results.png\")  # \u751f\u6210\u7ed3\u679c\u56fe\n    plot_sim_and_obs(t_range_train, best_simulation, obs, prcp[:], save_fig)", "namespace": "pandas.date_range", "completions": ["date_range(\n                start=calibrate_"]}
{"id": 78, "prompt": "def nmse(y_true: np.ndarray, y_pred: np.ndarray, squared: bool = False) -> float:\n    y_true = y_true.reshape(-1)\n    y_pred = y_pred.reshape(-1)\n\n    y_std = np.", "completion_part": "std(y_true)", "after_completion": "\n    if y_std == 0:\n        return mean_squared_error(y_true, y_pred, squared=squared)\n\n    return mean_squared_error(y_true, y_pred, squared=squared) / (np.var(y_true) if squared else y_std)", "namespace": "numpy.std", "completions": ["std(y_true)\n    y_mean"]}
{"id": 79, "prompt": "def _get_device_type_from_env() -> str:\n    if torch.cuda.", "completion_part": "is_available()", "after_completion": ":\n        device = \"cuda\"\n    else:\n        device = \"cpu\"\n    return device", "namespace": "torch.cuda.is_available", "completions": ["is_available():\n        return \"cuda\"\n   "]}
{"id": 80, "prompt": "def run_fpgrowth(data: DataFrame, min_support: float = 0.001) -> DataFrame:\n    sets = data.to_numpy().tolist()\n    te = TransactionEncoder()\n    sets_onehot = te.fit_transform(sets)\n    df = ", "completion_part": "DataFrame(sets_onehot, columns=te.columns_)", "after_completion": "  # type: ignore\n    frequent_itemsets = fpgrowth(df, min_support=min_support, use_colnames=True)\n    return frequent_itemsets.sort_values([\"support\"], ascending=[False])", "namespace": "pandas.DataFrame", "completions": [" pd.DataFrame(sets_onehot, columns"]}
{"id": 81, "prompt": "\ndef main():\n    parser = argparse.ArgumentParser()\n\n    # Required parameters\n    parser.add_argument(\n        \"--data_dir\",\n        default=None,\n        type=str,\n        required=True,\n        help=\"The input data dir. Should contain the .tsv files (or other data files) for the task.\",\n    )\n    parser.add_argument(\n        \"--model_type\",\n        default=None,\n        type=str,\n        required=True,\n        help=\"Model type selected in the list: \" + \", \".join(MODEL_CLASSES.keys()),\n    )\n    parser.add_argument(\n        \"--model_name_or_path\",\n        default=None,\n        type=str,\n        required=True,\n        help=\"Path to pre-trained model or shortcut name.\",\n    )\n    parser.add_argument(\n        \"--task_name\",\n        default=None,\n        type=str,\n        required=True,\n        help=\"The name of the task to train selected in the list: \" + \", \".join(processors.keys()),\n    )\n    parser.add_argument(\n        \"--output_dir\",\n        default=None,\n        type=str,\n        required=True,\n        help=\"The output directory where the model predictions and checkpoints will be written.\",\n    )\n    parser.add_argument(\n        \"--patience\",\n        default=\"0\",\n        type=str,\n        required=False,\n    )\n    parser.add_argument(\n        \"--regression_threshold\",\n        default=0,\n        type=float,\n        required=False,\n    )\n\n    # Other parameters\n    parser.add_argument(\n        \"--config_name\",\n        default=\"\",\n        type=str,\n        help=\"Pretrained config name or path if not the same as model_name\",\n    )\n    parser.add_argument(\n        \"--tokenizer_name\",\n        default=\"\",\n        type=str,\n        help=\"Pretrained tokenizer name or path if not the same as model_name\",\n    )\n    parser.add_argument(\n        \"--cache_dir\",\n        default=\"\",\n        type=str,\n        help=\"Where do you want to store the pre-trained models downloaded from huggingface.co\",\n    )\n    parser.add_argument(\n        \"--max_seq_length\",\n        default=128,\n        type=int,\n        help=(\n            \"The maximum total input sequence length after tokenization. Sequences longer \"\n            \"than this will be truncated, sequences shorter will be padded.\"\n        ),\n    )\n    parser.add_argument(\"--do_train\", action=\"store_true\", help=\"Whether to run training.\")\n    parser.add_argument(\"--do_eval\", action=\"store_true\", help=\"Whether to run eval on the dev set.\")\n    parser.add_argument(\n        \"--evaluate_during_training\",\n        action=\"store_true\",\n        help=\"Run evaluation during training at each logging step.\",\n    )\n    parser.add_argument(\n        \"--do_lower_case\",\n        action=\"store_true\",\n        help=\"Set this flag if you are using an uncased model.\",\n    )\n\n    parser.add_argument(\n        \"--per_gpu_train_batch_size\",\n        default=8,\n        type=int,\n        help=\"Batch size per GPU/CPU for training.\",\n    )\n    parser.add_argument(\n        \"--per_gpu_eval_batch_size\",\n        default=1,\n        type=int,\n        help=\"Batch size per GPU/CPU for evaluation.\",\n    )\n    parser.add_argument(\n        \"--gradient_accumulation_steps\",\n        type=int,\n        default=1,\n        help=\"Number of updates steps to accumulate before performing a backward/update pass.\",\n    )\n    parser.add_argument(\n        \"--learning_rate\",\n        default=5e-5,\n        type=float,\n        help=\"The initial learning rate for Adam.\",\n    )\n    parser.add_argument(\"--weight_decay\", default=0.0, type=float, help=\"Weight decay if we apply some.\")\n    parser.add_argument(\"--adam_epsilon\", default=1e-8, type=float, help=\"Epsilon for Adam optimizer.\")\n    parser.add_argument(\"--max_grad_norm\", default=1.0, type=float, help=\"Max gradient norm.\")\n    parser.add_argument(\n        \"--num_train_epochs\",\n        default=3.0,\n        type=float,\n        help=\"Total number of training epochs to perform.\",\n    )\n    parser.add_argument(\n        \"--max_steps\",\n        default=-1,\n        type=int,\n        help=\"If > 0: set total number of training steps to perform. Override num_train_epochs.\",\n    )\n    parser.add_argument(\"--warmup_steps\", default=0, type=int, help=\"Linear warmup over warmup_steps.\")\n\n    parser.add_argument(\"--logging_steps\", type=int, default=500, help=\"Log every X updates steps.\")\n    parser.add_argument(\n        \"--save_steps\",\n        type=int,\n        default=500,\n        help=\"Save checkpoint every X updates steps.\",\n    )\n    parser.add_argument(\n        \"--eval_all_checkpoints\",\n        action=\"store_true\",\n        help=\"Evaluate all checkpoints starting with the same prefix as model_name ending and ending with step number\",\n    )\n    parser.add_argument(\"--no_cuda\", action=\"store_true\", help=\"Avoid using CUDA when available\")\n    parser.add_argument(\n        \"--overwrite_output_dir\",\n        action=\"store_true\",\n        help=\"Overwrite the content of the output directory\",\n    )\n    parser.add_argument(\n        \"--overwrite_cache\",\n        action=\"store_true\",\n        help=\"Overwrite the cached training and evaluation sets\",\n    )\n    parser.add_argument(\"--seed\", type=int, default=42, help=\"random seed for initialization\")\n\n    parser.add_argument(\n        \"--fp16\",\n        action=\"store_true\",\n        help=\"Whether to use 16-bit (mixed) precision (through NVIDIA apex) instead of 32-bit\",\n    )\n    parser.add_argument(\n        \"--fp16_opt_level\",\n        type=str,\n        default=\"O1\",\n        help=(\n            \"For fp16: Apex AMP optimization level selected in ['O0', 'O1', 'O2', and 'O3']. \"\n            \"See details at https://nvidia.github.io/apex/amp.html\"\n        ),\n    )\n    parser.add_argument(\n        \"--local_rank\",\n        type=int,\n        default=-1,\n        help=\"For distributed training: local_rank\",\n    )\n    parser.add_argument(\"--server_ip\", type=str, default=\"\", help=\"For distant debugging.\")\n    parser.add_argument(\"--server_port\", type=str, default=\"\", help=\"For distant debugging.\")\n    args = parser.parse_args()\n\n    if (\n        os.path.exists(args.output_dir)\n        and os.listdir(args.output_dir)\n        and args.do_train\n        and not args.overwrite_output_dir\n    ):\n        raise ValueError(\n            \"Output directory ({}) already exists and is not empty. Use --overwrite_output_dir to overcome.\".format(\n                args.output_dir\n            )\n        )\n\n    # Setup distant debugging if needed\n    if args.server_ip and args.server_port:\n        # Distant debugging - see https://code.visualstudio.com/docs/python/debugging#_attach-to-a-local-script\n        import ptvsd\n\n        print(\"Waiting for debugger attach\")\n        ptvsd.enable_attach(address=(args.server_ip, args.server_port), redirect_output=True)\n        ptvsd.wait_for_attach()\n\n    # Setup CUDA, GPU & distributed training\n    if args.local_rank == -1 or args.no_cuda:\n        device = torch.device(\"cuda\" if torch.cuda.is_available() and not args.no_cuda else \"cpu\")\n        args.n_gpu = torch.cuda.device_count()\n    else:  # Initializes the distributed backend which will take care of synchronizing nodes/GPUs\n        torch.cuda.set_device(args.local_rank)\n        device = torch.device(\"cuda\", args.local_rank)\n        torch.distributed.init_process_group(backend=\"nccl\")\n        args.n_gpu = 1\n    args.device = device\n\n    # Setup logging\n    logging.basicConfig(\n        format=\"%(asctime)s - %(levelname)s - %(name)s - %(message)s\",\n        datefmt=\"%m/%d/%Y %H:%M:%S\",\n        level=logging.INFO if args.local_rank in [-1, 0] else logging.WARN,\n    )\n    logger.warning(\n        \"Process rank: %s, device: %s, n_gpu: %s, distributed training: %s, 16-bits training: %s\",\n        args.local_rank,\n        device,\n        args.n_gpu,\n        bool(args.local_rank != -1),\n        args.fp16,\n    )\n    # Set the verbosity to info of the Transformers logger (on main process only):\n    if is_main_process(args.local_rank):\n        transformers.utils.logging.set_verbosity_info()\n        transformers.utils.logging.enable_default_handler()\n        transformers.utils.logging.enable_explicit_format()\n    # Set seed\n    set_seed(args)\n\n    # Prepare GLUE task\n    args.task_name = args.task_name.lower()\n    if args.task_name not in processors:\n        raise ValueError(\"Task not found: %s\" % (args.task_name))\n    processor = processors[args.task_name]()\n    args.output_mode = output_modes[args.task_name]\n    label_list = processor.get_labels()\n    num_labels = len(label_list)\n\n    if args.patience != \"0\" and args.per_gpu_eval_batch_size != 1:\n        raise ValueError(\"The eval batch size must be 1 with PABEE inference on.\")\n\n    # Load pretrained model and tokenizer\n    if args.local_rank not in [-1, 0]:\n        torch.distributed.barrier()  # Make sure only the first process in distributed training will download model & vocab\n\n    args.model_type = args.model_type.lower()\n    config_class, model_class, tokenizer_class = MODEL_CLASSES[args.model_type]\n    config = config_class.from_pretrained(\n        args.config_name if args.config_name else args.model_name_or_path,\n        num_labels=num_labels,\n        finetuning_task=args.task_name,\n        cache_dir=args.cache_dir if args.cache_dir else None,\n    )\n    tokenizer = tokenizer_class.from_pretrained(\n        args.tokenizer_name if args.tokenizer_name else args.model_name_or_path,\n        do_lower_case=args.do_lower_case,\n        cache_dir=args.cache_dir if args.cache_dir else None,\n    )\n    model = model_class.from_pretrained(\n        args.model_name_or_path,\n        from_tf=bool(\".ckpt\" in args.model_name_or_path),\n        config=config,\n        cache_dir=args.cache_dir if args.cache_dir else None,\n    )\n\n    if args.local_rank == 0:\n        torch.distributed.barrier()  # Make sure only the first process in distributed training will download model & vocab\n\n    model.to(args.device)\n\n    print(\"Total Model Parameters:\", sum(param.numel() for param in model.parameters()))\n    output_layers_param_num = sum(param.numel() for param in model.classifiers.parameters())\n    print(\"Output Layers Parameters:\", output_layers_param_num)\n    single_output_layer_param_num = sum(param.numel() for param in model.classifiers[0].parameters())\n    print(\n        \"Added Output Layers Parameters:\",\n        output_layers_param_num - single_output_layer_param_num,\n    )\n\n    logger.info(\"Training/evaluation parameters %s\", args)\n\n    # Training\n    if args.do_train:\n        train_dataset = load_and_cache_examples(args, args.task_name, tokenizer, evaluate=False)\n        global_step, tr_loss = train(args, train_dataset, model, tokenizer)\n        logger.info(\" global_step = %s, average loss = %s\", global_step, tr_loss)\n\n    # Saving best-practices: if you use defaults names for the model, you can reload it using from_pretrained()\n    if args.do_train and (args.local_rank == -1 or torch.distributed.get_rank() == 0):\n        logger.info(\"Saving model checkpoint to %s\", args.output_dir)\n        # Save a trained model, configuration and tokenizer using `save_pretrained()`.\n        # They can then be reloaded using `from_pretrained()`\n        model_to_save = (\n            model.module if hasattr(model, \"module\") else model\n        )  # Take care of distributed/parallel training\n        model_to_save.save_pretrained(args.output_dir)\n        tokenizer.save_pretrained(args.output_dir)\n\n        # Good practice: save your training arguments together with the trained model\n        torch.", "completion_part": "save(args, os.path.join(args.output_dir, \"training_args.bin\"))", "after_completion": "\n\n        # Load a trained model and vocabulary that you have fine-tuned\n        model = model_class.from_pretrained(args.output_dir)\n        tokenizer = tokenizer_class.from_pretrained(args.output_dir)\n        model.to(args.device)\n\n    # Evaluation\n    results = {}\n    if args.do_eval and args.local_rank in [-1, 0]:\n        patience_list = [int(x) for x in args.patience.split(\",\")]\n        tokenizer = tokenizer_class.from_pretrained(args.output_dir, do_lower_case=args.do_lower_case)\n        checkpoints = [args.output_dir]\n        if args.eval_all_checkpoints:\n            checkpoints = [\n                os.path.dirname(c) for c in sorted(glob.glob(args.output_dir + \"/**/\" + WEIGHTS_NAME, recursive=True))\n            ]\n\n        logger.info(\"Evaluate the following checkpoints: %s\", checkpoints)\n\n        for checkpoint in checkpoints:\n            global_step = checkpoint.split(\"-\")[-1] if len(checkpoints) > 1 else \"\"\n            prefix = checkpoint.split(\"/\")[-1] if checkpoint.find(\"checkpoint\") != -1 else \"\"\n\n            model = model_class.from_pretrained(checkpoint)\n            model.to(args.device)\n\n            print(f\"Evaluation for checkpoint {prefix}\")\n            for patience in patience_list:\n                result = evaluate(args, model, tokenizer, prefix=prefix, patience=patience)\n                result = {k + \"_{}\".format(global_step): v for k, v in result.items()}\n                results.update(result)\n    return results", "namespace": "torch.save", "completions": ["save(args, os.path.join("]}
{"id": 82, "prompt": "def nargsort(\n    items,\n    kind: str = \"quicksort\",\n    ascending: bool = True,\n    na_position: str = \"last\",\n    key: Callable | None = None,\n    mask: npt.NDArray[np.bool_] | None = None,\n) -> npt.NDArray[np.intp]:\n\n    if key is not None:\n        items = ensure_key_mapped(items, key)\n        return nargsort(\n            items,\n            kind=kind,\n            ascending=ascending,\n            na_position=na_position,\n            key=None,\n            mask=mask,\n        )\n\n    if isinstance(items, ABCRangeIndex):\n        return items.argsort(ascending=ascending)  # TODO: test coverage with key?\n    elif not isinstance(items, ABCMultiIndex):\n        items = extract_array(items)\n    if mask is None:\n        mask = np.asarray(isna(items))  # TODO: does this exclude MultiIndex too?\n\n    if is_extension_array_dtype(items):\n        return items.argsort(ascending=ascending, kind=kind, na_position=na_position)\n    else:\n        items = np.asanyarray(items)\n\n    idx = np.arange(len(items))\n    non_nans = items[~mask]\n    non_nan_idx = idx[~mask]\n\n    nan_idx = np.", "completion_part": "nonzero(mask)", "after_completion": "[0]\n    if not ascending:\n        non_nans = non_nans[::-1]\n        non_nan_idx = non_nan_idx[::-1]\n    indexer = non_nan_idx[non_nans.argsort(kind=kind)]\n    if not ascending:\n        indexer = indexer[::-1]\n    # Finally, place the NaNs at the end or the beginning according to\n    # na_position\n    if na_position == \"last\":\n        indexer = np.concatenate([indexer, nan_idx])\n    elif na_position == \"first\":\n        indexer = np.concatenate([nan_idx, indexer])\n    else:\n        raise ValueError(f\"invalid na_position: {na_position}\")\n    return ensure_platform_int(indexer)", "namespace": "numpy.nonzero", "completions": ["nonzero(mask)[0]\n    if len("]}
{"id": 83, "prompt": "    def set_data(self, x, y, A):\n        x = np.array(x, np.float32)\n        y = np.array(y, np.float32)\n        A = cbook.", "completion_part": "safe_masked_invalid(A, copy=True)", "after_completion": "\n        if not (x.ndim == y.ndim == 1 and A.shape[0:2] == y.shape + x.shape):\n            raise TypeError(\"Axes don't match array shape\")\n        if A.ndim not in [2, 3]:\n            raise TypeError(\"Can only plot 2D or 3D data\")\n        if A.ndim == 3 and A.shape[2] not in [1, 3, 4]:\n            raise TypeError(\"3D arrays must have three (RGB) \"\n                            \"or four (RGBA) color components\")\n        if A.ndim == 3 and A.shape[2] == 1:\n            A = A.squeeze(axis=-1)\n        self._A = A\n        self._Ax = x\n        self._Ay = y\n        self._imcache = None\n\n        self.stale = True", "namespace": "matplotlib.cbook.safe_masked_invalid", "completions": ["asarray(A, dtype=np.float3"]}
{"id": 84, "prompt": "\ndef main():\n    parser = argparse.ArgumentParser()\n\n    # Required parameters\n    parser.add_argument(\n        \"--data_dir\",\n        default=None,\n        type=str,\n        required=True,\n        help=\"The input data dir. Should contain the .tsv files (or other data files) for the task.\",\n    )\n    parser.add_argument(\n        \"--model_type\",\n        default=None,\n        type=str,\n        required=True,\n        help=\"Model type selected in the list: \" + \", \".join(MODEL_CLASSES.keys()),\n    )\n    parser.add_argument(\n        \"--model_name_or_path\",\n        default=None,\n        type=str,\n        required=True,\n        help=\"Path to pre-trained model or shortcut name.\",\n    )\n    parser.add_argument(\n        \"--task_name\",\n        default=None,\n        type=str,\n        required=True,\n        help=\"The name of the task to train selected in the list: \" + \", \".join(processors.keys()),\n    )\n    parser.add_argument(\n        \"--output_dir\",\n        default=None,\n        type=str,\n        required=True,\n        help=\"The output directory where the model predictions and checkpoints will be written.\",\n    )\n    parser.add_argument(\n        \"--patience\",\n        default=\"0\",\n        type=str,\n        required=False,\n    )\n    parser.add_argument(\n        \"--regression_threshold\",\n        default=0,\n        type=float,\n        required=False,\n    )\n\n    # Other parameters\n    parser.add_argument(\n        \"--config_name\",\n        default=\"\",\n        type=str,\n        help=\"Pretrained config name or path if not the same as model_name\",\n    )\n    parser.add_argument(\n        \"--tokenizer_name\",\n        default=\"\",\n        type=str,\n        help=\"Pretrained tokenizer name or path if not the same as model_name\",\n    )\n    parser.add_argument(\n        \"--cache_dir\",\n        default=\"\",\n        type=str,\n        help=\"Where do you want to store the pre-trained models downloaded from huggingface.co\",\n    )\n    parser.add_argument(\n        \"--max_seq_length\",\n        default=128,\n        type=int,\n        help=(\n            \"The maximum total input sequence length after tokenization. Sequences longer \"\n            \"than this will be truncated, sequences shorter will be padded.\"\n        ),\n    )\n    parser.add_argument(\"--do_train\", action=\"store_true\", help=\"Whether to run training.\")\n    parser.add_argument(\"--do_eval\", action=\"store_true\", help=\"Whether to run eval on the dev set.\")\n    parser.add_argument(\n        \"--evaluate_during_training\",\n        action=\"store_true\",\n        help=\"Run evaluation during training at each logging step.\",\n    )\n    parser.add_argument(\n        \"--do_lower_case\",\n        action=\"store_true\",\n        help=\"Set this flag if you are using an uncased model.\",\n    )\n\n    parser.add_argument(\n        \"--per_gpu_train_batch_size\",\n        default=8,\n        type=int,\n        help=\"Batch size per GPU/CPU for training.\",\n    )\n    parser.add_argument(\n        \"--per_gpu_eval_batch_size\",\n        default=1,\n        type=int,\n        help=\"Batch size per GPU/CPU for evaluation.\",\n    )\n    parser.add_argument(\n        \"--gradient_accumulation_steps\",\n        type=int,\n        default=1,\n        help=\"Number of updates steps to accumulate before performing a backward/update pass.\",\n    )\n    parser.add_argument(\n        \"--learning_rate\",\n        default=5e-5,\n        type=float,\n        help=\"The initial learning rate for Adam.\",\n    )\n    parser.add_argument(\"--weight_decay\", default=0.0, type=float, help=\"Weight decay if we apply some.\")\n    parser.add_argument(\"--adam_epsilon\", default=1e-8, type=float, help=\"Epsilon for Adam optimizer.\")\n    parser.add_argument(\"--max_grad_norm\", default=1.0, type=float, help=\"Max gradient norm.\")\n    parser.add_argument(\n        \"--num_train_epochs\",\n        default=3.0,\n        type=float,\n        help=\"Total number of training epochs to perform.\",\n    )\n    parser.add_argument(\n        \"--max_steps\",\n        default=-1,\n        type=int,\n        help=\"If > 0: set total number of training steps to perform. Override num_train_epochs.\",\n    )\n    parser.add_argument(\"--warmup_steps\", default=0, type=int, help=\"Linear warmup over warmup_steps.\")\n\n    parser.add_argument(\"--logging_steps\", type=int, default=500, help=\"Log every X updates steps.\")\n    parser.add_argument(\n        \"--save_steps\",\n        type=int,\n        default=500,\n        help=\"Save checkpoint every X updates steps.\",\n    )\n    parser.add_argument(\n        \"--eval_all_checkpoints\",\n        action=\"store_true\",\n        help=\"Evaluate all checkpoints starting with the same prefix as model_name ending and ending with step number\",\n    )\n    parser.add_argument(\"--no_cuda\", action=\"store_true\", help=\"Avoid using CUDA when available\")\n    parser.add_argument(\n        \"--overwrite_output_dir\",\n        action=\"store_true\",\n        help=\"Overwrite the content of the output directory\",\n    )\n    parser.add_argument(\n        \"--overwrite_cache\",\n        action=\"store_true\",\n        help=\"Overwrite the cached training and evaluation sets\",\n    )\n    parser.add_argument(\"--seed\", type=int, default=42, help=\"random seed for initialization\")\n\n    parser.add_argument(\n        \"--fp16\",\n        action=\"store_true\",\n        help=\"Whether to use 16-bit (mixed) precision (through NVIDIA apex) instead of 32-bit\",\n    )\n    parser.add_argument(\n        \"--fp16_opt_level\",\n        type=str,\n        default=\"O1\",\n        help=(\n            \"For fp16: Apex AMP optimization level selected in ['O0', 'O1', 'O2', and 'O3']. \"\n            \"See details at https://nvidia.github.io/apex/amp.html\"\n        ),\n    )\n    parser.add_argument(\n        \"--local_rank\",\n        type=int,\n        default=-1,\n        help=\"For distributed training: local_rank\",\n    )\n    parser.add_argument(\"--server_ip\", type=str, default=\"\", help=\"For distant debugging.\")\n    parser.add_argument(\"--server_port\", type=str, default=\"\", help=\"For distant debugging.\")\n    args = parser.parse_args()\n\n    if (\n        os.path.exists(args.output_dir)\n        and os.listdir(args.output_dir)\n        and args.do_train\n        and not args.overwrite_output_dir\n    ):\n        raise ValueError(\n            \"Output directory ({}) already exists and is not empty. Use --overwrite_output_dir to overcome.\".format(\n                args.output_dir\n            )\n        )\n\n    # Setup distant debugging if needed\n    if args.server_ip and args.server_port:\n        # Distant debugging - see https://code.visualstudio.com/docs/python/debugging#_attach-to-a-local-script\n        import ptvsd\n\n        print(\"Waiting for debugger attach\")\n        ptvsd.enable_attach(address=(args.server_ip, args.server_port), redirect_output=True)\n        ptvsd.wait_for_attach()\n\n    # Setup CUDA, GPU & distributed training\n    if args.local_rank == -1 or args.no_cuda:\n        device = torch.", "completion_part": "device(\"cuda\" if torch.cuda.is_available() and not args.no_cuda else \"cpu\")", "after_completion": "\n        args.n_gpu = torch.cuda.device_count()\n    else:  # Initializes the distributed backend which will take care of synchronizing nodes/GPUs\n        torch.cuda.set_device(args.local_rank)\n        device = torch.device(\"cuda\", args.local_rank)\n        torch.distributed.init_process_group(backend=\"nccl\")\n        args.n_gpu = 1\n    args.device = device\n\n    # Setup logging\n    logging.basicConfig(\n        format=\"%(asctime)s - %(levelname)s - %(name)s - %(message)s\",\n        datefmt=\"%m/%d/%Y %H:%M:%S\",\n        level=logging.INFO if args.local_rank in [-1, 0] else logging.WARN,\n    )\n    logger.warning(\n        \"Process rank: %s, device: %s, n_gpu: %s, distributed training: %s, 16-bits training: %s\",\n        args.local_rank,\n        device,\n        args.n_gpu,\n        bool(args.local_rank != -1),\n        args.fp16,\n    )\n    # Set the verbosity to info of the Transformers logger (on main process only):\n    if is_main_process(args.local_rank):\n        transformers.utils.logging.set_verbosity_info()\n        transformers.utils.logging.enable_default_handler()\n        transformers.utils.logging.enable_explicit_format()\n    # Set seed\n    set_seed(args)\n\n    # Prepare GLUE task\n    args.task_name = args.task_name.lower()\n    if args.task_name not in processors:\n        raise ValueError(\"Task not found: %s\" % (args.task_name))\n    processor = processors[args.task_name]()\n    args.output_mode = output_modes[args.task_name]\n    label_list = processor.get_labels()\n    num_labels = len(label_list)\n\n    if args.patience != \"0\" and args.per_gpu_eval_batch_size != 1:\n        raise ValueError(\"The eval batch size must be 1 with PABEE inference on.\")\n\n    # Load pretrained model and tokenizer\n    if args.local_rank not in [-1, 0]:\n        torch.distributed.barrier()  # Make sure only the first process in distributed training will download model & vocab\n\n    args.model_type = args.model_type.lower()\n    config_class, model_class, tokenizer_class = MODEL_CLASSES[args.model_type]\n    config = config_class.from_pretrained(\n        args.config_name if args.config_name else args.model_name_or_path,\n        num_labels=num_labels,\n        finetuning_task=args.task_name,\n        cache_dir=args.cache_dir if args.cache_dir else None,\n    )\n    tokenizer = tokenizer_class.from_pretrained(\n        args.tokenizer_name if args.tokenizer_name else args.model_name_or_path,\n        do_lower_case=args.do_lower_case,\n        cache_dir=args.cache_dir if args.cache_dir else None,\n    )\n    model = model_class.from_pretrained(\n        args.model_name_or_path,\n        from_tf=bool(\".ckpt\" in args.model_name_or_path),\n        config=config,\n        cache_dir=args.cache_dir if args.cache_dir else None,\n    )\n\n    if args.local_rank == 0:\n        torch.distributed.barrier()  # Make sure only the first process in distributed training will download model & vocab\n\n    model.to(args.device)\n\n    print(\"Total Model Parameters:\", sum(param.numel() for param in model.parameters()))\n    output_layers_param_num = sum(param.numel() for param in model.classifiers.parameters())\n    print(\"Output Layers Parameters:\", output_layers_param_num)\n    single_output_layer_param_num = sum(param.numel() for param in model.classifiers[0].parameters())\n    print(\n        \"Added Output Layers Parameters:\",\n        output_layers_param_num - single_output_layer_param_num,\n    )\n\n    logger.info(\"Training/evaluation parameters %s\", args)\n\n    # Training\n    if args.do_train:\n        train_dataset = load_and_cache_examples(args, args.task_name, tokenizer, evaluate=False)\n        global_step, tr_loss = train(args, train_dataset, model, tokenizer)\n        logger.info(\" global_step = %s, average loss = %s\", global_step, tr_loss)\n\n    # Saving best-practices: if you use defaults names for the model, you can reload it using from_pretrained()\n    if args.do_train and (args.local_rank == -1 or torch.distributed.get_rank() == 0):\n        logger.info(\"Saving model checkpoint to %s\", args.output_dir)\n        # Save a trained model, configuration and tokenizer using `save_pretrained()`.\n        # They can then be reloaded using `from_pretrained()`\n        model_to_save = (\n            model.module if hasattr(model, \"module\") else model\n        )  # Take care of distributed/parallel training\n        model_to_save.save_pretrained(args.output_dir)\n        tokenizer.save_pretrained(args.output_dir)\n\n        # Good practice: save your training arguments together with the trained model\n        torch.save(args, os.path.join(args.output_dir, \"training_args.bin\"))\n\n        # Load a trained model and vocabulary that you have fine-tuned\n        model = model_class.from_pretrained(args.output_dir)\n        tokenizer = tokenizer_class.from_pretrained(args.output_dir)\n        model.to(args.device)\n\n    # Evaluation\n    results = {}\n    if args.do_eval and args.local_rank in [-1, 0]:\n        patience_list = [int(x) for x in args.patience.split(\",\")]\n        tokenizer = tokenizer_class.from_pretrained(args.output_dir, do_lower_case=args.do_lower_case)\n        checkpoints = [args.output_dir]\n        if args.eval_all_checkpoints:\n            checkpoints = [\n                os.path.dirname(c) for c in sorted(glob.glob(args.output_dir + \"/**/\" + WEIGHTS_NAME, recursive=True))\n            ]\n\n        logger.info(\"Evaluate the following checkpoints: %s\", checkpoints)\n\n        for checkpoint in checkpoints:\n            global_step = checkpoint.split(\"-\")[-1] if len(checkpoints) > 1 else \"\"\n            prefix = checkpoint.split(\"/\")[-1] if checkpoint.find(\"checkpoint\") != -1 else \"\"\n\n            model = model_class.from_pretrained(checkpoint)\n            model.to(args.device)\n\n            print(f\"Evaluation for checkpoint {prefix}\")\n            for patience in patience_list:\n                result = evaluate(args, model, tokenizer, prefix=prefix, patience=patience)\n                result = {k + \"_{}\".format(global_step): v for k, v in result.items()}\n                results.update(result)\n    return results", "namespace": "torch.device", "completions": ["device(\"cuda\" if torch.cuda.is"]}
{"id": 85, "prompt": "def points_in_polygon2D(p, poly_points):\n    if len(p.shape) == 1:\n        # Single point provided\n        points = p.copy()[np.newaxis, :]\n    else:\n        points = p.copy()\n\n    points += 1E-3\n\n    # Ensure polygon is closed\n    if np.any(poly_points[0, :] != poly_points[-1, :]):\n        poly_points = np.append(poly_points, poly_points[0, :][np.newaxis, :], axis=0)\n\n    npoints = points.shape[0]\n\n    mask = np.zeros(npoints, dtype=bool)\n\n    # Get random point that is definitely outside the polygon\n    a, b = 10 + np.random.", "completion_part": "random(size=2)", "after_completion": " * 10\n\n    test_point = np.array([a * np.max(poly_points[:, 0]), b * np.max(poly_points[:, 1])])\n\n    for i in range(npoints):\n        intersections = 0\n        for j in range(poly_points.shape[0] - 1):\n            intersections += line_intersect_2D(poly_points[j, :2], poly_points[j+1, :2], test_point, points[i, :2])\n        if intersections % 2:\n            # Even number of intersections, point is inside polygon\n            mask[i] = True\n    return mask.astype(bool)", "namespace": "numpy.random.random", "completions": ["rand(2) * 100\n   "]}
{"id": 86, "prompt": "\n    def _get_logits(\n        self,\n        hidden_states: torch.Tensor,\n        embedding: torch.Tensor,\n        embedding_bias: Optional[torch.Tensor] = None,\n    ) -> Optional[torch.Tensor]:\n        # Get the logits for the next tokens.\n        logits = torch.", "completion_part": "matmul(hidden_states, embedding.t())", "after_completion": "\n        if embedding_bias is not None:\n            logits += embedding_bias\n        logits = tensor_model_parallel_gather(logits)\n        if logits is None:\n            return None\n\n        lora_logits = torch.empty(\n            self.embeddings_tensors.shape[0] + 1,\n            self.embeddings_tensors.shape[1],\n            hidden_states.shape[0],\n            dtype=self.embeddings_tensors.dtype,\n            device=self.embeddings_tensors.device,\n        )\n        torch.matmul(self.embeddings_tensors,\n                     hidden_states.T,\n                     out=lora_logits[:-1])\n        lora_logits[-1] = float(\"-inf\")\n        lora_logits = lora_logits.mT\n        lora_logits = (lora_logits.reshape(\n            lora_logits.shape[0] * lora_logits.shape[1],\n            lora_logits.shape[2],\n        ).index_select(0,\n                       self.indices_padded[:self.indices_len[2]]).nan_to_num_(\n                           nan=float(\"-inf\"),\n                           posinf=float(\"inf\"),\n                           neginf=float(\"-inf\")))\n        logits[:,\n               self.base_layer.org_vocab_size:self.base_layer.org_vocab_size +\n               lora_logits.shape[1]] = lora_logits\n\n        _apply_lora(\n            hidden_states,\n            self.lora_a_stacked,\n            self.lora_b_stacked,\n            self.indices[:self.indices_len[1]],\n            logits,\n        )\n\n        # Remove paddings in vocab (if any).\n        logits = logits[:, :self.base_layer.vocab_size]\n\n        return logits", "namespace": "torch.matmul", "completions": ["matmul(hidden_states, embedding.transpose("]}
{"id": 87, "prompt": "    def to_dataframe(\n        self, index: Optional[Union[str, None]] = \"date\", sort_by: Optional[str] = None\n    ) -> pd.DataFrame:\n\n        def is_list_of_basemodel(items: Union[List[T], T]) -> bool:\n            return isinstance(items, list) and all(\n                isinstance(item, BaseModel) for item in items\n            )\n\n        if self.results is None or not self.results:\n            raise OpenBBError(\"Results not found.\")\n\n        if isinstance(self.results, pd.DataFrame):\n            return self.results\n\n        try:\n            res = self.results\n            df = None\n            sort_columns = True\n\n            # List[Dict]\n            if isinstance(res, list) and len(res) == 1 and isinstance(res[0], dict):\n                r = res[0]\n                dict_of_df = {}\n\n                for k, v in r.items():\n                    # Dict[str, List[BaseModel]]\n                    if is_list_of_basemodel(v):\n                        dict_of_df[k] = basemodel_to_df(v, index)\n                        sort_columns = False\n                    # Dict[str, Any]\n                    else:\n                        dict_of_df[k] = pd.", "completion_part": "DataFrame(v)", "after_completion": "\n\n                df = pd.concat(dict_of_df, axis=1)\n\n            # List[BaseModel]\n            elif is_list_of_basemodel(res):\n                dt: Union[List[Data], Data] = res  # type: ignore\n                df = basemodel_to_df(dt, index)\n                sort_columns = False\n            # List[List | str | int | float] | Dict[str, Dict | List | BaseModel]\n            else:\n                try:\n                    df = pd.DataFrame(res)\n                    # Set index, if any\n                    if index is not None and index in df.columns:\n                        df.set_index(index, inplace=True)\n\n                except ValueError:\n                    if isinstance(res, dict):\n                        df = pd.DataFrame([res])\n\n            if df is None:\n                raise OpenBBError(\"Unsupported data format.\")\n\n            # Drop columns that are all NaN, but don't rearrange columns\n            if sort_columns:\n                df.sort_index(axis=1, inplace=True)\n            df = df.dropna(axis=1, how=\"all\")\n\n            # Sort by specified column\n            if sort_by:\n                df.sort_values(by=sort_by, inplace=True)\n\n        except OpenBBError as e:\n            raise e\n        except ValueError as ve:\n            raise OpenBBError(\n                f\"ValueError: {ve}. Ensure the data format matches the expected format.\"\n            ) from ve\n        except TypeError as te:\n            raise OpenBBError(\n                f\"TypeError: {te}. Check the data types in your results.\"\n            ) from te\n        except Exception as ex:\n            raise OpenBBError(f\"An unexpected error occurred: {ex}\") from ex\n\n        return df", "namespace": "pandas.DataFrame", "completions": ["DataFrame(v, index=[0])\n\n                df"]}
{"id": 88, "prompt": "def unflatten_lower_triangle(flattened: torch.Tensor, M: int, N: int) -> torch.Tensor:\n    # Get the indices of the lower triangular part\n    row_indices, col_indices = torch.tril_indices(M, N)\n\n    # Create an empty tensor to fill in the original shape\n    new_shape = list(flattened.shape[:-1]) + [M, N]\n    reconstructed = torch.", "completion_part": "zeros(new_shape, dtype=flattened.dtype, device=flattened.device)", "after_completion": "\n\n    # Fill in the lower triangular part of the reconstructed tensor\n    reconstructed[..., row_indices, col_indices] = flattened\n\n    return reconstructed", "namespace": "torch.zeros", "completions": ["zeros(new_shape, dtype=flattened"]}
{"id": 89, "prompt": "def sample_intervention(\n    sem: SEM, tensordict_data: TensorDict, num_intervention_samples: int, treatment: str | None = None\n) -> InterventionData:\n    intervention_a = sample_intervention_dict(tensordict_data, treatment=treatment)\n\n    intervention_a_samples = sem.do(intervention_a).sample(\n        torch.", "completion_part": "Size(\n            [\n                num_intervention_samples,\n            ]\n        )", "after_completion": "\n    )\n\n    return InterventionData(\n        intervention_a_samples,\n        intervention_a,\n        TensorDict({}, batch_size=torch.Size()),\n    )", "namespace": "torch.Size", "completions": ["Size([num_intervention_samples])\n   "]}
{"id": 90, "prompt": "def unary_encoder_random_gaussian(nqubits: int, architecture: str = \"tree\", seed=None):\n    if not isinstance(nqubits, int):\n        raise_error(\n            TypeError, f\"nqubits must be type int, but it is type {type(nqubits)}.\"\n        )\n\n    if nqubits <= 0.0:\n        raise_error(\n            ValueError, f\"nqubits must be a positive integer, but it is {nqubits}.\"\n        )\n\n    if not isinstance(architecture, str):\n        raise_error(\n            TypeError,\n            f\"``architecture`` must be type str, but it is type {type(architecture)}.\",\n        )\n\n    if architecture != \"tree\":\n        raise_error(\n            NotImplementedError,\n            f\"Currently, this function only accepts ``architecture=='tree'``.\",\n        )\n\n    if not math.log2(nqubits).is_integer():\n        raise_error(ValueError, f\"nqubits must be a power of 2, but it is {nqubits}.\")\n\n    if (\n        seed is not None\n        and not isinstance(seed, int)\n        and not isinstance(seed, np.random.Generator)\n    ):\n        raise_error(\n            TypeError, \"seed must be either type int or numpy.random.Generator.\"\n        )\n\n    from qibo.quantum_info.random_ensembles import (  # pylint: disable=C0415\n        _ProbabilityDistributionGaussianLoader,\n    )\n\n    local_state = (\n        np.random.", "completion_part": "default_rng(seed)", "after_completion": " if seed is None or isinstance(seed, int) else seed\n    )\n\n    sampler = _ProbabilityDistributionGaussianLoader(\n        a=0, b=2 * math.pi, seed=local_state\n    )\n\n    circuit = Circuit(nqubits)\n    circuit.add(gates.X(nqubits - 1))\n    circuit_rbs, pairs_rbs = _generate_rbs_pairs(nqubits, architecture)\n    circuit += circuit_rbs\n\n    phases = []\n    for depth, row in enumerate(pairs_rbs, 1):\n        phases.extend(sampler.rvs(depth=depth, size=len(row)))\n\n    circuit.set_parameters(phases)\n\n    return circuit", "namespace": "numpy.random.default_rng", "completions": ["default_rng(seed)\n        if isinstance("]}
{"id": 91, "prompt": "def obj_fcn_decorator(\n    model_fcn_full,\n    weight_fcn,\n    TSS_fcn,\n    T,\n    obs,\n    settings,\n    alpha=2.0,\n    coef_id=[],\n    initial_fit=True,\n):\n\n    N = np.", "completion_part": "shape(obs)", "after_completion": "[0]\n    N_min = settings.segment_minimum_count  # N minimum for a sloped segment\n    sigma = 2.698  # 1.5 IQR\n    quantile = 0.25\n    min_weight = 0.00\n\n    T_fit_bnds = np.array([np.min(T), np.max(T)])\n    T_range = T_fit_bnds[1] - T_fit_bnds[0]\n\n    model_fcn = model_fcn_dec(model_fcn_full, T_fit_bnds, T)\n\n    lasso_a = settings.regularization_percent_lasso * settings.regularization_alpha\n    ridge_a = (\n        1 - settings.regularization_percent_lasso\n    ) * settings.regularization_alpha\n\n    idx_k = get_idx([\"dd_k\"], coef_id)\n    idx_beta = get_idx([\"dd_beta\"], coef_id)\n    idx_bp = get_idx([\"dd_bp\"], coef_id)\n    # idx_reg = get_idx([\"dd_beta\", \"dd_k\"], coef_id) # drop bps and intercept from regularization\n    idx_reg = get_idx(\n        [\"dd_beta\", \"dd_k\", \"dd_bp\"], coef_id\n    )  # drop intercept from regularization\n\n    def elastic_net_penalty(X, T_sorted, obs_sorted, weight_sorted, wRMSE):\n        \"\"\"\n        Calculates the elastic net penalty for a given set of inputs. The elastic net is a regularized\n        regression method that linearly combines the L1 and L2 penalties of the lasso and ridge methods.\n\n        Parameters:\n        X (array-like): The input array.\n        T_sorted (array-like): The sorted temperature array.\n        obs_sorted (array-like): The sorted observation array.\n        weight_sorted (array-like): The sorted weight array.\n        wRMSE (float): The weighted root mean squared error.\n\n        Returns:\n        penalty (float): The elastic net penalty.\n        \"\"\"\n\n        # Elastic net\n        X_enet = np.array(X).copy()\n\n        ## Scale break points ##\n        if len(idx_bp) > 0:\n            X_enet[idx_bp] = [\n                np.min(np.abs(X_enet[idx] - T_fit_bnds)) for idx in idx_bp\n            ]\n\n            if len(idx_bp) == 2:\n                X_enet[idx_bp] += (X[idx_bp][1] - X[idx_bp][0]) / 2\n\n            X_enet[idx_bp] *= wRMSE / T_range\n\n        # Find idx for regions\n        if len(idx_bp) == 2:\n            [hdd_bp, cdd_bp] = X[idx_bp]\n\n            idx_hdd = np.argwhere(T_sorted < hdd_bp).flatten()\n            idx_tidd = np.argwhere(\n                (hdd_bp <= T_sorted) & (T_sorted <= cdd_bp)\n            ).flatten()\n            idx_cdd = np.argwhere(cdd_bp < T_sorted).flatten()\n\n        elif len(idx_bp) == 1:\n            bp = X[idx_bp]\n            if X_enet[idx_beta] < 0:  # HDD_TIDD\n                idx_hdd = np.argwhere(T_sorted <= bp).flatten()\n                idx_tidd = np.argwhere(bp < T_sorted).flatten()\n                idx_cdd = np.array([])\n\n            else:\n                idx_hdd = np.array([])  # CDD_TIDD\n                idx_tidd = np.argwhere(T_sorted < bp).flatten()\n                idx_cdd = np.argwhere(bp <= T_sorted).flatten()\n\n        else:\n            idx_hdd = np.array([])\n            idx_tidd = np.arange(0, len(T_sorted))\n            idx_cdd = np.array([])\n\n        len_hdd = len(idx_hdd)\n        len_tidd = len(idx_tidd)\n        len_cdd = len(idx_cdd)\n\n        # combine tidd with hdd/cdd if cdd/hdd are large enough to get stdev\n        if (len_hdd < N_min) and (len_cdd >= N_min):\n            idx_hdd = np.hstack([idx_hdd, idx_tidd])\n        elif (len_hdd >= N_min) and (len_cdd < N_min):\n            idx_cdd = np.hstack([idx_tidd, idx_cdd])\n\n        # change to idx_hdd and idx_cdd to int arrays\n        idx_hdd = idx_hdd.astype(int)\n        idx_cdd = idx_cdd.astype(int)\n\n        ## Normalize slopes ##\n        # calculate stdevs\n        if (len(idx_bp) == 2) and (len(idx_hdd) >= N_min) and (len(idx_cdd) >= N_min):\n            N_beta = np.array([len_hdd, len_cdd])\n            T_stdev = np.array(\n                [\n                    stdev(T_sorted[idx_hdd], weights=weight_sorted[idx_hdd]),\n                    stdev(T_sorted[idx_cdd], weights=weight_sorted[idx_cdd]),\n                ]\n            )\n            obs_stdev = np.array(\n                [\n                    stdev(obs_sorted[idx_hdd], weights=weight_sorted[idx_hdd]),\n                    stdev(obs_sorted[idx_cdd], weights=weight_sorted[idx_cdd]),\n                ]\n            )\n\n        elif (len(idx_bp) == 1) and (len(idx_hdd) >= N_min):\n            N_beta = np.array([len_hdd])\n            T_stdev = stdev(T_sorted[idx_hdd], weights=weight_sorted[idx_hdd])\n            obs_stdev = stdev(obs_sorted[idx_hdd], weights=weight_sorted[idx_hdd])\n\n        elif (len(idx_bp) == 1) and (len(idx_cdd) >= N_min):\n            N_beta = np.array([len_cdd])\n            T_stdev = stdev(T_sorted[idx_cdd], weights=weight_sorted[idx_cdd])\n            obs_stdev = stdev(obs_sorted[idx_cdd], weights=weight_sorted[idx_cdd])\n\n        else:\n            N_beta = np.array([len_tidd])\n            T_stdev = stdev(T_sorted, weights=weight_sorted)\n            obs_stdev = stdev(obs_sorted, weights=weight_sorted)\n\n        X_enet[idx_beta] *= T_stdev / obs_stdev\n\n        # add penalty to slope for not having enough datapoints\n        X_enet[idx_beta] = np.where(\n            N_beta < N_min, X_enet[idx_beta] * 1e30, X_enet[idx_beta]\n        )\n\n        ## Scale smoothing parameter ##\n        if len(idx_k) > 0:  # reducing X_enet size allows for more smoothing\n            X_enet[idx_k] = X[idx_k]\n\n            if (len(idx_k) == 2) and (np.sum(X_enet[idx_k]) > 1):\n                X_enet[idx_k] /= np.sum(X_enet[idx_k])\n\n            X_enet[idx_k] *= (\n                X_enet[idx_beta] / 2\n            )  # uncertain what to divide by, this seems to work well\n\n        X_enet = X_enet[idx_reg]\n\n        if ridge_a == 0:\n            penalty = lasso_a * np.linalg.norm(X_enet, 1)\n        else:\n            penalty = lasso_a * np.linalg.norm(X_enet, 1) + ridge_a * np.linalg.norm(\n                X_enet, 2\n            )\n\n        return penalty\n\n    def obj_fcn(X, grad=[], optimize_flag=True):\n        \"\"\"\n        Creates an objective function having the required inputs for optimization via SciPy and NLopt. If the optimize_flag is true,\n        only return the loss. If the optimize_flag is false, return the loss and the model output parameters.\n\n        Parameters:\n        - X: array-like\n            Array of coefficients.\n        - grad: array-like, optional\n            Gradient array. Default is an empty list.\n        - optimize_flag: bool, optional\n            Whether to optimize. Default is True.\n\n        Returns:\n        - obj: float\n            Objective function value.\n        \"\"\"\n        X = np.array(X)\n\n        model = model_fcn(X)\n        idx_sorted = np.argsort(T).flatten()\n        idx_initial = np.argsort(idx_sorted).flatten()\n        resid = model - obs\n\n        T_sorted = T[idx_sorted]\n        # model_sorted = model[idx_sorted]\n        obs_sorted = obs[idx_sorted]\n        resid_sorted = resid[idx_sorted]\n        weight_sorted, c, a = weight_fcn(\n            *X, T_sorted, resid_sorted, sigma, quantile, alpha, min_weight\n        )\n\n        weight = weight_sorted[idx_initial]\n        wSSE = np.sum(weight * resid**2)\n        loss = wSSE / N\n\n        if settings.regularization_alpha != 0:\n            loss += elastic_net_penalty(\n                X, T_sorted, obs_sorted, weight_sorted, np.sqrt(loss)\n            )\n\n        if optimize_flag:\n            return loss\n\n        else:\n            if (\"r_squared\" in settings.split_selection_criteria) and callable(TSS_fcn):\n                TSS = TSS_fcn(*X, T_sorted, obs_sorted)\n            else:\n                TSS = wSSE\n\n            if initial_fit:\n                jac = None\n            else:\n                eps = 10 ** (OoM(X, method=\"floor\") - 2)\n                X_lower = X - eps\n                X_upper = X + eps\n\n                # select correct finite difference scheme based on variable type and value\n                # NOTE: finite differencing was not returning great results. Looking into switching to JAX autodiff\n                fd_type = [\"central\"] * len(X)\n                for i in range(len(X)):\n                    if i in idx_k:\n                        if X_lower[i] < 0:\n                            fd_type[i] = \"forward\"\n                        elif X_upper[i] > 1:\n                            fd_type[i] = \"backward\"\n                    elif i in idx_beta:\n                        if (X[i] > 0) and (X_lower[i] < 0):\n                            fd_type[i] = \"forward\"\n                        elif (X[i] < 0) and (X_upper[i] > 0):\n                            fd_type[i] = \"backward\"\n                    elif i in idx_bp:\n                        if X_lower[i] < T_sorted[0]:\n                            fd_type[i] = \"forward\"\n                        elif X_lower[i] > T_sorted[-1]:\n                            fd_type[i] = \"backward\"\n\n                # https://stackoverflow.com/questions/70572362/compute-efficiently-hessian-matrices-in-jax\n                # hess = jit(jacfwd(jacrev(no_weights_obj_fcn), has_aux=True), has_aux=True)(X, [model_fcn, obs, idx_bp])\n                # print(hess)\n                # obj_grad_fcn = lambda X: no_weights_obj_fcn(X, [model_fcn, obs, idx_bp])\n\n                # jac = numerical_jacobian(obj_grad_fcn, X, dx=eps, fd_type=fd_type)\n                jac = None\n\n            return X, loss, TSS, T, model, weight, resid, jac, np.mean(a), c\n\n    return obj_fcn", "namespace": "numpy.shape", "completions": ["shape(obs)[0]\n    n_coef"]}
{"id": 92, "prompt": "\ndef clip_grad_norm_(params, max_norm, aggregate_norm_fn=None) -> torch.Tensor:\n    def grad_exists(p):\n        return p is not None and getattr(p, \"grad\", None) is not None\n\n    if isinstance(params, torch.Tensor):\n        params = [params]\n    params = list(params)\n    grads = [\n        p.grad.detach() for p in params if grad_exists(p) and not hasattr(p, \"expert\")\n    ]\n    expert_grads = [\n        p.grad.detach() for p in params if grad_exists(p) and hasattr(p, \"expert\")\n    ]\n\n    if len(grads) == 0:\n        if len(params) > 0:\n            return params[0].new_tensor(0.0)\n        else:\n            return torch.tensor(0.0)\n\n    if len(grads) == 1:\n        total_norm = torch.", "completion_part": "norm(grads[0], p=2, dtype=torch.float32)", "after_completion": "\n    else:\n        if multi_tensor_l2norm_available:\n            total_norm = multi_tensor_total_norm(grads)\n        else:\n            if torch.cuda.is_available():\n                warnings.warn(\n                    \"amp_C fused kernels unavailable, disabling multi_tensor_l2norm; \"\n                    \"you may get better performance by installing NVIDIA's apex library\"\n                )\n                device = torch.cuda.current_device()\n            elif grads[0].device.type == \"xla\":\n                device = grads[0].device\n            else:\n                device = torch.device(\"cpu\")\n            total_norm = torch.norm(\n                torch.stack(\n                    [torch.norm(g, p=2, dtype=torch.float32).to(device) for g in grads]\n                )\n            )\n\n    if aggregate_norm_fn is not None:\n        total_norm = aggregate_norm_fn(total_norm)\n\n    if max_norm > 0:\n        max_norm = float(max_norm)\n        clip_coef = (max_norm / (total_norm + 1e-6)).clamp_(max=1)\n        for g in grads + expert_grads:\n            g.mul_(clip_coef)\n    return total_norm", "namespace": "torch.norm", "completions": ["norm(grads[0], p=2"]}
{"id": 93, "prompt": "def pandasSQL_builder(\n    con,\n    schema: str | None = None,\n    need_transaction: bool = False,\n) -> PandasSQL:\n    import sqlite3\n\n    if isinstance(con, sqlite3.Connection) or con is None:\n        return SQLiteDatabase(con)\n\n    sqlalchemy = ", "completion_part": "import_optional_dependency(\"sqlalchemy\", errors=\"ignore\")", "after_completion": "\n\n    if isinstance(con, str) and sqlalchemy is None:\n        raise ImportError(\"Using URI string without sqlalchemy installed.\")\n\n    if sqlalchemy is not None and isinstance(con, (str, sqlalchemy.engine.Connectable)):\n        return SQLDatabase(con, schema, need_transaction)\n\n    warnings.warn(\n        \"pandas only supports SQLAlchemy connectable (engine/connection) or \"\n        \"database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 \"\n        \"objects are not tested. Please consider using SQLAlchemy.\",\n        UserWarning,\n        stacklevel=find_stack_level(),\n    )\n    return SQLiteDatabase(con)", "namespace": "pandas.compat._optional.import_optional_dependency", "completions": ["0\n    try:\n        import sqlalchemy\n    except ImportError"]}
{"id": 94, "prompt": "def import_mesh(path):\n    gltf = GLTF2.load(path)\n    gltf.convert_buffers(BufferFormat.BINARYBLOB)\n    materials = _get_materials(gltf)\n    meshes = _get_meshes(gltf)\n    for sampler in gltf.samplers:\n        if sampler.wrapS != 10497 or sampler.wrapT != 10497:\n            warnings.warn(\n                \"wrapping mode is not support yet. Please make a github request if needed.\",\n                UserWarning\n            )\n    default_scene = gltf.scenes[gltf.scene]\n    scene_meshes = []\n\n    has_tangents = False\n    has_uvs = False\n    has_normals = False\n    for mesh in meshes:\n        if mesh.has_attribute('vertex_tangents'):\n            has_tangents = True\n        if mesh.has_attribute('uvs'):\n            has_uvs = True\n        if mesh.has_attribute('normals'):\n            has_normals = True\n\n    def _traverse_scene(node_idx, cur_transform):\n        node = gltf.nodes[node_idx]\n        if node.matrix is not None:\n            node_transform = torch.", "completion_part": "tensor(node.matrix, dtype=torch.double)", "after_completion": ".reshape(4, 4)\n        else:\n            node_transform = None\n            if node.scale is not None:\n                node_transform = torch.tensor([\n                    [node.scale[0], 0., 0., 0.],\n                    [0., node.scale[1], 0., 0.],\n                    [0., 0., node.scale[2], 0.],\n                    [0., 0., 0., 1.]\n                ], dtype=torch.double)\n            if node.rotation is not None:\n                rotation_mat = _make_rotation_mat(node.rotation)\n                if node_transform is None:\n                    node_transform = rotation_mat\n                else:\n                    node_transform = node_transform @ rotation_mat\n\n            if node.translation is not None:\n                translation_mat = torch.tensor([\n                    [1., 0., 0., 0.],\n                    [0., 1., 0., 0.],\n                    [0., 0., 1., 0.],\n                    [node.translation[0], node.translation[1], node.translation[2], 1.]\n                ], dtype=torch.double)\n                if node_transform is None:\n                    node_transform = translation_mat\n                else:\n                    node_transform = node_transform @ translation_mat\n        if node_transform is not None:\n            cur_transform = node_transform @ cur_transform\n        if node.mesh is not None:\n            mesh = copy.copy(meshes[node.mesh])\n            if mesh is not None:\n                vertices = torch.nn.functional.pad(\n                    mesh.vertices, (0, 1), value=1., mode='constant')\n                vertices = vertices @ cur_transform.float()\n                mesh.vertices = vertices[..., :3]\n                if has_tangents:\n                    tangents = mesh.vertex_tangents @ cur_transform[:3, :3].float()\n                    mesh.vertex_tangents = torch.nn.functional.normalize(\n                        tangents, dim=-1)\n                if has_normals:\n                    inv_cur_transform = torch.linalg.inv(cur_transform[:3, :3]).T.float()\n                    normals = mesh.vertex_normals @ inv_cur_transform\n                    mesh.vertex_normals = torch.nn.functional.normalize(\n                        normals, dim=-1)\n\n                scene_meshes.append(mesh)\n        for next_node_idx in node.children:\n            _traverse_scene(next_node_idx, cur_transform)\n    for node_idx in default_scene.nodes:\n        _traverse_scene(node_idx, torch.eye(4, dtype=torch.double))\n    outputs = _join_meshes(scene_meshes)\n    outputs.materials = materials\n    return outputs", "namespace": "torch.tensor", "completions": ["from_numpy(node.matrix).float()"]}
