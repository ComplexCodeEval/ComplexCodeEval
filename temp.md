# To Reviewer A

### Do You Assume Test Functions Are Self-Contained?

Thanks for the comments. In the paper, the test functions can be either self-contained or not. We extract test functions because they can serve as ground truth to validate the tests generated by a model. Therefore, whether the test functions are self-contained is not important for our benchmark.

### At Line 340, You Write: "We First Manually Define API Filtering Rules for Each Selected Library, Identifying the Common Characteristic of the APIs Within Each Library." Can You Elaborate? What Do You Mean by "Common Characteristics"?

We apologize for the confusion. By "common characteristics," we mean the common prefix of the APIs within a library. We implement the API filtering rules by defining an Accept field and a Reject field for each library in the configuration file ([profile.yaml](https://anonymous.4open.science/r/ComplexCodeEval-7A5F/benchmark_construct/setup/profile.yaml)) of our code repository. The `Accept` field contains the common prefix of the APIs within the library, while the `Reject` field contains prefixes that match the common prefix but actually belong to a different library. For example, in the case of the Spring library, the `Accept` field might contain "org.springframework," while the `Reject` field might contain "org.springframework.boot" because we consider this to be the common prefix for APIs under the Spring Boot library, not the Spring library. This ensures that our pipeline can obtain the correct libraries.

### What Prompt Is Used for Generating Docstrings (3.2.3)?

Thanks for your suggestion. Due to space limitations, we did not include the prompt template for generating docstrings in the paper. These can be seen in our [code repository](https://github.com/ComplexCodeEval/ComplexCodeEval/blob/master/figures/prompt_template.png). 

### Can You Formally Define a Sample in the Benchmark?

 In our paper, Figure 3 shows part of the field information of the data. Due to space constraints, we did not provide a detailed explanation of all the data fields. This information is thoroughly explained in the [README](https://anonymous.4open.science/r/ComplexCodeEval-7A5F/benchmark_construct/README.md) of the benchmark construction tool section of our code repository. 

### Do You Have Multiple Versions of the Same Function (Given That You Have Different Commits Available in the Repo)?

In our benchmark, there are no multiple versions of the same function. We only select the latest version of the code repository (as of May 1, 2024).

### Line 405: What Is Import Message?

The term “import message” refers to the details of external libraries, modules, or packages that a code file imports. For example, if a code file contains the import statement `from transformers import AutoModelForCausalLM`, the import message would be `transformers.AutoModelForCausalLM`.

### Two Versions of ComplexCodeEval

We are pleased that you are interested in this topic. In fact, we already have the version of ComplexCodeEval you mentioned, which removes the restriction of extracting only one sample per API in a project. We have already added this version to our code repository.

### Suggestions for experimental results

Thanks for your suggestions. We have added standard deviations and precision to Table 3. Additionally, we have included CodeBERTScore as an evaluation metric. These updates can be found in our [code repository](https://github.com/ComplexCodeEval/ComplexCodeEval/blob/master/figures/CodeBERTScore.png).

# To Reviewer B

### What Are the Numbers of Samples for Non-Leaked Data and Leaked Data Used in Section 5.3? How Can Data Leakage Be Avoided for New Models in the Future? How Challenging Is It to Run the Data Generation Process Again to Update the Benchmark with the Latest Repositories?

1. Based on the cutoff date of the training data for the selected models, there are 84 non-leaked samples for the Java version. To maintain consistency across two programming languages, we randomly selected 84 non-leaked samples for Python as well. Similarly, 84 leaked samples were randomly chosen.

2. To address the emergence of new models, we refer to the work of LiveCodeBench [1] and plan to update our benchmark regularly (every 6 months) to accommodate mainstream popular LCMs. 

3. In our benchmark construction process, the only parts that require manual intervention are the definition of API filtering rules and the design of prompts for generating docstrings (note that manually checking the generated docstrings is part of the prompt design). Both of these parts are reusable, so updating the benchmark using our tool only requires running the script. Additionally, our code repository includes corresponding [guidance](https://anonymous.4open.science/r/ComplexCodeEval-7A5F/benchmark_contruct/README.md) to help readers reconstruct the benchmark according to their own needs.

### What is the performance of baseline methods for each task on this benchmark?

We apologize for any confusion. In our paper, baseline methods refer to LCMs with basic prompts where no additional context (e.g., import information) is provided. Since existing non-LCM methods have some issues, such as requiring training, they are not suitable for our benchmark. However, this limitation does not affect our experimental conclusions, as theyare specific to LCMs.

### Why Are Only a Small Number of Samples Used for the Evaluation?

We apologize for the confusion. We presented the temporal distribution of benchmark samples in our [code repository](https://github.com/ComplexCodeEval/ComplexCodeEval/tree/master/dataset). The reason for using a limited number of samples for evaluation is primarily due to the limited amount of non-leaked Java data (84 samples). Since our evaluation involves comparing different LCMs, we have based our evaluation mainly on non-leaked data (except for the comparison of leaked and non-leaked data in Section 5.3). We believe that this sample size is sufficient to reflect the performance of LCMs on various tasks, similar to how HumanEval uses only 164 samples.

### What Are the API Filtering Rules?

API filtering rules are used to determine whether an API belongs to a specific library. We implement these rules by defining `Accept` and `Reject` fields for each library in the configuration file ([profile.yaml](https://anonymous.4open.science/r/ComplexCodeEval-7A5F/benchmark_construct/setup/profile.yaml)) of our code repository. The `Accept` field contains the common prefix of the APIs for that library, while the `Reject` field contains prefixes that, although matching the common prefix of the library’s APIs, actually belong to other libraries. For example, for the Spring library, the `Accept` field contains "org.springframework", while the `Reject` field contains "org.springframework.boot", because "org.springframework.boot" is considered to be a prefix for the Spring Boot library, not for the Spring library.

### What Metrics or Standards Are Used for the Manual Quality Evaluation of Generated Docstrings (Section 3.2.3)? What Are the “Desired Quality Standards”?

Our manual quality evaluation aims to ensure the effectiveness of the prompts used for generating docstrings. Specifically, we randomly sample 100 examples and have the model generate docstrings based on our prompts (prompt templates can be seen in our [code repository](https://github.com/ComplexCodeEval/ComplexCodeEval/blob/master/figures/prompt_template.png)). We then evaluate the generated docstrings according to the following three criteria:

1. The docstrings should not contain any additional content beyond the required description.
2. We check that the parameters and return values in the docstrings match those of the original function.
3. The first and second authors of the paper separately verify that the docstring descriptions accurately reflect the content of the original function.

If any docstring fails to meet these criteria, we revise the prompt and repeat the process. We will include this aspect in our paper.

### Only the most frequently used APIs and libraries are considered (Section 3.1), and only functions with tests are selected (Section 3.2.1). Will this introduce bias to the benchmark?

Our benchmark focuses on the aspects most relevant and relied upon in actual development. By concentrating on popular libraries and APIs, we ensure that the selected samples reflect areas of genuine interest to developers. Additionally, functions with tests are usually subjected to more validation, so by selecting functions with tests, we can ensure that our samples are more reliable. Furthermore, our benchmark construction tool is highly flexible, allowing the addition or removal of APIs and libraries as needed, thus reducing bias.

### Comparison of the Original Docstring and Generated Docstring

Thanks for your suggestion. Due to space constraints, we did not include examples comparing the original docstrings with the generated docstrings in our paper. To alleviate readers' confusion, we will provide an example in the paper.  A comparison between the original docstring and the generated docstring can be seen [here](https://github.com/ComplexCodeEval/ComplexCodeEval/blob/master/figures/docstring_compare.png):


### For the Data Leakage Impact Study (Section 5.3), Why Are Dependencies Not Considered for the Code Generation Task?

The main reason is that dependency information is not directly obtained from the original code repositories but is instead extracted additionally by us. In Section 5.3, when exploring the impact of data leakage, we only consider information that is inherently present in the original code repositories (such as import information and context information).

### In Table 6, why is only a subset of LLMs evaluated?

Due to computational resource constraints, we only chose a subset of LLMs for evaluation. For more comprehensive comparison, we have completed experiments with the remaining models, the sesults can be seen in our code repository. We will update Table 6 in the paper.

# To Reviewer C

### Examples of the Prompts Used to Generate Answers from the LCMs Are Missing in the Paper, Are Different Prompts Used to Improve the Quality of the Results?

We used the same prompt template with different selected contexts and ensured that other general parts in the prompts such as the role and task description remain the same in different experiments. We presented all the prompts used for inference in our code repository. Our prompt design is based on previous works (LiveCodeBench [1] and EvoCodeBench [2]). An example of one of our prompt templates can been seen [here](https://github.com/ComplexCodeEval/ComplexCodeEval/blob/master/experiment/prompt/prompt_template.png). 

### Decision Taken by Only Seeing the CodeBLEU Score

Due to time constraints, it is hard for us to conduct developer surveys and manual evaluations during the rebuttal. Following Reviewer A's suggestion, we have added CodeBERTScore [3] as an extra evaluation metric. CodeBERTScore, which uses the CodeBERT model, provides a more accurate assessment by better understanding the semantics and structure of the code. We have included these results in our [code repository](https://github.com/ComplexCodeEval/ComplexCodeEval/blob/master/figures/CodeBERTScore.png).

### Manual Evaluation of the LLM generated docstring

Our manual quality evaluation aims to ensure the effectiveness of the prompts used for generating docstrings. Specifically, we randomly sample 100 examples and generate docstrings based on our prompts (prompt templates can be seen in our [code repository](https://github.com/ComplexCodeEval/ComplexCodeEval/blob/master/figures/prompt_template.png)). We then evaluate the generated docstrings according to the following three criteria:

1. The docstrings should not contain any additional content beyond the required description.
2. We checked that the parameters and return values in the docstrings match those of the original function.
3. The first and second authors of the paper separately verified that the docstring descriptions accurately reflect the content of the original function.

If any docstring failed to meet these criteria, we revised the prompt and repeated the process.

### Experiment with Latest LLMs, like GPT and Gemini

Thanks for your suggestion. We have evaluated GPT-3.5-turbo on our benchmark across four tasks. We will include these results in the paper, which can also be seen below.

### Comparison with Other Benchmarks

Thanks for your suggestion. In Section 5.1, CodeLlama-34b achieved the best overall performance. Therefore, we chose to compare the proposed ComplexCodeEval with other benchmarks using CodeLlama-34b. For code generation tasks, we selected CoderEval [4]; for code completion, we chose CrossCodeEval [5]; for test case generation, we used Method2Test [6]; and for API recommendation, we selected APIBench [7]. The sesults can be seen in our code repository.
References:
[1] Li J, Li G, Zhang X, et al. EvoCodeBench: An Evolving Code Generation Benchmark Aligned with Real-World Code Repositories[J]. arXiv preprint arXiv:2404.00599, 2024.
[2] Jain N, Han K, Gu A, et al. Livecodebench: Holistic and contamination free evaluation of large language models for code[J]. arXiv preprint arXiv:2403.07974, 2024.
[3] Zhou S, Alon U, Agarwal S, et al. Codebertscore: Evaluating code generation with pretrained models of code[J]. arXiv preprint arXiv:2302.05527, 2023.
[4] Yu H, Shen B, Ran D, et al. Codereval: A benchmark of pragmatic code generation with generative pre-trained models[C]//Proceedings of the 46th IEEE/ACM International Conference on Software Engineering. 2024: 1-12.
[5] Ding Y, Wang Z, Ahmad W, et al. Crosscodeeval: A diverse and multilingual benchmark for cross-file code completion[J]. Advances in Neural Information Processing Systems, 2024, 36.
[6] Tufano M, Deng S K, Sundaresan N, et al. Methods2Test: A dataset of focal methods mapped to test cases[C]//Proceedings of the 19th International Conference on Mining Software Repositories. 2022: 299-303.
[7] Peng Y, Li S, Gu W, et al. Revisiting, benchmarking and exploring API recommendation: How far are we?[J]. IEEE Transactions on Software Engineering, 2022, 49(4): 1876-1897.
